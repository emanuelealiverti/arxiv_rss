<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-04-29T09:31:01+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Continuous Relaxation for Discrete Bayesian Optimization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AContinuousRelaxationforDiscreteBayesianOptimization.html" rel="alternate" type="text/html" title="A Continuous Relaxation for Discrete Bayesian Optimization" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AContinuousRelaxationforDiscreteBayesianOptimization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AContinuousRelaxationforDiscreteBayesianOptimization.html">&lt;p&gt;To optimize efficiently over discrete data and with only few available target observations is a challenge in Bayesian optimization. We propose a continuous relaxation of the objective function and show that inference and optimization can be computationally tractable. We consider in particular the optimization domain where very few observations and strict budgets exist; motivated by optimizing protein sequences for expensive to evaluate bio-chemical properties. The advantages of our approach are two-fold: the problem is treated in the continuous setting, and available prior knowledge over sequences can be incorporated directly. More specifically, we utilize available and learned distributions over the problem domain for a weighting of the Hellinger distance which yields a covariance function. We show that the resulting acquisition function can be optimized with both continuous or discrete optimization algorithms and empirically assess our method on two bio-chemical sequence optimization tasks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17452&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Richard Michael, Simon Bartels, Miguel González-Duque, Yevgen Zainchkovskyy, Jes Frellsen, S{\o}ren Hauberg, Wouter Boomsma</name></author><category term="stat.ML" /><summary type="html">To optimize efficiently over discrete data and with only few available target observations is a challenge in Bayesian optimization. We propose a continuous relaxation of the objective function and show that inference and optimization can be computationally tractable. We consider in particular the optimization domain where very few observations and strict budgets exist; motivated by optimizing protein sequences for expensive to evaluate bio-chemical properties. The advantages of our approach are two-fold: the problem is treated in the continuous setting, and available prior knowledge over sequences can be incorporated directly. More specifically, we utilize available and learned distributions over the problem domain for a weighting of the Hellinger distance which yields a covariance function. We show that the resulting acquisition function can be optimized with both continuous or discrete optimization algorithms and empirically assess our method on two bio-chemical sequence optimization tasks.</summary></entry><entry><title type="html">A Notion of Uniqueness for the Adversarial Bayes Classifier</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/ANotionofUniquenessfortheAdversarialBayesClassifier.html" rel="alternate" type="text/html" title="A Notion of Uniqueness for the Adversarial Bayes Classifier" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/ANotionofUniquenessfortheAdversarialBayesClassifier</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/ANotionofUniquenessfortheAdversarialBayesClassifier.html">&lt;p&gt;We propose a new notion of uniqueness for the adversarial Bayes classifier in the setting of binary classification. Analyzing this notion of uniqueness produces a simple procedure for computing all adversarial Bayes classifiers for a well-motivated family of one dimensional data distributions. This characterization is then leveraged to show that as the perturbation radius increases, certain notions of regularity improve for adversarial Bayes classifiers. We demonstrate with various examples that the boundary of the adversarial Bayes classifier frequently lies near the boundary of the Bayes classifier.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.16956&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Natalie S. Frank</name></author><category term="stat.ML," /><category term="stat.TH" /><summary type="html">We propose a new notion of uniqueness for the adversarial Bayes classifier in the setting of binary classification. Analyzing this notion of uniqueness produces a simple procedure for computing all adversarial Bayes classifiers for a well-motivated family of one dimensional data distributions. This characterization is then leveraged to show that as the perturbation radius increases, certain notions of regularity improve for adversarial Bayes classifiers. We demonstrate with various examples that the boundary of the adversarial Bayes classifier frequently lies near the boundary of the Bayes classifier.</summary></entry><entry><title type="html">A Novel Context driven Critical Integrative Levels (CIL) Approach: Advancing Human-Centric and Integrative Lighting Asset Management in Public Libraries with Practical Thresholds</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/ANovelContextdrivenCriticalIntegrativeLevelsCILApproachAdvancingHumanCentricandIntegrativeLightingAssetManagementinPublicLibrarieswithPracticalThresholds.html" rel="alternate" type="text/html" title="A Novel Context driven Critical Integrative Levels (CIL) Approach: Advancing Human-Centric and Integrative Lighting Asset Management in Public Libraries with Practical Thresholds" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/ANovelContextdrivenCriticalIntegrativeLevelsCILApproachAdvancingHumanCentricandIntegrativeLightingAssetManagementinPublicLibrarieswithPracticalThresholds</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/ANovelContextdrivenCriticalIntegrativeLevelsCILApproachAdvancingHumanCentricandIntegrativeLightingAssetManagementinPublicLibrarieswithPracticalThresholds.html">&lt;p&gt;This paper proposes the context driven Critical Integrative Levels (CIL), a novel approach to lighting asset management in public libraries that aligns with the transformative vision of human-centric and integrative lighting. This approach encompasses not only the visual aspects of lighting performance but also prioritizes the physiological and psychological well-being of library users. Incorporating a newly defined metric, Mean Time of Exposure (MTOE), the approach quantifies user-light interaction, enabling tailored lighting strategies that respond to diverse activities and needs in library spaces. Case studies demonstrate how the CIL matrix can be practically applied, offering significant improvements over conventional methods by focusing on optimized user experiences from both visual impacts and non-visual effects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17554&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jing Lin, Nina Mylly, Per Olof Hedekvist, Jingchun Shen</name></author><category term="stat.AP" /><summary type="html">This paper proposes the context driven Critical Integrative Levels (CIL), a novel approach to lighting asset management in public libraries that aligns with the transformative vision of human-centric and integrative lighting. This approach encompasses not only the visual aspects of lighting performance but also prioritizes the physiological and psychological well-being of library users. Incorporating a newly defined metric, Mean Time of Exposure (MTOE), the approach quantifies user-light interaction, enabling tailored lighting strategies that respond to diverse activities and needs in library spaces. Case studies demonstrate how the CIL matrix can be practically applied, offering significant improvements over conventional methods by focusing on optimized user experiences from both visual impacts and non-visual effects.</summary></entry><entry><title type="html">A Weibull Mixture Cure Frailty Model for High-dimensional Covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AWeibullMixtureCureFrailtyModelforHighdimensionalCovariates.html" rel="alternate" type="text/html" title="A Weibull Mixture Cure Frailty Model for High-dimensional Covariates" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AWeibullMixtureCureFrailtyModelforHighdimensionalCovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AWeibullMixtureCureFrailtyModelforHighdimensionalCovariates.html">&lt;p&gt;A novel mixture cure frailty model is introduced for handling censored survival data. Mixture cure models are preferable when the existence of a cured fraction among patients can be assumed. However, such models are heavily underexplored: frailty structures within cure models remain largely undeveloped, and furthermore, most existing methods do not work for high-dimensional datasets, when the number of predictors is significantly larger than the number of observations. In this study, we introduce a novel extension of the Weibull mixture cure model that incorporates a frailty component, employed to model an underlying latent population heterogeneity with respect to the outcome risk. Additionally, high-dimensional covariates are integrated into both the cure rate and survival part of the model, providing a comprehensive approach to employ the model in the context of high-dimensional omics data. We also perform variable selection via an adaptive elastic-net penalization, and propose a novel approach to inference using the expectation-maximization (EM) algorithm. Extensive simulation studies are conducted across various scenarios to demonstrate the performance of the model, and results indicate that our proposed method outperforms competitor models. We apply the novel approach to analyze RNAseq gene expression data from bulk breast cancer patients included in The Cancer Genome Atlas (TCGA) database. A set of prognostic biomarkers is then derived from selected genes, and subsequently validated via both functional enrichment analysis and comparison to the existing biological literature. Finally, a prognostic risk score index based on the identified biomarkers is proposed and validated by exploring the patients’ survival.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.06575&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fatih K{\i}z{\i}laslan, David Michael Swanson, Valeria Vitelli</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">A novel mixture cure frailty model is introduced for handling censored survival data. Mixture cure models are preferable when the existence of a cured fraction among patients can be assumed. However, such models are heavily underexplored: frailty structures within cure models remain largely undeveloped, and furthermore, most existing methods do not work for high-dimensional datasets, when the number of predictors is significantly larger than the number of observations. In this study, we introduce a novel extension of the Weibull mixture cure model that incorporates a frailty component, employed to model an underlying latent population heterogeneity with respect to the outcome risk. Additionally, high-dimensional covariates are integrated into both the cure rate and survival part of the model, providing a comprehensive approach to employ the model in the context of high-dimensional omics data. We also perform variable selection via an adaptive elastic-net penalization, and propose a novel approach to inference using the expectation-maximization (EM) algorithm. Extensive simulation studies are conducted across various scenarios to demonstrate the performance of the model, and results indicate that our proposed method outperforms competitor models. We apply the novel approach to analyze RNAseq gene expression data from bulk breast cancer patients included in The Cancer Genome Atlas (TCGA) database. A set of prognostic biomarkers is then derived from selected genes, and subsequently validated via both functional enrichment analysis and comparison to the existing biological literature. Finally, a prognostic risk score index based on the identified biomarkers is proposed and validated by exploring the patients’ survival.</summary></entry><entry><title type="html">A comparison of the discrimination performance of lasso and maximum likelihood estimation in logistic regression model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Acomparisonofthediscriminationperformanceoflassoandmaximumlikelihoodestimationinlogisticregressionmodel.html" rel="alternate" type="text/html" title="A comparison of the discrimination performance of lasso and maximum likelihood estimation in logistic regression model" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Acomparisonofthediscriminationperformanceoflassoandmaximumlikelihoodestimationinlogisticregressionmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Acomparisonofthediscriminationperformanceoflassoandmaximumlikelihoodestimationinlogisticregressionmodel.html">&lt;p&gt;Logistic regression is widely used in many areas of knowledge. Several works compare the performance of lasso and maximum likelihood estimation in logistic regression. However, part of these works do not perform simulation studies and the remaining ones do not consider scenarios in which the ratio of the number of covariates to sample size is high. In this work, we compare the discrimination performance of lasso and maximum likelihood estimation in logistic regression using simulation studies and applications. Variable selection is done both by lasso and by stepwise when maximum likelihood estimation is used. We consider a wide range of values for the ratio of the number of covariates to sample size. The main conclusion of the work is that lasso has a better discrimination performance than maximum likelihood estimation when the ratio of the number of covariates to sample size is high.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17482&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Gilberto P. Alcântara Junior, Gustavo H. A. Pereira</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Logistic regression is widely used in many areas of knowledge. Several works compare the performance of lasso and maximum likelihood estimation in logistic regression. However, part of these works do not perform simulation studies and the remaining ones do not consider scenarios in which the ratio of the number of covariates to sample size is high. In this work, we compare the discrimination performance of lasso and maximum likelihood estimation in logistic regression using simulation studies and applications. Variable selection is done both by lasso and by stepwise when maximum likelihood estimation is used. We consider a wide range of values for the ratio of the number of covariates to sample size. The main conclusion of the work is that lasso has a better discrimination performance than maximum likelihood estimation when the ratio of the number of covariates to sample size is high.</summary></entry><entry><title type="html">A comprehensive survey of the home advantage in American football</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AcomprehensivesurveyofthehomeadvantageinAmericanfootball.html" rel="alternate" type="text/html" title="A comprehensive survey of the home advantage in American football" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AcomprehensivesurveyofthehomeadvantageinAmericanfootball</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AcomprehensivesurveyofthehomeadvantageinAmericanfootball.html">&lt;p&gt;The existence and justification to the home advantage – the benefit a sports team receives when playing at home – has been studied across sport. The majority of research on this topic is limited to individual leagues in short time frames, which hinders extrapolation and a deeper understanding of possible causes. Using nearly two decades of data from the National Football League (NFL), the National Collegiate Athletic Association (NCAA), and high schools from across the United States, we provide a uniform approach to understanding the home advantage in American football. Our findings suggest home advantage is declining in the NFL and the highest levels of collegiate football, but not in amateur football. This increases the possibility that characteristics of the NCAA and NFL, such as travel improvements and instant replay, have helped level the playing field.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.16392&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luke S. Benz, Thompson J. Bliss, Michael J. Lopez</name></author><category term="stat.AP" /><summary type="html">The existence and justification to the home advantage – the benefit a sports team receives when playing at home – has been studied across sport. The majority of research on this topic is limited to individual leagues in short time frames, which hinders extrapolation and a deeper understanding of possible causes. Using nearly two decades of data from the National Football League (NFL), the National Collegiate Athletic Association (NCAA), and high schools from across the United States, we provide a uniform approach to understanding the home advantage in American football. Our findings suggest home advantage is declining in the NFL and the highest levels of collegiate football, but not in amateur football. This increases the possibility that characteristics of the NCAA and NFL, such as travel improvements and instant replay, have helped level the playing field.</summary></entry><entry><title type="html">Adversarial Consistency and the Uniqueness of the Adversarial Bayes Classifier</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AdversarialConsistencyandtheUniquenessoftheAdversarialBayesClassifier.html" rel="alternate" type="text/html" title="Adversarial Consistency and the Uniqueness of the Adversarial Bayes Classifier" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AdversarialConsistencyandtheUniquenessoftheAdversarialBayesClassifier</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AdversarialConsistencyandtheUniquenessoftheAdversarialBayesClassifier.html">&lt;p&gt;Adversarial training is a common technique for learning robust classifiers. Prior work showed that convex surrogate losses are not statistically consistent in the adversarial context – or in other words, a minimizing sequence of the adversarial surrogate risk will not necessarily minimize the adversarial classification error. We connect the consistency of adversarial surrogate losses to properties of minimizers to the adversarial classification risk, known as \emph{adversarial Bayes classifiers}. Specifically, under reasonable distributional assumptions, a convex loss is statistically consistent for adversarial learning iff the adversarial Bayes classifier satisfies a certain notion of uniqueness.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17358&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Natalie S. Frank</name></author><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Adversarial training is a common technique for learning robust classifiers. Prior work showed that convex surrogate losses are not statistically consistent in the adversarial context – or in other words, a minimizing sequence of the adversarial surrogate risk will not necessarily minimize the adversarial classification error. We connect the consistency of adversarial surrogate losses to properties of minimizers to the adversarial classification risk, known as \emph{adversarial Bayes classifiers}. Specifically, under reasonable distributional assumptions, a convex loss is statistically consistent for adversarial learning iff the adversarial Bayes classifier satisfies a certain notion of uniqueness.</summary></entry><entry><title type="html">Adversarial Estimation of Riesz Representers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AdversarialEstimationofRieszRepresenters.html" rel="alternate" type="text/html" title="Adversarial Estimation of Riesz Representers" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AdversarialEstimationofRieszRepresenters</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AdversarialEstimationofRieszRepresenters.html">&lt;p&gt;Many causal parameters are linear functionals of an underlying regression. The Riesz representer is a key component in the asymptotic variance of a semiparametrically estimated linear functional. We propose an adversarial framework to estimate the Riesz representer using general function spaces. We prove a nonasymptotic mean square rate in terms of an abstract quantity called the critical radius, then specialize it for neural networks, random forests, and reproducing kernel Hilbert spaces as leading cases. Our estimators are highly compatible with targeted and debiased machine learning with sample splitting; our guarantees directly verify general conditions for inference that allow mis-specification. We also use our guarantees to prove inference without sample splitting, based on stability or complexity. Our estimators achieve nominal coverage in highly nonlinear simulations where some previous methods break down. They shed new light on the heterogeneous effects of matching grants.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2101.00009&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Victor Chernozhukov, Whitney Newey, Rahul Singh, Vasilis Syrgkanis</name></author><category term="stat.ML" /><summary type="html">Many causal parameters are linear functionals of an underlying regression. The Riesz representer is a key component in the asymptotic variance of a semiparametrically estimated linear functional. We propose an adversarial framework to estimate the Riesz representer using general function spaces. We prove a nonasymptotic mean square rate in terms of an abstract quantity called the critical radius, then specialize it for neural networks, random forests, and reproducing kernel Hilbert spaces as leading cases. Our estimators are highly compatible with targeted and debiased machine learning with sample splitting; our guarantees directly verify general conditions for inference that allow mis-specification. We also use our guarantees to prove inference without sample splitting, based on stability or complexity. Our estimators achieve nominal coverage in highly nonlinear simulations where some previous methods break down. They shed new light on the heterogeneous effects of matching grants.</summary></entry><entry><title type="html">An adaptive standardisation methodology for Day-Ahead electricity price forecasting</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AnadaptivestandardisationmethodologyforDayAheadelectricitypriceforecasting.html" rel="alternate" type="text/html" title="An adaptive standardisation methodology for Day-Ahead electricity price forecasting" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AnadaptivestandardisationmethodologyforDayAheadelectricitypriceforecasting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AnadaptivestandardisationmethodologyforDayAheadelectricitypriceforecasting.html">&lt;p&gt;The study of Day-Ahead prices in the electricity market is one of the most popular problems in time series forecasting. Previous research has focused on employing increasingly complex learning algorithms to capture the sophisticated dynamics of the market. However, there is a threshold where increased complexity fails to yield substantial improvements. In this work, we propose an alternative approach by introducing an adaptive standardisation to mitigate the effects of dataset shifts that commonly occur in the market. By doing so, learning algorithms can prioritize uncovering the true relationship between the target variable and the explanatory variables. We investigate five distinct markets, including two novel datasets, previously unexplored in the literature. These datasets provide a more realistic representation of the current market context, that conventional datasets do not show. The results demonstrate a significant improvement across all five markets using the widely accepted learning algorithms in the literature (LEAR and DNN). In particular, the combination of the proposed methodology with the methodology previously presented in the literature obtains the best results. This significant advancement unveils new lines of research in this field, highlighting the potential of adaptive transformations in enhancing the performance of forecasting models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.02610&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carlos Sebastián, Carlos E. González-Guillén, Jesús Juan</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">The study of Day-Ahead prices in the electricity market is one of the most popular problems in time series forecasting. Previous research has focused on employing increasingly complex learning algorithms to capture the sophisticated dynamics of the market. However, there is a threshold where increased complexity fails to yield substantial improvements. In this work, we propose an alternative approach by introducing an adaptive standardisation to mitigate the effects of dataset shifts that commonly occur in the market. By doing so, learning algorithms can prioritize uncovering the true relationship between the target variable and the explanatory variables. We investigate five distinct markets, including two novel datasets, previously unexplored in the literature. These datasets provide a more realistic representation of the current market context, that conventional datasets do not show. The results demonstrate a significant improvement across all five markets using the widely accepted learning algorithms in the literature (LEAR and DNN). In particular, the combination of the proposed methodology with the methodology previously presented in the literature obtains the best results. This significant advancement unveils new lines of research in this field, highlighting the potential of adaptive transformations in enhancing the performance of forecasting models.</summary></entry><entry><title type="html">An exactly solvable model for emergence and scaling laws</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Anexactlysolvablemodelforemergenceandscalinglaws.html" rel="alternate" type="text/html" title="An exactly solvable model for emergence and scaling laws" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Anexactlysolvablemodelforemergenceandscalinglaws</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Anexactlysolvablemodelforemergenceandscalinglaws.html">&lt;p&gt;Deep learning models can exhibit what appears to be a sudden ability to solve a new problem as training time ($T$), training data ($D$), or model size ($N$) increases, a phenomenon known as emergence. In this paper, we present a framework where each new ability (a skill) is represented as a basis function. We solve a simple multi-linear model in this skill-basis, finding analytic expressions for the emergence of new skills, as well as for scaling laws of the loss with training time, data size, model size, and optimal compute ($C$). We compare our detailed calculations to direct simulations of a two-layer neural network trained on multitask sparse parity, where the tasks in the dataset are distributed according to a power-law. Our simple model captures, using a single fit parameter, the sigmoidal emergence of multiple new skills as training time, data size or model size increases in the neural network.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17563&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yoonsoo Nam, Nayara Fonseca, Seok Hyeong Lee, Ard Louis</name></author><category term="stat.ML" /><summary type="html">Deep learning models can exhibit what appears to be a sudden ability to solve a new problem as training time ($T$), training data ($D$), or model size ($N$) increases, a phenomenon known as emergence. In this paper, we present a framework where each new ability (a skill) is represented as a basis function. We solve a simple multi-linear model in this skill-basis, finding analytic expressions for the emergence of new skills, as well as for scaling laws of the loss with training time, data size, model size, and optimal compute ($C$). We compare our detailed calculations to direct simulations of a two-layer neural network trained on multitask sparse parity, where the tasks in the dataset are distributed according to a power-law. Our simple model captures, using a single fit parameter, the sigmoidal emergence of multiple new skills as training time, data size or model size increases in the neural network.</summary></entry></feed>