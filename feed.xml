<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-10T07:14:28+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Connection Between Covariate Adjustment and Stratified Randomization in Randomized Clinical Trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/AConnectionBetweenCovariateAdjustmentandStratifiedRandomizationinRandomizedClinicalTrials.html" rel="alternate" type="text/html" title="A Connection Between Covariate Adjustment and Stratified Randomization in Randomized Clinical Trials" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/AConnectionBetweenCovariateAdjustmentandStratifiedRandomizationinRandomizedClinicalTrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/AConnectionBetweenCovariateAdjustmentandStratifiedRandomizationinRandomizedClinicalTrials.html">&lt;p&gt;The statistical efficiency of randomized clinical trials can be improved by incorporating information from baseline covariates (i.e., pre-treatment patient characteristics). This can be done in the design stage using stratified (permutated block) randomization or in the analysis stage through covariate adjustment. This article makes a connection between covariate adjustment and stratified randomization in a general framework where all regular, asymptotically linear estimators are identified as augmented estimators. From a geometric perspective, covariate adjustment can be viewed as an attempt to approximate the optimal augmentation function, and stratified randomization improves a given approximation by moving it closer to the optimal augmentation function. The efficiency benefit of stratified randomization is asymptotically equivalent to attaching an optimal augmentation term based on the stratification factor. Under stratified randomization, adjusting for the stratification factor only in data analysis is not expected to improve efficiency, and the key to efficient estimation is incorporating new prognostic information from other covariates. In designing a trial with stratified randomization, it is not essential to include all important covariates in the stratification, because their prognostic information can be incorporated through covariate adjustment. These observations are confirmed in a simulation study and illustrated using real clinical trial data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.11352&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhiwei Zhang</name></author><category term="stat.ME" /><summary type="html">The statistical efficiency of randomized clinical trials can be improved by incorporating information from baseline covariates (i.e., pre-treatment patient characteristics). This can be done in the design stage using stratified (permutated block) randomization or in the analysis stage through covariate adjustment. This article makes a connection between covariate adjustment and stratified randomization in a general framework where all regular, asymptotically linear estimators are identified as augmented estimators. From a geometric perspective, covariate adjustment can be viewed as an attempt to approximate the optimal augmentation function, and stratified randomization improves a given approximation by moving it closer to the optimal augmentation function. The efficiency benefit of stratified randomization is asymptotically equivalent to attaching an optimal augmentation term based on the stratification factor. Under stratified randomization, adjusting for the stratification factor only in data analysis is not expected to improve efficiency, and the key to efficient estimation is incorporating new prognostic information from other covariates. In designing a trial with stratified randomization, it is not essential to include all important covariates in the stratification, because their prognostic information can be incorporated through covariate adjustment. These observations are confirmed in a simulation study and illustrated using real clinical trial data.</summary></entry><entry><title type="html">An algorithm for forensic toolmark comparisons</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Analgorithmforforensictoolmarkcomparisons.html" rel="alternate" type="text/html" title="An algorithm for forensic toolmark comparisons" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Analgorithmforforensictoolmarkcomparisons</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Analgorithmforforensictoolmarkcomparisons.html">&lt;p&gt;Forensic toolmark analysis traditionally relies on subjective human judgment, leading to inconsistencies and lack of transparency. The multitude of variables, including angles and directions of mark generation, further complicates comparisons. To address this, we first generate a dataset of 3D toolmarks from various angles and directions using consecutively manufactured slotted screwdrivers. By using PAM clustering, we find that there is clustering by tool rather than angle or direction. Using Known Match and Known Non-Match densities, we establish thresholds for classification. Fitting Beta distributions to the densities, we allow for the derivation of likelihood ratios for new toolmark pairs. With a cross-validated sensitivity of 98% and specificity of 96%, our approach enhances the reliability of toolmark analysis. This approach is applicable to slotted screwdrivers, and for screwdrivers that are made with a similar production method. With data collection of other tools and factors, it could be applied to compare toolmarks of other types. This empirically trained, open-source solution offers forensic examiners a standardized means to objectively compare toolmarks, potentially decreasing the number of miscarriages of justice in the legal system.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.00032&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maria Cuellar, Sheng Gao, Heike Hofmann</name></author><category term="stat.AP" /><summary type="html">Forensic toolmark analysis traditionally relies on subjective human judgment, leading to inconsistencies and lack of transparency. The multitude of variables, including angles and directions of mark generation, further complicates comparisons. To address this, we first generate a dataset of 3D toolmarks from various angles and directions using consecutively manufactured slotted screwdrivers. By using PAM clustering, we find that there is clustering by tool rather than angle or direction. Using Known Match and Known Non-Match densities, we establish thresholds for classification. Fitting Beta distributions to the densities, we allow for the derivation of likelihood ratios for new toolmark pairs. With a cross-validated sensitivity of 98% and specificity of 96%, our approach enhances the reliability of toolmark analysis. This approach is applicable to slotted screwdrivers, and for screwdrivers that are made with a similar production method. With data collection of other tools and factors, it could be applied to compare toolmarks of other types. This empirically trained, open-source solution offers forensic examiners a standardized means to objectively compare toolmarks, potentially decreasing the number of miscarriages of justice in the legal system.</summary></entry><entry><title type="html">A novel multivariate regression model for unbalanced binary data : a strong conjugacy under random effect approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Anovelmultivariateregressionmodelforunbalancedbinarydataastrongconjugacyunderrandomeffectapproach.html" rel="alternate" type="text/html" title="A novel multivariate regression model for unbalanced binary data : a strong conjugacy under random effect approach" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Anovelmultivariateregressionmodelforunbalancedbinarydataastrongconjugacyunderrandomeffectapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Anovelmultivariateregressionmodelforunbalancedbinarydataastrongconjugacyunderrandomeffectapproach.html">&lt;p&gt;In this paper, we deduce a new multivariate regression model designed to fit correlated binary data. The multivariate distribution is derived from a Bernoulli mixed model with a nonnormal random intercept on the marginal approach. The random effect distribution is assumed to be the generalized log-gamma (GLG) distribution by considering a particular parameter setting. The complement log-log function is specified to lead to strong conjugacy between the response variable and random effect. The new discrete multivariate distribution, named MBerGLG distribution, has location and dispersion parameters. The MBerGLG distribution leads to the MBerGLG regression (MBerGLGR) model, providing an alternative approach to fitting both unbalanced and balanced correlated response binary data. Monte Carlo simulation studies show that its maximum likelihood estimators are unbiased, efficient, and consistent asymptotically. The randomized quantile residuals are performed to identify possible departures from the proposal model and the data and detect atypical subjects. Finally, two applications are presented in the data analysis section.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04518&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lizandra C. Fabio, Vanessa Barros, Cristian Villegas, Jalmar M. F. Carrasco</name></author><category term="stat.ME" /><summary type="html">In this paper, we deduce a new multivariate regression model designed to fit correlated binary data. The multivariate distribution is derived from a Bernoulli mixed model with a nonnormal random intercept on the marginal approach. The random effect distribution is assumed to be the generalized log-gamma (GLG) distribution by considering a particular parameter setting. The complement log-log function is specified to lead to strong conjugacy between the response variable and random effect. The new discrete multivariate distribution, named MBerGLG distribution, has location and dispersion parameters. The MBerGLG distribution leads to the MBerGLG regression (MBerGLGR) model, providing an alternative approach to fitting both unbalanced and balanced correlated response binary data. Monte Carlo simulation studies show that its maximum likelihood estimators are unbiased, efficient, and consistent asymptotically. The randomized quantile residuals are performed to identify possible departures from the proposal model and the data and detect atypical subjects. Finally, two applications are presented in the data analysis section.</summary></entry><entry><title type="html">Approximate Bayesian Computation with Deep Learning and Conformal prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/ApproximateBayesianComputationwithDeepLearningandConformalprediction.html" rel="alternate" type="text/html" title="Approximate Bayesian Computation with Deep Learning and Conformal prediction" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/ApproximateBayesianComputationwithDeepLearningandConformalprediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/ApproximateBayesianComputationwithDeepLearningandConformalprediction.html">&lt;p&gt;Approximate Bayesian Computation (ABC) methods are commonly used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Classical ABC methods are based on nearest neighbor type algorithms and rely on the choice of so-called summary statistics, distances between datasets and a tolerance threshold. Recently, methods combining ABC with more complex machine learning algorithms have been proposed to mitigate the impact of these “user-choices”. In this paper, we propose the first, to our knowledge, ABC method completely free of summary statistics, distance and tolerance threshold. Moreover, in contrast with usual generalizations of the ABC method, it associates a confidence interval (having a proper frequentist marginal coverage) with the posterior mean estimation (or other moment-type estimates).
  Our method, ABCD-Conformal, uses a neural network with Monte Carlo Dropout to provide an estimation of the posterior mean (or others moment type functional), and conformal theory to obtain associated confidence sets. Efficient for estimating multidimensional parameters, we test this new method on three different applications and compare it with other ABC methods in the literature.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04874&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Meili Baragatti, Bertrand Cloez, David Métivier, Isabelle Sanchez</name></author><category term="stat.ME" /><summary type="html">Approximate Bayesian Computation (ABC) methods are commonly used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Classical ABC methods are based on nearest neighbor type algorithms and rely on the choice of so-called summary statistics, distances between datasets and a tolerance threshold. Recently, methods combining ABC with more complex machine learning algorithms have been proposed to mitigate the impact of these “user-choices”. In this paper, we propose the first, to our knowledge, ABC method completely free of summary statistics, distance and tolerance threshold. Moreover, in contrast with usual generalizations of the ABC method, it associates a confidence interval (having a proper frequentist marginal coverage) with the posterior mean estimation (or other moment-type estimates). Our method, ABCD-Conformal, uses a neural network with Monte Carlo Dropout to provide an estimation of the posterior mean (or others moment type functional), and conformal theory to obtain associated confidence sets. Efficient for estimating multidimensional parameters, we test this new method on three different applications and compare it with other ABC methods in the literature.</summary></entry><entry><title type="html">Bayesian Inference for Spatial-temporal Non-Gaussian Data Using Predictive Stacking</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/BayesianInferenceforSpatialtemporalNonGaussianDataUsingPredictiveStacking.html" rel="alternate" type="text/html" title="Bayesian Inference for Spatial-temporal Non-Gaussian Data Using Predictive Stacking" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/BayesianInferenceforSpatialtemporalNonGaussianDataUsingPredictiveStacking</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/BayesianInferenceforSpatialtemporalNonGaussianDataUsingPredictiveStacking.html">&lt;p&gt;Analysing non-Gaussian spatial-temporal data typically requires introducing spatial dependence in generalised linear models through the link function of an exponential family distribution. However, unlike in Gaussian likelihoods, inference is considerably encumbered by the inability to analytically integrate out the random effects and reduce the dimension of the parameter space. Iterative estimation algorithms struggle to converge due to the presence of weakly identified parameters. We devise an approach that obviates these issues by exploiting generalised conjugate multivariate distribution theory for exponential families, which enables exact sampling from analytically available posterior distributions conditional upon some fixed process parameters. More specifically, we expand upon the Diaconis-Ylvisaker family of conjugate priors to achieve analytically tractable posterior inference for spatially-temporally varying regression models conditional on some kernel parameters. Subsequently, we assimilate inference from these individual posterior distributions over a range of values of these parameters using Bayesian predictive stacking. We evaluate inferential performance on simulated data, compare with fully Bayesian inference using Markov chain Monte Carlo and apply our proposed method to analyse spatially-temporally referenced avian count data from the North American Breeding Bird Survey database.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04655&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Soumyakanti Pan, Lu Zhang, Jonathan R. Bradley, Sudipto Banerjee</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Analysing non-Gaussian spatial-temporal data typically requires introducing spatial dependence in generalised linear models through the link function of an exponential family distribution. However, unlike in Gaussian likelihoods, inference is considerably encumbered by the inability to analytically integrate out the random effects and reduce the dimension of the parameter space. Iterative estimation algorithms struggle to converge due to the presence of weakly identified parameters. We devise an approach that obviates these issues by exploiting generalised conjugate multivariate distribution theory for exponential families, which enables exact sampling from analytically available posterior distributions conditional upon some fixed process parameters. More specifically, we expand upon the Diaconis-Ylvisaker family of conjugate priors to achieve analytically tractable posterior inference for spatially-temporally varying regression models conditional on some kernel parameters. Subsequently, we assimilate inference from these individual posterior distributions over a range of values of these parameters using Bayesian predictive stacking. We evaluate inferential performance on simulated data, compare with fully Bayesian inference using Markov chain Monte Carlo and apply our proposed method to analyse spatially-temporally referenced avian count data from the North American Breeding Bird Survey database.</summary></entry><entry><title type="html">Bayesian Methods to Improve The Accuracy of Differentially Private Measurements of Constrained Parameters</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/BayesianMethodstoImproveTheAccuracyofDifferentiallyPrivateMeasurementsofConstrainedParameters.html" rel="alternate" type="text/html" title="Bayesian Methods to Improve The Accuracy of Differentially Private Measurements of Constrained Parameters" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/BayesianMethodstoImproveTheAccuracyofDifferentiallyPrivateMeasurementsofConstrainedParameters</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/BayesianMethodstoImproveTheAccuracyofDifferentiallyPrivateMeasurementsofConstrainedParameters.html">&lt;p&gt;Formal disclosure avoidance techniques are necessary to ensure that published data can not be used to identify information about individuals. The addition of statistical noise to unpublished data can be implemented to achieve differential privacy, which provides a formal mathematical privacy guarantee. However, the infusion of noise results in data releases which are less precise than if no noise had been added, and can lead to some of the individual data points being nonsensical. Examples of this are estimates of population counts which are negative, or estimates of the ratio of counts which violate known constraints. A straightforward way to guarantee that published estimates satisfy these known constraints is to specify a statistical model and incorporate a prior on census counts and ratios which properly constrains the parameter space. We utilize rejection sampling methods for drawing samples from the posterior distribution and we show that this implementation produces estimates of population counts and ratios which maintain formal privacy, are more precise than the original unconstrained noisy measurements, and are guaranteed to satisfy prior constraints.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04448&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ryan Janicki, Scott H. Holan, Kyle M. Irimata, James Livsey, Andrew Raim</name></author><category term="stat.ME" /><summary type="html">Formal disclosure avoidance techniques are necessary to ensure that published data can not be used to identify information about individuals. The addition of statistical noise to unpublished data can be implemented to achieve differential privacy, which provides a formal mathematical privacy guarantee. However, the infusion of noise results in data releases which are less precise than if no noise had been added, and can lead to some of the individual data points being nonsensical. Examples of this are estimates of population counts which are negative, or estimates of the ratio of counts which violate known constraints. A straightforward way to guarantee that published estimates satisfy these known constraints is to specify a statistical model and incorporate a prior on census counts and ratios which properly constrains the parameter space. We utilize rejection sampling methods for drawing samples from the posterior distribution and we show that this implementation produces estimates of population counts and ratios which maintain formal privacy, are more precise than the original unconstrained noisy measurements, and are guaranteed to satisfy prior constraints.</summary></entry><entry><title type="html">Bayesian Statistics: A Review and a Reminder for the Practicing Reliability Engineer</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/BayesianStatisticsAReviewandaReminderforthePracticingReliabilityEngineer.html" rel="alternate" type="text/html" title="Bayesian Statistics: A Review and a Reminder for the Practicing Reliability Engineer" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/BayesianStatisticsAReviewandaReminderforthePracticingReliabilityEngineer</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/BayesianStatisticsAReviewandaReminderforthePracticingReliabilityEngineer.html">&lt;p&gt;This paper introduces and reviews some of the principles and methods used in Bayesian reliability. It specifically discusses methods used in the analysis of success/no-success data and then reminds the reader of a simple Monte Carlo algorithm that can be used to calculate the posterior distribution of a system’s reliability. This algorithm is especially useful when a system’s reliability is modeled through the reliability of its subcomponents, yet only system-level data is available.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02751&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carsten H. Botts</name></author><category term="stat.ME" /><summary type="html">This paper introduces and reviews some of the principles and methods used in Bayesian reliability. It specifically discusses methods used in the analysis of success/no-success data and then reminds the reader of a simple Monte Carlo algorithm that can be used to calculate the posterior distribution of a system’s reliability. This algorithm is especially useful when a system’s reliability is modeled through the reliability of its subcomponents, yet only system-level data is available.</summary></entry><entry><title type="html">Bayesian inference of Latent Spectral Shapes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/BayesianinferenceofLatentSpectralShapes.html" rel="alternate" type="text/html" title="Bayesian inference of Latent Spectral Shapes" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/BayesianinferenceofLatentSpectralShapes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/BayesianinferenceofLatentSpectralShapes.html">&lt;p&gt;This paper proposes a hierarchical spatial-temporal model for modelling the spectrograms of animal calls. The motivation stems from analyzing recordings of the so-called grunt calls emitted by various lemur species. Our goal is to identify a latent spectral shape that characterizes each species and facilitates measuring dissimilarities between them. The model addresses the synchronization of animal vocalizations, due to varying time-lengths and speeds, with non-stationary temporal patterns and accounts for periodic sampling artifacts produced by the time discretization of analog signals. The former is achieved through a synchronization function, and the latter is modeled using a circular representation of time. To overcome the curse of dimensionality inherent in the model’s implementation, we employ the Nearest Neighbor Gaussian Process, and posterior samples are obtained using the Markov Chain Monte Carlo method. We apply the model to a real dataset comprising sounds from 8 different species. We define a representative sound for each species and compare them using a simple distance measure. Cross-validation is used to evaluate the predictive capability of our proposal and explore special cases. Additionally, a simulation example is provided to demonstrate that the algorithm is capable of retrieving the true parameters.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04915&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hiu Ching Yip, Daria Valente, Enrico Bibbona, Olivier Friard, Gianluca Mastrantonio, Marco Gamba</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">This paper proposes a hierarchical spatial-temporal model for modelling the spectrograms of animal calls. The motivation stems from analyzing recordings of the so-called grunt calls emitted by various lemur species. Our goal is to identify a latent spectral shape that characterizes each species and facilitates measuring dissimilarities between them. The model addresses the synchronization of animal vocalizations, due to varying time-lengths and speeds, with non-stationary temporal patterns and accounts for periodic sampling artifacts produced by the time discretization of analog signals. The former is achieved through a synchronization function, and the latter is modeled using a circular representation of time. To overcome the curse of dimensionality inherent in the model’s implementation, we employ the Nearest Neighbor Gaussian Process, and posterior samples are obtained using the Markov Chain Monte Carlo method. We apply the model to a real dataset comprising sounds from 8 different species. We define a representative sound for each species and compare them using a simple distance measure. Cross-validation is used to evaluate the predictive capability of our proposal and explore special cases. Additionally, a simulation example is provided to demonstrate that the algorithm is capable of retrieving the true parameters.</summary></entry><entry><title type="html">Bayesian multi-exposure image fusion for robust high dynamic range ptychography</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Bayesianmultiexposureimagefusionforrobusthighdynamicrangeptychography.html" rel="alternate" type="text/html" title="Bayesian multi-exposure image fusion for robust high dynamic range ptychography" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Bayesianmultiexposureimagefusionforrobusthighdynamicrangeptychography</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Bayesianmultiexposureimagefusionforrobusthighdynamicrangeptychography.html">&lt;p&gt;The limited dynamic range of the detector can impede coherent diffractive imaging (CDI) schemes from achieving diffraction-limited resolution. To overcome this limitation, a straightforward approach is to utilize high dynamic range (HDR) imaging through multi-exposure image fusion (MEF). This method involves capturing measurements at different exposure times, spanning from under to overexposure and fusing them into a single HDR image. The conventional MEF technique in ptychography typically involves subtracting the background noise, ignoring the saturated pixels and then merging the acquisitions. However, this approach is inadequate under conditions of low signal-to-noise ratio (SNR). Additionally, variations in illumination intensity significantly affect the phase retrieval process. To address these issues, we propose a Bayesian MEF modeling approach based on a modified Poisson distribution that takes the background and saturation into account. The expectation-maximization (EM) algorithm is employed to infer the model parameters. As demonstrated with synthetic and experimental data, our approach outperforms the conventional MEF method, offering superior phase retrieval under challenging experimental conditions. This work underscores the significance of robust multi-exposure image fusion for ptychography, particularly in imaging shot-noise-dominated weakly scattering specimens or in cases where access to HDR detectors with high SNR is limited. Furthermore, the applicability of the Bayesian MEF approach extends beyond CDI to any imaging scheme that requires HDR treatment. Given this versatility, we provide the implementation of our algorithm as a Python package.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.11344&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shantanu Kodgirwar, Lars Loetgering, Chang Liu, Aleena Joseph, Leona Licht, Daniel S. Penagos Molina, Wilhelm Eschen, Jan Rothhardt, Michael Habeck</name></author><category term="stat.AP" /><summary type="html">The limited dynamic range of the detector can impede coherent diffractive imaging (CDI) schemes from achieving diffraction-limited resolution. To overcome this limitation, a straightforward approach is to utilize high dynamic range (HDR) imaging through multi-exposure image fusion (MEF). This method involves capturing measurements at different exposure times, spanning from under to overexposure and fusing them into a single HDR image. The conventional MEF technique in ptychography typically involves subtracting the background noise, ignoring the saturated pixels and then merging the acquisitions. However, this approach is inadequate under conditions of low signal-to-noise ratio (SNR). Additionally, variations in illumination intensity significantly affect the phase retrieval process. To address these issues, we propose a Bayesian MEF modeling approach based on a modified Poisson distribution that takes the background and saturation into account. The expectation-maximization (EM) algorithm is employed to infer the model parameters. As demonstrated with synthetic and experimental data, our approach outperforms the conventional MEF method, offering superior phase retrieval under challenging experimental conditions. This work underscores the significance of robust multi-exposure image fusion for ptychography, particularly in imaging shot-noise-dominated weakly scattering specimens or in cases where access to HDR detectors with high SNR is limited. Furthermore, the applicability of the Bayesian MEF approach extends beyond CDI to any imaging scheme that requires HDR treatment. Given this versatility, we provide the implementation of our algorithm as a Python package.</summary></entry><entry><title type="html">Bounded-memory adjusted scores estimation in generalized linear models with large data sets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Boundedmemoryadjustedscoresestimationingeneralizedlinearmodelswithlargedatasets.html" rel="alternate" type="text/html" title="Bounded-memory adjusted scores estimation in generalized linear models with large data sets" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Boundedmemoryadjustedscoresestimationingeneralizedlinearmodelswithlargedatasets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Boundedmemoryadjustedscoresestimationingeneralizedlinearmodelswithlargedatasets.html">&lt;p&gt;The widespread use of maximum Jeffreys’-prior penalized likelihood in binomial-response generalized linear models, and in logistic regression, in particular, are supported by the results of Kosmidis and Firth (2021, Biometrika), who show that the resulting estimates are always finite-valued, even in cases where the maximum likelihood estimates are not, which is a practical issue regardless of the size of the data set. In logistic regression, the implied adjusted score equations are formally bias-reducing in asymptotic frameworks with a fixed number of parameters and appear to deliver a substantial reduction in the persistent bias of the maximum likelihood estimator in high-dimensional settings where the number of parameters grows asymptotically as a proportion of the number of observations. In this work, we develop and present two new variants of iteratively reweighted least squares for estimating generalized linear models with adjusted score equations for mean bias reduction and maximization of the likelihood penalized by a positive power of the Jeffreys-prior penalty, which eliminate the requirement of storing $O(n)$ quantities in memory, and can operate with data sets that exceed computer memory or even hard drive capacity. We achieve that through incremental QR decompositions, which enable IWLS iterations to have access only to data chunks of predetermined size. Both procedures can also be readily adapted to fit generalized linear models when distinct parts of the data is stored across different sites and, due to privacy concerns, cannot be fully transferred across sites. We assess the procedures through a real-data application with millions of observations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.07342&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Patrick Zietkiewicz, Ioannis Kosmidis</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">The widespread use of maximum Jeffreys’-prior penalized likelihood in binomial-response generalized linear models, and in logistic regression, in particular, are supported by the results of Kosmidis and Firth (2021, Biometrika), who show that the resulting estimates are always finite-valued, even in cases where the maximum likelihood estimates are not, which is a practical issue regardless of the size of the data set. In logistic regression, the implied adjusted score equations are formally bias-reducing in asymptotic frameworks with a fixed number of parameters and appear to deliver a substantial reduction in the persistent bias of the maximum likelihood estimator in high-dimensional settings where the number of parameters grows asymptotically as a proportion of the number of observations. In this work, we develop and present two new variants of iteratively reweighted least squares for estimating generalized linear models with adjusted score equations for mean bias reduction and maximization of the likelihood penalized by a positive power of the Jeffreys-prior penalty, which eliminate the requirement of storing $O(n)$ quantities in memory, and can operate with data sets that exceed computer memory or even hard drive capacity. We achieve that through incremental QR decompositions, which enable IWLS iterations to have access only to data chunks of predetermined size. Both procedures can also be readily adapted to fit generalized linear models when distinct parts of the data is stored across different sites and, due to privacy concerns, cannot be fully transferred across sites. We assess the procedures through a real-data application with millions of observations.</summary></entry><entry><title type="html">Causal Inference in Randomized Trials with Partial Clustering and Imbalanced Dependence Structures</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/CausalInferenceinRandomizedTrialswithPartialClusteringandImbalancedDependenceStructures.html" rel="alternate" type="text/html" title="Causal Inference in Randomized Trials with Partial Clustering and Imbalanced Dependence Structures" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/CausalInferenceinRandomizedTrialswithPartialClusteringandImbalancedDependenceStructures</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/CausalInferenceinRandomizedTrialswithPartialClusteringandImbalancedDependenceStructures.html">&lt;p&gt;In many randomized trials, participants are grouped into clusters, such as neighborhoods or schools, and these clusters are assumed to be the independent unit. This assumption, however, might not reflect the underlying dependence structure, with serious consequences to statistical power. First, consider a cluster randomized trial where participants are artificially grouped together for the purposes of randomization. For intervention participants the groups are the basis for intervention delivery, but for control participants the groups are dissolved. Second, consider an individually randomized group treatment trial where participants are randomized and then post-randomization, intervention participants are grouped together for intervention delivery, while the control participants continue with the standard of care. In both trial designs, outcomes among intervention participants will be dependent within each cluster, while outcomes for control participants will be effectively independent. We use causal models to non-parametrically describe the data generating process for each trial design and formalize the conditional independence in the observed data distribution. For estimation and inference, we propose a novel implementation of targeted minimum loss-based estimation (TMLE) accounting for partial clustering and the imbalanced dependence structure. TMLE is a model-robust approach, leverages covariate adjustment and machine learning to improve precision, and facilitates estimation of a large set of causal effects. In finite sample simulations, TMLE achieved comparable or markedly higher statistical power than common alternatives. Finally, application of TMLE to real data from the SEARCH-IPT trial resulted in 20-57\% efficiency gains, demonstrating the real-world consequences of our proposed approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04505&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joshua R. Nugent, Elijah Kakande, Gabriel Chamie, Jane Kabami, Asiphas Owaraganise, Diane V. Havlir, Moses Kamya, Laura B. Balzer</name></author><category term="stat.ME" /><summary type="html">In many randomized trials, participants are grouped into clusters, such as neighborhoods or schools, and these clusters are assumed to be the independent unit. This assumption, however, might not reflect the underlying dependence structure, with serious consequences to statistical power. First, consider a cluster randomized trial where participants are artificially grouped together for the purposes of randomization. For intervention participants the groups are the basis for intervention delivery, but for control participants the groups are dissolved. Second, consider an individually randomized group treatment trial where participants are randomized and then post-randomization, intervention participants are grouped together for intervention delivery, while the control participants continue with the standard of care. In both trial designs, outcomes among intervention participants will be dependent within each cluster, while outcomes for control participants will be effectively independent. We use causal models to non-parametrically describe the data generating process for each trial design and formalize the conditional independence in the observed data distribution. For estimation and inference, we propose a novel implementation of targeted minimum loss-based estimation (TMLE) accounting for partial clustering and the imbalanced dependence structure. TMLE is a model-robust approach, leverages covariate adjustment and machine learning to improve precision, and facilitates estimation of a large set of causal effects. In finite sample simulations, TMLE achieved comparable or markedly higher statistical power than common alternatives. Finally, application of TMLE to real data from the SEARCH-IPT trial resulted in 20-57\% efficiency gains, demonstrating the real-world consequences of our proposed approach.</summary></entry><entry><title type="html">Conformal Multi-Target Hyperrectangles</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/ConformalMultiTargetHyperrectangles.html" rel="alternate" type="text/html" title="Conformal Multi-Target Hyperrectangles" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/ConformalMultiTargetHyperrectangles</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/ConformalMultiTargetHyperrectangles.html">&lt;p&gt;We propose conformal hyperrectangular prediction regions for multi-target regression. We propose split conformal prediction algorithms for both point and quantile regression to form hyperrectangular prediction regions, which allow for easy marginal interpretation and do not require covariance estimation. In practice, it is preferable that a prediction region is balanced, that is, having identical marginal prediction coverage, since prediction accuracy is generally equally important across components of the response vector. The proposed algorithms possess two desirable properties, namely, tight asymptotic overall nominal coverage as well as asymptotic balance, that is, identical asymptotic marginal coverage, under mild conditions. We then compare our methods to some existing methods on both simulated and real data sets. Our simulation results and real data analysis show that our methods outperform existing methods while achieving the desired nominal coverage and good balance between dimensions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04498&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Max Sampson, Kung-Sik Chan</name></author><category term="stat.ME" /><summary type="html">We propose conformal hyperrectangular prediction regions for multi-target regression. We propose split conformal prediction algorithms for both point and quantile regression to form hyperrectangular prediction regions, which allow for easy marginal interpretation and do not require covariance estimation. In practice, it is preferable that a prediction region is balanced, that is, having identical marginal prediction coverage, since prediction accuracy is generally equally important across components of the response vector. The proposed algorithms possess two desirable properties, namely, tight asymptotic overall nominal coverage as well as asymptotic balance, that is, identical asymptotic marginal coverage, under mild conditions. We then compare our methods to some existing methods on both simulated and real data sets. Our simulation results and real data analysis show that our methods outperform existing methods while achieving the desired nominal coverage and good balance between dimensions.</summary></entry><entry><title type="html">Determining the Number of Communities in Sparse and Imbalanced Settings</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/DeterminingtheNumberofCommunitiesinSparseandImbalancedSettings.html" rel="alternate" type="text/html" title="Determining the Number of Communities in Sparse and Imbalanced Settings" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/DeterminingtheNumberofCommunitiesinSparseandImbalancedSettings</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/DeterminingtheNumberofCommunitiesinSparseandImbalancedSettings.html">&lt;p&gt;Community structures represent a crucial aspect of network analysis, and various methods have been developed to identify these communities. However, a common hurdle lies in determining the number of communities K, a parameter that often requires estimation in practice. Existing approaches for estimating K face two notable challenges: the weak community signal present in sparse networks and the imbalance in community sizes or edge densities that result in unequal per-community expected degree. We propose a spectral method based on a novel network operator whose spectral properties effectively overcome both challenges. This operator is a refined version of the non-backtracking operator, adapted from a “centered” adjacency matrix. Its leading eigenvalues are more concentrated than those of the adjacency matrix for sparse networks, while they also demonstrate enhanced signal under imbalance scenarios, a benefit attributed to the centering step. This is justified, either theoretically or numerically, under the null model K = 1, in both dense and ultra-sparse settings. A goodness-of-fit test based on the leading eigenvalue can be applied to determine the number of communities K.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04423&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhixuan Shao, Can M. Le</name></author><category term="stat.ME" /><summary type="html">Community structures represent a crucial aspect of network analysis, and various methods have been developed to identify these communities. However, a common hurdle lies in determining the number of communities K, a parameter that often requires estimation in practice. Existing approaches for estimating K face two notable challenges: the weak community signal present in sparse networks and the imbalance in community sizes or edge densities that result in unequal per-community expected degree. We propose a spectral method based on a novel network operator whose spectral properties effectively overcome both challenges. This operator is a refined version of the non-backtracking operator, adapted from a “centered” adjacency matrix. Its leading eigenvalues are more concentrated than those of the adjacency matrix for sparse networks, while they also demonstrate enhanced signal under imbalance scenarios, a benefit attributed to the centering step. This is justified, either theoretically or numerically, under the null model K = 1, in both dense and ultra-sparse settings. A goodness-of-fit test based on the leading eigenvalue can be applied to determine the number of communities K.</summary></entry><entry><title type="html">Diffusion posterior sampling for simulation-based inference in tall data settings</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Diffusionposteriorsamplingforsimulationbasedinferenceintalldatasettings.html" rel="alternate" type="text/html" title="Diffusion posterior sampling for simulation-based inference in tall data settings" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Diffusionposteriorsamplingforsimulationbasedinferenceintalldatasettings</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Diffusionposteriorsamplingforsimulationbasedinferenceintalldatasettings.html">&lt;p&gt;Determining which parameters of a non-linear model best describe a set of experimental data is a fundamental problem in science and it has gained much traction lately with the rise of complex large-scale simulators. The likelihood of such models is typically intractable, which is why classical MCMC methods can not be used. Simulation-based inference (SBI) stands out in this context by only requiring a dataset of simulations to train deep generative models capable of approximating the posterior distribution that relates input parameters to a given observation. In this work, we consider a tall data extension in which multiple observations are available to better infer the parameters of the model. The proposed method is built upon recent developments from the flourishing score-based diffusion literature and allows to estimate the tall data posterior distribution, while simply using information from a score network trained for a single context observation. We compare our method to recently proposed competing approaches on various numerical experiments and demonstrate its superiority in terms of numerical stability and computational cost.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.07593&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Julia Linhart, Gabriel Victorino Cardoso, Alexandre Gramfort, Sylvain Le Corff, Pedro L. C. Rodrigues</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Determining which parameters of a non-linear model best describe a set of experimental data is a fundamental problem in science and it has gained much traction lately with the rise of complex large-scale simulators. The likelihood of such models is typically intractable, which is why classical MCMC methods can not be used. Simulation-based inference (SBI) stands out in this context by only requiring a dataset of simulations to train deep generative models capable of approximating the posterior distribution that relates input parameters to a given observation. In this work, we consider a tall data extension in which multiple observations are available to better infer the parameters of the model. The proposed method is built upon recent developments from the flourishing score-based diffusion literature and allows to estimate the tall data posterior distribution, while simply using information from a score network trained for a single context observation. We compare our method to recently proposed competing approaches on various numerical experiments and demonstrate its superiority in terms of numerical stability and computational cost.</summary></entry><entry><title type="html">Dynamical mixture modeling with fast, automatic determination of Markov chains</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/DynamicalmixturemodelingwithfastautomaticdeterminationofMarkovchains.html" rel="alternate" type="text/html" title="Dynamical mixture modeling with fast, automatic determination of Markov chains" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/DynamicalmixturemodelingwithfastautomaticdeterminationofMarkovchains</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/DynamicalmixturemodelingwithfastautomaticdeterminationofMarkovchains.html">&lt;p&gt;Markov state modeling has gained popularity in various scientific fields due to its ability to reduce complex time series data into transitions between a few states. Yet, current frameworks are limited by assuming a single Markov chain describes the data, and they suffer an inability to discern heterogeneities. As a solution, this paper proposes a variational expectation-maximization algorithm that identifies a mixture of Markov chains in a time-series data set. The method is agnostic to the definition of the Markov states, whether data-driven (e.g. by spectral clustering) or based on domain knowledge. Variational EM efficiently and organically identifies the number of Markov chains and dynamics of each chain without expensive model comparisons or posterior sampling. The approach is supported by a theoretical analysis and numerical experiments, including simulated and observational data sets based on ${\tt Last.fm}$ music listening, ultramarathon running, and gene expression. The results show the new algorithm is competitive with contemporary mixture modeling approaches and powerful in identifying meaningful heterogeneities in time series data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04653&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Christopher E. Miles, Robert J. Webber</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Markov state modeling has gained popularity in various scientific fields due to its ability to reduce complex time series data into transitions between a few states. Yet, current frameworks are limited by assuming a single Markov chain describes the data, and they suffer an inability to discern heterogeneities. As a solution, this paper proposes a variational expectation-maximization algorithm that identifies a mixture of Markov chains in a time-series data set. The method is agnostic to the definition of the Markov states, whether data-driven (e.g. by spectral clustering) or based on domain knowledge. Variational EM efficiently and organically identifies the number of Markov chains and dynamics of each chain without expensive model comparisons or posterior sampling. The approach is supported by a theoretical analysis and numerical experiments, including simulated and observational data sets based on ${\tt Last.fm}$ music listening, ultramarathon running, and gene expression. The results show the new algorithm is competitive with contemporary mixture modeling approaches and powerful in identifying meaningful heterogeneities in time series data.</summary></entry><entry><title type="html">Dynamic prediction of death risk given a renewal hospitalization process</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Dynamicpredictionofdeathriskgivenarenewalhospitalizationprocess.html" rel="alternate" type="text/html" title="Dynamic prediction of death risk given a renewal hospitalization process" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Dynamicpredictionofdeathriskgivenarenewalhospitalizationprocess</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Dynamicpredictionofdeathriskgivenarenewalhospitalizationprocess.html">&lt;p&gt;Predicting the risk of death for chronic patients is highly valuable for informed medical decision-making. This paper proposes a general framework for dynamic prediction of the risk of death of a patient given her hospitalization history, which is generally available to physicians. Predictions are based on a joint model for the death and hospitalization processes, thereby avoiding the potential bias arising from selection of survivors. The framework accommodates various submodels for the hospitalization process. In particular, we study prediction of the risk of death in a renewal model for hospitalizations, a common approach to recurrent event modelling. In the renewal model, the distribution of hospitalizations throughout the follow-up period impacts the risk of death. This result differs from prediction in the Poisson model, previously studied, where only the number of hospitalizations matters. We apply our methodology to a prospective, observational cohort study of 401 patients treated for COPD in one of six outpatient respiratory clinics run by the Respiratory Service of Galdakao University Hospital, with a median follow-up of 4.16 years. We find that more concentrated hospitalizations increase the risk of death.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04849&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Telmo J. Pérez-Izquierdo, Irantzu Barrio, Cristobal Esteban</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Predicting the risk of death for chronic patients is highly valuable for informed medical decision-making. This paper proposes a general framework for dynamic prediction of the risk of death of a patient given her hospitalization history, which is generally available to physicians. Predictions are based on a joint model for the death and hospitalization processes, thereby avoiding the potential bias arising from selection of survivors. The framework accommodates various submodels for the hospitalization process. In particular, we study prediction of the risk of death in a renewal model for hospitalizations, a common approach to recurrent event modelling. In the renewal model, the distribution of hospitalizations throughout the follow-up period impacts the risk of death. This result differs from prediction in the Poisson model, previously studied, where only the number of hospitalizations matters. We apply our methodology to a prospective, observational cohort study of 401 patients treated for COPD in one of six outpatient respiratory clinics run by the Respiratory Service of Galdakao University Hospital, with a median follow-up of 4.16 years. We find that more concentrated hospitalizations increase the risk of death.</summary></entry><entry><title type="html">Gaining Insights into Group-Level Course Difficulty via Differential Course Functioning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/GainingInsightsintoGroupLevelCourseDifficultyviaDifferentialCourseFunctioning.html" rel="alternate" type="text/html" title="Gaining Insights into Group-Level Course Difficulty via Differential Course Functioning" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/GainingInsightsintoGroupLevelCourseDifficultyviaDifferentialCourseFunctioning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/GainingInsightsintoGroupLevelCourseDifficultyviaDifferentialCourseFunctioning.html">&lt;p&gt;Curriculum Analytics (CA) studies curriculum structure and student data to ensure the quality of educational programs. One desirable property of courses within curricula is that they are not unexpectedly more difficult for students of different backgrounds. While prior work points to likely variations in course difficulty across student groups, robust methodologies for capturing such variations are scarce, and existing approaches do not adequately decouple course-specific difficulty from students’ general performance levels. The present study introduces Differential Course Functioning (DCF) as an Item Response Theory (IRT)-based CA methodology. DCF controls for student performance levels and examines whether significant differences exist in how distinct student groups succeed in a given course. Leveraging data from over 20,000 students at a large public university, we demonstrate DCF’s ability to detect inequities in undergraduate course difficulty across student groups described by grade achievement. We compare major pairs with high co-enrollment and transfer students to their non-transfer peers. For the former, our findings suggest a link between DCF effect sizes and the alignment of course content to student home department motivating interventions targeted towards improving course preparedness. For the latter, results suggest minor variations in course-specific difficulty between transfer and non-transfer students. While this is desirable, it also suggests that interventions targeted toward mitigating grade achievement gaps in transfer students should encompass comprehensive support beyond enhancing preparedness for individual courses. By providing more nuanced and equitable assessments of academic performance and difficulties experienced by diverse student populations, DCF could support policymakers, course articulation officers, and student advisors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04348&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Frederik Baucks, Robin Schmucker, Conrad Borchers, Zachary A. Pardos, Laurenz Wiskott</name></author><category term="stat.AP" /><summary type="html">Curriculum Analytics (CA) studies curriculum structure and student data to ensure the quality of educational programs. One desirable property of courses within curricula is that they are not unexpectedly more difficult for students of different backgrounds. While prior work points to likely variations in course difficulty across student groups, robust methodologies for capturing such variations are scarce, and existing approaches do not adequately decouple course-specific difficulty from students’ general performance levels. The present study introduces Differential Course Functioning (DCF) as an Item Response Theory (IRT)-based CA methodology. DCF controls for student performance levels and examines whether significant differences exist in how distinct student groups succeed in a given course. Leveraging data from over 20,000 students at a large public university, we demonstrate DCF’s ability to detect inequities in undergraduate course difficulty across student groups described by grade achievement. We compare major pairs with high co-enrollment and transfer students to their non-transfer peers. For the former, our findings suggest a link between DCF effect sizes and the alignment of course content to student home department motivating interventions targeted towards improving course preparedness. For the latter, results suggest minor variations in course-specific difficulty between transfer and non-transfer students. While this is desirable, it also suggests that interventions targeted toward mitigating grade achievement gaps in transfer students should encompass comprehensive support beyond enhancing preparedness for individual courses. By providing more nuanced and equitable assessments of academic performance and difficulties experienced by diverse student populations, DCF could support policymakers, course articulation officers, and student advisors.</summary></entry><entry><title type="html">Group Differences in Opinion Instability and Measurement Errors: A G-Theory Analysis of College Students</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/GroupDifferencesinOpinionInstabilityandMeasurementErrorsAGTheoryAnalysisofCollegeStudents.html" rel="alternate" type="text/html" title="Group Differences in Opinion Instability and Measurement Errors: A G-Theory Analysis of College Students" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/GroupDifferencesinOpinionInstabilityandMeasurementErrorsAGTheoryAnalysisofCollegeStudents</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/GroupDifferencesinOpinionInstabilityandMeasurementErrorsAGTheoryAnalysisofCollegeStudents.html">&lt;p&gt;This study examines opinion instability among individuals from different ethnic groups (White, Latino, and Asian Americans) by analyzing measurement errors in survey measures. Using a multi-wave panel dataset of college students and employing generalizability theory, the study uncovers significant patterns. The results reveal that White students exhibit higher attitude reliability, characterized by larger variances in true opinions and smaller measurement errors. In contrast, Latino and Asian American students display lower attitude stability, with lower variances in true opinions and higher variances in both item-specific and measurement errors. Disparities in political socialization and issue concerns contribute to the observed attitude instability among Latino and Asian American students. Moreover, Asian American and Latino respondents require a greater number of survey items to mitigate measurement error compared to their White counterparts. However, the impact of multiple waves of surveys on improving reliability is limited for Latino and Asian American students compared to White students. These findings deepen our understanding of attitude instability across ethnic groups and underscore the importance of further research in this area.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.17311&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bang Quan Zheng</name></author><category term="stat.AP" /><summary type="html">This study examines opinion instability among individuals from different ethnic groups (White, Latino, and Asian Americans) by analyzing measurement errors in survey measures. Using a multi-wave panel dataset of college students and employing generalizability theory, the study uncovers significant patterns. The results reveal that White students exhibit higher attitude reliability, characterized by larger variances in true opinions and smaller measurement errors. In contrast, Latino and Asian American students display lower attitude stability, with lower variances in true opinions and higher variances in both item-specific and measurement errors. Disparities in political socialization and issue concerns contribute to the observed attitude instability among Latino and Asian American students. Moreover, Asian American and Latino respondents require a greater number of survey items to mitigate measurement error compared to their White counterparts. However, the impact of multiple waves of surveys on improving reliability is limited for Latino and Asian American students compared to White students. These findings deepen our understanding of attitude instability across ethnic groups and underscore the importance of further research in this area.</summary></entry><entry><title type="html">Improving Model Chain Approaches for Probabilistic Solar Energy Forecasting through Post-processing and Machine Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/ImprovingModelChainApproachesforProbabilisticSolarEnergyForecastingthroughPostprocessingandMachineLearning.html" rel="alternate" type="text/html" title="Improving Model Chain Approaches for Probabilistic Solar Energy Forecasting through Post-processing and Machine Learning" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/ImprovingModelChainApproachesforProbabilisticSolarEnergyForecastingthroughPostprocessingandMachineLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/ImprovingModelChainApproachesforProbabilisticSolarEnergyForecastingthroughPostprocessingandMachineLearning.html">&lt;p&gt;Weather forecasts from numerical weather prediction models play a central role in solar energy forecasting, where a cascade of physics-based models is used in a model chain approach to convert forecasts of solar irradiance to solar power production, using additional weather variables as auxiliary information. Ensemble weather forecasts aim to quantify uncertainty in the future development of the weather, and can be used to propagate this uncertainty through the model chain to generate probabilistic solar energy predictions. However, ensemble prediction systems are known to exhibit systematic errors, and thus require post-processing to obtain accurate and reliable probabilistic forecasts. The overarching aim of our study is to systematically evaluate different strategies to apply post-processing methods in model chain approaches: Not applying any post-processing at all; post-processing only the irradiance predictions before the conversion; post-processing only the solar power predictions obtained from the model chain; or applying post-processing in both steps. In a case study based on a benchmark dataset for the Jacumba solar plant in the U.S., we develop statistical and machine learning methods for post-processing ensemble predictions of global horizontal irradiance and solar power generation. Further, we propose a neural network-based model for direct solar power forecasting that bypasses the model chain. Our results indicate that post-processing substantially improves the solar power generation forecasts, in particular when post-processing is applied to the power predictions. The machine learning methods for post-processing yield slightly better probabilistic forecasts, and the direct forecasting approach performs comparable to the post-processing strategies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04424&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nina Horat, Sina Klerings, Sebastian Lerch</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Weather forecasts from numerical weather prediction models play a central role in solar energy forecasting, where a cascade of physics-based models is used in a model chain approach to convert forecasts of solar irradiance to solar power production, using additional weather variables as auxiliary information. Ensemble weather forecasts aim to quantify uncertainty in the future development of the weather, and can be used to propagate this uncertainty through the model chain to generate probabilistic solar energy predictions. However, ensemble prediction systems are known to exhibit systematic errors, and thus require post-processing to obtain accurate and reliable probabilistic forecasts. The overarching aim of our study is to systematically evaluate different strategies to apply post-processing methods in model chain approaches: Not applying any post-processing at all; post-processing only the irradiance predictions before the conversion; post-processing only the solar power predictions obtained from the model chain; or applying post-processing in both steps. In a case study based on a benchmark dataset for the Jacumba solar plant in the U.S., we develop statistical and machine learning methods for post-processing ensemble predictions of global horizontal irradiance and solar power generation. Further, we propose a neural network-based model for direct solar power forecasting that bypasses the model chain. Our results indicate that post-processing substantially improves the solar power generation forecasts, in particular when post-processing is applied to the power predictions. The machine learning methods for post-processing yield slightly better probabilistic forecasts, and the direct forecasting approach performs comparable to the post-processing strategies.</summary></entry><entry><title type="html">Imputation of Nonignorable Missing Data in Surveys Using Auxiliary Margins Via Hot Deck and Sequential Imputation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/ImputationofNonignorableMissingDatainSurveysUsingAuxiliaryMarginsViaHotDeckandSequentialImputation.html" rel="alternate" type="text/html" title="Imputation of Nonignorable Missing Data in Surveys Using Auxiliary Margins Via Hot Deck and Sequential Imputation" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/ImputationofNonignorableMissingDatainSurveysUsingAuxiliaryMarginsViaHotDeckandSequentialImputation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/ImputationofNonignorableMissingDatainSurveysUsingAuxiliaryMarginsViaHotDeckandSequentialImputation.html">&lt;p&gt;Survey data collection often is plagued by unit and item nonresponse. To reduce reliance on strong assumptions about the missingness mechanisms, statisticians can use information about population marginal distributions known, for example, from censuses or administrative databases. One approach that does so is the Missing Data with Auxiliary Margins, or MD-AM, framework, which uses multiple imputation for both unit and item nonresponse so that survey-weighted estimates accord with the known marginal distributions. However, this framework relies on specifying and estimating a joint distribution for the survey data and nonresponse indicators, which can be computationally and practically daunting in data with many variables of mixed types. We propose two adaptations to the MD-AM framework to simplify the imputation task. First, rather than specifying a joint model for unit respondents’ data, we use random hot deck imputation while still leveraging the known marginal distributions. Second, instead of sampling from conditional distributions implied by the joint model for the missing data due to item nonresponse, we apply multiple imputation by chained equations for item nonresponse before imputation for unit nonresponse. Using simulation studies with nonignorable missingness mechanisms, we demonstrate that the proposed approach can provide more accurate point and interval estimates than models that do not leverage the auxiliary information. We illustrate the approach using data on voter turnout from the U.S. Current Population Survey.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04599&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yanjiao Yang, Jerome P. Reiter</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Survey data collection often is plagued by unit and item nonresponse. To reduce reliance on strong assumptions about the missingness mechanisms, statisticians can use information about population marginal distributions known, for example, from censuses or administrative databases. One approach that does so is the Missing Data with Auxiliary Margins, or MD-AM, framework, which uses multiple imputation for both unit and item nonresponse so that survey-weighted estimates accord with the known marginal distributions. However, this framework relies on specifying and estimating a joint distribution for the survey data and nonresponse indicators, which can be computationally and practically daunting in data with many variables of mixed types. We propose two adaptations to the MD-AM framework to simplify the imputation task. First, rather than specifying a joint model for unit respondents’ data, we use random hot deck imputation while still leveraging the known marginal distributions. Second, instead of sampling from conditional distributions implied by the joint model for the missing data due to item nonresponse, we apply multiple imputation by chained equations for item nonresponse before imputation for unit nonresponse. Using simulation studies with nonignorable missingness mechanisms, we demonstrate that the proposed approach can provide more accurate point and interval estimates than models that do not leverage the auxiliary information. We illustrate the approach using data on voter turnout from the U.S. Current Population Survey.</summary></entry><entry><title type="html">Kernel Three Pass Regression Filter</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/KernelThreePassRegressionFilter.html" rel="alternate" type="text/html" title="Kernel Three Pass Regression Filter" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/KernelThreePassRegressionFilter</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/KernelThreePassRegressionFilter.html">&lt;p&gt;We forecast a single time series using a high-dimensional set of predictors. When these predictors share common underlying dynamics, an approximate latent factor model provides a powerful characterization of their co-movements Bai(2003). These latent factors succinctly summarize the data and can also be used for prediction, alleviating the curse of dimensionality in high-dimensional prediction exercises, see Stock &amp;amp; Watson (2002a). However, forecasting using these latent factors suffers from two potential drawbacks. First, not all pervasive factors among the set of predictors may be relevant, and using all of them can lead to inefficient forecasts. The second shortcoming is the assumption of linear dependence of predictors on the underlying factors. The first issue can be addressed by using some form of supervision, which leads to the omission of irrelevant information. One example is the three-pass regression filter proposed by Kelly &amp;amp; Pruitt (2015). We extend their framework to cases where the form of dependence might be nonlinear by developing a new estimator, which we refer to as the Kernel Three-Pass Regression Filter (K3PRF). This alleviates the aforementioned second shortcoming. The estimator is computationally efficient and performs well empirically. The short-term performance matches or exceeds that of established models, while the long-term performance shows significant improvement.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07292&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rajveer Jat, Daanish Padha</name></author><category term="stat.ME" /><summary type="html">We forecast a single time series using a high-dimensional set of predictors. When these predictors share common underlying dynamics, an approximate latent factor model provides a powerful characterization of their co-movements Bai(2003). These latent factors succinctly summarize the data and can also be used for prediction, alleviating the curse of dimensionality in high-dimensional prediction exercises, see Stock &amp;amp; Watson (2002a). However, forecasting using these latent factors suffers from two potential drawbacks. First, not all pervasive factors among the set of predictors may be relevant, and using all of them can lead to inefficient forecasts. The second shortcoming is the assumption of linear dependence of predictors on the underlying factors. The first issue can be addressed by using some form of supervision, which leads to the omission of irrelevant information. One example is the three-pass regression filter proposed by Kelly &amp;amp; Pruitt (2015). We extend their framework to cases where the form of dependence might be nonlinear by developing a new estimator, which we refer to as the Kernel Three-Pass Regression Filter (K3PRF). This alleviates the aforementioned second shortcoming. The estimator is computationally efficient and performs well empirically. The short-term performance matches or exceeds that of established models, while the long-term performance shows significant improvement.</summary></entry><entry><title type="html">Optimization of geological carbon storage operations with multimodal latent dynamic model and deep reinforcement learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Optimizationofgeologicalcarbonstorageoperationswithmultimodallatentdynamicmodelanddeepreinforcementlearning.html" rel="alternate" type="text/html" title="Optimization of geological carbon storage operations with multimodal latent dynamic model and deep reinforcement learning" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Optimizationofgeologicalcarbonstorageoperationswithmultimodallatentdynamicmodelanddeepreinforcementlearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Optimizationofgeologicalcarbonstorageoperationswithmultimodallatentdynamicmodelanddeepreinforcementlearning.html">&lt;p&gt;Maximizing storage performance in geological carbon storage (GCS) is crucial for commercial deployment, but traditional optimization demands resource-intensive simulations, posing computational challenges. This study introduces the multimodal latent dynamic (MLD) model, a deep learning framework for fast flow prediction and well control optimization in GCS. The MLD model includes a representation module for compressed latent representations, a transition module for system state evolution, and a prediction module for flow responses. A novel training strategy combining regression loss and joint-embedding consistency loss enhances temporal consistency and multi-step prediction accuracy. Unlike existing models, the MLD supports diverse input modalities, allowing comprehensive data interactions. The MLD model, resembling a Markov decision process (MDP), can train deep reinforcement learning agents, specifically using the soft actor-critic (SAC) algorithm, to maximize net present value (NPV) through continuous interactions. The approach outperforms traditional methods, achieving the highest NPV while reducing computational resources by over 60%. It also demonstrates strong generalization performance, providing improved decisions for new scenarios based on knowledge from previous ones.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04575&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhongzheng Wang, Yuntian Chen, Guodong Chen, Dongxiao Zhang</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Maximizing storage performance in geological carbon storage (GCS) is crucial for commercial deployment, but traditional optimization demands resource-intensive simulations, posing computational challenges. This study introduces the multimodal latent dynamic (MLD) model, a deep learning framework for fast flow prediction and well control optimization in GCS. The MLD model includes a representation module for compressed latent representations, a transition module for system state evolution, and a prediction module for flow responses. A novel training strategy combining regression loss and joint-embedding consistency loss enhances temporal consistency and multi-step prediction accuracy. Unlike existing models, the MLD supports diverse input modalities, allowing comprehensive data interactions. The MLD model, resembling a Markov decision process (MDP), can train deep reinforcement learning agents, specifically using the soft actor-critic (SAC) algorithm, to maximize net present value (NPV) through continuous interactions. The approach outperforms traditional methods, achieving the highest NPV while reducing computational resources by over 60%. It also demonstrates strong generalization performance, providing improved decisions for new scenarios based on knowledge from previous ones.</summary></entry><entry><title type="html">Robust Inference of Dynamic Covariance Using Wishart Processes and Sequential Monte Carlo</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/RobustInferenceofDynamicCovarianceUsingWishartProcessesandSequentialMonteCarlo.html" rel="alternate" type="text/html" title="Robust Inference of Dynamic Covariance Using Wishart Processes and Sequential Monte Carlo" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/RobustInferenceofDynamicCovarianceUsingWishartProcessesandSequentialMonteCarlo</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/RobustInferenceofDynamicCovarianceUsingWishartProcessesandSequentialMonteCarlo.html">&lt;p&gt;Several disciplines, such as econometrics, neuroscience, and computational psychology, study the dynamic interactions between variables over time. A Bayesian nonparametric model known as the Wishart process has been shown to be effective in this situation, but its inference remains highly challenging. In this work, we introduce a Sequential Monte Carlo (SMC) sampler for the Wishart process, and show how it compares to conventional inference approaches, namely MCMC and variational inference. Using simulations we show that SMC sampling results in the most robust estimates and out-of-sample predictions of dynamic covariance. SMC especially outperforms the alternative approaches when using composite covariance functions with correlated parameters. We demonstrate the practical applicability of our proposed approach on a dataset of clinical depression (n=1), and show how using an accurate representation of the posterior distribution can be used to test for dynamics on covariance&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04796&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hester Huijsdens, David Leeftink, Linda Geerligs, Max Hinne</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Several disciplines, such as econometrics, neuroscience, and computational psychology, study the dynamic interactions between variables over time. A Bayesian nonparametric model known as the Wishart process has been shown to be effective in this situation, but its inference remains highly challenging. In this work, we introduce a Sequential Monte Carlo (SMC) sampler for the Wishart process, and show how it compares to conventional inference approaches, namely MCMC and variational inference. Using simulations we show that SMC sampling results in the most robust estimates and out-of-sample predictions of dynamic covariance. SMC especially outperforms the alternative approaches when using composite covariance functions with correlated parameters. We demonstrate the practical applicability of our proposed approach on a dataset of clinical depression (n=1), and show how using an accurate representation of the posterior distribution can be used to test for dynamics on covariance</summary></entry><entry><title type="html">Robust Survival Analysis with Adversarial Regularization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/RobustSurvivalAnalysiswithAdversarialRegularization.html" rel="alternate" type="text/html" title="Robust Survival Analysis with Adversarial Regularization" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/RobustSurvivalAnalysiswithAdversarialRegularization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/RobustSurvivalAnalysiswithAdversarialRegularization.html">&lt;p&gt;Survival Analysis (SA) is about modeling the time for an event of interest to occur, which has important applications in many fields, including medicine, defense, finance, and aerospace. Recent work has demonstrated the benefits of using Neural Networks (NNs) to capture complicated relationships in SA. However, the datasets used to train these models are often subject to uncertainty (e.g., noisy measurements, human error), which we show can substantially degrade the performance of existing techniques. To address this issue, this work leverages recent advances in NN verification to provide new algorithms for generating fully parametric survival models that are robust to such uncertainties. In particular, we introduce a robust loss function for training the models and use CROWN-IBP regularization to address the computational challenges with solving the resulting Min-Max problem. To evaluate the proposed approach, we apply relevant perturbations to publicly available datasets in the SurvSet repository and compare survival models against several baselines. We empirically show that Survival Analysis with Adversarial Regularization (SAWAR) method on average ranks best for dataset perturbations of varying magnitudes on metrics such as Negative Log Likelihood (NegLL), Integrated Brier Score (IBS), and Concordance Index (CI), concluding that adversarial regularization enhances performance in SA. Code: https://github.com/mlpotter/SAWAR&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.16019&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael Potter, Stefano Maxenti, Michael Everett</name></author><category term="stat.ML," /><category term="stat.AP" /><summary type="html">Survival Analysis (SA) is about modeling the time for an event of interest to occur, which has important applications in many fields, including medicine, defense, finance, and aerospace. Recent work has demonstrated the benefits of using Neural Networks (NNs) to capture complicated relationships in SA. However, the datasets used to train these models are often subject to uncertainty (e.g., noisy measurements, human error), which we show can substantially degrade the performance of existing techniques. To address this issue, this work leverages recent advances in NN verification to provide new algorithms for generating fully parametric survival models that are robust to such uncertainties. In particular, we introduce a robust loss function for training the models and use CROWN-IBP regularization to address the computational challenges with solving the resulting Min-Max problem. To evaluate the proposed approach, we apply relevant perturbations to publicly available datasets in the SurvSet repository and compare survival models against several baselines. We empirically show that Survival Analysis with Adversarial Regularization (SAWAR) method on average ranks best for dataset perturbations of varying magnitudes on metrics such as Negative Log Likelihood (NegLL), Integrated Brier Score (IBS), and Concordance Index (CI), concluding that adversarial regularization enhances performance in SA. Code: https://github.com/mlpotter/SAWAR</summary></entry><entry><title type="html">Sensitivity analysis for publication bias in meta-analysis of sparse data based on exact likelihood</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Sensitivityanalysisforpublicationbiasinmetaanalysisofsparsedatabasedonexactlikelihood.html" rel="alternate" type="text/html" title="Sensitivity analysis for publication bias in meta-analysis of sparse data based on exact likelihood" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Sensitivityanalysisforpublicationbiasinmetaanalysisofsparsedatabasedonexactlikelihood</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Sensitivityanalysisforpublicationbiasinmetaanalysisofsparsedatabasedonexactlikelihood.html">&lt;p&gt;Meta-analysis is a powerful tool to synthesize findings from multiple studies. The normal-normal random-effects model is widely used to account for between-study heterogeneity. However, meta-analysis of sparse data, which may arise when the event rate is low for binary or count outcomes, poses a challenge to the normal-normal random-effects model in the accuracy and stability in inference since the normal approximation in the within-study model may not be good. To reduce bias arising from data sparsity, the generalized linear mixed model can be used by replacing the approximate normal within-study model with an exact model. Publication bias is one of the most serious threats in meta-analysis. Several quantitative sensitivity analysis methods for evaluating the potential impacts of selective publication are available for the normal-normal random-effects model. We propose a sensitivity analysis method by extending the likelihood-based sensitivity analysis with the t-statistic selection function of Copas to several generalized linear mixed-effects models. Through applications of our proposed method to several real-world meta-analysis and simulation studies, the proposed method was proven to outperform the likelihood-based sensitivity analysis based on the normal-normal model. The proposed method would give useful guidance to address publication bias in meta-analysis of sparse data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.06837&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Taojun Hu, Yi Zhou, Satoshi Hattori</name></author><category term="stat.ME" /><summary type="html">Meta-analysis is a powerful tool to synthesize findings from multiple studies. The normal-normal random-effects model is widely used to account for between-study heterogeneity. However, meta-analysis of sparse data, which may arise when the event rate is low for binary or count outcomes, poses a challenge to the normal-normal random-effects model in the accuracy and stability in inference since the normal approximation in the within-study model may not be good. To reduce bias arising from data sparsity, the generalized linear mixed model can be used by replacing the approximate normal within-study model with an exact model. Publication bias is one of the most serious threats in meta-analysis. Several quantitative sensitivity analysis methods for evaluating the potential impacts of selective publication are available for the normal-normal random-effects model. We propose a sensitivity analysis method by extending the likelihood-based sensitivity analysis with the t-statistic selection function of Copas to several generalized linear mixed-effects models. Through applications of our proposed method to several real-world meta-analysis and simulation studies, the proposed method was proven to outperform the likelihood-based sensitivity analysis based on the normal-normal model. The proposed method would give useful guidance to address publication bias in meta-analysis of sparse data.</summary></entry><entry><title type="html">Small area estimation of forest biomass via a two-stage model for continuous zero-inflated data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Smallareaestimationofforestbiomassviaatwostagemodelforcontinuouszeroinflateddata.html" rel="alternate" type="text/html" title="Small area estimation of forest biomass via a two-stage model for continuous zero-inflated data" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Smallareaestimationofforestbiomassviaatwostagemodelforcontinuouszeroinflateddata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Smallareaestimationofforestbiomassviaatwostagemodelforcontinuouszeroinflateddata.html">&lt;p&gt;The United States (US) Forest Inventory &amp;amp; Analysis Program (FIA) collects data on and monitors the trends of forests in the US. FIA is increasingly interested in monitoring forest attributes such as biomass at fine geographic and temporal scales, resulting in a need for assessment and development of small area estimation techniques in forest inventory. We implement a small area estimator and parametric bootstrap estimator that account for zero-inflation in biomass data via a two-stage model-based approach and compare its performance to a post-stratified estimator and to the unit- and area-level empirical best linear unbiased prediction (EBLUP) estimators. For estimator comparison, we conduct a simulation study with counties in the US state Nevada as domains based on sampled plot data and remote sensing data products. Results show the zero-inflated estimator has the lowest relative bias and the smallest empirical root mean square error. Moreover, the 95% confidence interval coverages of the zero-inflated estimator and the unit-level EBLUP are more accurate than the other two estimators. To further illustrate the practical utility, we employ a data application across the 2019 measurement year in Nevada. We introduce the R package, saeczi, which efficiently implements the zero-inflated estimator and its mean squared error estimator.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.03263&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Grayson W. White, Josh K. Yamamoto, Dinan H. Elsyad, Julian F. Schmitt, Niels H. Korsgaard, Jie Kate Hu, George C. Gaines III, Tracey S. Frescino, Kelly S. McConville</name></author><category term="stat.AP" /><summary type="html">The United States (US) Forest Inventory &amp;amp; Analysis Program (FIA) collects data on and monitors the trends of forests in the US. FIA is increasingly interested in monitoring forest attributes such as biomass at fine geographic and temporal scales, resulting in a need for assessment and development of small area estimation techniques in forest inventory. We implement a small area estimator and parametric bootstrap estimator that account for zero-inflation in biomass data via a two-stage model-based approach and compare its performance to a post-stratified estimator and to the unit- and area-level empirical best linear unbiased prediction (EBLUP) estimators. For estimator comparison, we conduct a simulation study with counties in the US state Nevada as domains based on sampled plot data and remote sensing data products. Results show the zero-inflated estimator has the lowest relative bias and the smallest empirical root mean square error. Moreover, the 95% confidence interval coverages of the zero-inflated estimator and the unit-level EBLUP are more accurate than the other two estimators. To further illustrate the practical utility, we employ a data application across the 2019 measurement year in Nevada. We introduce the R package, saeczi, which efficiently implements the zero-inflated estimator and its mean squared error estimator.</summary></entry><entry><title type="html">Stochastic full waveform inversion with deep generative prior for uncertainty quantification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Stochasticfullwaveforminversionwithdeepgenerativepriorforuncertaintyquantification.html" rel="alternate" type="text/html" title="Stochastic full waveform inversion with deep generative prior for uncertainty quantification" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Stochasticfullwaveforminversionwithdeepgenerativepriorforuncertaintyquantification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Stochasticfullwaveforminversionwithdeepgenerativepriorforuncertaintyquantification.html">&lt;p&gt;To obtain high-resolution images of subsurface structures from seismic data, seismic imaging techniques such as Full Waveform Inversion (FWI) serve as crucial tools. However, FWI involves solving a nonlinear and often non-unique inverse problem, presenting challenges such as local minima trapping and inadequate handling of inherent uncertainties. In addressing these challenges, we propose leveraging deep generative models as the prior distribution of geophysical parameters for stochastic Bayesian inversion. This approach integrates the adjoint state gradient for efficient back-propagation from the numerical solution of partial differential equations. Additionally, we introduce explicit and implicit variational Bayesian inference methods. The explicit method computes variational distribution density using a normalizing flow-based neural network, enabling computation of the Bayesian posterior of parameters. Conversely, the implicit method employs an inference network attached to a pretrained generative model to estimate density, incorporating an entropy estimator. Furthermore, we also experimented with the Stein Variational Gradient Descent (SVGD) method as another variational inference technique, using particles. We compare these variational Bayesian inference methods with conventional Markov chain Monte Carlo (McMC) sampling. Each method is able to quantify uncertainties and to generate seismic data-conditioned realizations of subsurface geophysical parameters. This framework provides insights into subsurface structures while accounting for inherent uncertainties.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04859&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuke Xie, Hervé Chauris, Nicolas Desassis</name></author><category term="stat.CO" /><summary type="html">To obtain high-resolution images of subsurface structures from seismic data, seismic imaging techniques such as Full Waveform Inversion (FWI) serve as crucial tools. However, FWI involves solving a nonlinear and often non-unique inverse problem, presenting challenges such as local minima trapping and inadequate handling of inherent uncertainties. In addressing these challenges, we propose leveraging deep generative models as the prior distribution of geophysical parameters for stochastic Bayesian inversion. This approach integrates the adjoint state gradient for efficient back-propagation from the numerical solution of partial differential equations. Additionally, we introduce explicit and implicit variational Bayesian inference methods. The explicit method computes variational distribution density using a normalizing flow-based neural network, enabling computation of the Bayesian posterior of parameters. Conversely, the implicit method employs an inference network attached to a pretrained generative model to estimate density, incorporating an entropy estimator. Furthermore, we also experimented with the Stein Variational Gradient Descent (SVGD) method as another variational inference technique, using particles. We compare these variational Bayesian inference methods with conventional Markov chain Monte Carlo (McMC) sampling. Each method is able to quantify uncertainties and to generate seismic data-conditioned realizations of subsurface geophysical parameters. This framework provides insights into subsurface structures while accounting for inherent uncertainties.</summary></entry><entry><title type="html">Testing common invariant subspace of multilayer networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Testingcommoninvariantsubspaceofmultilayernetworks.html" rel="alternate" type="text/html" title="Testing common invariant subspace of multilayer networks" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Testingcommoninvariantsubspaceofmultilayernetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Testingcommoninvariantsubspaceofmultilayernetworks.html">&lt;p&gt;Graph (or network) is a mathematical structure that has been widely used to model relational data. As real-world systems get more complex, multilayer (or multiple) networks are employed to represent diverse patterns of relationships among the objects in the systems. One active research problem in multilayer networks analysis is to study the common invariant subspace of the networks, because such common invariant subspace could capture the fundamental structural patterns and interactions across all layers. Many methods have been proposed to estimate the common invariant subspace. However, whether real-world multilayer networks share the same common subspace remains unknown. In this paper, we first attempt to answer this question by means of hypothesis testing. The null hypothesis states that the multilayer networks share the same subspace, and under the alternative hypothesis, there exist at least two networks that do not have the same subspace. We propose a Weighted Degree Difference Test, derive the limiting distribution of the test statistic and provide an analytical analysis of the power. Simulation study shows that the proposed test has satisfactory performance, and a real data application is provided.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05010&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mingao Yuan, Qianqian Yao</name></author><category term="stat.ME" /><summary type="html">Graph (or network) is a mathematical structure that has been widely used to model relational data. As real-world systems get more complex, multilayer (or multiple) networks are employed to represent diverse patterns of relationships among the objects in the systems. One active research problem in multilayer networks analysis is to study the common invariant subspace of the networks, because such common invariant subspace could capture the fundamental structural patterns and interactions across all layers. Many methods have been proposed to estimate the common invariant subspace. However, whether real-world multilayer networks share the same common subspace remains unknown. In this paper, we first attempt to answer this question by means of hypothesis testing. The null hypothesis states that the multilayer networks share the same subspace, and under the alternative hypothesis, there exist at least two networks that do not have the same subspace. We propose a Weighted Degree Difference Test, derive the limiting distribution of the test statistic and provide an analytical analysis of the power. Simulation study shows that the proposed test has satisfactory performance, and a real data application is provided.</summary></entry><entry><title type="html">Time-lag bias induced by unobserved heterogeneity: comparing treated patients to controls with a different start of follow-up</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Timelagbiasinducedbyunobservedheterogeneitycomparingtreatedpatientstocontrolswithadifferentstartoffollowup.html" rel="alternate" type="text/html" title="Time-lag bias induced by unobserved heterogeneity: comparing treated patients to controls with a different start of follow-up" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Timelagbiasinducedbyunobservedheterogeneitycomparingtreatedpatientstocontrolswithadifferentstartoffollowup</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/Timelagbiasinducedbyunobservedheterogeneitycomparingtreatedpatientstocontrolswithadifferentstartoffollowup.html">&lt;p&gt;In comparative effectiveness research, treated and control patients might have a different start of follow-up as treatment is often started later in the disease trajectory. This typically occurs when data from treated and controls are not collected within the same source. Only patients who did not yet experience the event of interest whilst in the control condition end up in the treatment data source. In case of unobserved heterogeneity, these treated patients will have a lower average risk than the controls. We illustrate how failing to account for this time-lag between treated and controls leads to bias in the estimated treatment effect. We define estimands and time axes, then explore five methods to adjust for this time-lag bias by utilising the time between diagnosis and treatment initiation in different ways. We conducted a simulation study to evaluate whether these methods reduce the bias and then applied the methods to a comparison between fertility patients treated with insemination and similar but untreated patients. We conclude that time-lag bias can be vast and that the time between diagnosis and treatment initiation should be taken into account in the analysis to respect the chronology of the disease and treatment trajectory.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.07685&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rik van Eekelen, Patrick M. M. Bossuyt, Nan van Geloven</name></author><category term="stat.ME" /><summary type="html">In comparative effectiveness research, treated and control patients might have a different start of follow-up as treatment is often started later in the disease trajectory. This typically occurs when data from treated and controls are not collected within the same source. Only patients who did not yet experience the event of interest whilst in the control condition end up in the treatment data source. In case of unobserved heterogeneity, these treated patients will have a lower average risk than the controls. We illustrate how failing to account for this time-lag between treated and controls leads to bias in the estimated treatment effect. We define estimands and time axes, then explore five methods to adjust for this time-lag bias by utilising the time between diagnosis and treatment initiation in different ways. We conducted a simulation study to evaluate whether these methods reduce the bias and then applied the methods to a comparison between fertility patients treated with insemination and similar but untreated patients. We conclude that time-lag bias can be vast and that the time between diagnosis and treatment initiation should be taken into account in the analysis to respect the chronology of the disease and treatment trajectory.</summary></entry><entry><title type="html">TrendLSW: Trend and Spectral Estimation of Nonstationary Time Series in R</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/TrendLSWTrendandSpectralEstimationofNonstationaryTimeSeriesinR.html" rel="alternate" type="text/html" title="TrendLSW: Trend and Spectral Estimation of Nonstationary Time Series in R" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/TrendLSWTrendandSpectralEstimationofNonstationaryTimeSeriesinR</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/TrendLSWTrendandSpectralEstimationofNonstationaryTimeSeriesinR.html">&lt;p&gt;The TrendLSW R package has been developed to provide users with a suite of wavelet-based techniques to analyse the statistical properties of nonstationary time series. The key components of the package are (a) two approaches for the estimation of the evolutionary wavelet spectrum in the presence of trend; and (b) wavelet-based trend estimation in the presence of locally stationary wavelet errors via both linear and nonlinear wavelet thresholding; and (c) the calculation of associated pointwise confidence intervals. Lastly, the package directly implements boundary handling methods that enable the methods to be performed on data of arbitrary length, not just dyadic length as is common for wavelet-based methods, ensuring no pre-processing of data is necessary. The key functionality of the package is demonstrated through two data examples, arising from biology and activity monitoring.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05012&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Euan T. McGonigle, Rebecca Killick, Matthew A. Nunes</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">The TrendLSW R package has been developed to provide users with a suite of wavelet-based techniques to analyse the statistical properties of nonstationary time series. The key components of the package are (a) two approaches for the estimation of the evolutionary wavelet spectrum in the presence of trend; and (b) wavelet-based trend estimation in the presence of locally stationary wavelet errors via both linear and nonlinear wavelet thresholding; and (c) the calculation of associated pointwise confidence intervals. Lastly, the package directly implements boundary handling methods that enable the methods to be performed on data of arbitrary length, not just dyadic length as is common for wavelet-based methods, ensuring no pre-processing of data is necessary. The key functionality of the package is demonstrated through two data examples, arising from biology and activity monitoring.</summary></entry><entry><title type="html">Unguided structure learning of DAGs for count data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/UnguidedstructurelearningofDAGsforcountdata.html" rel="alternate" type="text/html" title="Unguided structure learning of DAGs for count data" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/UnguidedstructurelearningofDAGsforcountdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/UnguidedstructurelearningofDAGsforcountdata.html">&lt;p&gt;Mainly motivated by the problem of modelling directional dependence relationships for multivariate count data in high-dimensional settings, we present a new algorithm, called learnDAG, for learning the structure of directed acyclic graphs (DAGs). In particular, the proposed algorithm tackled the problem of learning DAGs from observational data in two main steps: (i) estimation of candidate parent sets; and (ii) feature selection. We experimentally compare learnDAG to several popular competitors in recovering the true structure of the graphs in situations where relatively moderate sample sizes are available. Furthermore, to make our algorithm is stronger, a validation of the algorithm is presented through the analysis of real datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04994&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thi Kim Hue Nguyen, Monica Chiogna, Davide Risso</name></author><category term="stat.ME" /><summary type="html">Mainly motivated by the problem of modelling directional dependence relationships for multivariate count data in high-dimensional settings, we present a new algorithm, called learnDAG, for learning the structure of directed acyclic graphs (DAGs). In particular, the proposed algorithm tackled the problem of learning DAGs from observational data in two main steps: (i) estimation of candidate parent sets; and (ii) feature selection. We experimentally compare learnDAG to several popular competitors in recovering the true structure of the graphs in situations where relatively moderate sample sizes are available. Furthermore, to make our algorithm is stronger, a validation of the algorithm is presented through the analysis of real datasets.</summary></entry><entry><title type="html">Uplift Modeling Under Limited Supervision</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/UpliftModelingUnderLimitedSupervision.html" rel="alternate" type="text/html" title="Uplift Modeling Under Limited Supervision" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/UpliftModelingUnderLimitedSupervision</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/UpliftModelingUnderLimitedSupervision.html">&lt;p&gt;Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of the training set in settings with extremely low experimental budget. The framework is flexible since each step can be used separately with other models or treatment policies. The experiments on real large-scale networks indicate a clear advantage of our methodology over the state of the art, which in many cases performs close to random, underlining the need for models that can generalize with limited supervision to reduce experimental risks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.19289&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>George Panagopoulos, Daniele Malitesta, Fragkiskos D. Malliaros, Jun Pang</name></author><category term="stat.ME" /><summary type="html">Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of the training set in settings with extremely low experimental budget. The framework is flexible since each step can be used separately with other models or treatment policies. The experiments on real large-scale networks indicate a clear advantage of our methodology over the state of the art, which in many cases performs close to random, underlining the need for models that can generalize with limited supervision to reduce experimental risks.</summary></entry><entry><title type="html">Variational Inference for Uncertainty Quantification: an Analysis of Trade-offs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/VariationalInferenceforUncertaintyQuantificationanAnalysisofTradeoffs.html" rel="alternate" type="text/html" title="Variational Inference for Uncertainty Quantification: an Analysis of Trade-offs" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/VariationalInferenceforUncertaintyQuantificationanAnalysisofTradeoffs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/VariationalInferenceforUncertaintyQuantificationanAnalysisofTradeoffs.html">&lt;p&gt;Given an intractable distribution $p$, the problem of variational inference (VI) is to find the best approximation from some more tractable family $Q$. Commonly, one chooses $Q$ to be a family of factorized distributions (i.e., the mean-field assumption), even though~$p$ itself does not factorize. We show that this mismatch leads to an impossibility theorem: if $p$ does not factorize, then any factorized approximation $q\in Q$ can correctly estimate at most one of the following three measures of uncertainty: (i) the marginal variances, (ii) the marginal precisions, or (iii) the generalized variance (which can be related to the entropy). In practice, the best variational approximation in $Q$ is found by minimizing some divergence $D(q,p)$ between distributions, and so we ask: how does the choice of divergence determine which measure of uncertainty, if any, is correctly estimated by VI? We consider the classic Kullback-Leibler divergences, the more general R&apos;enyi divergences, and a score-based divergence which compares $\nabla \log p$ and $\nabla \log q$. We provide a thorough theoretical analysis in the setting where $p$ is a Gaussian and $q$ is a (factorized) Gaussian. We show that all the considered divergences can be \textit{ordered} based on the estimates of uncertainty they yield as objective functions for~VI. Finally, we empirically evaluate the validity of this ordering when the target distribution $p$ is not Gaussian.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.13748&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Charles C. Margossian, Loucas Pillaud-Vivien, Lawrence K. Saul</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Given an intractable distribution $p$, the problem of variational inference (VI) is to find the best approximation from some more tractable family $Q$. Commonly, one chooses $Q$ to be a family of factorized distributions (i.e., the mean-field assumption), even though~$p$ itself does not factorize. We show that this mismatch leads to an impossibility theorem: if $p$ does not factorize, then any factorized approximation $q\in Q$ can correctly estimate at most one of the following three measures of uncertainty: (i) the marginal variances, (ii) the marginal precisions, or (iii) the generalized variance (which can be related to the entropy). In practice, the best variational approximation in $Q$ is found by minimizing some divergence $D(q,p)$ between distributions, and so we ask: how does the choice of divergence determine which measure of uncertainty, if any, is correctly estimated by VI? We consider the classic Kullback-Leibler divergences, the more general R&apos;enyi divergences, and a score-based divergence which compares $\nabla \log p$ and $\nabla \log q$. We provide a thorough theoretical analysis in the setting where $p$ is a Gaussian and $q$ is a (factorized) Gaussian. We show that all the considered divergences can be \textit{ordered} based on the estimates of uncertainty they yield as objective functions for~VI. Finally, we empirically evaluate the validity of this ordering when the target distribution $p$ is not Gaussian.</summary></entry><entry><title type="html">When Swarm Learning meets energy series data: A decentralized collaborative learning design based on blockchain</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/WhenSwarmLearningmeetsenergyseriesdataAdecentralizedcollaborativelearningdesignbasedonblockchain.html" rel="alternate" type="text/html" title="When Swarm Learning meets energy series data: A decentralized collaborative learning design based on blockchain" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/WhenSwarmLearningmeetsenergyseriesdataAdecentralizedcollaborativelearningdesignbasedonblockchain</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/WhenSwarmLearningmeetsenergyseriesdataAdecentralizedcollaborativelearningdesignbasedonblockchain.html">&lt;p&gt;Machine learning models offer the capability to forecast future energy production or consumption and infer essential unknown variables from existing data. However, legal and policy constraints within specific energy sectors render the data sensitive, presenting technical hurdles in utilizing data from diverse sources. Therefore, we propose adopting a Swarm Learning (SL) scheme, which replaces the centralized server with a blockchain-based distributed network to address the security and privacy issues inherent in Federated Learning (FL)’s centralized architecture. Within this distributed Collaborative Learning framework, each participating organization governs nodes for inter-organizational communication. Devices from various organizations utilize smart contracts for parameter uploading and retrieval. Consensus mechanism ensures distributed consistency throughout the learning process, guarantees the transparent trustworthiness and immutability of parameters on-chain. The efficacy of the proposed framework is substantiated across three real-world energy series modeling scenarios with superior performance compared to Local Learning approaches, simultaneously emphasizing enhanced data security and privacy over Centralized Learning and FL method. Notably, as the number of data volume and the count of local epochs increases within a threshold, there is an improvement in model performance accompanied by a reduction in the variance of performance errors. Consequently, this leads to an increased stability and reliability in the outcomes produced by the model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04743&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lei Xu, Yulong Chen, Yuntian Chen, Longfeng Nie, Xuetao Wei, Liang Xue, Dongxiao Zhang</name></author><category term="stat.AP" /><summary type="html">Machine learning models offer the capability to forecast future energy production or consumption and infer essential unknown variables from existing data. However, legal and policy constraints within specific energy sectors render the data sensitive, presenting technical hurdles in utilizing data from diverse sources. Therefore, we propose adopting a Swarm Learning (SL) scheme, which replaces the centralized server with a blockchain-based distributed network to address the security and privacy issues inherent in Federated Learning (FL)’s centralized architecture. Within this distributed Collaborative Learning framework, each participating organization governs nodes for inter-organizational communication. Devices from various organizations utilize smart contracts for parameter uploading and retrieval. Consensus mechanism ensures distributed consistency throughout the learning process, guarantees the transparent trustworthiness and immutability of parameters on-chain. The efficacy of the proposed framework is substantiated across three real-world energy series modeling scenarios with superior performance compared to Local Learning approaches, simultaneously emphasizing enhanced data security and privacy over Centralized Learning and FL method. Notably, as the number of data volume and the count of local epochs increases within a threshold, there is an improvement in model performance accompanied by a reduction in the variance of performance errors. Consequently, this leads to an increased stability and reliability in the outcomes produced by the model.</summary></entry><entry><title type="html">W-kernel and essential subspace for frequentist evaluation of Bayesian estimators</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/WkernelandessentialsubspaceforfrequentistevaluationofBayesianestimators.html" rel="alternate" type="text/html" title="W-kernel and essential subspace for frequentist evaluation of Bayesian estimators" /><published>2024-06-10T00:00:00+00:00</published><updated>2024-06-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/WkernelandessentialsubspaceforfrequentistevaluationofBayesianestimators</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/10/WkernelandessentialsubspaceforfrequentistevaluationofBayesianestimators.html">&lt;p&gt;The posterior covariance matrix W defined by the log-likelihood of each observation plays important roles both in the sensitivity analysis and frequentist evaluation of the Bayesian estimators. This study is focused on the matrix W and its principal space; we term the latter as an essential subspace. Projections to the essential subspace realize dimensional reduction in the sensitivity analysis and frequentist evaluation. A key tool for treating frequentist properties is the recently proposed Bayesian infinitesimal jackknife approximation(Giordano and Broderick (2023)). The matrix W can be interpreted as a reproducing kernel and is denoted as W-kernel. Using W-kernel, the essential subspace is expressed as a principal space given by the kernel principal component analysis. A relation to the Fisher kernel and neural tangent kernel is established, which elucidates the connection to the classical asymptotic theory. We also discuss a type of Bayesian-frequentist duality, naturally appeared from the kernel framework. Two applications are discussed: the selection of a representative set of observations and dimensional reduction in the approximate bootstrap. In the former, incomplete Cholesky decomposition is introduced as an efficient method for computing the essential subspace. In the latter, different implementations of the approximate bootstrap for posterior means are compared.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.13017&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yukito Iba</name></author><category term="stat.ME," /><category term="cond-mat.stat-mech," /><category term="stat.ML" /><summary type="html">The posterior covariance matrix W defined by the log-likelihood of each observation plays important roles both in the sensitivity analysis and frequentist evaluation of the Bayesian estimators. This study is focused on the matrix W and its principal space; we term the latter as an essential subspace. Projections to the essential subspace realize dimensional reduction in the sensitivity analysis and frequentist evaluation. A key tool for treating frequentist properties is the recently proposed Bayesian infinitesimal jackknife approximation(Giordano and Broderick (2023)). The matrix W can be interpreted as a reproducing kernel and is denoted as W-kernel. Using W-kernel, the essential subspace is expressed as a principal space given by the kernel principal component analysis. A relation to the Fisher kernel and neural tangent kernel is established, which elucidates the connection to the classical asymptotic theory. We also discuss a type of Bayesian-frequentist duality, naturally appeared from the kernel framework. Two applications are discussed: the selection of a representative set of observations and dimensional reduction in the approximate bootstrap. In the former, incomplete Cholesky decomposition is introduced as an efficient method for computing the essential subspace. In the latter, different implementations of the approximate bootstrap for posterior means are compared.</summary></entry></feed>