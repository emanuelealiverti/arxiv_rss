<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-29T07:14:06+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ABayesianApproachtoOnlineLearningforContextualRestlessBanditswithApplicationstoPublicHealth.html" rel="alternate" type="text/html" title="A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ABayesianApproachtoOnlineLearningforContextualRestlessBanditswithApplicationstoPublicHealth</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ABayesianApproachtoOnlineLearningforContextualRestlessBanditswithApplicationstoPublicHealth.html">&lt;p&gt;Public health programs often provide interventions to encourage beneficiary adherence,and effectively allocating interventions is vital for producing the greatest overall health outcomes. Such resource allocation problems are often modeled as restless multi-armed bandits (RMABs) with unknown underlying transition dynamics, hence requiring online reinforcement learning (RL). We present Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs that novelly combines techniques in Bayesian modeling with Thompson sampling to flexibly model the complex RMAB settings present in public health program adherence problems, such as context and non-stationarity. BCoR’s key strength is the ability to leverage shared information within and between arms to learn the unknown RMAB transition dynamics quickly in intervention-scarce settings with relatively short time horizons, which is common in public health applications. Empirically, BCoR achieves substantially higher finite-sample performance over a range of experimental settings, including an example based on real-world adherence data that was developed in collaboration with ARMMAN, an NGO in India which runs a large-scale maternal health program, showcasing BCoR practical utility and potential for real-world deployment.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.04933&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Biyonka Liang, Lily Xu, Aparna Taneja, Milind Tambe, Lucas Janson</name></author><category term="stat.AP" /><summary type="html">Public health programs often provide interventions to encourage beneficiary adherence,and effectively allocating interventions is vital for producing the greatest overall health outcomes. Such resource allocation problems are often modeled as restless multi-armed bandits (RMABs) with unknown underlying transition dynamics, hence requiring online reinforcement learning (RL). We present Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs that novelly combines techniques in Bayesian modeling with Thompson sampling to flexibly model the complex RMAB settings present in public health program adherence problems, such as context and non-stationarity. BCoR’s key strength is the ability to leverage shared information within and between arms to learn the unknown RMAB transition dynamics quickly in intervention-scarce settings with relatively short time horizons, which is common in public health applications. Empirically, BCoR achieves substantially higher finite-sample performance over a range of experimental settings, including an example based on real-world adherence data that was developed in collaboration with ARMMAN, an NGO in India which runs a large-scale maternal health program, showcasing BCoR practical utility and potential for real-world deployment.</summary></entry><entry><title type="html">A Note on the Prediction-Powered Bootstrap</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ANoteonthePredictionPoweredBootstrap.html" rel="alternate" type="text/html" title="A Note on the Prediction-Powered Bootstrap" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ANoteonthePredictionPoweredBootstrap</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ANoteonthePredictionPoweredBootstrap.html">&lt;p&gt;We introduce PPBoot: a bootstrap-based method for prediction-powered inference. PPBoot is applicable to arbitrary estimation problems and is very simple to implement, essentially only requiring one application of the bootstrap. Through a series of examples, we demonstrate that PPBoot often performs nearly identically to (and sometimes better than) the earlier PPI(++) method based on asymptotic normality$\unicode{x2013}$when the latter is applicable$\unicode{x2013}$without requiring any asymptotic characterizations. Given its versatility, PPBoot could simplify and expand the scope of application of prediction-powered inference to problems where central limit theorems are hard to prove.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18379&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tijana Zrnic</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">We introduce PPBoot: a bootstrap-based method for prediction-powered inference. PPBoot is applicable to arbitrary estimation problems and is very simple to implement, essentially only requiring one application of the bootstrap. Through a series of examples, we demonstrate that PPBoot often performs nearly identically to (and sometimes better than) the earlier PPI(++) method based on asymptotic normality$\unicode{x2013}$when the latter is applicable$\unicode{x2013}$without requiring any asymptotic characterizations. Given its versatility, PPBoot could simplify and expand the scope of application of prediction-powered inference to problems where central limit theorems are hard to prove.</summary></entry><entry><title type="html">Acquiring Better Load Estimates by Combining Anomaly and Change-point Detection in Power Grid Time-series Measurements</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/AcquiringBetterLoadEstimatesbyCombiningAnomalyandChangepointDetectioninPowerGridTimeseriesMeasurements.html" rel="alternate" type="text/html" title="Acquiring Better Load Estimates by Combining Anomaly and Change-point Detection in Power Grid Time-series Measurements" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/AcquiringBetterLoadEstimatesbyCombiningAnomalyandChangepointDetectioninPowerGridTimeseriesMeasurements</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/AcquiringBetterLoadEstimatesbyCombiningAnomalyandChangepointDetectioninPowerGridTimeseriesMeasurements.html">&lt;p&gt;In this paper we present novel methodology for automatic anomaly and switch event filtering to improve load estimation in power grid systems. By leveraging unsupervised methods with supervised optimization, our approach prioritizes interpretability while ensuring robust and generalizable performance on unseen data. Through experimentation, a combination of binary segmentation for change point detection and statistical process control for anomaly detection emerges as the most effective strategy, specifically when ensembled in a novel sequential manner. Results indicate the clear wasted potential when filtering is not applied. The automatic load estimation is also fairly accurate, with approximately 90% of estimates falling within a 10% error margin, with only a single significant failure in both the minimum and maximum load estimates across 60 measurements in the test set. Our methodology’s interpretability makes it particularly suitable for critical infrastructure planning, thereby enhancing decision-making processes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16164&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roel Bouman, Linda Schmeitz, Luco Buise, Jacco Heres, Yuliya Shapovalova, Tom Heskes</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">In this paper we present novel methodology for automatic anomaly and switch event filtering to improve load estimation in power grid systems. By leveraging unsupervised methods with supervised optimization, our approach prioritizes interpretability while ensuring robust and generalizable performance on unseen data. Through experimentation, a combination of binary segmentation for change point detection and statistical process control for anomaly detection emerges as the most effective strategy, specifically when ensembled in a novel sequential manner. Results indicate the clear wasted potential when filtering is not applied. The automatic load estimation is also fairly accurate, with approximately 90% of estimates falling within a 10% error margin, with only a single significant failure in both the minimum and maximum load estimates across 60 measurements in the test set. Our methodology’s interpretability makes it particularly suitable for critical infrastructure planning, thereby enhancing decision-making processes.</summary></entry><entry><title type="html">Aligning the Western Balkans power sectors with the European Green Deal</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/AligningtheWesternBalkanspowersectorswiththeEuropeanGreenDeal.html" rel="alternate" type="text/html" title="Aligning the Western Balkans power sectors with the European Green Deal" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/AligningtheWesternBalkanspowersectorswiththeEuropeanGreenDeal</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/AligningtheWesternBalkanspowersectorswiththeEuropeanGreenDeal.html">&lt;p&gt;Located in Southern Europe, the Drina River Basin is shared between Bosnia and Herzegovina, Montenegro, and Serbia. The power sectors of the three countries have an exceptionally high dependence on coal for power generation. In this paper, we analyse different development pathways for achieving climate neutrality in these countries and explore the potential of variable renewable energy (VRE) and its role in power sector decarbonization. We investigate whether hydro and non-hydro renewables can enable a net-zero transition by 2050 and how VRE might affect the hydropower cascade shared by the three countries. The Open-Source Energy Modelling System (OSeMOSYS) was used to develop a model representation of the countries’ power sectors. Findings show that the renewable potential of the countries is a significant 94.4 GW. This potential is 68% higher than previous assessments have shown. Under an Emission Limit scenario assuming net zero by 2050, 17% of this VRE potential is utilized to support the decarbonization of the power sectors. Additional findings show a limited impact of VRE technologies on total power generation output from the hydropower cascade. However, increased solar deployment shifts the operation of the cascade to increased short-term balancing, moving from baseload to more responsive power generation patterns. Prolonged use of thermal power plants is observed under scenarios assuming high wholesale electricity prices, leading to increased emissions. Results from scenarios with low cost of electricity trade suggest power sector developments that lead to decreased energy security.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.07433&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emir Fejzić, Taco Niet, Cameron Wade, Will Usher</name></author><category term="stat.AP" /><summary type="html">Located in Southern Europe, the Drina River Basin is shared between Bosnia and Herzegovina, Montenegro, and Serbia. The power sectors of the three countries have an exceptionally high dependence on coal for power generation. In this paper, we analyse different development pathways for achieving climate neutrality in these countries and explore the potential of variable renewable energy (VRE) and its role in power sector decarbonization. We investigate whether hydro and non-hydro renewables can enable a net-zero transition by 2050 and how VRE might affect the hydropower cascade shared by the three countries. The Open-Source Energy Modelling System (OSeMOSYS) was used to develop a model representation of the countries’ power sectors. Findings show that the renewable potential of the countries is a significant 94.4 GW. This potential is 68% higher than previous assessments have shown. Under an Emission Limit scenario assuming net zero by 2050, 17% of this VRE potential is utilized to support the decarbonization of the power sectors. Additional findings show a limited impact of VRE technologies on total power generation output from the hydropower cascade. However, increased solar deployment shifts the operation of the cascade to increased short-term balancing, moving from baseload to more responsive power generation patterns. Prolonged use of thermal power plants is observed under scenarios assuming high wholesale electricity prices, leading to increased emissions. Results from scenarios with low cost of electricity trade suggest power sector developments that lead to decreased energy security.</summary></entry><entry><title type="html">Augmented Risk Prediction for the Onset of Alzheimer’s Disease from Electronic Health Records with Large Language Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/AugmentedRiskPredictionfortheOnsetofAlzheimersDiseasefromElectronicHealthRecordswithLargeLanguageModels.html" rel="alternate" type="text/html" title="Augmented Risk Prediction for the Onset of Alzheimer’s Disease from Electronic Health Records with Large Language Models" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/AugmentedRiskPredictionfortheOnsetofAlzheimersDiseasefromElectronicHealthRecordswithLargeLanguageModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/AugmentedRiskPredictionfortheOnsetofAlzheimersDiseasefromElectronicHealthRecordswithLargeLanguageModels.html">&lt;p&gt;Alzheimer’s disease (AD) is the fifth-leading cause of death among Americans aged 65 and older. Screening and early detection of AD and related dementias (ADRD) are critical for timely intervention and for identifying clinical trial participants. The widespread adoption of electronic health records (EHRs) offers an important resource for developing ADRD screening tools such as machine learning based predictive models. Recent advancements in large language models (LLMs) demonstrate their unprecedented capability of encoding knowledge and performing reasoning, which offers them strong potential for enhancing risk prediction. This paper proposes a novel pipeline that augments risk prediction by leveraging the few-shot inference power of LLMs to make predictions on cases where traditional supervised learning methods (SLs) may not excel. Specifically, we develop a collaborative pipeline that combines SLs and LLMs via a confidence-driven decision-making mechanism, leveraging the strengths of SLs in clear-cut cases and LLMs in more complex scenarios. We evaluate this pipeline using a real-world EHR data warehouse from Oregon Health \&amp;amp; Science University (OHSU) Hospital, encompassing EHRs from over 2.5 million patients and more than 20 million patient encounters. Our results show that our proposed approach effectively combines the power of SLs and LLMs, offering significant improvements in predictive performance. This advancement holds promise for revolutionizing ADRD screening and early detection practices, with potential implications for better strategies of patient management and thus improving healthcare.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16413&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiankun Wang, Sumyeong Ahn, Taykhoom Dalal, Xiaodan Zhang, Weishen Pan, Qiannan Zhang, Bin Chen, Hiroko H. Dodge, Fei Wang, Jiayu Zhou</name></author><category term="stat.AP" /><summary type="html">Alzheimer’s disease (AD) is the fifth-leading cause of death among Americans aged 65 and older. Screening and early detection of AD and related dementias (ADRD) are critical for timely intervention and for identifying clinical trial participants. The widespread adoption of electronic health records (EHRs) offers an important resource for developing ADRD screening tools such as machine learning based predictive models. Recent advancements in large language models (LLMs) demonstrate their unprecedented capability of encoding knowledge and performing reasoning, which offers them strong potential for enhancing risk prediction. This paper proposes a novel pipeline that augments risk prediction by leveraging the few-shot inference power of LLMs to make predictions on cases where traditional supervised learning methods (SLs) may not excel. Specifically, we develop a collaborative pipeline that combines SLs and LLMs via a confidence-driven decision-making mechanism, leveraging the strengths of SLs in clear-cut cases and LLMs in more complex scenarios. We evaluate this pipeline using a real-world EHR data warehouse from Oregon Health \&amp;amp; Science University (OHSU) Hospital, encompassing EHRs from over 2.5 million patients and more than 20 million patient encounters. Our results show that our proposed approach effectively combines the power of SLs and LLMs, offering significant improvements in predictive performance. This advancement holds promise for revolutionizing ADRD screening and early detection practices, with potential implications for better strategies of patient management and thus improving healthcare.</summary></entry><entry><title type="html">AutoEval Done Right: Using Synthetic Data for Model Evaluation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/AutoEvalDoneRightUsingSyntheticDataforModelEvaluation.html" rel="alternate" type="text/html" title="AutoEval Done Right: Using Synthetic Data for Model Evaluation" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/AutoEvalDoneRightUsingSyntheticDataforModelEvaluation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/AutoEvalDoneRightUsingSyntheticDataforModelEvaluation.html">&lt;p&gt;The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.07008&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Pierre Boyeau, Anastasios N. Angelopoulos, Nir Yosef, Jitendra Malik, Michael I. Jordan</name></author><category term="stat.ME" /><summary type="html">The evaluation of machine learning models using human-labeled validation data can be expensive and time-consuming. AI-labeled synthetic data can be used to decrease the number of human annotations required for this purpose in a process called autoevaluation. We suggest efficient and statistically principled algorithms for this purpose that improve sample efficiency while remaining unbiased. These algorithms increase the effective human-labeled sample size by up to 50% on experiments with GPT-4.</summary></entry><entry><title type="html">Bayesian Nonparametrics for Principal Stratification with Continuous Post-Treatment Variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/BayesianNonparametricsforPrincipalStratificationwithContinuousPostTreatmentVariables.html" rel="alternate" type="text/html" title="Bayesian Nonparametrics for Principal Stratification with Continuous Post-Treatment Variables" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/BayesianNonparametricsforPrincipalStratificationwithContinuousPostTreatmentVariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/BayesianNonparametricsforPrincipalStratificationwithContinuousPostTreatmentVariables.html">&lt;p&gt;Principal stratification provides a causal inference framework that allows adjustment for confounded post-treatment variables when comparing treatments. Although the literature has focused mainly on binary post-treatment variables, there is a growing interest in principal stratification involving continuous post-treatment variables. However, characterizing the latent principal strata with a continuous post-treatment presents a significant challenge, which is further complicated in observational studies where the treatment is not randomized. In this paper, we introduce the Confounders-Aware SHared atoms BAyesian mixture (CASBAH), a novel approach for principal stratification with continuous post-treatment variables that can be directly applied to observational studies. CASBAH leverages a dependent Dirichlet process, utilizing shared atoms across treatment levels, to effectively control for measured confounders and facilitate information sharing between treatment groups in the identification of principal strata membership. CASBAH also offers a comprehensive quantification of uncertainty surrounding the membership of the principal strata. Through Monte Carlo simulations, we show that the proposed methodology has excellent performance in characterizing the latent principal strata and estimating the effects of treatment on post-treatment variables and outcomes. Finally, CASBAH is applied to a case study in which we estimate the causal effects of US national air quality regulations on pollution levels and health outcomes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17669&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Dafne Zorzetto, Antonio Canale, Fabrizia Mealli, Francesca Dominici, Falco J. Bargagli-Stoffi</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Principal stratification provides a causal inference framework that allows adjustment for confounded post-treatment variables when comparing treatments. Although the literature has focused mainly on binary post-treatment variables, there is a growing interest in principal stratification involving continuous post-treatment variables. However, characterizing the latent principal strata with a continuous post-treatment presents a significant challenge, which is further complicated in observational studies where the treatment is not randomized. In this paper, we introduce the Confounders-Aware SHared atoms BAyesian mixture (CASBAH), a novel approach for principal stratification with continuous post-treatment variables that can be directly applied to observational studies. CASBAH leverages a dependent Dirichlet process, utilizing shared atoms across treatment levels, to effectively control for measured confounders and facilitate information sharing between treatment groups in the identification of principal strata membership. CASBAH also offers a comprehensive quantification of uncertainty surrounding the membership of the principal strata. Through Monte Carlo simulations, we show that the proposed methodology has excellent performance in characterizing the latent principal strata and estimating the effects of treatment on post-treatment variables and outcomes. Finally, CASBAH is applied to a case study in which we estimate the causal effects of US national air quality regulations on pollution levels and health outcomes.</summary></entry><entry><title type="html">Bayesian material flow analysis for systems with multiple levels of disaggregation and high dimensional data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Bayesianmaterialflowanalysisforsystemswithmultiplelevelsofdisaggregationandhighdimensionaldata.html" rel="alternate" type="text/html" title="Bayesian material flow analysis for systems with multiple levels of disaggregation and high dimensional data" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Bayesianmaterialflowanalysisforsystemswithmultiplelevelsofdisaggregationandhighdimensionaldata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Bayesianmaterialflowanalysisforsystemswithmultiplelevelsofdisaggregationandhighdimensionaldata.html">&lt;p&gt;Material Flow Analysis (MFA) is used to quantify and understand the life cycles of materials from production to end of use, which enables environmental, social and economic impacts and interventions. MFA is challenging as available data is often limited and uncertain, leading to an underdetermined system with an infinite number of possible stocks and flows values. Bayesian statistics is an effective way to address these challenges by principally incorporating domain knowledge, and quantifying uncertainty in the data and providing probabilities associated with model solutions.
  This paper presents a novel MFA methodology under the Bayesian framework. By relaxing the mass balance constraints, we improve the computational scalability and reliability of the posterior samples compared to existing Bayesian MFA methods. We propose a mass based, child and parent process framework to model systems with disaggregated processes and flows. We show posterior predictive checks can be used to identify inconsistencies in the data and aid noise and hyperparameter selection. The proposed approach is demonstrated on case studies, including a global aluminium cycle with significant disaggregation, under weakly informative priors and significant data gaps to investigate the feasibility of Bayesian MFA. We illustrate just a weakly informative prior can greatly improve the performance of Bayesian methods, for both estimation accuracy and uncertainty quantification.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.06178&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Junyang Wang, Kolyan Ray, Pablo Brito-Parada, Yves Plancherel, Tom Bide, Joseph Mankelow, John Morley, Julia Stegemann, Rupert Myers</name></author><category term="stat.AP" /><summary type="html">Material Flow Analysis (MFA) is used to quantify and understand the life cycles of materials from production to end of use, which enables environmental, social and economic impacts and interventions. MFA is challenging as available data is often limited and uncertain, leading to an underdetermined system with an infinite number of possible stocks and flows values. Bayesian statistics is an effective way to address these challenges by principally incorporating domain knowledge, and quantifying uncertainty in the data and providing probabilities associated with model solutions. This paper presents a novel MFA methodology under the Bayesian framework. By relaxing the mass balance constraints, we improve the computational scalability and reliability of the posterior samples compared to existing Bayesian MFA methods. We propose a mass based, child and parent process framework to model systems with disaggregated processes and flows. We show posterior predictive checks can be used to identify inconsistencies in the data and aid noise and hyperparameter selection. The proposed approach is demonstrated on case studies, including a global aluminium cycle with significant disaggregation, under weakly informative priors and significant data gaps to investigate the feasibility of Bayesian MFA. We illustrate just a weakly informative prior can greatly improve the performance of Bayesian methods, for both estimation accuracy and uncertainty quantification.</summary></entry><entry><title type="html">Bayesian sample size determination using robust commensurate priors with interpretable discrepancy weights</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Bayesiansamplesizedeterminationusingrobustcommensuratepriorswithinterpretablediscrepancyweights.html" rel="alternate" type="text/html" title="Bayesian sample size determination using robust commensurate priors with interpretable discrepancy weights" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Bayesiansamplesizedeterminationusingrobustcommensuratepriorswithinterpretablediscrepancyweights</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Bayesiansamplesizedeterminationusingrobustcommensuratepriorswithinterpretablediscrepancyweights.html">&lt;p&gt;Randomized controlled clinical trials provide the gold standard for evidence generation in relation to the efficacy of a new treatment in medical research. Relevant information from previous studies may be desirable to incorporate in the design and analysis of a new trial, with the Bayesian paradigm providing a coherent framework to formally incorporate prior knowledge. Many established methods involve the use of a discounting factor, sometimes related to a measure of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;similarity&apos; between historical and the new trials. However, it is often the case that the sample size is highly nonlinear in those discounting factors. This hinders communication with subject-matter experts to elicit sensible values for borrowing strength at the trial design stage. Focusing on a commensurate predictive prior method that can incorporate historical data from multiple sources, we highlight a particular issue of nonmonotonicity and explain why this causes issues with interpretability of the discounting factors (hereafter referred to as &lt;/code&gt;weights’). We propose a solution for this, from which an analytical sample size formula is derived. We then propose a linearization technique such that the sample size changes uniformly over the weights. Our approach leads to interpretable weights that represent the probability that historical data are (ir)relevant to the new trial, and could therefore facilitate easier elicitation of expert opinion on their values.
  Keywords: Bayesian sample size determination; Commensurate priors; Historical borrowing; Prior aggregation; Uniform shrinkage.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.10592&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lou E. Whitehead, James M. S. Wason, Oliver Sailer, Haiyan Zheng</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Randomized controlled clinical trials provide the gold standard for evidence generation in relation to the efficacy of a new treatment in medical research. Relevant information from previous studies may be desirable to incorporate in the design and analysis of a new trial, with the Bayesian paradigm providing a coherent framework to formally incorporate prior knowledge. Many established methods involve the use of a discounting factor, sometimes related to a measure of similarity&apos; between historical and the new trials. However, it is often the case that the sample size is highly nonlinear in those discounting factors. This hinders communication with subject-matter experts to elicit sensible values for borrowing strength at the trial design stage. Focusing on a commensurate predictive prior method that can incorporate historical data from multiple sources, we highlight a particular issue of nonmonotonicity and explain why this causes issues with interpretability of the discounting factors (hereafter referred to as weights’). We propose a solution for this, from which an analytical sample size formula is derived. We then propose a linearization technique such that the sample size changes uniformly over the weights. Our approach leads to interpretable weights that represent the probability that historical data are (ir)relevant to the new trial, and could therefore facilitate easier elicitation of expert opinion on their values. Keywords: Bayesian sample size determination; Commensurate priors; Historical borrowing; Prior aggregation; Uniform shrinkage.</summary></entry><entry><title type="html">Chauhan Weighted Trajectory Analysis reduces sample size requirements and expedites time-to-efficacy signals in advanced cancer clinical trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ChauhanWeightedTrajectoryAnalysisreducessamplesizerequirementsandexpeditestimetoefficacysignalsinadvancedcancerclinicaltrials.html" rel="alternate" type="text/html" title="Chauhan Weighted Trajectory Analysis reduces sample size requirements and expedites time-to-efficacy signals in advanced cancer clinical trials" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ChauhanWeightedTrajectoryAnalysisreducessamplesizerequirementsandexpeditestimetoefficacysignalsinadvancedcancerclinicaltrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ChauhanWeightedTrajectoryAnalysisreducessamplesizerequirementsandexpeditestimetoefficacysignalsinadvancedcancerclinicaltrials.html">&lt;p&gt;As Kaplan-Meier (KM) analysis is limited to single unidirectional endpoints, most advanced cancer randomized clinical trials (RCTs) are powered for either progression free survival (PFS) or overall survival (OS). This discards efficacy information carried by partial responses, complete responses, and stable disease that frequently precede progressive disease and death. Chauhan Weighted Trajectory Analysis (CWTA) is a generalization of KM that simultaneously assesses multiple rank-ordered endpoints. We hypothesized that CWTA could use this efficacy information to reduce sample size requirements and expedite efficacy signals in advanced cancer trials. We performed 100-fold and 1000-fold simulations of solid tumour systemic therapy RCTs with health statuses rank ordered from complete response (Stage 0) to death (Stage 4). At increments of sample size and hazard ratio, we compared KM PFS and OS with CWTA for (i) sample size requirements to achieve a power of 0.8 and (ii) time-to-first significant efficacy signal. CWTA consistently demonstrated greater power, and reduced sample size requirements by 18% to 35% compared to KM PFS and 14% to 20% compared to KM OS. CWTA also expedited time-to-efficacy signals 2- to 6-fold. CWTA, by incorporating all efficacy signals in the cancer treatment trajectory, provides clinically relevant reduction in required sample size and meaningfully expedites the efficacy signals of cancer treatments compared to KM PFS and KM OS. Using CWTA rather than KM as the primary trial outcome has the potential to meaningfully reduce the numbers of patients, trial duration, and costs to evaluate therapies in advanced cancer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02529&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Utkarsh Chauhan, Daylen Mackey, John R. Mackey</name></author><category term="stat.ME" /><summary type="html">As Kaplan-Meier (KM) analysis is limited to single unidirectional endpoints, most advanced cancer randomized clinical trials (RCTs) are powered for either progression free survival (PFS) or overall survival (OS). This discards efficacy information carried by partial responses, complete responses, and stable disease that frequently precede progressive disease and death. Chauhan Weighted Trajectory Analysis (CWTA) is a generalization of KM that simultaneously assesses multiple rank-ordered endpoints. We hypothesized that CWTA could use this efficacy information to reduce sample size requirements and expedite efficacy signals in advanced cancer trials. We performed 100-fold and 1000-fold simulations of solid tumour systemic therapy RCTs with health statuses rank ordered from complete response (Stage 0) to death (Stage 4). At increments of sample size and hazard ratio, we compared KM PFS and OS with CWTA for (i) sample size requirements to achieve a power of 0.8 and (ii) time-to-first significant efficacy signal. CWTA consistently demonstrated greater power, and reduced sample size requirements by 18% to 35% compared to KM PFS and 14% to 20% compared to KM OS. CWTA also expedited time-to-efficacy signals 2- to 6-fold. CWTA, by incorporating all efficacy signals in the cancer treatment trajectory, provides clinically relevant reduction in required sample size and meaningfully expedites the efficacy signals of cancer treatments compared to KM PFS and KM OS. Using CWTA rather than KM as the primary trial outcome has the potential to meaningfully reduce the numbers of patients, trial duration, and costs to evaluate therapies in advanced cancer.</summary></entry><entry><title type="html">Combining Evidence Across Filtrations Using Adjusters</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/CombiningEvidenceAcrossFiltrationsUsingAdjusters.html" rel="alternate" type="text/html" title="Combining Evidence Across Filtrations Using Adjusters" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/CombiningEvidenceAcrossFiltrationsUsingAdjusters</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/CombiningEvidenceAcrossFiltrationsUsingAdjusters.html">&lt;p&gt;In anytime-valid sequential inference, it is known that any admissible procedure must be based on e-processes, which are composite generalizations of test martingales that quantify the accumulated evidence against a composite null hypothesis at any arbitrary stopping time. This paper studies methods for combining e-processes constructed using different information sets (filtrations) for the same null. Although e-processes constructed in the same filtration can be combined effortlessly (e.g., by averaging), e-processes constructed in different filtrations cannot, because their validity in a coarser filtration does not translate to validity in a finer filtration. This issue arises in exchangeability tests, independence tests, and tests for comparing forecasts with lags. We first establish that a class of functions called adjusters allows us to lift e-processes from a coarser filtration into any finer filtration. We then introduce a characterization theorem for adjusters, formalizing a sense in which using adjusters is necessary. There are two major implications. First, if we have a powerful e-process in a coarsened filtration, then we readily have a powerful e-process in the original filtration. Second, when we coarsen the filtration to construct an e-process, there is an asymptotically logarithmic cost of recovering anytime-validity in the original filtration.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.09698&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yo Joong Choe, Aaditya Ramdas</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">In anytime-valid sequential inference, it is known that any admissible procedure must be based on e-processes, which are composite generalizations of test martingales that quantify the accumulated evidence against a composite null hypothesis at any arbitrary stopping time. This paper studies methods for combining e-processes constructed using different information sets (filtrations) for the same null. Although e-processes constructed in the same filtration can be combined effortlessly (e.g., by averaging), e-processes constructed in different filtrations cannot, because their validity in a coarser filtration does not translate to validity in a finer filtration. This issue arises in exchangeability tests, independence tests, and tests for comparing forecasts with lags. We first establish that a class of functions called adjusters allows us to lift e-processes from a coarser filtration into any finer filtration. We then introduce a characterization theorem for adjusters, formalizing a sense in which using adjusters is necessary. There are two major implications. First, if we have a powerful e-process in a coarsened filtration, then we readily have a powerful e-process in the original filtration. Second, when we coarsen the filtration to construct an e-process, there is an asymptotically logarithmic cost of recovering anytime-validity in the original filtration.</summary></entry><entry><title type="html">Comparison of predictive values with paired samples</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Comparisonofpredictivevalueswithpairedsamples.html" rel="alternate" type="text/html" title="Comparison of predictive values with paired samples" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Comparisonofpredictivevalueswithpairedsamples</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Comparisonofpredictivevalueswithpairedsamples.html">&lt;p&gt;Positive predictive value and negative predictive value are two widely used parameters to assess the clinical usefulness of a medical diagnostic test. When there are two diagnostic tests, it is recommendable to make a comparative assessment of the values of these two parameters after applying the two tests to the same subjects (paired samples). The objective is then to make individual or global inferences about the difference or the ratio of the predictive value of the two diagnostic tests. These inferences are usually based on complex and not very intuitive expressions, some of which have subsequently been reformulated. We define the two properties of symmetry which any inference method must verify - symmetry in diagnoses and symmetry in the tests -, we propose new inference methods, and we define them with simple expressions. All of the methods are compared with each other, selecting the optimal method: (a) to obtain a confidence interval for the difference or ratio; (b) to perform an individual homogeneity test of the two predictive values; and (c) to carry out a global homogeneity test of the two predictive values.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17954&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Antonio Martín Andrés, Pedro Femia Marzo</name></author><category term="stat.ME" /><summary type="html">Positive predictive value and negative predictive value are two widely used parameters to assess the clinical usefulness of a medical diagnostic test. When there are two diagnostic tests, it is recommendable to make a comparative assessment of the values of these two parameters after applying the two tests to the same subjects (paired samples). The objective is then to make individual or global inferences about the difference or the ratio of the predictive value of the two diagnostic tests. These inferences are usually based on complex and not very intuitive expressions, some of which have subsequently been reformulated. We define the two properties of symmetry which any inference method must verify - symmetry in diagnoses and symmetry in the tests -, we propose new inference methods, and we define them with simple expressions. All of the methods are compared with each other, selecting the optimal method: (a) to obtain a confidence interval for the difference or ratio; (b) to perform an individual homogeneity test of the two predictive values; and (c) to carry out a global homogeneity test of the two predictive values.</summary></entry><entry><title type="html">Connecting the Dots: Is Mode-Connectedness the Key to Feasible Sample-Based Inference in Bayesian Neural Networks?</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ConnectingtheDotsIsModeConnectednesstheKeytoFeasibleSampleBasedInferenceinBayesianNeuralNetworks.html" rel="alternate" type="text/html" title="Connecting the Dots: Is Mode-Connectedness the Key to Feasible Sample-Based Inference in Bayesian Neural Networks?" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ConnectingtheDotsIsModeConnectednesstheKeytoFeasibleSampleBasedInferenceinBayesianNeuralNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ConnectingtheDotsIsModeConnectednesstheKeytoFeasibleSampleBasedInferenceinBayesianNeuralNetworks.html">&lt;p&gt;A major challenge in sample-based inference (SBI) for Bayesian neural networks is the size and structure of the networks’ parameter space. Our work shows that successful SBI is possible by embracing the characteristic relationship between weight and function space, uncovering a systematic link between overparameterization and the difficulty of the sampling problem. Through extensive experiments, we establish practical guidelines for sampling and convergence diagnosis. As a result, we present a deep ensemble initialized approach as an effective solution with competitive performance and uncertainty quantification.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.01484&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuel Sommer, Lisa Wimmer, Theodore Papamarkou, Ludwig Bothmann, Bernd Bischl, David Rügamer</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">A major challenge in sample-based inference (SBI) for Bayesian neural networks is the size and structure of the networks’ parameter space. Our work shows that successful SBI is possible by embracing the characteristic relationship between weight and function space, uncovering a systematic link between overparameterization and the difficulty of the sampling problem. Through extensive experiments, we establish practical guidelines for sampling and convergence diagnosis. As a result, we present a deep ensemble initialized approach as an effective solution with competitive performance and uncertainty quantification.</summary></entry><entry><title type="html">Convergence rates of particle approximation of forward-backward splitting algorithm for granular medium equations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Convergenceratesofparticleapproximationofforwardbackwardsplittingalgorithmforgranularmediumequations.html" rel="alternate" type="text/html" title="Convergence rates of particle approximation of forward-backward splitting algorithm for granular medium equations" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Convergenceratesofparticleapproximationofforwardbackwardsplittingalgorithmforgranularmediumequations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Convergenceratesofparticleapproximationofforwardbackwardsplittingalgorithmforgranularmediumequations.html">&lt;p&gt;We study the spatially homogeneous granular medium equation [\partial_t\mu=\rm{div}(\mu\nabla V)+\rm{div}(\mu(\nabla W \ast \mu))+\Delta\mu\,,] within a large and natural class of the confinement potentials $V$ and interaction potentials $W$. The considered problem do not need to assume that $\nabla V$ or $\nabla W$ are globally Lipschitz. With the aim of providing particle approximation of solutions, we design efficient forward-backward splitting algorithms. Sharp convergence rates in terms of the Wasserstein distance are provided.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18034&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matej Benko, Iwona Chlebicka, J{\o}rgen Endal, B{\l}ażej Miasojedow</name></author><category term="stat.CO" /><summary type="html">We study the spatially homogeneous granular medium equation [\partial_t\mu=\rm{div}(\mu\nabla V)+\rm{div}(\mu(\nabla W \ast \mu))+\Delta\mu\,,] within a large and natural class of the confinement potentials $V$ and interaction potentials $W$. The considered problem do not need to assume that $\nabla V$ or $\nabla W$ are globally Lipschitz. With the aim of providing particle approximation of solutions, we design efficient forward-backward splitting algorithms. Sharp convergence rates in terms of the Wasserstein distance are provided.</summary></entry><entry><title type="html">Efficient Prior Calibration From Indirect Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/EfficientPriorCalibrationFromIndirectData.html" rel="alternate" type="text/html" title="Efficient Prior Calibration From Indirect Data" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/EfficientPriorCalibrationFromIndirectData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/EfficientPriorCalibrationFromIndirectData.html">&lt;p&gt;Bayesian inversion is central to the quantification of uncertainty within problems arising from numerous applications in science and engineering. To formulate the approach, four ingredients are required: a forward model mapping the unknown parameter to an element of a solution space, often the solution space for a differential equation; an observation operator mapping an element of the solution space to the data space; a noise model describing how noise pollutes the observations; and a prior model describing knowledge about the unknown parameter before the data is acquired. This paper is concerned with learning the prior model from data; in particular, learning the prior from multiple realizations of indirect data obtained through the noisy observation process. The prior is represented, using a generative model, as the pushforward of a Gaussian in a latent space; the pushforward map is learned by minimizing an appropriate loss function. A metric that is well-defined under empirical approximation is used to define the loss function for the pushforward map to make an implementable methodology. Furthermore, an efficient residual-based neural operator approximation of the forward model is proposed and it is shown that this may be learned concurrently with the pushforward map, using a bilevel optimization formulation of the problem; this use of neural operator approximation has the potential to make prior learning from indirect data more computationally efficient, especially when the observation process is expensive, non-smooth or not known. The ideas are illustrated with the Darcy flow inverse problem of finding permeability from piezometric head measurements.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17955&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>O. Deniz Akyildiz, Mark Girolami, Andrew M. Stuart, Arnaud Vadeboncoeur</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Bayesian inversion is central to the quantification of uncertainty within problems arising from numerous applications in science and engineering. To formulate the approach, four ingredients are required: a forward model mapping the unknown parameter to an element of a solution space, often the solution space for a differential equation; an observation operator mapping an element of the solution space to the data space; a noise model describing how noise pollutes the observations; and a prior model describing knowledge about the unknown parameter before the data is acquired. This paper is concerned with learning the prior model from data; in particular, learning the prior from multiple realizations of indirect data obtained through the noisy observation process. The prior is represented, using a generative model, as the pushforward of a Gaussian in a latent space; the pushforward map is learned by minimizing an appropriate loss function. A metric that is well-defined under empirical approximation is used to define the loss function for the pushforward map to make an implementable methodology. Furthermore, an efficient residual-based neural operator approximation of the forward model is proposed and it is shown that this may be learned concurrently with the pushforward map, using a bilevel optimization formulation of the problem; this use of neural operator approximation has the potential to make prior learning from indirect data more computationally efficient, especially when the observation process is expensive, non-smooth or not known. The ideas are illustrated with the Darcy flow inverse problem of finding permeability from piezometric head measurements.</summary></entry><entry><title type="html">Estimating Conditional Distributions with Neural Networks using R package deeptrafo</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/EstimatingConditionalDistributionswithNeuralNetworksusingRpackagedeeptrafo.html" rel="alternate" type="text/html" title="Estimating Conditional Distributions with Neural Networks using R package deeptrafo" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/EstimatingConditionalDistributionswithNeuralNetworksusingRpackagedeeptrafo</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/EstimatingConditionalDistributionswithNeuralNetworksusingRpackagedeeptrafo.html">&lt;p&gt;Contemporary empirical applications frequently require flexible regression models for complex response types and large tabular or non-tabular, including image or text, data. Classical regression models either break down under the computational load of processing such data or require additional manual feature extraction to make these problems tractable. Here, we present deeptrafo, a package for fitting flexible regression models for conditional distributions using a tensorflow backend with numerous additional processors, such as neural networks, penalties, and smoothing splines. Package deeptrafo implements deep conditional transformation models (DCTMs) for binary, ordinal, count, survival, continuous, and time series responses, potentially with uninformative censoring. Unlike other available methods, DCTMs do not assume a parametric family of distributions for the response. Further, the data analyst may trade off interpretability and flexibility by supplying custom neural network architectures and smoothers for each term in an intuitive formula interface. We demonstrate how to set up, fit, and work with DCTMs for several response types. We further showcase how to construct ensembles of these models, evaluate models using inbuilt cross-validation, and use other convenience functions for DCTMs in several applications. Lastly, we discuss DCTMs in light of other approaches to regression with non-tabular data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.13665&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lucas Kook, Philipp FM Baumann, Oliver Dürr, Beate Sick, David Rügamer</name></author><category term="stat.CO" /><summary type="html">Contemporary empirical applications frequently require flexible regression models for complex response types and large tabular or non-tabular, including image or text, data. Classical regression models either break down under the computational load of processing such data or require additional manual feature extraction to make these problems tractable. Here, we present deeptrafo, a package for fitting flexible regression models for conditional distributions using a tensorflow backend with numerous additional processors, such as neural networks, penalties, and smoothing splines. Package deeptrafo implements deep conditional transformation models (DCTMs) for binary, ordinal, count, survival, continuous, and time series responses, potentially with uninformative censoring. Unlike other available methods, DCTMs do not assume a parametric family of distributions for the response. Further, the data analyst may trade off interpretability and flexibility by supplying custom neural network architectures and smoothers for each term in an intuitive formula interface. We demonstrate how to set up, fit, and work with DCTMs for several response types. We further showcase how to construct ensembles of these models, evaluate models using inbuilt cross-validation, and use other convenience functions for DCTMs in several applications. Lastly, we discuss DCTMs in light of other approaches to regression with non-tabular data.</summary></entry><entry><title type="html">Factor Augmented Matrix Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/FactorAugmentedMatrixRegression.html" rel="alternate" type="text/html" title="Factor Augmented Matrix Regression" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/FactorAugmentedMatrixRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/FactorAugmentedMatrixRegression.html">&lt;p&gt;We introduce \underline{F}actor-\underline{A}ugmented \underline{Ma}trix \underline{R}egression (FAMAR) to address the growing applications of matrix-variate data and their associated challenges, particularly with high-dimensionality and covariate correlations. FAMAR encompasses two key algorithms. The first is a novel non-iterative approach that efficiently estimates the factors and loadings of the matrix factor model, utilizing techniques of pre-training, diverse projection, and block-wise averaging. The second algorithm offers an accelerated solution for penalized matrix factor regression. Both algorithms are supported by established statistical and numerical convergence properties. Empirical evaluations, conducted on synthetic and real economics datasets, demonstrate FAMAR’s superiority in terms of accuracy, interpretability, and computational speed. Our application to economic data showcases how matrix factors can be incorporated to predict the GDPs of the countries of interest, and the influence of these factors on the GDPs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17744&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elynn Chen, Jianqing Fan, Xiaonan Zhu</name></author><category term="stat.ME" /><summary type="html">We introduce \underline{F}actor-\underline{A}ugmented \underline{Ma}trix \underline{R}egression (FAMAR) to address the growing applications of matrix-variate data and their associated challenges, particularly with high-dimensionality and covariate correlations. FAMAR encompasses two key algorithms. The first is a novel non-iterative approach that efficiently estimates the factors and loadings of the matrix factor model, utilizing techniques of pre-training, diverse projection, and block-wise averaging. The second algorithm offers an accelerated solution for penalized matrix factor regression. Both algorithms are supported by established statistical and numerical convergence properties. Empirical evaluations, conducted on synthetic and real economics datasets, demonstrate FAMAR’s superiority in terms of accuracy, interpretability, and computational speed. Our application to economic data showcases how matrix factors can be incorporated to predict the GDPs of the countries of interest, and the influence of these factors on the GDPs.</summary></entry><entry><title type="html">Finding Pareto Efficient Redistricting Plans with Short Bursts</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/FindingParetoEfficientRedistrictingPlanswithShortBursts.html" rel="alternate" type="text/html" title="Finding Pareto Efficient Redistricting Plans with Short Bursts" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/FindingParetoEfficientRedistrictingPlanswithShortBursts</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/FindingParetoEfficientRedistrictingPlanswithShortBursts.html">&lt;p&gt;Redistricting practitioners must balance many competing constraints and criteria when drawing district boundaries. To aid in this process, researchers have developed many methods for optimizing districting plans according to one or more criteria. This research note extends a recently-proposed single-criterion optimization method, short bursts (Cannon et al., 2023), to handle the multi-criterion case, and in doing so approximate the Pareto frontier for any set of constraints. We study the empirical performance of the method in a realistic setting and find it behaves as expected and is not very sensitive to algorithmic parameters. The proposed approach, which is implemented in open-source software, should allow researchers and practitioners to better understand the tradeoffs inherent to the redistricting process.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.00427&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Cory McCartan</name></author><category term="stat.AP" /><summary type="html">Redistricting practitioners must balance many competing constraints and criteria when drawing district boundaries. To aid in this process, researchers have developed many methods for optimizing districting plans according to one or more criteria. This research note extends a recently-proposed single-criterion optimization method, short bursts (Cannon et al., 2023), to handle the multi-criterion case, and in doing so approximate the Pareto frontier for any set of constraints. We study the empirical performance of the method in a realistic setting and find it behaves as expected and is not very sensitive to algorithmic parameters. The proposed approach, which is implemented in open-source software, should allow researchers and practitioners to better understand the tradeoffs inherent to the redistricting process.</summary></entry><entry><title type="html">Fisher’s Legacy of Directional Statistics, and Beyond to Statistics on Manifolds</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/FishersLegacyofDirectionalStatisticsandBeyondtoStatisticsonManifolds.html" rel="alternate" type="text/html" title="Fisher’s Legacy of Directional Statistics, and Beyond to Statistics on Manifolds" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/FishersLegacyofDirectionalStatisticsandBeyondtoStatisticsonManifolds</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/FishersLegacyofDirectionalStatisticsandBeyondtoStatisticsonManifolds.html">&lt;p&gt;It will not be an exaggeration to say that R A Fisher is the Albert Einstein of Statistics. He pioneered almost all the main branches of statistics, but it is not as well known that he opened the area of Directional Statistics with his 1953 paper introducing a distribution on the sphere which is now known as the Fisher distribution. He stressed that for spherical data one should take into account that the data is on a manifold. We will describe this Fisher distribution and reanalyse his geological data. We also comment on the two goals he set himself in that paper, and how he reinvented the von Mises distribution on the circle. Since then, many extensions of this distribution have appeared bearing Fisher’s name such as the von Mises Fisher distribution and the matrix Fisher distribution. In fact, the subject of Directional Statistics has grown tremendously in the last two decades with new applications emerging in Life Sciences, Image Analysis, Machine Learning and so on. We give a recent new method of constructing the Fisher type distribution which has been motivated by some problems in Machine Learning. The subject related to his distribution has evolved since then more broadly as Statistics on Manifolds which also includes the new field of Shape Analysis. We end with a historical note pointing out some correspondence between D’Arcy Thompson and R A Fisher related to Shape Analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17919&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kanti V. Mardia</name></author><category term="stat.ME" /><summary type="html">It will not be an exaggeration to say that R A Fisher is the Albert Einstein of Statistics. He pioneered almost all the main branches of statistics, but it is not as well known that he opened the area of Directional Statistics with his 1953 paper introducing a distribution on the sphere which is now known as the Fisher distribution. He stressed that for spherical data one should take into account that the data is on a manifold. We will describe this Fisher distribution and reanalyse his geological data. We also comment on the two goals he set himself in that paper, and how he reinvented the von Mises distribution on the circle. Since then, many extensions of this distribution have appeared bearing Fisher’s name such as the von Mises Fisher distribution and the matrix Fisher distribution. In fact, the subject of Directional Statistics has grown tremendously in the last two decades with new applications emerging in Life Sciences, Image Analysis, Machine Learning and so on. We give a recent new method of constructing the Fisher type distribution which has been motivated by some problems in Machine Learning. The subject related to his distribution has evolved since then more broadly as Statistics on Manifolds which also includes the new field of Shape Analysis. We end with a historical note pointing out some correspondence between D’Arcy Thompson and R A Fisher related to Shape Analysis.</summary></entry><entry><title type="html">Homophily-adjusted social influence estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Homophilyadjustedsocialinfluenceestimation.html" rel="alternate" type="text/html" title="Homophily-adjusted social influence estimation" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Homophilyadjustedsocialinfluenceestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Homophilyadjustedsocialinfluenceestimation.html">&lt;p&gt;Homophily and social influence are two key concepts of social network analysis. Distinguishing between these phenomena is difficult, and approaches to disambiguate the two have been primarily limited to longitudinal data analyses. In this study, we provide sufficient conditions for valid estimation of social influence through cross-sectional data, leading to a novel homophily-adjusted social influence model which addresses the backdoor pathway of latent homophilic features. The oft-used network autocorrelation model (NAM) is the special case of our proposed model with no latent homophily, suggesting that the NAM is only valid when all homophilic attributes are observed. We conducted an extensive simulation study to evaluate the performance of our proposed homophily-adjusted model, comparing its results with those from the conventional NAM. Our findings shed light on the nuanced dynamics of social networks, presenting a valuable tool for researchers seeking to estimate the effects of social influence while accounting for homophily. Code to implement our approach is available at https://github.com/hanhtdpham/hanam.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18413&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hanh T. D. Pham, Daniel K. Sewell</name></author><category term="stat.ME" /><summary type="html">Homophily and social influence are two key concepts of social network analysis. Distinguishing between these phenomena is difficult, and approaches to disambiguate the two have been primarily limited to longitudinal data analyses. In this study, we provide sufficient conditions for valid estimation of social influence through cross-sectional data, leading to a novel homophily-adjusted social influence model which addresses the backdoor pathway of latent homophilic features. The oft-used network autocorrelation model (NAM) is the special case of our proposed model with no latent homophily, suggesting that the NAM is only valid when all homophilic attributes are observed. We conducted an extensive simulation study to evaluate the performance of our proposed homophily-adjusted model, comparing its results with those from the conventional NAM. Our findings shed light on the nuanced dynamics of social networks, presenting a valuable tool for researchers seeking to estimate the effects of social influence while accounting for homophily. Code to implement our approach is available at https://github.com/hanhtdpham/hanam.</summary></entry><entry><title type="html">Identifiability, Observability, Uncertainty and Bayesian System Identification of Epidemiological Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/IdentifiabilityObservabilityUncertaintyandBayesianSystemIdentificationofEpidemiologicalModels.html" rel="alternate" type="text/html" title="Identifiability, Observability, Uncertainty and Bayesian System Identification of Epidemiological Models" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/IdentifiabilityObservabilityUncertaintyandBayesianSystemIdentificationofEpidemiologicalModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/IdentifiabilityObservabilityUncertaintyandBayesianSystemIdentificationofEpidemiologicalModels.html">&lt;p&gt;In this project, identifiability, observability and uncertainty properties of the deterministic and Chain Binomial stochastic SIR, SEIR and SEIAR epidemiological models are studied. Techniques for modeling overdispersion are investigated and used to compare simulated trajectories for moderately sized, homogenous populations. With the chosen model parameters overdispersion was found to have small impact, but larger impact on smaller populations and simulations closer to the initial outbreak of an epidemic. Using a software tool for model identifiability and observability (DAISY[Bellu et al. 2007]), the deterministic SIR and SEIR models was found to be structurally identifiable and observable under mild conditions, while SEIAR in general remains structurally unidentifiable and unobservable. Sequential Monte Carlo and Markov Chain Monte Carlo methods were implemented in a custom C++ library and applied to stochastic SIR, SEIR and SEIAR models in order to generate parameter distributions. With the chosen model parameters overdispersion was found to have a small impact on parameter distributions for SIR and SEIR models. For SEIAR, the algorithm did not converge around the true parameters of the deterministic model. The custom C++ library was found to be computationally efficient, and is very likely to be used in future projects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18279&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jonas Hjulstad</name></author><category term="stat.AP" /><summary type="html">In this project, identifiability, observability and uncertainty properties of the deterministic and Chain Binomial stochastic SIR, SEIR and SEIAR epidemiological models are studied. Techniques for modeling overdispersion are investigated and used to compare simulated trajectories for moderately sized, homogenous populations. With the chosen model parameters overdispersion was found to have small impact, but larger impact on smaller populations and simulations closer to the initial outbreak of an epidemic. Using a software tool for model identifiability and observability (DAISY[Bellu et al. 2007]), the deterministic SIR and SEIR models was found to be structurally identifiable and observable under mild conditions, while SEIAR in general remains structurally unidentifiable and unobservable. Sequential Monte Carlo and Markov Chain Monte Carlo methods were implemented in a custom C++ library and applied to stochastic SIR, SEIR and SEIAR models in order to generate parameter distributions. With the chosen model parameters overdispersion was found to have a small impact on parameter distributions for SIR and SEIR models. For SEIAR, the algorithm did not converge around the true parameters of the deterministic model. The custom C++ library was found to be computationally efficient, and is very likely to be used in future projects.</summary></entry><entry><title type="html">Improving prediction models by incorporating external data with weights based on similarity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Improvingpredictionmodelsbyincorporatingexternaldatawithweightsbasedonsimilarity.html" rel="alternate" type="text/html" title="Improving prediction models by incorporating external data with weights based on similarity" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Improvingpredictionmodelsbyincorporatingexternaldatawithweightsbasedonsimilarity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Improvingpredictionmodelsbyincorporatingexternaldatawithweightsbasedonsimilarity.html">&lt;p&gt;In clinical settings, we often face the challenge of building prediction models based on small observational data sets. For example, such a data set might be from a medical center in a multi-center study. Differences between centers might be large, thus requiring specific models based on the data set from the target center. Still, we want to borrow information from the external centers, to deal with small sample sizes. There are approaches that either assign weights to each external data set or each external observation. To incorporate information on differences between data sets and observations, we propose an approach that combines both into weights that can be incorporated into a likelihood for fitting regression models. Specifically, we suggest weights at the data set level that incorporate information on how well the models that provide the observation weights distinguish between data sets. Technically, this takes the form of inverse probability weighting. We explore different scenarios where covariates and outcomes differ among data sets, informing our simulation design for method evaluation. The concept of effective sample size is used for understanding the effectiveness of our subgroup modeling approach. We demonstrate our approach through a clinical application, predicting applied radiotherapy doses for cancer patients. Generally, the proposed approach provides improved prediction performance when external data sets are similar. We thus provide a method for quantifying similarity of external data sets to the target data set and use this similarity to include external observations for improving performance in a target data set prediction modeling task with small data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07631&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Max Behrens, Maryam Farhadizadeh, Angelika Rohde, Alexander Rühle, Nils H. Nicolay, Harald Binder, Daniela Zöller</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In clinical settings, we often face the challenge of building prediction models based on small observational data sets. For example, such a data set might be from a medical center in a multi-center study. Differences between centers might be large, thus requiring specific models based on the data set from the target center. Still, we want to borrow information from the external centers, to deal with small sample sizes. There are approaches that either assign weights to each external data set or each external observation. To incorporate information on differences between data sets and observations, we propose an approach that combines both into weights that can be incorporated into a likelihood for fitting regression models. Specifically, we suggest weights at the data set level that incorporate information on how well the models that provide the observation weights distinguish between data sets. Technically, this takes the form of inverse probability weighting. We explore different scenarios where covariates and outcomes differ among data sets, informing our simulation design for method evaluation. The concept of effective sample size is used for understanding the effectiveness of our subgroup modeling approach. We demonstrate our approach through a clinical application, predicting applied radiotherapy doses for cancer patients. Generally, the proposed approach provides improved prediction performance when external data sets are similar. We thus provide a method for quantifying similarity of external data sets to the target data set and use this similarity to include external observations for improving performance in a target data set prediction modeling task with small data.</summary></entry><entry><title type="html">Independence Testing for Temporal Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/IndependenceTestingforTemporalData.html" rel="alternate" type="text/html" title="Independence Testing for Temporal Data" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/IndependenceTestingforTemporalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/IndependenceTestingforTemporalData.html">&lt;p&gt;Temporal data are increasingly prevalent in modern data science. A fundamental question is whether two time series are related or not. Existing approaches often have limitations, such as relying on parametric assumptions, detecting only linear associations, and requiring multiple tests and corrections. While many non-parametric and universally consistent dependence measures have recently been proposed, directly applying them to temporal data can inflate the p-value and result in an invalid test. To address these challenges, this paper introduces the temporal dependence statistic with block permutation to test independence between temporal data. Under proper assumptions, the proposed procedure is asymptotically valid and universally consistent for testing independence between stationary time series, and capable of estimating the optimal dependence lag that maximizes the dependence. Moreover, it is compatible with a rich family of distance and kernel based dependence measures, eliminates the need for multiple testing, and exhibits excellent testing power in various simulation settings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.06486&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Cencheng Shen, Jaewon Chung, Ronak Mehta, Ting Xu, Joshua T. Vogelstein</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Temporal data are increasingly prevalent in modern data science. A fundamental question is whether two time series are related or not. Existing approaches often have limitations, such as relying on parametric assumptions, detecting only linear associations, and requiring multiple tests and corrections. While many non-parametric and universally consistent dependence measures have recently been proposed, directly applying them to temporal data can inflate the p-value and result in an invalid test. To address these challenges, this paper introduces the temporal dependence statistic with block permutation to test independence between temporal data. Under proper assumptions, the proposed procedure is asymptotically valid and universally consistent for testing independence between stationary time series, and capable of estimating the optimal dependence lag that maximizes the dependence. Moreover, it is compatible with a rich family of distance and kernel based dependence measures, eliminates the need for multiple testing, and exhibits excellent testing power in various simulation settings.</summary></entry><entry><title type="html">Individualized Dynamic Mediation Analysis Using Latent Factor Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/IndividualizedDynamicMediationAnalysisUsingLatentFactorModels.html" rel="alternate" type="text/html" title="Individualized Dynamic Mediation Analysis Using Latent Factor Models" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/IndividualizedDynamicMediationAnalysisUsingLatentFactorModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/IndividualizedDynamicMediationAnalysisUsingLatentFactorModels.html">&lt;p&gt;Mediation analysis plays a crucial role in causal inference as it can investigate the pathways through which treatment influences outcome. Most existing mediation analysis assumes that mediation effects are static and homogeneous within populations. However, mediation effects usually change over time and exhibit significant heterogeneity in many real-world applications. Additionally, the presence of unobserved confounding variables imposes a significant challenge to inferring both causal effect and mediation effect. To address these issues, we propose an individualized dynamic mediation analysis method. Our approach can identify the significant mediators of the population level while capturing the time-varying and heterogeneous mediation effects via latent factor modeling on coefficients of structural equation models. Another advantage of our method is that we can infer individualized mediation effects in the presence of unmeasured time-varying confounders. We provide estimation consistency for our proposed causal estimand and selection consistency for significant mediators. Extensive simulation studies and an application to a DNA methylation study demonstrate the effectiveness and advantages of our method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17591&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yijiao Zhang, Yubai Yuan, Yuexia Zhang, Zhongyi Zhu, Annie Qu</name></author><category term="stat.ME" /><summary type="html">Mediation analysis plays a crucial role in causal inference as it can investigate the pathways through which treatment influences outcome. Most existing mediation analysis assumes that mediation effects are static and homogeneous within populations. However, mediation effects usually change over time and exhibit significant heterogeneity in many real-world applications. Additionally, the presence of unobserved confounding variables imposes a significant challenge to inferring both causal effect and mediation effect. To address these issues, we propose an individualized dynamic mediation analysis method. Our approach can identify the significant mediators of the population level while capturing the time-varying and heterogeneous mediation effects via latent factor modeling on coefficients of structural equation models. Another advantage of our method is that we can infer individualized mediation effects in the presence of unmeasured time-varying confounders. We provide estimation consistency for our proposed causal estimand and selection consistency for significant mediators. Extensive simulation studies and an application to a DNA methylation study demonstrate the effectiveness and advantages of our method.</summary></entry><entry><title type="html">Inference for the stochastic FitzHugh-Nagumo model from real action potential data via approximate Bayesian computation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/InferenceforthestochasticFitzHughNagumomodelfromrealactionpotentialdataviaapproximateBayesiancomputation.html" rel="alternate" type="text/html" title="Inference for the stochastic FitzHugh-Nagumo model from real action potential data via approximate Bayesian computation" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/InferenceforthestochasticFitzHughNagumomodelfromrealactionpotentialdataviaapproximateBayesiancomputation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/InferenceforthestochasticFitzHughNagumomodelfromrealactionpotentialdataviaapproximateBayesiancomputation.html">&lt;p&gt;The stochastic FitzHugh-Nagumo (FHN) model considered here is a two-dimensional nonlinear stochastic differential equation with additive degenerate noise, whose first component, the only one observed, describes the membrane voltage evolution of a single neuron. Due to its low dimensionality, its analytical and numerical tractability, and its neuronal interpretation, it has been used as a case study to test the performance of different statistical methods in estimating the underlying model parameters. Existing methods, however, often require complete observations, non-degeneracy of the noise or a complex architecture (e.g., to estimate the transition density of the process, “recovering” the unobserved second component), and they may not (satisfactorily) estimate all model parameters simultaneously. Moreover, these studies lack real data applications for the stochastic FHN model. Here, we tackle all challenges (non-globally Lipschitz drift, non-explicit solution, lack of available transition density, degeneracy of the noise, and partial observations) via an intuitive and easy-to-implement sequential Monte Carlo approximate Bayesian computation algorithm. The proposed method relies on a recent computationally efficient and structure-preserving numerical splitting scheme for synthetic data generation, and on summary statistics exploiting the structural properties of the process. We succeed in estimating all model parameters from simulated data and, more remarkably, real action potential data of rats. The presented novel real-data fit may broaden the scope and credibility of this classic and widely used neuronal model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17972&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Adeline Samson, Massimiliano Tamborrino, Irene Tubikanec</name></author><category term="stat.CO" /><summary type="html">The stochastic FitzHugh-Nagumo (FHN) model considered here is a two-dimensional nonlinear stochastic differential equation with additive degenerate noise, whose first component, the only one observed, describes the membrane voltage evolution of a single neuron. Due to its low dimensionality, its analytical and numerical tractability, and its neuronal interpretation, it has been used as a case study to test the performance of different statistical methods in estimating the underlying model parameters. Existing methods, however, often require complete observations, non-degeneracy of the noise or a complex architecture (e.g., to estimate the transition density of the process, “recovering” the unobserved second component), and they may not (satisfactorily) estimate all model parameters simultaneously. Moreover, these studies lack real data applications for the stochastic FHN model. Here, we tackle all challenges (non-globally Lipschitz drift, non-explicit solution, lack of available transition density, degeneracy of the noise, and partial observations) via an intuitive and easy-to-implement sequential Monte Carlo approximate Bayesian computation algorithm. The proposed method relies on a recent computationally efficient and structure-preserving numerical splitting scheme for synthetic data generation, and on summary statistics exploiting the structural properties of the process. We succeed in estimating all model parameters from simulated data and, more remarkably, real action potential data of rats. The presented novel real-data fit may broaden the scope and credibility of this classic and widely used neuronal model.</summary></entry><entry><title type="html">Inference in parametric models with many L-moments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/InferenceinparametricmodelswithmanyLmoments.html" rel="alternate" type="text/html" title="Inference in parametric models with many L-moments" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/InferenceinparametricmodelswithmanyLmoments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/InferenceinparametricmodelswithmanyLmoments.html">&lt;p&gt;L-moments are expected values of linear combinations of order statistics that provide robust alternatives to traditional moments. The estimation of parametric models by matching sample L-moments has been shown to outperform maximum likelihood estimation (MLE) in small samples from popular distributions. The choice of the number of L-moments to be used in estimation remains ad-hoc, though: researchers typically set the number of L-moments equal to the number of parameters, as to achieve an order condition for identification. This approach is generally inefficient in larger samples. In this paper, we show that, by properly choosing the number of L-moments and weighting these accordingly, we are able to construct an estimator that outperforms MLE in finite samples, and yet does not suffer from efficiency losses asymptotically. We do so by considering a “generalised” method of L-moments estimator and deriving its asymptotic properties in a framework where the number of L-moments varies with sample size. We then propose methods to automatically select the number of L-moments in a given sample. Monte Carlo evidence shows our proposed approach is able to outperform (in a mean-squared error sense) MLE in smaller samples, whilst working as well as it in larger samples. We then consider extensions of our approach to conditional and semiparametric models, and apply the latter to study expenditure patterns in a ridesharing platform in Brazil.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.04146&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luis Alvarez, Chang Chiann, Pedro Morettin</name></author><category term="stat.ME" /><summary type="html">L-moments are expected values of linear combinations of order statistics that provide robust alternatives to traditional moments. The estimation of parametric models by matching sample L-moments has been shown to outperform maximum likelihood estimation (MLE) in small samples from popular distributions. The choice of the number of L-moments to be used in estimation remains ad-hoc, though: researchers typically set the number of L-moments equal to the number of parameters, as to achieve an order condition for identification. This approach is generally inefficient in larger samples. In this paper, we show that, by properly choosing the number of L-moments and weighting these accordingly, we are able to construct an estimator that outperforms MLE in finite samples, and yet does not suffer from efficiency losses asymptotically. We do so by considering a “generalised” method of L-moments estimator and deriving its asymptotic properties in a framework where the number of L-moments varies with sample size. We then propose methods to automatically select the number of L-moments in a given sample. Monte Carlo evidence shows our proposed approach is able to outperform (in a mean-squared error sense) MLE in smaller samples, whilst working as well as it in larger samples. We then consider extensions of our approach to conditional and semiparametric models, and apply the latter to study expenditure patterns in a ridesharing platform in Brazil.</summary></entry><entry><title type="html">Instrumental Variable Estimation for Compositional Treatments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/InstrumentalVariableEstimationforCompositionalTreatments.html" rel="alternate" type="text/html" title="Instrumental Variable Estimation for Compositional Treatments" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/InstrumentalVariableEstimationforCompositionalTreatments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/InstrumentalVariableEstimationforCompositionalTreatments.html">&lt;p&gt;Many scientific datasets are compositional in nature. Important biological examples include species abundances in ecology, cell-type compositions derived from single-cell sequencing data, and amplicon abundance data in microbiome research. Here, we provide a causal view on compositional data in an instrumental variable setting where the composition acts as the cause. First, we crisply articulate potential pitfalls for practitioners regarding the interpretation of compositional causes from the viewpoint of interventions and warn against attributing causal meaning to common summary statistics such as diversity indices in microbiome data analysis. We then advocate for and develop multivariate methods using statistical data transformations and regression techniques that take the special structure of the compositional sample space into account while still yielding scientifically interpretable results. In a comparative analysis on synthetic and real microbiome data we show the advantages and limitations of our proposal. We posit that our analysis provides a useful framework and guidance for valid and informative cause-effect estimation in the context of compositional data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.11234&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elisabeth Ailer, Christian L. Müller, Niki Kilbertus</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Many scientific datasets are compositional in nature. Important biological examples include species abundances in ecology, cell-type compositions derived from single-cell sequencing data, and amplicon abundance data in microbiome research. Here, we provide a causal view on compositional data in an instrumental variable setting where the composition acts as the cause. First, we crisply articulate potential pitfalls for practitioners regarding the interpretation of compositional causes from the viewpoint of interventions and warn against attributing causal meaning to common summary statistics such as diversity indices in microbiome data analysis. We then advocate for and develop multivariate methods using statistical data transformations and regression techniques that take the special structure of the compositional sample space into account while still yielding scientifically interpretable results. In a comparative analysis on synthetic and real microbiome data we show the advantages and limitations of our proposal. We posit that our analysis provides a useful framework and guidance for valid and informative cause-effect estimation in the context of compositional data.</summary></entry><entry><title type="html">MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/MCGTAMetricConstrainedModelBasedClusteringusingGoodnessoffitTestswithAutocorrelations.html" rel="alternate" type="text/html" title="MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/MCGTAMetricConstrainedModelBasedClusteringusingGoodnessoffitTestswithAutocorrelations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/MCGTAMetricConstrainedModelBasedClusteringusingGoodnessoffitTestswithAutocorrelations.html">&lt;p&gt;A wide range of (multivariate) temporal (1D) and spatial (2D) data analysis tasks, such as grouping vehicle sensor trajectories, can be formulated as clustering with given metric constraints. Existing metric-constrained clustering algorithms overlook the rich correlation between feature similarity and metric distance, i.e., metric autocorrelation. The model-based variations of these clustering algorithms (e.g. TICC and STICC) achieve SOTA performance, yet suffer from computational instability and complexity by using a metric-constrained Expectation-Maximization procedure. In order to address these two problems, we propose a novel clustering algorithm, MC-GTA (Model-based Clustering via Goodness-of-fit Tests with Autocorrelations). Its objective is only composed of pairwise weighted sums of feature similarity terms (square Wasserstein-2 distance) and metric autocorrelation terms (a novel multivariate generalization of classic semivariogram). We show that MC-GTA is effectively minimizing the total hinge loss for intra-cluster observation pairs not passing goodness-of-fit tests, i.e., statistically not originating from the same distribution. Experiments on 1D/2D synthetic and real-world datasets demonstrate that MC-GTA successfully incorporates metric autocorrelation. It outperforms strong baselines by large margins (up to 14.3% in ARI and 32.1% in NMI) with faster and stabler optimization (&amp;gt;10x speedup).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18395&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhangyu Wang, Gengchen Mai, Krzysztof Janowicz, Ni Lao</name></author><category term="stat.AP" /><summary type="html">A wide range of (multivariate) temporal (1D) and spatial (2D) data analysis tasks, such as grouping vehicle sensor trajectories, can be formulated as clustering with given metric constraints. Existing metric-constrained clustering algorithms overlook the rich correlation between feature similarity and metric distance, i.e., metric autocorrelation. The model-based variations of these clustering algorithms (e.g. TICC and STICC) achieve SOTA performance, yet suffer from computational instability and complexity by using a metric-constrained Expectation-Maximization procedure. In order to address these two problems, we propose a novel clustering algorithm, MC-GTA (Model-based Clustering via Goodness-of-fit Tests with Autocorrelations). Its objective is only composed of pairwise weighted sums of feature similarity terms (square Wasserstein-2 distance) and metric autocorrelation terms (a novel multivariate generalization of classic semivariogram). We show that MC-GTA is effectively minimizing the total hinge loss for intra-cluster observation pairs not passing goodness-of-fit tests, i.e., statistically not originating from the same distribution. Experiments on 1D/2D synthetic and real-world datasets demonstrate that MC-GTA successfully incorporates metric autocorrelation. It outperforms strong baselines by large margins (up to 14.3% in ARI and 32.1% in NMI) with faster and stabler optimization (&amp;gt;10x speedup).</summary></entry><entry><title type="html">Measurement That Matches Theory: Theory-Driven Identification in IRT Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/MeasurementThatMatchesTheoryTheoryDrivenIdentificationinIRTModels.html" rel="alternate" type="text/html" title="Measurement That Matches Theory: Theory-Driven Identification in IRT Models" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/MeasurementThatMatchesTheoryTheoryDrivenIdentificationinIRTModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/MeasurementThatMatchesTheoryTheoryDrivenIdentificationinIRTModels.html">&lt;p&gt;Measurement bridges theory and empirics. Without measures that appropriately capture theoretical concepts, description will fail to represent reality and true causal inference will be impossible. Yet, the social sciences traffic in complex concepts and their measurement is difficult. Item Response Theory (IRT) models reduce variation in multiple variables to continuous variation along one or more latent dimensions intended to capture key theoretical concepts. Unfortunately, those latent dimensions have no intrinsic conceptual meaning. Partial solutions to that problem include limiting the number of dimensions to one or assigning meaning post-analysis, but either can lead to potential bias and a lack of reliability across data sources. We propose, detail, and validate a semi-supervised approach employing Bayesian Item Response Theory on multiple latent dimensions and binary data. Our approach, which we validate on simulated and real data, yields conceptually meaningful latent dimensions that are reliable across different data sources without additional exogenous assumptions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.11979&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marco Morucci, Margaret Foster, Kaitlyn Webster, So Jin Lee, David Siegel</name></author><category term="stat.AP" /><summary type="html">Measurement bridges theory and empirics. Without measures that appropriately capture theoretical concepts, description will fail to represent reality and true causal inference will be impossible. Yet, the social sciences traffic in complex concepts and their measurement is difficult. Item Response Theory (IRT) models reduce variation in multiple variables to continuous variation along one or more latent dimensions intended to capture key theoretical concepts. Unfortunately, those latent dimensions have no intrinsic conceptual meaning. Partial solutions to that problem include limiting the number of dimensions to one or assigning meaning post-analysis, but either can lead to potential bias and a lack of reliability across data sources. We propose, detail, and validate a semi-supervised approach employing Bayesian Item Response Theory on multiple latent dimensions and binary data. Our approach, which we validate on simulated and real data, yields conceptually meaningful latent dimensions that are reliable across different data sources without additional exogenous assumptions.</summary></entry><entry><title type="html">Mixed Semi-Supervised Generalized-Linear-Regression with applications to Deep-Learning and Interpolators</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/MixedSemiSupervisedGeneralizedLinearRegressionwithapplicationstoDeepLearningandInterpolators.html" rel="alternate" type="text/html" title="Mixed Semi-Supervised Generalized-Linear-Regression with applications to Deep-Learning and Interpolators" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/MixedSemiSupervisedGeneralizedLinearRegressionwithapplicationstoDeepLearningandInterpolators</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/MixedSemiSupervisedGeneralizedLinearRegressionwithapplicationstoDeepLearningandInterpolators.html">&lt;p&gt;We present a methodology for using unlabeled data to design semi supervised learning (SSL) methods that improve the prediction performance of supervised learning for regression tasks. The main idea is to design different mechanisms for integrating the unlabeled data, and include in each of them a mixing parameter $\alpha$, controlling the weight given to the unlabeled data. Focusing on Generalized Linear Models (GLM) and linear interpolators classes of models, we analyze the characteristics of different mixing mechanisms, and prove that in all cases, it is invariably beneficial to integrate the unlabeled data with some nonzero mixing ratio $\alpha&amp;gt;0$, in terms of predictive performance. Moreover, we provide a rigorous framework to estimate the best mixing ratio $\alpha^*$ where mixed SSL delivers the best predictive performance, while using the labeled and unlabeled data on hand.
  The effectiveness of our methodology in delivering substantial improvement compared to the standard supervised models, in a variety of settings, is demonstrated empirically through extensive simulation, in a manner that supports the theoretical analysis. We also demonstrate the applicability of our methodology (with some intuitive modifications) to improve more complex models, such as deep neural networks, in real-world regression tasks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.09526&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Oren Yuval, Saharon Rosset</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We present a methodology for using unlabeled data to design semi supervised learning (SSL) methods that improve the prediction performance of supervised learning for regression tasks. The main idea is to design different mechanisms for integrating the unlabeled data, and include in each of them a mixing parameter $\alpha$, controlling the weight given to the unlabeled data. Focusing on Generalized Linear Models (GLM) and linear interpolators classes of models, we analyze the characteristics of different mixing mechanisms, and prove that in all cases, it is invariably beneficial to integrate the unlabeled data with some nonzero mixing ratio $\alpha&amp;gt;0$, in terms of predictive performance. Moreover, we provide a rigorous framework to estimate the best mixing ratio $\alpha^*$ where mixed SSL delivers the best predictive performance, while using the labeled and unlabeled data on hand. The effectiveness of our methodology in delivering substantial improvement compared to the standard supervised models, in a variety of settings, is demonstrated empirically through extensive simulation, in a manner that supports the theoretical analysis. We also demonstrate the applicability of our methodology (with some intuitive modifications) to improve more complex models, such as deep neural networks, in real-world regression tasks.</summary></entry><entry><title type="html">Multi-CATE: Multi-Accurate Conditional Average Treatment Effect Estimation Robust to Unknown Covariate Shifts</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/MultiCATEMultiAccurateConditionalAverageTreatmentEffectEstimationRobusttoUnknownCovariateShifts.html" rel="alternate" type="text/html" title="Multi-CATE: Multi-Accurate Conditional Average Treatment Effect Estimation Robust to Unknown Covariate Shifts" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/MultiCATEMultiAccurateConditionalAverageTreatmentEffectEstimationRobusttoUnknownCovariateShifts</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/MultiCATEMultiAccurateConditionalAverageTreatmentEffectEstimationRobusttoUnknownCovariateShifts.html">&lt;p&gt;Estimating heterogeneous treatment effects is important to tailor treatments to those individuals who would most likely benefit. However, conditional average treatment effect predictors may often be trained on one population but possibly deployed on different, possibly unknown populations. We use methodology for learning multi-accurate predictors to post-process CATE T-learners (differenced regressions) to become robust to unknown covariate shifts at the time of deployment. The method works in general for pseudo-outcome regression, such as the DR-learner. We show how this approach can combine (large) confounded observational and (smaller) randomized datasets by learning a confounded predictor from the observational dataset, and auditing for multi-accuracy on the randomized controlled trial. We show improvements in bias and mean squared error in simulations with increasingly larger covariate shift, and on a semi-synthetic case study of a parallel large observational study and smaller randomized controlled experiment. Overall, we establish a connection between methods developed for multi-distribution learning and achieve appealing desiderata (e.g. external validity) in causal inference and machine learning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18206&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Christoph Kern, Michael Kim, Angela Zhou</name></author><category term="stat.ME" /><summary type="html">Estimating heterogeneous treatment effects is important to tailor treatments to those individuals who would most likely benefit. However, conditional average treatment effect predictors may often be trained on one population but possibly deployed on different, possibly unknown populations. We use methodology for learning multi-accurate predictors to post-process CATE T-learners (differenced regressions) to become robust to unknown covariate shifts at the time of deployment. The method works in general for pseudo-outcome regression, such as the DR-learner. We show how this approach can combine (large) confounded observational and (smaller) randomized datasets by learning a confounded predictor from the observational dataset, and auditing for multi-accuracy on the randomized controlled trial. We show improvements in bias and mean squared error in simulations with increasingly larger covariate shift, and on a semi-synthetic case study of a parallel large observational study and smaller randomized controlled experiment. Overall, we establish a connection between methods developed for multi-distribution learning and achieve appealing desiderata (e.g. external validity) in causal inference and machine learning.</summary></entry><entry><title type="html">On Robust Clustering of Temporal Point Process</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/OnRobustClusteringofTemporalPointProcess.html" rel="alternate" type="text/html" title="On Robust Clustering of Temporal Point Process" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/OnRobustClusteringofTemporalPointProcess</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/OnRobustClusteringofTemporalPointProcess.html">&lt;p&gt;Clustering of event stream data is of great importance in many application scenarios, including but not limited to, e-commerce, electronic health, online testing, mobile music service, etc. Existing clustering algorithms fail to take outlier data into consideration and are implemented without theoretical guarantees. In this paper, we propose a robust temporal point processes clustering framework which works under mild assumptions and meanwhile addresses several important issues in the event stream clustering problem.Specifically, we introduce a computationally efficient model-free distance function to quantify the dissimilarity between different event streams so that the outliers can be detected and the good initial clusters could be obtained. We further consider an expectation-maximization-type algorithm incorporated with a Catoni’s influence function for robust estimation and fine-tuning of clusters. We also establish the theoretical results including algorithmic convergence, estimation error bound, outlier detection, etc. Simulation results corroborate our theoretical findings and real data applications show the effectiveness of our proposed methodology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17828&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuecheng Zhang, Guanhua Fang, Wen Yu</name></author><category term="stat.ME" /><summary type="html">Clustering of event stream data is of great importance in many application scenarios, including but not limited to, e-commerce, electronic health, online testing, mobile music service, etc. Existing clustering algorithms fail to take outlier data into consideration and are implemented without theoretical guarantees. In this paper, we propose a robust temporal point processes clustering framework which works under mild assumptions and meanwhile addresses several important issues in the event stream clustering problem.Specifically, we introduce a computationally efficient model-free distance function to quantify the dissimilarity between different event streams so that the outliers can be detected and the good initial clusters could be obtained. We further consider an expectation-maximization-type algorithm incorporated with a Catoni’s influence function for robust estimation and fine-tuning of clusters. We also establish the theoretical results including algorithmic convergence, estimation error bound, outlier detection, etc. Simulation results corroborate our theoretical findings and real data applications show the effectiveness of our proposed methodology.</summary></entry><entry><title type="html">On Robust Inference in Time Series Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/OnRobustInferenceinTimeSeriesRegression.html" rel="alternate" type="text/html" title="On Robust Inference in Time Series Regression" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/OnRobustInferenceinTimeSeriesRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/OnRobustInferenceinTimeSeriesRegression.html">&lt;p&gt;Least squares regression with heteroskedasticity consistent standard errors (“OLS-HC regression”) has proved very useful in cross section environments. However, several major difficulties, which are generally overlooked, must be confronted when transferring the HC technology to time series environments via heteroskedasticity and autocorrelation consistent standard errors (“OLS-HAC regression”). First, in plausible time-series environments, OLS parameter estimates can be inconsistent, so that OLS-HAC inference fails even asymptotically. Second, most economic time series have autocorrelation, which renders OLS parameter estimates inefficient. Third, autocorrelation similarly renders conditional predictions based on OLS parameter estimates inefficient. Finally, the structure of popular HAC covariance matrix estimators is ill-suited for capturing the autoregressive autocorrelation typically present in economic time series, which produces large size distortions and reduced power in HAC-based hypothesis testing, in all but the largest samples. We show that all four problems are largely avoided by the use of a simple and easily-implemented dynamic regression procedure, which we call DURBIN. We demonstrate the advantages of DURBIN with detailed simulations covering a range of practical issues.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.04080&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Richard T. Baillie, Francis X. Diebold, George Kapetanios, Kun Ho Kim, Aaron Mora</name></author><category term="stat.AP" /><summary type="html">Least squares regression with heteroskedasticity consistent standard errors (“OLS-HC regression”) has proved very useful in cross section environments. However, several major difficulties, which are generally overlooked, must be confronted when transferring the HC technology to time series environments via heteroskedasticity and autocorrelation consistent standard errors (“OLS-HAC regression”). First, in plausible time-series environments, OLS parameter estimates can be inconsistent, so that OLS-HAC inference fails even asymptotically. Second, most economic time series have autocorrelation, which renders OLS parameter estimates inefficient. Third, autocorrelation similarly renders conditional predictions based on OLS parameter estimates inefficient. Finally, the structure of popular HAC covariance matrix estimators is ill-suited for capturing the autoregressive autocorrelation typically present in economic time series, which produces large size distortions and reduced power in HAC-based hypothesis testing, in all but the largest samples. We show that all four problems are largely avoided by the use of a simple and easily-implemented dynamic regression procedure, which we call DURBIN. We demonstrate the advantages of DURBIN with detailed simulations covering a range of practical issues.</summary></entry><entry><title type="html">Online conformal prediction with decaying step sizes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Onlineconformalpredictionwithdecayingstepsizes.html" rel="alternate" type="text/html" title="Online conformal prediction with decaying step sizes" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Onlineconformalpredictionwithdecayingstepsizes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Onlineconformalpredictionwithdecayingstepsizes.html">&lt;p&gt;We introduce a method for online conformal prediction with decaying step sizes. Like previous methods, ours possesses a retrospective guarantee of coverage for arbitrary sequences. However, unlike previous methods, we can simultaneously estimate a population quantile when it exists. Our theory and experiments indicate substantially improved practical properties: in particular, when the distribution is stable, the coverage is close to the desired level for every time point, not just on average over the observed sequence.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.01139&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anastasios N. Angelopoulos, Rina Foygel Barber, Stephen Bates</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">We introduce a method for online conformal prediction with decaying step sizes. Like previous methods, ours possesses a retrospective guarantee of coverage for arbitrary sequences. However, unlike previous methods, we can simultaneously estimate a population quantile when it exists. Our theory and experiments indicate substantially improved practical properties: in particular, when the distribution is stable, the coverage is close to the desired level for every time point, not just on average over the observed sequence.</summary></entry><entry><title type="html">On the uses and abuses of regression models: a call for reform of statistical practice and teaching</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Ontheusesandabusesofregressionmodelsacallforreformofstatisticalpracticeandteaching.html" rel="alternate" type="text/html" title="On the uses and abuses of regression models: a call for reform of statistical practice and teaching" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Ontheusesandabusesofregressionmodelsacallforreformofstatisticalpracticeandteaching</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/Ontheusesandabusesofregressionmodelsacallforreformofstatisticalpracticeandteaching.html">&lt;p&gt;Regression methods dominate the practice of biostatistical analysis, but biostatistical training emphasises the details of regression models and methods ahead of the purposes for which such modelling might be useful. More broadly, statistics is widely understood to provide a body of techniques for “modelling data”, underpinned by what we describe as the “true model myth”: that the task of the statistician/data analyst is to build a model that closely approximates the true data generating process. By way of our own historical examples and a brief review of mainstream clinical research journals, we describe how this perspective has led to a range of problems in the application of regression methods, including misguided “adjustment” for covariates, misinterpretation of regression coefficients and the widespread fitting of regression models without a clear purpose. We then outline a new approach to the teaching and application of biostatistical methods, which situates them within a framework that first requires clear definition of the substantive research question at hand within one of three categories: descriptive, predictive, or causal. Within this approach, the simple univariable regression model may be introduced as a tool for description, while the development and application of multivariable regression models as well as other advanced biostatistical methods should proceed differently according to the type of question. Regression methods will no doubt remain central to statistical practice as they provide a powerful tool for representing variation in a response or outcome variable as a function of “input” variables, but their conceptualisation and usage should follow from the purpose at hand.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.06668&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>John B. Carlin, Margarita Moreno-Betancur</name></author><category term="stat.ME" /><summary type="html">Regression methods dominate the practice of biostatistical analysis, but biostatistical training emphasises the details of regression models and methods ahead of the purposes for which such modelling might be useful. More broadly, statistics is widely understood to provide a body of techniques for “modelling data”, underpinned by what we describe as the “true model myth”: that the task of the statistician/data analyst is to build a model that closely approximates the true data generating process. By way of our own historical examples and a brief review of mainstream clinical research journals, we describe how this perspective has led to a range of problems in the application of regression methods, including misguided “adjustment” for covariates, misinterpretation of regression coefficients and the widespread fitting of regression models without a clear purpose. We then outline a new approach to the teaching and application of biostatistical methods, which situates them within a framework that first requires clear definition of the substantive research question at hand within one of three categories: descriptive, predictive, or causal. Within this approach, the simple univariable regression model may be introduced as a tool for description, while the development and application of multivariable regression models as well as other advanced biostatistical methods should proceed differently according to the type of question. Regression methods will no doubt remain central to statistical practice as they provide a powerful tool for representing variation in a response or outcome variable as a function of “input” variables, but their conceptualisation and usage should follow from the purpose at hand.</summary></entry><entry><title type="html">Optimal Design in Repeated Testing for Count Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/OptimalDesigninRepeatedTestingforCountData.html" rel="alternate" type="text/html" title="Optimal Design in Repeated Testing for Count Data" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/OptimalDesigninRepeatedTestingforCountData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/OptimalDesigninRepeatedTestingforCountData.html">&lt;p&gt;In this paper, we develop optimal designs for growth curve models with count data based on the Rasch Poisson-Gamma counts (RPGCM) model. This model is often used in educational and psychological testing when test results yield count data. In the RPGCM, the test scores are determined by respondents ability and item difficulty. Locally D-optimal designs are derived for maximum quasi-likelihood estimation to efficiently estimate the mean abilities of the respondents over time. Using the log link, both unstructured, linear and nonlinear growth curves of log mean abilities are taken into account. Finally, the sensitivity of the derived optimal designs due to an imprecise choice of parameter values is analyzed using D-efficiency.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18323&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Parisa Parsamaram, Heinz Holling, Rainer Schwabe</name></author><category term="stat.ME" /><summary type="html">In this paper, we develop optimal designs for growth curve models with count data based on the Rasch Poisson-Gamma counts (RPGCM) model. This model is often used in educational and psychological testing when test results yield count data. In the RPGCM, the test scores are determined by respondents ability and item difficulty. Locally D-optimal designs are derived for maximum quasi-likelihood estimation to efficiently estimate the mean abilities of the respondents over time. Using the log link, both unstructured, linear and nonlinear growth curves of log mean abilities are taken into account. Finally, the sensitivity of the derived optimal designs due to an imprecise choice of parameter values is analyzed using D-efficiency.</summary></entry><entry><title type="html">Parameter Inference for Degenerate Diffusion Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ParameterInferenceforDegenerateDiffusionProcesses.html" rel="alternate" type="text/html" title="Parameter Inference for Degenerate Diffusion Processes" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ParameterInferenceforDegenerateDiffusionProcesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ParameterInferenceforDegenerateDiffusionProcesses.html">&lt;p&gt;We study parametric inference for ergodic diffusion processes with a degenerate diffusion matrix. Existing research focuses on a particular class of hypo-elliptic SDEs, with components split into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rough&apos;/&lt;/code&gt;smooth’ and noise from rough components propagating directly onto smooth ones, but some critical model classes arising in applications have yet to be explored. We aim to cover this gap, thus analyse the highly degenerate class of SDEs, where components split into further sub-groups. Such models include e.g. the notable case of generalised Langevin equations. We propose a tailored time-discretisation scheme and provide asymptotic results supporting our scheme in the context of high-frequency, full observations. The proposed discretisation scheme is applicable in much more general data regimes and is shown to overcome biases via simulation studies also in the practical case when only a smooth component is observed. Joint consideration of our study for highly degenerate SDEs and existing research provides a general `recipe’ for the development of time-discretisation schemes to be used within statistical methods for general classes of hypo-elliptic SDEs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.16485&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuga Iguchi, Alexandros Beskos, Matthew Graham</name></author><category term="stat.AP," /><category term="stat.CO," /><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We study parametric inference for ergodic diffusion processes with a degenerate diffusion matrix. Existing research focuses on a particular class of hypo-elliptic SDEs, with components split into rough&apos;/smooth’ and noise from rough components propagating directly onto smooth ones, but some critical model classes arising in applications have yet to be explored. We aim to cover this gap, thus analyse the highly degenerate class of SDEs, where components split into further sub-groups. Such models include e.g. the notable case of generalised Langevin equations. We propose a tailored time-discretisation scheme and provide asymptotic results supporting our scheme in the context of high-frequency, full observations. The proposed discretisation scheme is applicable in much more general data regimes and is shown to overcome biases via simulation studies also in the practical case when only a smooth component is observed. Joint consideration of our study for highly degenerate SDEs and existing research provides a general `recipe’ for the development of time-discretisation schemes to be used within statistical methods for general classes of hypo-elliptic SDEs.</summary></entry><entry><title type="html">Predicting Progression Events in Multiple Myeloma from Routine Blood Work</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/PredictingProgressionEventsinMultipleMyelomafromRoutineBloodWork.html" rel="alternate" type="text/html" title="Predicting Progression Events in Multiple Myeloma from Routine Blood Work" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/PredictingProgressionEventsinMultipleMyelomafromRoutineBloodWork</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/PredictingProgressionEventsinMultipleMyelomafromRoutineBloodWork.html">&lt;p&gt;The ability to accurately predict disease progression is paramount for optimizing multiple myeloma patient care. This study introduces a hybrid neural network architecture, combining Long Short-Term Memory networks with a Conditional Restricted Boltzmann Machine, to predict future blood work of affected patients from a series of historical laboratory results. We demonstrate that our model can replicate the statistical moments of the time series ($0.95~\pm~0.01~\geq~R^2~\geq~0.83~\pm~0.03$) and forecast future blood work features with high correlation to actual patient data ($0.92\pm0.02~\geq~r~\geq~0.52~\pm~0.09$). Subsequently, a second Long Short-Term Memory network is employed to detect and annotate disease progression events within the forecasted blood work time series. We show that these annotations enable the prediction of progression events with significant reliability (AUROC$~=~0.88~\pm~0.01$), up to 12 months in advance (AUROC($t+12~$mos)$~=0.65~\pm~0.01$). Our system is designed in a modular fashion, featuring separate entities for forecasting and progression event annotation. This structure not only enhances interpretability but also facilitates the integration of additional modules to perform subsequent operations on the generated outputs. Our approach utilizes a minimal set of routine blood work measurements, which avoids the need for expensive or resource-intensive tests and ensures accessibility of the system in clinical routine. This capability allows for individualized risk assessment and making informed treatment decisions tailored to a patient’s unique disease kinetics. The represented approach contributes to the development of a scalable and cost-effective virtual human twin system for optimized healthcare resource utilization and improved patient outcomes in multiple myeloma care.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18051&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maximilian Ferle, Nora Grieb, Markus Kreuz, Uwe Platzbecker, Thomas Neumuth, Kristin Reiche, Alexander Oeser, Maximilian Merz</name></author><category term="stat.AP" /><summary type="html">The ability to accurately predict disease progression is paramount for optimizing multiple myeloma patient care. This study introduces a hybrid neural network architecture, combining Long Short-Term Memory networks with a Conditional Restricted Boltzmann Machine, to predict future blood work of affected patients from a series of historical laboratory results. We demonstrate that our model can replicate the statistical moments of the time series ($0.95~\pm~0.01~\geq~R^2~\geq~0.83~\pm~0.03$) and forecast future blood work features with high correlation to actual patient data ($0.92\pm0.02~\geq~r~\geq~0.52~\pm~0.09$). Subsequently, a second Long Short-Term Memory network is employed to detect and annotate disease progression events within the forecasted blood work time series. We show that these annotations enable the prediction of progression events with significant reliability (AUROC$~=~0.88~\pm~0.01$), up to 12 months in advance (AUROC($t+12~$mos)$~=0.65~\pm~0.01$). Our system is designed in a modular fashion, featuring separate entities for forecasting and progression event annotation. This structure not only enhances interpretability but also facilitates the integration of additional modules to perform subsequent operations on the generated outputs. Our approach utilizes a minimal set of routine blood work measurements, which avoids the need for expensive or resource-intensive tests and ensures accessibility of the system in clinical routine. This capability allows for individualized risk assessment and making informed treatment decisions tailored to a patient’s unique disease kinetics. The represented approach contributes to the development of a scalable and cost-effective virtual human twin system for optimized healthcare resource utilization and improved patient outcomes in multiple myeloma care.</summary></entry><entry><title type="html">Prediction of energy consumption in hotels using ANN</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/PredictionofenergyconsumptioninhotelsusingANN.html" rel="alternate" type="text/html" title="Prediction of energy consumption in hotels using ANN" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/PredictionofenergyconsumptioninhotelsusingANN</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/PredictionofenergyconsumptioninhotelsusingANN.html">&lt;p&gt;The increase in travelers and stays in tourist destinations is leading hotels to be aware of their ecological management and the need for efficient energy consumption. To achieve this, hotels are increasingly using digitalized systems and more frequent measurements are made of the variables that affect their management. Electricity can play a significant role, predicting electricity usage in hotels, which in turn can enhance their circularity - an approach aimed at sustainable and efficient resource use. In this study, neural networks are trained to predict electricity usage patterns in two hotels based on historical data. The results indicate that the predictions have a good accuracy level of around 2.5% in MAPE, showing the potential of using these techniques for electricity forecasting in hotels. Additionally, neural network models can use climatological data to improve predictions. By accurately forecasting energy demand, hotels can optimize their energy procurement and usage, moving energy-intensive activities to off-peak hours to reduce costs and strain on the grid, assisting in the better integration of renewable energy sources, or identifying patterns and anomalies in energy consumption, suggesting areas for efficiency improvements, among other. Hence, by optimizing the allocation of resources, reducing waste and improving efficiency these models can improve hotel’s circularity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18076&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Oscar Trull, Angel Peiro-Signes, J. Carlos Garcia-Diaz, Marival Segarra-Ona</name></author><category term="stat.CO" /><summary type="html">The increase in travelers and stays in tourist destinations is leading hotels to be aware of their ecological management and the need for efficient energy consumption. To achieve this, hotels are increasingly using digitalized systems and more frequent measurements are made of the variables that affect their management. Electricity can play a significant role, predicting electricity usage in hotels, which in turn can enhance their circularity - an approach aimed at sustainable and efficient resource use. In this study, neural networks are trained to predict electricity usage patterns in two hotels based on historical data. The results indicate that the predictions have a good accuracy level of around 2.5% in MAPE, showing the potential of using these techniques for electricity forecasting in hotels. Additionally, neural network models can use climatological data to improve predictions. By accurately forecasting energy demand, hotels can optimize their energy procurement and usage, moving energy-intensive activities to off-peak hours to reduce costs and strain on the grid, assisting in the better integration of renewable energy sources, or identifying patterns and anomalies in energy consumption, suggesting areas for efficiency improvements, among other. Hence, by optimizing the allocation of resources, reducing waste and improving efficiency these models can improve hotel’s circularity.</summary></entry><entry><title type="html">Probabilistically Plausible Counterfactual Explanations with Normalizing Flows</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ProbabilisticallyPlausibleCounterfactualExplanationswithNormalizingFlows.html" rel="alternate" type="text/html" title="Probabilistically Plausible Counterfactual Explanations with Normalizing Flows" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ProbabilisticallyPlausibleCounterfactualExplanationswithNormalizingFlows</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ProbabilisticallyPlausibleCounterfactualExplanationswithNormalizingFlows.html">&lt;p&gt;We present PPCEF, a novel method for generating probabilistically plausible counterfactual explanations (CFs). PPCEF advances beyond existing methods by combining a probabilistic formulation that leverages the data distribution with the optimization of plausibility within a unified framework. Compared to reference approaches, our method enforces plausibility by directly optimizing the explicit density function without assuming a particular family of parametrized distributions. This ensures CFs are not only valid (i.e., achieve class change) but also align with the underlying data’s probability density. For that purpose, our approach leverages normalizing flows as powerful density estimators to capture the complex high-dimensional data distribution. Furthermore, we introduce a novel loss that balances the trade-off between achieving class change and maintaining closeness to the original instance while also incorporating a probabilistic plausibility term. PPCEF’s unconstrained formulation allows for efficient gradient-based optimization with batch processing, leading to orders of magnitude faster computation compared to prior methods. Moreover, the unconstrained formulation of PPCEF allows for the seamless integration of future constraints tailored to specific counterfactual properties. Finally, extensive evaluations demonstrate PPCEF’s superiority in generating high-quality, probabilistically plausible counterfactual explanations in high-dimensional tabular settings. This makes PPCEF a powerful tool for not only interpreting complex machine learning models but also for improving fairness, accountability, and trust in AI systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17640&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Patryk Wielopolski, Oleksii Furman, Jerzy Stefanowski, Maciej Zięba</name></author><category term="stat.ME" /><summary type="html">We present PPCEF, a novel method for generating probabilistically plausible counterfactual explanations (CFs). PPCEF advances beyond existing methods by combining a probabilistic formulation that leverages the data distribution with the optimization of plausibility within a unified framework. Compared to reference approaches, our method enforces plausibility by directly optimizing the explicit density function without assuming a particular family of parametrized distributions. This ensures CFs are not only valid (i.e., achieve class change) but also align with the underlying data’s probability density. For that purpose, our approach leverages normalizing flows as powerful density estimators to capture the complex high-dimensional data distribution. Furthermore, we introduce a novel loss that balances the trade-off between achieving class change and maintaining closeness to the original instance while also incorporating a probabilistic plausibility term. PPCEF’s unconstrained formulation allows for efficient gradient-based optimization with batch processing, leading to orders of magnitude faster computation compared to prior methods. Moreover, the unconstrained formulation of PPCEF allows for the seamless integration of future constraints tailored to specific counterfactual properties. Finally, extensive evaluations demonstrate PPCEF’s superiority in generating high-quality, probabilistically plausible counterfactual explanations in high-dimensional tabular settings. This makes PPCEF a powerful tool for not only interpreting complex machine learning models but also for improving fairness, accountability, and trust in AI systems.</summary></entry><entry><title type="html">Random measure priors in Bayesian recovery from sketches</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/RandommeasurepriorsinBayesianrecoveryfromsketches.html" rel="alternate" type="text/html" title="Random measure priors in Bayesian recovery from sketches" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/RandommeasurepriorsinBayesianrecoveryfromsketches</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/RandommeasurepriorsinBayesianrecoveryfromsketches.html">&lt;p&gt;This paper introduces a Bayesian nonparametric approach to frequency recovery from lossy-compressed discrete data, leveraging all information contained in a sketch obtained through random hashing. By modeling the data points as random samples from an unknown discrete distribution endowed with a Poisson-Kingman prior, we derive the posterior distribution of a symbol’s empirical frequency given the sketch. This leads to principled frequency estimates through mean functionals, e.g., the posterior mean, median and mode. We highlight applications of this general result to Dirichlet process and Pitman-Yor process priors. Notably, we prove that the former prior uniquely satisfies a sufficiency property that simplifies the posterior distribution, while the latter enables a convenient large-sample asymptotic approximation. Additionally, we extend our approach to the problem of cardinality recovery, estimating the number of distinct symbols in the sketched dataset. Our approach to frequency recovery also adapts to a more general &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;traits&apos;&apos; setting, where each data point has integer levels of association with multiple symbols, typically referred to as&lt;/code&gt;traits’’. By employing a generalized Indian buffet process, we compute the posterior distribution of a trait’s frequency using both the Poisson and Bernoulli distributions for the trait association levels, respectively yielding exact and approximate posterior frequency distributions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.15029&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mario Beraha, Stefano Favaro, Matteo Sesia</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This paper introduces a Bayesian nonparametric approach to frequency recovery from lossy-compressed discrete data, leveraging all information contained in a sketch obtained through random hashing. By modeling the data points as random samples from an unknown discrete distribution endowed with a Poisson-Kingman prior, we derive the posterior distribution of a symbol’s empirical frequency given the sketch. This leads to principled frequency estimates through mean functionals, e.g., the posterior mean, median and mode. We highlight applications of this general result to Dirichlet process and Pitman-Yor process priors. Notably, we prove that the former prior uniquely satisfies a sufficiency property that simplifies the posterior distribution, while the latter enables a convenient large-sample asymptotic approximation. Additionally, we extend our approach to the problem of cardinality recovery, estimating the number of distinct symbols in the sketched dataset. Our approach to frequency recovery also adapts to a more general traits&apos;&apos; setting, where each data point has integer levels of association with multiple symbols, typically referred to astraits’’. By employing a generalized Indian buffet process, we compute the posterior distribution of a trait’s frequency using both the Poisson and Bernoulli distributions for the trait association levels, respectively yielding exact and approximate posterior frequency distributions.</summary></entry><entry><title type="html">SB-ETAS: using simulation based inference for scalable, likelihood-free inference for the ETAS model of earthquake occurrences</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/SBETASusingsimulationbasedinferenceforscalablelikelihoodfreeinferencefortheETASmodelofearthquakeoccurrences.html" rel="alternate" type="text/html" title="SB-ETAS: using simulation based inference for scalable, likelihood-free inference for the ETAS model of earthquake occurrences" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/SBETASusingsimulationbasedinferenceforscalablelikelihoodfreeinferencefortheETASmodelofearthquakeoccurrences</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/SBETASusingsimulationbasedinferenceforscalablelikelihoodfreeinferencefortheETASmodelofearthquakeoccurrences.html">&lt;p&gt;Performing Bayesian inference for the Epidemic-Type Aftershock Sequence (ETAS) model of earthquakes typically requires MCMC sampling using the likelihood function or estimating the latent branching structure. These tasks have computational complexity $O(n^2)$ with the number of earthquakes and therefore do not scale well with new enhanced catalogs, which can now contain an order of $10^6$ events. On the other hand, simulation from the ETAS model can be done more quickly $O(n \log n )$. We present SB-ETAS: simulation-based inference for the ETAS model. This is an approximate Bayesian method which uses Sequential Neural Posterior Estimation (SNPE), a machine learning based algorithm for learning posterior distributions from simulations. SB-ETAS can successfully approximate ETAS posterior distributions on shorter catalogues where it is computationally feasible to compare with MCMC sampling. Furthermore, the scaling of SB-ETAS makes it feasible to fit to very large earthquake catalogs, such as one for Southern California dating back to 1932. SB-ETAS can find Bayesian estimates of ETAS parameters for this catalog in less than 10 hours on a standard laptop, which would have taken over 2 weeks using MCMC. Looking beyond the standard ETAS model, this simulation based framework would allow earthquake modellers to define and infer parameters for much more complex models that have intractable likelihood functions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.16590&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Samuel Stockman, Daniel J. Lawson, Maximilian J. Werner</name></author><category term="stat.AP" /><summary type="html">Performing Bayesian inference for the Epidemic-Type Aftershock Sequence (ETAS) model of earthquakes typically requires MCMC sampling using the likelihood function or estimating the latent branching structure. These tasks have computational complexity $O(n^2)$ with the number of earthquakes and therefore do not scale well with new enhanced catalogs, which can now contain an order of $10^6$ events. On the other hand, simulation from the ETAS model can be done more quickly $O(n \log n )$. We present SB-ETAS: simulation-based inference for the ETAS model. This is an approximate Bayesian method which uses Sequential Neural Posterior Estimation (SNPE), a machine learning based algorithm for learning posterior distributions from simulations. SB-ETAS can successfully approximate ETAS posterior distributions on shorter catalogues where it is computationally feasible to compare with MCMC sampling. Furthermore, the scaling of SB-ETAS makes it feasible to fit to very large earthquake catalogs, such as one for Southern California dating back to 1932. SB-ETAS can find Bayesian estimates of ETAS parameters for this catalog in less than 10 hours on a standard laptop, which would have taken over 2 weeks using MCMC. Looking beyond the standard ETAS model, this simulation based framework would allow earthquake modellers to define and infer parameters for much more complex models that have intractable likelihood functions.</summary></entry><entry><title type="html">Simulation of Single-Phase Natural Circulation within the BEPU Framework: Sketching Scaling Uncertainty Principle by Multi-Scale CFD Approaches</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/SimulationofSinglePhaseNaturalCirculationwithintheBEPUFrameworkSketchingScalingUncertaintyPrinciplebyMultiScaleCFDApproaches.html" rel="alternate" type="text/html" title="Simulation of Single-Phase Natural Circulation within the BEPU Framework: Sketching Scaling Uncertainty Principle by Multi-Scale CFD Approaches" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/SimulationofSinglePhaseNaturalCirculationwithintheBEPUFrameworkSketchingScalingUncertaintyPrinciplebyMultiScaleCFDApproaches</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/SimulationofSinglePhaseNaturalCirculationwithintheBEPUFrameworkSketchingScalingUncertaintyPrinciplebyMultiScaleCFDApproaches.html">&lt;p&gt;In order to enhance safety, nuclear reactors in the design phase consider natural circulation as a mean to remove residual power. The simulation of this passive mechanism must be qualified between the validation range and the scope of utilization (reactor case), introducing potential physical and numerical distortion effects. In this study, we simulate the flow of liquid sodium using the TrioCFD code, employing both higher-fidelity (HF) LES and lower-fidelity (LF) URANS models. We tackle respectively numerical uncertainties through the Grid Convergence Index method, and physical modelling uncertainties through the Polynomial Chaos Expansion method available on the URANIE platform. HF simulations are shown to exhibit a strong resilience to physical distortion effects, with numerical uncertainties being intricately correlated. Conversely, the LF approach, the only one applicable at the reactor scale, is likely to present a reduced predictability. If so, the HF approach should be effective in pinpointing the LF weaknesses: the concept of scaling uncertainty is inline introduced as the growth of the LF simulation uncertainty associated with distortion effects. Thus, the paper outlines that a specific methodology within the BEPU framework - leveraging both HF and LF approaches - could pragmatically enable correlating distortion effects with scaling uncertainty, thereby providing a metric principle.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18108&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Haifu Huang , Jorge Perez , Nicolas Alpy , Marc Medale</name></author><category term="stat.AP" /><summary type="html">In order to enhance safety, nuclear reactors in the design phase consider natural circulation as a mean to remove residual power. The simulation of this passive mechanism must be qualified between the validation range and the scope of utilization (reactor case), introducing potential physical and numerical distortion effects. In this study, we simulate the flow of liquid sodium using the TrioCFD code, employing both higher-fidelity (HF) LES and lower-fidelity (LF) URANS models. We tackle respectively numerical uncertainties through the Grid Convergence Index method, and physical modelling uncertainties through the Polynomial Chaos Expansion method available on the URANIE platform. HF simulations are shown to exhibit a strong resilience to physical distortion effects, with numerical uncertainties being intricately correlated. Conversely, the LF approach, the only one applicable at the reactor scale, is likely to present a reduced predictability. If so, the HF approach should be effective in pinpointing the LF weaknesses: the concept of scaling uncertainty is inline introduced as the growth of the LF simulation uncertainty associated with distortion effects. Thus, the paper outlines that a specific methodology within the BEPU framework - leveraging both HF and LF approaches - could pragmatically enable correlating distortion effects with scaling uncertainty, thereby providing a metric principle.</summary></entry><entry><title type="html">Stagewise Boosting Distributional Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/StagewiseBoostingDistributionalRegression.html" rel="alternate" type="text/html" title="Stagewise Boosting Distributional Regression" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/StagewiseBoostingDistributionalRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/StagewiseBoostingDistributionalRegression.html">&lt;p&gt;Forward stagewise regression is a simple algorithm that can be used to estimate regularized models. The updating rule adds a small constant to a regression coefficient in each iteration, such that the underlying optimization problem is solved slowly with small improvements. This is similar to gradient boosting, with the essential difference that the step size is determined by the product of the gradient and a step length parameter in the latter algorithm. One often overlooked challenge in gradient boosting for distributional regression is the issue of a vanishing small gradient, which practically halts the algorithm’s progress. We show that gradient boosting in this case oftentimes results in suboptimal models, especially for complex problems certain distributional parameters are never updated due to the vanishing gradient. Therefore, we propose a stagewise boosting-type algorithm for distributional regression, combining stagewise regression ideas with gradient boosting. Additionally, we extend it with a novel regularization method, correlation filtering, to provide additional stability when the problem involves a large number of covariates. Furthermore, the algorithm includes best-subset selection for parameters and can be applied to big data problems by leveraging stochastic approximations of the updating steps. Besides the advantage of processing large datasets, the stochastic nature of the approximations can lead to better results, especially for complex distributions, by reducing the risk of being trapped in a local optimum. The performance of our proposed stagewise boosting distributional regression approach is investigated in an extensive simulation study and by estimating a full probabilistic model for lightning counts with data of more than 9.1 million observations and 672 covariates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18288&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mattias Wetscher, Johannes Seiler, Reto Stauffer, Nikolaus Umlauf</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Forward stagewise regression is a simple algorithm that can be used to estimate regularized models. The updating rule adds a small constant to a regression coefficient in each iteration, such that the underlying optimization problem is solved slowly with small improvements. This is similar to gradient boosting, with the essential difference that the step size is determined by the product of the gradient and a step length parameter in the latter algorithm. One often overlooked challenge in gradient boosting for distributional regression is the issue of a vanishing small gradient, which practically halts the algorithm’s progress. We show that gradient boosting in this case oftentimes results in suboptimal models, especially for complex problems certain distributional parameters are never updated due to the vanishing gradient. Therefore, we propose a stagewise boosting-type algorithm for distributional regression, combining stagewise regression ideas with gradient boosting. Additionally, we extend it with a novel regularization method, correlation filtering, to provide additional stability when the problem involves a large number of covariates. Furthermore, the algorithm includes best-subset selection for parameters and can be applied to big data problems by leveraging stochastic approximations of the updating steps. Besides the advantage of processing large datasets, the stochastic nature of the approximations can lead to better results, especially for complex distributions, by reducing the risk of being trapped in a local optimum. The performance of our proposed stagewise boosting distributional regression approach is investigated in an extensive simulation study and by estimating a full probabilistic model for lightning counts with data of more than 9.1 million observations and 672 covariates.</summary></entry><entry><title type="html">Stochastic Localization via Iterative Posterior Sampling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/StochasticLocalizationviaIterativePosteriorSampling.html" rel="alternate" type="text/html" title="Stochastic Localization via Iterative Posterior Sampling" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/StochasticLocalizationviaIterativePosteriorSampling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/StochasticLocalizationviaIterativePosteriorSampling.html">&lt;p&gt;Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, $\textit{Stochastic Localization via Iterative Posterior Sampling}$ (SLIPS), to obtain approximate samples of this dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimation of the denoiser and comes with detailed practical guidelines. We illustrate the benefits and applicability of SLIPS on several benchmarks of multi-modal distributions, including Gaussian mixtures in increasing dimensions, Bayesian logistic regression and a high-dimensional field system from statistical-mechanics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.10758&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Louis Grenioux, Maxence Noble, Marylou Gabrié, Alain Oliviero Durmus</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, $\textit{Stochastic Localization via Iterative Posterior Sampling}$ (SLIPS), to obtain approximate samples of this dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimation of the denoiser and comes with detailed practical guidelines. We illustrate the benefits and applicability of SLIPS on several benchmarks of multi-modal distributions, including Gaussian mixtures in increasing dimensions, Bayesian logistic regression and a high-dimensional field system from statistical-mechanics.</summary></entry><entry><title type="html">Tensor Methods in High Dimensional Data Analysis: Opportunities and Challenges</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TensorMethodsinHighDimensionalDataAnalysisOpportunitiesandChallenges.html" rel="alternate" type="text/html" title="Tensor Methods in High Dimensional Data Analysis: Opportunities and Challenges" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TensorMethodsinHighDimensionalDataAnalysisOpportunitiesandChallenges</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TensorMethodsinHighDimensionalDataAnalysisOpportunitiesandChallenges.html">&lt;p&gt;Large amount of multidimensional data represented by multiway arrays or tensors are prevalent in modern applications across various fields such as chemometrics, genomics, physics, psychology, and signal processing. The structural complexity of such data provides vast new opportunities for modeling and analysis, but efficiently extracting information content from them, both statistically and computationally, presents unique and fundamental challenges. Addressing these challenges requires an interdisciplinary approach that brings together tools and insights from statistics, optimization and numerical linear algebra among other fields. Despite these hurdles, significant progress has been made in the last decade. This review seeks to examine some of the key advancements and identify common threads among them, under eight different statistical settings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18412&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arnab Auddy, Dong Xia, Ming Yuan</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Large amount of multidimensional data represented by multiway arrays or tensors are prevalent in modern applications across various fields such as chemometrics, genomics, physics, psychology, and signal processing. The structural complexity of such data provides vast new opportunities for modeling and analysis, but efficiently extracting information content from them, both statistically and computationally, presents unique and fundamental challenges. Addressing these challenges requires an interdisciplinary approach that brings together tools and insights from statistics, optimization and numerical linear algebra among other fields. Despite these hurdles, significant progress has been made in the last decade. This review seeks to examine some of the key advancements and identify common threads among them, under eight different statistical settings.</summary></entry><entry><title type="html">The Multiplex $p_2$ Model: Mixed-Effects Modeling for Multiplex Social Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TheMultiplexp2ModelMixedEffectsModelingforMultiplexSocialNetworks.html" rel="alternate" type="text/html" title="The Multiplex $p_2$ Model: Mixed-Effects Modeling for Multiplex Social Networks" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TheMultiplexp2ModelMixedEffectsModelingforMultiplexSocialNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TheMultiplexp2ModelMixedEffectsModelingforMultiplexSocialNetworks.html">&lt;p&gt;Social actors are often embedded in multiple social networks, and there is a growing interest in studying social systems from a multiplex network perspective. In this paper, we propose a mixed-effects model for cross-sectional multiplex network data that assumes dyads to be conditionally independent. Building on the uniplex $p_2$ model, we incorporate dependencies between different network layers via cross-layer dyadic effects and actor random effects. These cross-layer effects model the tendencies for ties between two actors and the ties to and from the same actor to be dependent across different relational dimensions. The model can also study the effect of actor and dyad covariates. As simulation-based goodness-of-fit analyses are common practice in applied network studies, we here propose goodness-of-fit measures for multiplex network analyses. We evaluate our choice of priors and the computational faithfulness and inferential properties of the proposed method through simulation. We illustrate the utility of the multiplex $p_2$ model in a replication study of a toxic chemical policy network. An original study that reflects on gossip as perceived by gossip senders and gossip targets, and their differences in perspectives, based on data from 34 Hungarian elementary school classes, highlights the applicability of the proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17707&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anni Hong, Nynke M. D. Niezink</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Social actors are often embedded in multiple social networks, and there is a growing interest in studying social systems from a multiplex network perspective. In this paper, we propose a mixed-effects model for cross-sectional multiplex network data that assumes dyads to be conditionally independent. Building on the uniplex $p_2$ model, we incorporate dependencies between different network layers via cross-layer dyadic effects and actor random effects. These cross-layer effects model the tendencies for ties between two actors and the ties to and from the same actor to be dependent across different relational dimensions. The model can also study the effect of actor and dyad covariates. As simulation-based goodness-of-fit analyses are common practice in applied network studies, we here propose goodness-of-fit measures for multiplex network analyses. We evaluate our choice of priors and the computational faithfulness and inferential properties of the proposed method through simulation. We illustrate the utility of the multiplex $p_2$ model in a replication study of a toxic chemical policy network. An original study that reflects on gossip as perceived by gossip senders and gossip targets, and their differences in perspectives, based on data from 34 Hungarian elementary school classes, highlights the applicability of the proposed method.</summary></entry><entry><title type="html">The association between environmental variables and short-term mortality: evidence from Europe</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TheassociationbetweenenvironmentalvariablesandshorttermmortalityevidencefromEurope.html" rel="alternate" type="text/html" title="The association between environmental variables and short-term mortality: evidence from Europe" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TheassociationbetweenenvironmentalvariablesandshorttermmortalityevidencefromEurope</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TheassociationbetweenenvironmentalvariablesandshorttermmortalityevidencefromEurope.html">&lt;p&gt;Using fine-grained, publicly available data, this paper studies the association between environmental factors, i.e., variables capturing weather and air pollution characteristics, and weekly mortality rates in small geographical regions in Europe. Hereto, we develop a mortality modelling framework where a baseline captures a region-specific, seasonal historical trend observed within the weekly mortality rates. Using a machine learning algorithm, we then explain deviations from this baseline using anomalies and extreme indices constructed from the environmental data. We illustrate our proposed modelling framework through a case study on more than 550 NUTS 3 regions (Nomenclature of Territorial Units for Statistics, level 3) located in 20 different European countries. Through interpretation tools, we unravel insights into which environmental features are most important when estimating excess or deficit mortality with respect to the baseline and explore how these features interact. Moreover, we investigate harvesting effects of the environmental features through our constructed weekly mortality modelling framework. Our findings show that temperature-related features exert the most significant influence in explaining deviations in mortality from the baseline. Furthermore, we find that environmental features prove particularly beneficial in southern regions for explaining elevated levels of mortality over short time periods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18020&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jens Robben, Katrien Antonio, Torsten Kleinow</name></author><category term="stat.AP" /><summary type="html">Using fine-grained, publicly available data, this paper studies the association between environmental factors, i.e., variables capturing weather and air pollution characteristics, and weekly mortality rates in small geographical regions in Europe. Hereto, we develop a mortality modelling framework where a baseline captures a region-specific, seasonal historical trend observed within the weekly mortality rates. Using a machine learning algorithm, we then explain deviations from this baseline using anomalies and extreme indices constructed from the environmental data. We illustrate our proposed modelling framework through a case study on more than 550 NUTS 3 regions (Nomenclature of Territorial Units for Statistics, level 3) located in 20 different European countries. Through interpretation tools, we unravel insights into which environmental features are most important when estimating excess or deficit mortality with respect to the baseline and explore how these features interact. Moreover, we investigate harvesting effects of the environmental features through our constructed weekly mortality modelling framework. Our findings show that temperature-related features exert the most significant influence in explaining deviations in mortality from the baseline. Furthermore, we find that environmental features prove particularly beneficial in southern regions for explaining elevated levels of mortality over short time periods.</summary></entry><entry><title type="html">TimeGPT-1</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TimeGPT1.html" rel="alternate" type="text/html" title="TimeGPT-1" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TimeGPT1</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TimeGPT1.html">&lt;p&gt;In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.03589&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Azul Garza, Cristian Challu, Max Mergenthaler-Canseco</name></author><category term="stat.AP" /><summary type="html">In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.</summary></entry><entry><title type="html">Towards Efficient Disaster Response via Cost-effective Unbiased Class Rate Estimation through Neyman Allocation Stratified Sampling Active Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TowardsEfficientDisasterResponseviaCosteffectiveUnbiasedClassRateEstimationthroughNeymanAllocationStratifiedSamplingActiveLearning.html" rel="alternate" type="text/html" title="Towards Efficient Disaster Response via Cost-effective Unbiased Class Rate Estimation through Neyman Allocation Stratified Sampling Active Learning" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TowardsEfficientDisasterResponseviaCosteffectiveUnbiasedClassRateEstimationthroughNeymanAllocationStratifiedSamplingActiveLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TowardsEfficientDisasterResponseviaCosteffectiveUnbiasedClassRateEstimationthroughNeymanAllocationStratifiedSamplingActiveLearning.html">&lt;p&gt;With the rapid development of earth observation technology, we have entered an era of massively available satellite remote-sensing data. However, a large amount of satellite remote sensing data lacks a label or the label cost is too high to hinder the potential of AI technology mining satellite data. Especially in such an emergency response scenario that uses satellite data to evaluate the degree of disaster damage. Disaster damage assessment encountered bottlenecks due to excessive focus on the damage of a certain building in a specific geographical space or a certain area on a larger scale. In fact, in the early days of disaster emergency response, government departments were more concerned about the overall damage rate of the disaster area instead of single-building damage, because this helps the government decide the level of emergency response. We present an innovative algorithm that constructs Neyman stratified random sampling trees for binary classification and extends this approach to multiclass problems. Through extensive experimentation on various datasets and model structures, our findings demonstrate that our method surpasses both passive and conventional active learning techniques in terms of class rate estimation and model enhancement with only 30\%-60\% of the annotation cost of simple sampling. It effectively addresses the ‘sampling bias’ challenge in traditional active learning strategies and mitigates the ‘cold start’ dilemma. The efficacy of our approach is further substantiated through application to disaster evaluation tasks using Xview2 Satellite imagery, showcasing its practical utility in real-world contexts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17734&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yanbing Bai, Xinyi Wu, Lai Xu, Jihan Pei, Erick Mas, Shunichi Koshimura</name></author><category term="stat.AP" /><summary type="html">With the rapid development of earth observation technology, we have entered an era of massively available satellite remote-sensing data. However, a large amount of satellite remote sensing data lacks a label or the label cost is too high to hinder the potential of AI technology mining satellite data. Especially in such an emergency response scenario that uses satellite data to evaluate the degree of disaster damage. Disaster damage assessment encountered bottlenecks due to excessive focus on the damage of a certain building in a specific geographical space or a certain area on a larger scale. In fact, in the early days of disaster emergency response, government departments were more concerned about the overall damage rate of the disaster area instead of single-building damage, because this helps the government decide the level of emergency response. We present an innovative algorithm that constructs Neyman stratified random sampling trees for binary classification and extends this approach to multiclass problems. Through extensive experimentation on various datasets and model structures, our findings demonstrate that our method surpasses both passive and conventional active learning techniques in terms of class rate estimation and model enhancement with only 30\%-60\% of the annotation cost of simple sampling. It effectively addresses the ‘sampling bias’ challenge in traditional active learning strategies and mitigates the ‘cold start’ dilemma. The efficacy of our approach is further substantiated through application to disaster evaluation tasks using Xview2 Satellite imagery, showcasing its practical utility in real-world contexts.</summary></entry><entry><title type="html">Towards a Low-SWaP 1024-beam Digital Array: A 32-beam Sub-system at 5.8 GHz</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TowardsaLowSWaP1024beamDigitalArrayA32beamSubsystemat58GHz.html" rel="alternate" type="text/html" title="Towards a Low-SWaP 1024-beam Digital Array: A 32-beam Sub-system at 5.8 GHz" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TowardsaLowSWaP1024beamDigitalArrayA32beamSubsystemat58GHz</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TowardsaLowSWaP1024beamDigitalArrayA32beamSubsystemat58GHz.html">&lt;p&gt;Millimeter wave communications require multibeam beamforming in order to utilize wireless channels that suffer from obstructions, path loss, and multi-path effects. Digital multibeam beamforming has maximum degrees of freedom compared to analog phased arrays. However, circuit complexity and power consumption are important constraints for digital multibeam systems. A low-complexity digital computing architecture is proposed for a multiplication-free 32-point linear transform that approximates multiple simultaneous RF beams similar to a discrete Fourier transform (DFT). Arithmetic complexity due to multiplication is reduced from the FFT complexity of $\mathcal{O}(N: \log N)$ for DFT realizations, down to zero, thus yielding a 46% and 55% reduction in chip area and dynamic power consumption, respectively, for the $N=32$ case considered. The paper describes the proposed 32-point DFT approximation targeting a 1024-beams using a 2D array, and shows the multiplierless approximation and its mapping to a 32-beam sub-system consisting of 5.8 GHz antennas that can be used for generating 1024 digital beams without multiplications. Real-time beam computation is achieved using a Xilinx FPGA at 120 MHz bandwidth per beam. Theoretical beam performance is compared with measured RF patterns from both a fixed-point FFT as well as the proposed multiplier-free algorithm and are in good agreement.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2207.09054&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arjuna Madanayake, Viduneth Ariyarathna, Suresh Madishetty, Sravan Pulipati, R. J. Cintra, Diego Coelho, Raíza Oliveira, Fábio M. Bayer, Leonid Belostotski, Soumyajit Mandal, Theodore S. Rappaport</name></author><category term="stat.ME" /><summary type="html">Millimeter wave communications require multibeam beamforming in order to utilize wireless channels that suffer from obstructions, path loss, and multi-path effects. Digital multibeam beamforming has maximum degrees of freedom compared to analog phased arrays. However, circuit complexity and power consumption are important constraints for digital multibeam systems. A low-complexity digital computing architecture is proposed for a multiplication-free 32-point linear transform that approximates multiple simultaneous RF beams similar to a discrete Fourier transform (DFT). Arithmetic complexity due to multiplication is reduced from the FFT complexity of $\mathcal{O}(N: \log N)$ for DFT realizations, down to zero, thus yielding a 46% and 55% reduction in chip area and dynamic power consumption, respectively, for the $N=32$ case considered. The paper describes the proposed 32-point DFT approximation targeting a 1024-beams using a 2D array, and shows the multiplierless approximation and its mapping to a 32-beam sub-system consisting of 5.8 GHz antennas that can be used for generating 1024 digital beams without multiplications. Real-time beam computation is achieved using a Xilinx FPGA at 120 MHz bandwidth per beam. Theoretical beam performance is compared with measured RF patterns from both a fixed-point FFT as well as the proposed multiplier-free algorithm and are in good agreement.</summary></entry><entry><title type="html">Towards the use of multiple ROIs for radiomics-based survival modelling: finding a strategy of aggregating lesions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TowardstheuseofmultipleROIsforradiomicsbasedsurvivalmodellingfindingastrategyofaggregatinglesions.html" rel="alternate" type="text/html" title="Towards the use of multiple ROIs for radiomics-based survival modelling: finding a strategy of aggregating lesions" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TowardstheuseofmultipleROIsforradiomicsbasedsurvivalmodellingfindingastrategyofaggregatinglesions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TowardstheuseofmultipleROIsforradiomicsbasedsurvivalmodellingfindingastrategyofaggregatinglesions.html">&lt;p&gt;The main objective of this work is to explore the possibility of incorporating radiomic information from multiple lesions into survival models. We hypothesise that when more lesions are present, their inclusion can improve model performance, and we aim to find an optimal strategy for using multiple distinct regions in modelling.
  The idea of using multiple regions of interest (ROIs) to extract radiomic features for predictive models has been implemented in many recent works. However, in almost all studies, analogous regions were segmented according to particular criteria for all patients – for example, the primary tumour and peritumoral area, or subregions of the primary tumour. They can be included in a model in a straightforward way as additional features. A more interesting scenario occurs when multiple distinct ROIs are present, such as multiple lesions in a regionally disseminated cancer. Since the number of such regions may differ between patients, their inclusion in a model is non-trivial and requires additional processing steps.
  We proposed several methods of handling multiple ROIs representing either ROI or risk aggregation strategy, compared them to a published one, and evaluated their performance in different classes of survival models in a Monte Carlo Cross-Validation scheme. We demonstrated the effectiveness of the methods using a cohort of 115 non-small cell lung cancer patients, for whom we predicted the metastasis risk based on features extracted from PET images in original resolution or interpolated to CT image resolution. For both feature sets, incorporating all available lesions, as opposed to a singular ROI representing the primary tumour, allowed for considerable improvement of predictive ability regardless of the model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17668&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Agata Ma{\l}gorzata Wilk, Andrzej Swierniak, Andrea d&apos;Amico, Rafa{\l} Suwiński, Krzysztof Fujarewicz, Damian Borys</name></author><category term="stat.AP" /><summary type="html">The main objective of this work is to explore the possibility of incorporating radiomic information from multiple lesions into survival models. We hypothesise that when more lesions are present, their inclusion can improve model performance, and we aim to find an optimal strategy for using multiple distinct regions in modelling. The idea of using multiple regions of interest (ROIs) to extract radiomic features for predictive models has been implemented in many recent works. However, in almost all studies, analogous regions were segmented according to particular criteria for all patients – for example, the primary tumour and peritumoral area, or subregions of the primary tumour. They can be included in a model in a straightforward way as additional features. A more interesting scenario occurs when multiple distinct ROIs are present, such as multiple lesions in a regionally disseminated cancer. Since the number of such regions may differ between patients, their inclusion in a model is non-trivial and requires additional processing steps. We proposed several methods of handling multiple ROIs representing either ROI or risk aggregation strategy, compared them to a published one, and evaluated their performance in different classes of survival models in a Monte Carlo Cross-Validation scheme. We demonstrated the effectiveness of the methods using a cohort of 115 non-small cell lung cancer patients, for whom we predicted the metastasis risk based on features extracted from PET images in original resolution or interpolated to CT image resolution. For both feature sets, incorporating all available lesions, as opposed to a singular ROI representing the primary tumour, allowed for considerable improvement of predictive ability regardless of the model.</summary></entry><entry><title type="html">Trajectory-Based Individualized Treatment Rules</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TrajectoryBasedIndividualizedTreatmentRules.html" rel="alternate" type="text/html" title="Trajectory-Based Individualized Treatment Rules" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TrajectoryBasedIndividualizedTreatmentRules</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/TrajectoryBasedIndividualizedTreatmentRules.html">&lt;p&gt;A core component of precision medicine research involves optimizing individualized treatment rules (ITRs) based on patient characteristics. Many studies used to estimate ITRs are longitudinal in nature, collecting outcomes over time. Yet, to date, methods developed to estimate ITRs often ignore the longitudinal structure of the data. Information available from the longitudinal nature of the data can be especially useful in mental health studies. Although treatment means might appear similar, understanding the trajectory of outcomes over time can reveal important differences between treatments and placebo effects. This longitudinal perspective is especially beneficial in mental health research, where subtle shifts in outcome patterns can hold significant implications. Despite numerous studies involving the collection of outcome data across various time points, most precision medicine methods used to develop ITRs overlook the information available from the longitudinal structure. The prevalence of missing data in such studies exacerbates the issue, as neglecting the longitudinal nature of the data can significantly impair the effectiveness of treatment rules. This paper develops a powerful longitudinal trajectory-based ITR construction method that incorporates baseline variables, via a single-index or biosignature, into the modeling of longitudinal outcomes. This trajectory-based ITR approach substantially minimizes the negative impact of missing data compared to more traditional ITR approaches. The approach is illustrated through simulation studies and a clinical trial for depression, contrasting it with more traditional ITRs that ignore longitudinal information.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09810&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lanqiu Yao, Thaddeus Tarpey</name></author><category term="stat.ME" /><summary type="html">A core component of precision medicine research involves optimizing individualized treatment rules (ITRs) based on patient characteristics. Many studies used to estimate ITRs are longitudinal in nature, collecting outcomes over time. Yet, to date, methods developed to estimate ITRs often ignore the longitudinal structure of the data. Information available from the longitudinal nature of the data can be especially useful in mental health studies. Although treatment means might appear similar, understanding the trajectory of outcomes over time can reveal important differences between treatments and placebo effects. This longitudinal perspective is especially beneficial in mental health research, where subtle shifts in outcome patterns can hold significant implications. Despite numerous studies involving the collection of outcome data across various time points, most precision medicine methods used to develop ITRs overlook the information available from the longitudinal structure. The prevalence of missing data in such studies exacerbates the issue, as neglecting the longitudinal nature of the data can significantly impair the effectiveness of treatment rules. This paper develops a powerful longitudinal trajectory-based ITR construction method that incorporates baseline variables, via a single-index or biosignature, into the modeling of longitudinal outcomes. This trajectory-based ITR approach substantially minimizes the negative impact of missing data compared to more traditional ITR approaches. The approach is illustrated through simulation studies and a clinical trial for depression, contrasting it with more traditional ITRs that ignore longitudinal information.</summary></entry><entry><title type="html">Unifying Perspectives: Plausible Counterfactual Explanations on Global, Group-wise, and Local Levels</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/UnifyingPerspectivesPlausibleCounterfactualExplanationsonGlobalGroupwiseandLocalLevels.html" rel="alternate" type="text/html" title="Unifying Perspectives: Plausible Counterfactual Explanations on Global, Group-wise, and Local Levels" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/UnifyingPerspectivesPlausibleCounterfactualExplanationsonGlobalGroupwiseandLocalLevels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/UnifyingPerspectivesPlausibleCounterfactualExplanationsonGlobalGroupwiseandLocalLevels.html">&lt;p&gt;Growing regulatory and societal pressures demand increased transparency in AI, particularly in understanding the decisions made by complex machine learning models. Counterfactual Explanations (CFs) have emerged as a promising technique within Explainable AI (xAI), offering insights into individual model predictions. However, to understand the systemic biases and disparate impacts of AI models, it is crucial to move beyond local CFs and embrace global explanations, which offer a~holistic view across diverse scenarios and populations. Unfortunately, generating Global Counterfactual Explanations (GCEs) faces challenges in computational complexity, defining the scope of “global,” and ensuring the explanations are both globally representative and locally plausible. We introduce a novel unified approach for generating Local, Group-wise, and Global Counterfactual Explanations for differentiable classification models via gradient-based optimization to address these challenges. This framework aims to bridge the gap between individual and systemic insights, enabling a deeper understanding of model decisions and their potential impact on diverse populations. Our approach further innovates by incorporating a probabilistic plausibility criterion, enhancing actionability and trustworthiness. By offering a cohesive solution to the optimization and plausibility challenges in GCEs, our work significantly advances the interpretability and accountability of AI models, marking a step forward in the pursuit of transparent AI.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17642&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Patryk Wielopolski, Oleksii Furman, Jerzy Stefanowski, Maciej Zięba</name></author><category term="stat.ME" /><summary type="html">Growing regulatory and societal pressures demand increased transparency in AI, particularly in understanding the decisions made by complex machine learning models. Counterfactual Explanations (CFs) have emerged as a promising technique within Explainable AI (xAI), offering insights into individual model predictions. However, to understand the systemic biases and disparate impacts of AI models, it is crucial to move beyond local CFs and embrace global explanations, which offer a~holistic view across diverse scenarios and populations. Unfortunately, generating Global Counterfactual Explanations (GCEs) faces challenges in computational complexity, defining the scope of “global,” and ensuring the explanations are both globally representative and locally plausible. We introduce a novel unified approach for generating Local, Group-wise, and Global Counterfactual Explanations for differentiable classification models via gradient-based optimization to address these challenges. This framework aims to bridge the gap between individual and systemic insights, enabling a deeper understanding of model decisions and their potential impact on diverse populations. Our approach further innovates by incorporating a probabilistic plausibility criterion, enhancing actionability and trustworthiness. By offering a cohesive solution to the optimization and plausibility challenges in GCEs, our work significantly advances the interpretability and accountability of AI models, marking a step forward in the pursuit of transparent AI.</summary></entry><entry><title type="html">Unraveling Factors Influencing Shooting Incidents: Preliminary Analysis and Insights</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/UnravelingFactorsInfluencingShootingIncidentsPreliminaryAnalysisandInsights.html" rel="alternate" type="text/html" title="Unraveling Factors Influencing Shooting Incidents: Preliminary Analysis and Insights" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/UnravelingFactorsInfluencingShootingIncidentsPreliminaryAnalysisandInsights</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/UnravelingFactorsInfluencingShootingIncidentsPreliminaryAnalysisandInsights.html">&lt;p&gt;The following is a write up of the progress of modeling data from the K12 organization \cite{Riedman_2023}. Data was characterized and investigated for statistically significant factors. The incident data was spilt into three sets: the entire set of incidents, incidents from 1966 - 2017, and incidents from 2018 - 2023. This was done in an attempt to discern key factors for the acceleration of incidents over the last several years. The data set was cleaned and processed primarily through RStudio. The individual factors were studied and subjected to statistical analysis where appropriate. As it turns out, there are differences between media portrayals of shooters and actual shooters. Then, multiple regression techniques were performed then followed by ANOVA of the models to determine statistically significant independent variables and their influence on casualties. Thus far, linear regression and negative binomial regression have been attempted. Further refining of the methods will be necessary for Poisson regression and logistic regression to be viably attempted. At this point in time a common theme among each of the models is the presence of targeted attacks affecting casualties. Further study can lead to improved safe guarding strategies to eliminate or minimize casualties. Further, increased understanding of shooter demographics can also lead to outreach and prevention programs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18271&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Robert Hernandez, Linn Carothers</name></author><category term="stat.AP" /><summary type="html">The following is a write up of the progress of modeling data from the K12 organization \cite{Riedman_2023}. Data was characterized and investigated for statistically significant factors. The incident data was spilt into three sets: the entire set of incidents, incidents from 1966 - 2017, and incidents from 2018 - 2023. This was done in an attempt to discern key factors for the acceleration of incidents over the last several years. The data set was cleaned and processed primarily through RStudio. The individual factors were studied and subjected to statistical analysis where appropriate. As it turns out, there are differences between media portrayals of shooters and actual shooters. Then, multiple regression techniques were performed then followed by ANOVA of the models to determine statistically significant independent variables and their influence on casualties. Thus far, linear regression and negative binomial regression have been attempted. Further refining of the methods will be necessary for Poisson regression and logistic regression to be viably attempted. At this point in time a common theme among each of the models is the presence of targeted attacks affecting casualties. Further study can lead to improved safe guarding strategies to eliminate or minimize casualties. Further, increased understanding of shooter demographics can also lead to outreach and prevention programs.</summary></entry><entry><title type="html">ZIKQ: An innovative centile chart method for utilizing natural history data in rare disease clinical development</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ZIKQAninnovativecentilechartmethodforutilizingnaturalhistorydatainrarediseaseclinicaldevelopment.html" rel="alternate" type="text/html" title="ZIKQ: An innovative centile chart method for utilizing natural history data in rare disease clinical development" /><published>2024-05-29T00:00:00+00:00</published><updated>2024-05-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ZIKQAninnovativecentilechartmethodforutilizingnaturalhistorydatainrarediseaseclinicaldevelopment</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/29/ZIKQAninnovativecentilechartmethodforutilizingnaturalhistorydatainrarediseaseclinicaldevelopment.html">&lt;p&gt;Utilizing natural history data as external control plays an important role in the clinical development of rare diseases, since placebo groups in double-blind randomization trials may not be available due to ethical reasons and low disease prevalence. This article proposed an innovative approach for utilizing natural history data to support rare disease clinical development by constructing reference centile charts. Due to the deterioration nature of certain rare diseases, the distributions of clinical endpoints can be age-dependent and have an absorbing state of zero, which can result in censored natural history data. Existing methods of reference centile charts can not be directly used in the censored natural history data. Therefore, we propose a new calibrated zero-inflated kernel quantile (ZIKQ) estimation to construct reference centile charts from censored natural history data. Using the application to Duchenne Muscular Dystrophy drug development, we demonstrate that the reference centile charts using the ZIKQ method can be implemented to evaluate treatment efficacy and facilitate a more targeted patient enrollment in rare disease clinical development.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17684&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tianying Wang, Wenfei Zhang, Ying Wei</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Utilizing natural history data as external control plays an important role in the clinical development of rare diseases, since placebo groups in double-blind randomization trials may not be available due to ethical reasons and low disease prevalence. This article proposed an innovative approach for utilizing natural history data to support rare disease clinical development by constructing reference centile charts. Due to the deterioration nature of certain rare diseases, the distributions of clinical endpoints can be age-dependent and have an absorbing state of zero, which can result in censored natural history data. Existing methods of reference centile charts can not be directly used in the censored natural history data. Therefore, we propose a new calibrated zero-inflated kernel quantile (ZIKQ) estimation to construct reference centile charts from censored natural history data. Using the application to Duchenne Muscular Dystrophy drug development, we demonstrate that the reference centile charts using the ZIKQ method can be implemented to evaluate treatment efficacy and facilitate a more targeted patient enrollment in rare disease clinical development.</summary></entry></feed>