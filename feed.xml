<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-25T07:13:59+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html"></title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/2024-06-25-Influenceanalysesofdesignsforevaluatinginconsistencyinnetworkmetaanalysis.html" rel="alternate" type="text/html" title="" /><published>2024-06-25T07:13:59+00:00</published><updated>2024-06-25T07:13:59+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/2024-06-25-Influenceanalysesofdesignsforevaluatinginconsistencyinnetworkmetaanalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/2024-06-25-Influenceanalysesofdesignsforevaluatinginconsistencyinnetworkmetaanalysis.html">&lt;p&gt;Network meta-analysis is an evidence synthesis method for comparative effectiveness analyses of multiple available treatments. To justify evidence synthesis, consistency is a relevant assumption; however, existing methods founded on statistical testing possibly have substantial limitations of statistical powers or several drawbacks in treating multi-arm studies. Besides, inconsistency is theoretically explained as design-by-treatment interactions, and the primary purpose of these analyses is prioritizing “designs” for further investigations to explore sources of biases and irregular issues that might influence the overall results. In this article, we propose an alternative framework for inconsistency evaluations using influence diagnostic methods that enable quantitative evaluations of the influences of individual designs to the overall results. We provide four new methods to quantify the influences of individual designs through a “leave-one-design-out” analysis framework. We also propose a simple summary measure, the O-value, for prioritizing designs and interpreting these influential analyses straightforwardly. Furthermore, we propose another testing approach based on the leave-one-design-out analysis framework. By applying the new methods to a network meta-analysis of antihypertensive drugs, we demonstrate the new methods located potential sources of inconsistency accurately. The proposed methods provide new insights into alternatives to existing test-based methods, especially quantifications of influences of individual designs on the overall network meta-analysis results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16485&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Aliverti</name></author></entry><entry><title type="html">Accommodating informative visit times for analysing irregular longitudinal data: a sensitivity analysis approach with balancing weights estimators</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Accommodatinginformativevisittimesforanalysingirregularlongitudinaldataasensitivityanalysisapproachwithbalancingweightsestimators.html" rel="alternate" type="text/html" title="Accommodating informative visit times for analysing irregular longitudinal data: a sensitivity analysis approach with balancing weights estimators" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Accommodatinginformativevisittimesforanalysingirregularlongitudinaldataasensitivityanalysisapproachwithbalancingweightsestimators</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Accommodatinginformativevisittimesforanalysingirregularlongitudinaldataasensitivityanalysisapproachwithbalancingweightsestimators.html">&lt;p&gt;Irregular longitudinal data with informative visit times arise when patients’ visits are partly driven by concurrent disease outcomes. However, existing methods such as inverse intensity weighting (IIW), often overlook or have not adequately assess the influence of informative visit times on estimation and inference. Based on novel balancing weights estimators, we propose a new sensitivity analysis approach to addressing informative visit times within the IIW framework. The balancing weights are obtained by balancing observed history variable distributions over time and including a selection function with specified sensitivity parameters to characterise the additional influence of the concurrent outcome on the visit process. A calibration procedure is proposed to anchor the range of the sensitivity parameters to the amount of variation in the visit process that could be additionally explained by the concurrent outcome given the observed history and time. Simulations demonstrate that our balancing weights estimators outperform existing weighted estimators for robustness and efficiency. We provide an R Markdown tutorial of the proposed methods and apply them to analyse data from a clinic-based cohort of psoriatic arthritis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.16018&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sean Yiu, Li Su</name></author><category term="stat.ME" /><summary type="html">Irregular longitudinal data with informative visit times arise when patients’ visits are partly driven by concurrent disease outcomes. However, existing methods such as inverse intensity weighting (IIW), often overlook or have not adequately assess the influence of informative visit times on estimation and inference. Based on novel balancing weights estimators, we propose a new sensitivity analysis approach to addressing informative visit times within the IIW framework. The balancing weights are obtained by balancing observed history variable distributions over time and including a selection function with specified sensitivity parameters to characterise the additional influence of the concurrent outcome on the visit process. A calibration procedure is proposed to anchor the range of the sensitivity parameters to the amount of variation in the visit process that could be additionally explained by the concurrent outcome given the observed history and time. Simulations demonstrate that our balancing weights estimators outperform existing weighted estimators for robustness and efficiency. We provide an R Markdown tutorial of the proposed methods and apply them to analyse data from a clinic-based cohort of psoriatic arthritis.</summary></entry><entry><title type="html">Adaptive Online Experimental Design for Causal Discovery</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/AdaptiveOnlineExperimentalDesignforCausalDiscovery.html" rel="alternate" type="text/html" title="Adaptive Online Experimental Design for Causal Discovery" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/AdaptiveOnlineExperimentalDesignforCausalDiscovery</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/AdaptiveOnlineExperimentalDesignforCausalDiscovery.html">&lt;p&gt;Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11548&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Muhammad Qasim Elahi, Lai Wei, Murat Kocaoglu, Mahsa Ghasemi</name></author><category term="stat.AP" /><summary type="html">Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.</summary></entry><entry><title type="html">Adjusting for Selection Bias Due to Missing Eligibility Criteria in Emulated Target Trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/AdjustingforSelectionBiasDuetoMissingEligibilityCriteriainEmulatedTargetTrials.html" rel="alternate" type="text/html" title="Adjusting for Selection Bias Due to Missing Eligibility Criteria in Emulated Target Trials" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/AdjustingforSelectionBiasDuetoMissingEligibilityCriteriainEmulatedTargetTrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/AdjustingforSelectionBiasDuetoMissingEligibilityCriteriainEmulatedTargetTrials.html">&lt;p&gt;Target trial emulation (TTE) is a popular framework for observational studies based on electronic health records (EHR). A key component of this framework is determining the patient population eligible for inclusion in both a target trial of interest and its observational emulation. Missingness in variables that define eligibility criteria, however, presents a major challenge towards determining the eligible population when emulating a target trial with an observational study. In practice, patients with incomplete data are almost always excluded from analysis despite the possibility of selection bias, which can arise when subjects with observed eligibility data are fundamentally different than excluded subjects. Despite this, to the best of our knowledge, very little work has been done to mitigate this concern. In this paper, we propose a novel conceptual framework to address selection bias in TTE studies, tailored towards time-to-event endpoints, and describe estimation and inferential procedures via inverse probability weighting (IPW). Under an EHR-based simulation infrastructure, developed to reflect the complexity of EHR data, we characterize common settings under which missing eligibility data poses the threat of selection bias and investigate the ability of the proposed methods to address it. Finally, using EHR databases from Kaiser Permanente, we demonstrate the use of our method to evaluate the effect of bariatric surgery on microvascular outcomes among a cohort of severely obese patients with Type II diabetes mellitus (T2DM).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16830&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luke Benz, Rajarshi Mukherjee, Issa Dahabreh, Rui Wang, David Arterburn, Catherine Lee, Heidi Fischer, Susan Shortreed, Sebastien Haneuse</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Target trial emulation (TTE) is a popular framework for observational studies based on electronic health records (EHR). A key component of this framework is determining the patient population eligible for inclusion in both a target trial of interest and its observational emulation. Missingness in variables that define eligibility criteria, however, presents a major challenge towards determining the eligible population when emulating a target trial with an observational study. In practice, patients with incomplete data are almost always excluded from analysis despite the possibility of selection bias, which can arise when subjects with observed eligibility data are fundamentally different than excluded subjects. Despite this, to the best of our knowledge, very little work has been done to mitigate this concern. In this paper, we propose a novel conceptual framework to address selection bias in TTE studies, tailored towards time-to-event endpoints, and describe estimation and inferential procedures via inverse probability weighting (IPW). Under an EHR-based simulation infrastructure, developed to reflect the complexity of EHR data, we characterize common settings under which missing eligibility data poses the threat of selection bias and investigate the ability of the proposed methods to address it. Finally, using EHR databases from Kaiser Permanente, we demonstrate the use of our method to evaluate the effect of bariatric surgery on microvascular outcomes among a cohort of severely obese patients with Type II diabetes mellitus (T2DM).</summary></entry><entry><title type="html">Approximate Bayesian Computation sequential Monte Carlo via random forests</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ApproximateBayesianComputationsequentialMonteCarloviarandomforests.html" rel="alternate" type="text/html" title="Approximate Bayesian Computation sequential Monte Carlo via random forests" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ApproximateBayesianComputationsequentialMonteCarloviarandomforests</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ApproximateBayesianComputationsequentialMonteCarloviarandomforests.html">&lt;p&gt;Approximate Bayesian Computation (ABC) is a popular inference method when likelihoods are hard to come by. Practical bottlenecks of ABC applications include selecting statistics that summarize the data without losing too much information or introducing uncertainty, and choosing distance functions and tolerance thresholds that balance accuracy and computational efficiency. Recent studies have shown that ABC methods using random forest (RF) methodology perform well while circumventing many of ABC’s drawbacks. However, RF construction is computationally expensive for large numbers of trees and model simulations, and there can be high uncertainty in the posterior if the prior distribution is uninformative. Here we adapt distributional random forests to the ABC setting, and introduce Approximate Bayesian Computation sequential Monte Carlo with random forests (ABC-SMC-(D)RF). This updates the prior distribution iteratively to focus on the most likely regions in the parameter space. We show that ABC-SMC-(D)RF can accurately infer posterior distributions for a wide range of deterministic and stochastic models in different scientific areas.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15865&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Khanh N. Dinh, Zijin Xiang, Zhihan Liu, Simon Tavaré</name></author><category term="stat.CO" /><summary type="html">Approximate Bayesian Computation (ABC) is a popular inference method when likelihoods are hard to come by. Practical bottlenecks of ABC applications include selecting statistics that summarize the data without losing too much information or introducing uncertainty, and choosing distance functions and tolerance thresholds that balance accuracy and computational efficiency. Recent studies have shown that ABC methods using random forest (RF) methodology perform well while circumventing many of ABC’s drawbacks. However, RF construction is computationally expensive for large numbers of trees and model simulations, and there can be high uncertainty in the posterior if the prior distribution is uninformative. Here we adapt distributional random forests to the ABC setting, and introduce Approximate Bayesian Computation sequential Monte Carlo with random forests (ABC-SMC-(D)RF). This updates the prior distribution iteratively to focus on the most likely regions in the parameter space. We show that ABC-SMC-(D)RF can accurately infer posterior distributions for a wide range of deterministic and stochastic models in different scientific areas.</summary></entry><entry><title type="html">Assessing the Unobserved: Enhancing Causal Inference in Sociology with Sensitivity Analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/AssessingtheUnobservedEnhancingCausalInferenceinSociologywithSensitivityAnalysis.html" rel="alternate" type="text/html" title="Assessing the Unobserved: Enhancing Causal Inference in Sociology with Sensitivity Analysis" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/AssessingtheUnobservedEnhancingCausalInferenceinSociologywithSensitivityAnalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/AssessingtheUnobservedEnhancingCausalInferenceinSociologywithSensitivityAnalysis.html">&lt;p&gt;Explaining social events is a primary objective of applied data-driven sociology. To achieve that objective, many sociologists use statistical causal inference to identify causality using observational studies research context where the analyst does not control the data generating process. However, it is often challenging in observation studies to satisfy the unmeasured confounding assumption, namely, that there is no lurking third variable affecting the causal relationship of interest. In this article, we develop a framework enabling sociologists to employ a different strategy to enhance the quality of observational studies. Our framework builds on a surprisingly simple statistical approach, sensitivity analysis: a thought-experimental framework where the analyst imagines a lever, which they can pull for probing a variety of theoretically driven statistical magnitudes of posited unmeasured confounding which in turn distorts the causal effect of interest. By pulling that lever, the analyst can identify how strong an unmeasured confounder must be to wash away the estimated causal effect. Although each sensitivity analysis method requires its own assumptions, this sort of post-hoc analysis provides underutilized tools to bound causal quantities. Extending Lundberg et al, we develop a five-step approach to how applied sociological research can incorporate sensitivity analysis, empowering scholars to rejuvenate causal inference in observational studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.13410&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Cheng Lin, Jose M. Pena, Adel Daoud</name></author><category term="stat.ME" /><summary type="html">Explaining social events is a primary objective of applied data-driven sociology. To achieve that objective, many sociologists use statistical causal inference to identify causality using observational studies research context where the analyst does not control the data generating process. However, it is often challenging in observation studies to satisfy the unmeasured confounding assumption, namely, that there is no lurking third variable affecting the causal relationship of interest. In this article, we develop a framework enabling sociologists to employ a different strategy to enhance the quality of observational studies. Our framework builds on a surprisingly simple statistical approach, sensitivity analysis: a thought-experimental framework where the analyst imagines a lever, which they can pull for probing a variety of theoretically driven statistical magnitudes of posited unmeasured confounding which in turn distorts the causal effect of interest. By pulling that lever, the analyst can identify how strong an unmeasured confounder must be to wash away the estimated causal effect. Although each sensitivity analysis method requires its own assumptions, this sort of post-hoc analysis provides underutilized tools to bound causal quantities. Extending Lundberg et al, we develop a five-step approach to how applied sociological research can incorporate sensitivity analysis, empowering scholars to rejuvenate causal inference in observational studies.</summary></entry><entry><title type="html">Bayesian modeling of multi-species labeling errors in ecological studies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Bayesianmodelingofmultispecieslabelingerrorsinecologicalstudies.html" rel="alternate" type="text/html" title="Bayesian modeling of multi-species labeling errors in ecological studies" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Bayesianmodelingofmultispecieslabelingerrorsinecologicalstudies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Bayesianmodelingofmultispecieslabelingerrorsinecologicalstudies.html">&lt;p&gt;Ecological and conservation studies monitoring bird communities typically rely on species classification based on bird vocalizations. Historically, this has been based on expert volunteers going into the field and making lists of the bird species that they observe. Recently, machine learning algorithms have emerged that can accurately classify bird species based on audio recordings of their vocalizations. Such algorithms crucially rely on training data that are labeled by experts. Automated classification is challenging when multiple species are vocalizing simultaneously, there is background noise, and/or the bird is far from the microphone. In continuously monitoring different locations, the size of the audio data become immense and it is only possible for human experts to label a tiny proportion of the available data. In addition, experts can vary in their accuracy and breadth of knowledge about different species. This article focuses on the important problem of combining sparse expert annotations to improve bird species classification while providing uncertainty quantification. We additionally are interested in providing expert performance scores to increase their engagement and encourage improvements. We propose a Bayesian hierarchical modeling approach and evaluate this approach on a new community science platform developed in Finland.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15844&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Haoxuan Wang, Patrik Lauha, David B. Dunson</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Ecological and conservation studies monitoring bird communities typically rely on species classification based on bird vocalizations. Historically, this has been based on expert volunteers going into the field and making lists of the bird species that they observe. Recently, machine learning algorithms have emerged that can accurately classify bird species based on audio recordings of their vocalizations. Such algorithms crucially rely on training data that are labeled by experts. Automated classification is challenging when multiple species are vocalizing simultaneously, there is background noise, and/or the bird is far from the microphone. In continuously monitoring different locations, the size of the audio data become immense and it is only possible for human experts to label a tiny proportion of the available data. In addition, experts can vary in their accuracy and breadth of knowledge about different species. This article focuses on the important problem of combining sparse expert annotations to improve bird species classification while providing uncertainty quantification. We additionally are interested in providing expert performance scores to increase their engagement and encourage improvements. We propose a Bayesian hierarchical modeling approach and evaluate this approach on a new community science platform developed in Finland.</summary></entry><entry><title type="html">CLEAR: Can Language Models Really Understand Causal Graphs?</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/CLEARCanLanguageModelsReallyUnderstandCausalGraphs.html" rel="alternate" type="text/html" title="CLEAR: Can Language Models Really Understand Causal Graphs?" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/CLEARCanLanguageModelsReallyUnderstandCausalGraphs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/CLEARCanLanguageModelsReallyUnderstandCausalGraphs.html">&lt;p&gt;Causal reasoning is a cornerstone of how humans interpret the world. To model and reason about causality, causal graphs offer a concise yet effective solution. Given the impressive advancements in language models, a crucial question arises: can they really understand causal graphs? To this end, we pioneer an investigation into language models’ understanding of causal graphs. Specifically, we develop a framework to define causal graph understanding, by assessing language models’ behaviors through four practical criteria derived from diverse disciplines (e.g., philosophy and psychology). We then develop CLEAR, a novel benchmark that defines three complexity levels and encompasses 20 causal graph-based tasks across these levels. Finally, based on our framework and benchmark, we conduct extensive experiments on six leading language models and summarize five empirical findings. Our results indicate that while language models demonstrate a preliminary understanding of causal graphs, significant potential for improvement remains. Our project website is at https://github.com/OpenCausaLab/CLEAR.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16605&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sirui Chen, Mengying Xu, Kun Wang, Xingyu Zeng, Rui Zhao, Shengjie Zhao, Chaochao Lu</name></author><category term="stat.ME" /><summary type="html">Causal reasoning is a cornerstone of how humans interpret the world. To model and reason about causality, causal graphs offer a concise yet effective solution. Given the impressive advancements in language models, a crucial question arises: can they really understand causal graphs? To this end, we pioneer an investigation into language models’ understanding of causal graphs. Specifically, we develop a framework to define causal graph understanding, by assessing language models’ behaviors through four practical criteria derived from diverse disciplines (e.g., philosophy and psychology). We then develop CLEAR, a novel benchmark that defines three complexity levels and encompasses 20 causal graph-based tasks across these levels. Finally, based on our framework and benchmark, we conduct extensive experiments on six leading language models and summarize five empirical findings. Our results indicate that while language models demonstrate a preliminary understanding of causal graphs, significant potential for improvement remains. Our project website is at https://github.com/OpenCausaLab/CLEAR.</summary></entry><entry><title type="html">CausalFormer: An Interpretable Transformer for Temporal Causal Discovery</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/CausalFormerAnInterpretableTransformerforTemporalCausalDiscovery.html" rel="alternate" type="text/html" title="CausalFormer: An Interpretable Transformer for Temporal Causal Discovery" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/CausalFormerAnInterpretableTransformerforTemporalCausalDiscovery</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/CausalFormerAnInterpretableTransformerforTemporalCausalDiscovery.html">&lt;p&gt;Temporal causal discovery is a crucial task aimed at uncovering the causal relations within time series data. The latest temporal causal discovery methods usually train deep learning models on prediction tasks to uncover the causality between time series. They capture causal relations by analyzing the parameters of some components of the trained models, e.g., attention weights and convolution weights. However, this is an incomplete mapping process from the model parameters to the causality and fails to investigate the other components, e.g., fully connected layers and activation functions, that are also significant for causal discovery. To facilitate the utilization of the whole deep learning models in temporal causal discovery, we proposed an interpretable transformer-based causal discovery model termed CausalFormer, which consists of the causality-aware transformer and the decomposition-based causality detector. The causality-aware transformer learns the causal representation of time series data using a prediction task with the designed multi-kernel causal convolution which aggregates each input time series along the temporal dimension under the temporal priority constraint. Then, the decomposition-based causality detector interprets the global structure of the trained causality-aware transformer with the proposed regression relevance propagation to identify potential causal relations and finally construct the causal graph. Experiments on synthetic, simulated, and real datasets demonstrate the state-of-the-art performance of CausalFormer on discovering temporal causality. Our code is available at https://github.com/lingbai-kong/CausalFormer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16708&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lingbai Kong, Wengen Li, Hanchen Yang, Yichao Zhang, Jihong Guan, Shuigeng Zhou</name></author><category term="stat.ME" /><summary type="html">Temporal causal discovery is a crucial task aimed at uncovering the causal relations within time series data. The latest temporal causal discovery methods usually train deep learning models on prediction tasks to uncover the causality between time series. They capture causal relations by analyzing the parameters of some components of the trained models, e.g., attention weights and convolution weights. However, this is an incomplete mapping process from the model parameters to the causality and fails to investigate the other components, e.g., fully connected layers and activation functions, that are also significant for causal discovery. To facilitate the utilization of the whole deep learning models in temporal causal discovery, we proposed an interpretable transformer-based causal discovery model termed CausalFormer, which consists of the causality-aware transformer and the decomposition-based causality detector. The causality-aware transformer learns the causal representation of time series data using a prediction task with the designed multi-kernel causal convolution which aggregates each input time series along the temporal dimension under the temporal priority constraint. Then, the decomposition-based causality detector interprets the global structure of the trained causality-aware transformer with the proposed regression relevance propagation to identify potential causal relations and finally construct the causal graph. Experiments on synthetic, simulated, and real datasets demonstrate the state-of-the-art performance of CausalFormer on discovering temporal causality. Our code is available at https://github.com/lingbai-kong/CausalFormer.</summary></entry><entry><title type="html">Causal Inference on Process Graphs, Part I: The Structural Equation Process Representation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/CausalInferenceonProcessGraphsPartITheStructuralEquationProcessRepresentation.html" rel="alternate" type="text/html" title="Causal Inference on Process Graphs, Part I: The Structural Equation Process Representation" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/CausalInferenceonProcessGraphsPartITheStructuralEquationProcessRepresentation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/CausalInferenceonProcessGraphsPartITheStructuralEquationProcessRepresentation.html">&lt;p&gt;When dealing with time series data, causal inference methods often employ structural vector autoregressive (SVAR) processes to model time-evolving random systems. In this work, we rephrase recursive SVAR processes with possible latent component processes as a linear Structural Causal Model (SCM) of stochastic processes on a simple causal graph, the \emph{process graph}, that models every process as a single node. Using this reformulation, we generalise Wright’s well-known path-rule for linear Gaussian SCMs to the newly introduced process SCMs and we express the auto-covariance sequence of an SVAR process by means of a generalised trek-rule. Employing the Fourier-Transformation, we derive compact expressions for causal effects in the frequency domain that allow us to efficiently visualise the causal interactions in a multivariate SVAR process. Finally, we observe that the process graph can be used to formulate graphical criteria for identifying causal effects and to derive algebraic relations with which these frequency domain causal effects can be recovered from the observed spectral density.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.11561&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nicolas-Domenic Reiter, Andreas Gerhardus, Jonas Wahl, Jakob Runge</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">When dealing with time series data, causal inference methods often employ structural vector autoregressive (SVAR) processes to model time-evolving random systems. In this work, we rephrase recursive SVAR processes with possible latent component processes as a linear Structural Causal Model (SCM) of stochastic processes on a simple causal graph, the \emph{process graph}, that models every process as a single node. Using this reformulation, we generalise Wright’s well-known path-rule for linear Gaussian SCMs to the newly introduced process SCMs and we express the auto-covariance sequence of an SVAR process by means of a generalised trek-rule. Employing the Fourier-Transformation, we derive compact expressions for causal effects in the frequency domain that allow us to efficiently visualise the causal interactions in a multivariate SVAR process. Finally, we observe that the process graph can be used to formulate graphical criteria for identifying causal effects and to derive algebraic relations with which these frequency domain causal effects can be recovered from the observed spectral density.</summary></entry><entry><title type="html">Chauhan Weighted Trajectory Analysis reduces sample size requirements and expedites time-to-efficacy signals in advanced cancer clinical trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ChauhanWeightedTrajectoryAnalysisreducessamplesizerequirementsandexpeditestimetoefficacysignalsinadvancedcancerclinicaltrials.html" rel="alternate" type="text/html" title="Chauhan Weighted Trajectory Analysis reduces sample size requirements and expedites time-to-efficacy signals in advanced cancer clinical trials" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ChauhanWeightedTrajectoryAnalysisreducessamplesizerequirementsandexpeditestimetoefficacysignalsinadvancedcancerclinicaltrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ChauhanWeightedTrajectoryAnalysisreducessamplesizerequirementsandexpeditestimetoefficacysignalsinadvancedcancerclinicaltrials.html">&lt;p&gt;As Kaplan-Meier (KM) analysis is limited to single unidirectional endpoints, most advanced cancer randomized clinical trials (RCTs) are powered for either progression free survival (PFS) or overall survival (OS). This discards efficacy information carried by partial responses, complete responses, and stable disease that frequently precede progressive disease and death. Chauhan Weighted Trajectory Analysis (CWTA) is a generalization of KM that simultaneously assesses multiple rank-ordered endpoints. We hypothesized that CWTA could use this efficacy information to reduce sample size requirements and expedite efficacy signals in advanced cancer trials. We performed 100-fold and 1000-fold simulations of solid tumour systemic therapy RCTs with health statuses rank ordered from complete response (Stage 0) to death (Stage 4). At increments of sample size and hazard ratio, we compared KM PFS and OS with CWTA for (i) sample size requirements to achieve a power of 0.8 and (ii) time-to-first significant efficacy signal. CWTA consistently demonstrated greater power, and reduced sample size requirements by 18% to 35% compared to KM PFS and 14% to 20% compared to KM OS. CWTA also expedited time-to-efficacy signals 2- to 6-fold. CWTA, by incorporating all efficacy signals in the cancer treatment trajectory, provides clinically relevant reduction in required sample size and meaningfully expedites the efficacy signals of cancer treatments compared to KM PFS and KM OS. Using CWTA rather than KM as the primary trial outcome has the potential to meaningfully reduce the numbers of patients, trial duration, and costs to evaluate therapies in advanced cancer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02529&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Utkarsh Chauhan, Daylen Mackey, John R. Mackey</name></author><category term="stat.ME" /><summary type="html">As Kaplan-Meier (KM) analysis is limited to single unidirectional endpoints, most advanced cancer randomized clinical trials (RCTs) are powered for either progression free survival (PFS) or overall survival (OS). This discards efficacy information carried by partial responses, complete responses, and stable disease that frequently precede progressive disease and death. Chauhan Weighted Trajectory Analysis (CWTA) is a generalization of KM that simultaneously assesses multiple rank-ordered endpoints. We hypothesized that CWTA could use this efficacy information to reduce sample size requirements and expedite efficacy signals in advanced cancer trials. We performed 100-fold and 1000-fold simulations of solid tumour systemic therapy RCTs with health statuses rank ordered from complete response (Stage 0) to death (Stage 4). At increments of sample size and hazard ratio, we compared KM PFS and OS with CWTA for (i) sample size requirements to achieve a power of 0.8 and (ii) time-to-first significant efficacy signal. CWTA consistently demonstrated greater power, and reduced sample size requirements by 18% to 35% compared to KM PFS and 14% to 20% compared to KM OS. CWTA also expedited time-to-efficacy signals 2- to 6-fold. CWTA, by incorporating all efficacy signals in the cancer treatment trajectory, provides clinically relevant reduction in required sample size and meaningfully expedites the efficacy signals of cancer treatments compared to KM PFS and KM OS. Using CWTA rather than KM as the primary trial outcome has the potential to meaningfully reduce the numbers of patients, trial duration, and costs to evaluate therapies in advanced cancer.</summary></entry><entry><title type="html">Clustering and Meta-Analysis Using a Mixture of Dependent Linear Tail-Free Priors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ClusteringandMetaAnalysisUsingaMixtureofDependentLinearTailFreePriors.html" rel="alternate" type="text/html" title="Clustering and Meta-Analysis Using a Mixture of Dependent Linear Tail-Free Priors" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ClusteringandMetaAnalysisUsingaMixtureofDependentLinearTailFreePriors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ClusteringandMetaAnalysisUsingaMixtureofDependentLinearTailFreePriors.html">&lt;p&gt;We propose a novel nonparametric Bayesian approach for meta-analysis with event time outcomes. The model is an extension of linear dependent tail-free processes. The extension includes a modification to facilitate (conditionally) conjugate posterior updating and a hierarchical extension with a random partition of studies. The partition is formalized as a Dirichlet process mixture. The model development is motivated by a meta-analysis of cancer immunotherapy studies. The aim is to validate the use of relevant biomarkers in the design of immunotherapy studies. The hypothesis is about immunotherapy in general, rather than about a specific tumor type, therapy and marker. This broad hypothesis leads to a very diverse set of studies being included in the analysis and gives rise to substantial heterogeneity across studies&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15912&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bernardo Flores, Peter Mueller</name></author><category term="stat.ME" /><summary type="html">We propose a novel nonparametric Bayesian approach for meta-analysis with event time outcomes. The model is an extension of linear dependent tail-free processes. The extension includes a modification to facilitate (conditionally) conjugate posterior updating and a hierarchical extension with a random partition of studies. The partition is formalized as a Dirichlet process mixture. The model development is motivated by a meta-analysis of cancer immunotherapy studies. The aim is to validate the use of relevant biomarkers in the design of immunotherapy studies. The hypothesis is about immunotherapy in general, rather than about a specific tumor type, therapy and marker. This broad hypothesis leads to a very diverse set of studies being included in the analysis and gives rise to substantial heterogeneity across studies</summary></entry><entry><title type="html">Communication-Efficient Distributed Estimation and Inference for Cox’s Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/CommunicationEfficientDistributedEstimationandInferenceforCoxsModel.html" rel="alternate" type="text/html" title="Communication-Efficient Distributed Estimation and Inference for Cox’s Model" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/CommunicationEfficientDistributedEstimationandInferenceforCoxsModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/CommunicationEfficientDistributedEstimationandInferenceforCoxsModel.html">&lt;p&gt;Motivated by multi-center biomedical studies that cannot share individual data due to privacy and ownership concerns, we develop communication-efficient iterative distributed algorithms for estimation and inference in the high-dimensional sparse Cox proportional hazards model. We demonstrate that our estimator, even with a relatively small number of iterations, achieves the same convergence rate as the ideal full-sample estimator under very mild conditions. To construct confidence intervals for linear combinations of high-dimensional hazard regression coefficients, we introduce a novel debiased method, establish central limit theorems, and provide consistent variance estimators that yield asymptotically valid distributed confidence intervals. In addition, we provide valid and powerful distributed hypothesis tests for any coordinate element based on a decorrelated score test. We allow time-dependent covariates as well as censored survival times. Extensive numerical experiments on both simulated and real data lend further support to our theory and demonstrate that our communication-efficient distributed estimators, confidence intervals, and hypothesis tests improve upon alternative methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.12111&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Pierre Bayle, Jianqing Fan, Zhipeng Lou</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Motivated by multi-center biomedical studies that cannot share individual data due to privacy and ownership concerns, we develop communication-efficient iterative distributed algorithms for estimation and inference in the high-dimensional sparse Cox proportional hazards model. We demonstrate that our estimator, even with a relatively small number of iterations, achieves the same convergence rate as the ideal full-sample estimator under very mild conditions. To construct confidence intervals for linear combinations of high-dimensional hazard regression coefficients, we introduce a novel debiased method, establish central limit theorems, and provide consistent variance estimators that yield asymptotically valid distributed confidence intervals. In addition, we provide valid and powerful distributed hypothesis tests for any coordinate element based on a decorrelated score test. We allow time-dependent covariates as well as censored survival times. Extensive numerical experiments on both simulated and real data lend further support to our theory and demonstrate that our communication-efficient distributed estimators, confidence intervals, and hypothesis tests improve upon alternative methods.</summary></entry><entry><title type="html">Comparing statistical likelihoods with diagnostic probabilities based on directly observed proportions to help understand the replication crisis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Comparingstatisticallikelihoodswithdiagnosticprobabilitiesbasedondirectlyobservedproportionstohelpunderstandthereplicationcrisis.html" rel="alternate" type="text/html" title="Comparing statistical likelihoods with diagnostic probabilities based on directly observed proportions to help understand the replication crisis" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Comparingstatisticallikelihoodswithdiagnosticprobabilitiesbasedondirectlyobservedproportionstohelpunderstandthereplicationcrisis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Comparingstatisticallikelihoodswithdiagnosticprobabilitiesbasedondirectlyobservedproportionstohelpunderstandthereplicationcrisis.html">&lt;p&gt;Diagnosticians use an observed proportion as a direct estimate of the posterior probability of a diagnosis. Therefore, a diagnostician might regard a continuous Gaussian probability distribution of possible numerical outcomes conditional on the information in the study methods and data as posterior probabilities. Similarly, they might regard the distribution of possible means based on a SEM as a posterior probability distribution too. If the converse likelihood distribution of the observed mean conditional on any hypothetical mean (e.g. the null hypothesis) is assumed to be the same as the above posterior distribution (as is customary) then by Bayes rule, the prior distribution of all possible hypothetical means is uniform. It follows that the probability Q of any theoretically true mean falling into a tail beyond a null hypothesis would be equal to that tails area as a proportion of the whole. It also follows that the P value (the probability of the observed mean or something more extreme conditional on the null hypothesis) is equal to Q. Replication involves doing two independent studies, thus doubling the variance for the combined posterior probability distribution. So, if the original effect size was 1.96, the number of observations was 100, the SEM was 1 and the original P value was 0.025, the theoretical probability of a replicating study getting a P value of up to 0.025 again is only 0.283. By applying this double variance to achieve a power of 80%, the required number of observations is doubled compared to conventional approaches. If some replicating study is to achieve a P value of up to 0.025 yet again with a probability of 0.8, then this requires 3 times as many observations in the power calculation. This might explain the replication crisis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.16906&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Huw Llewelyn</name></author><category term="stat.ME" /><summary type="html">Diagnosticians use an observed proportion as a direct estimate of the posterior probability of a diagnosis. Therefore, a diagnostician might regard a continuous Gaussian probability distribution of possible numerical outcomes conditional on the information in the study methods and data as posterior probabilities. Similarly, they might regard the distribution of possible means based on a SEM as a posterior probability distribution too. If the converse likelihood distribution of the observed mean conditional on any hypothetical mean (e.g. the null hypothesis) is assumed to be the same as the above posterior distribution (as is customary) then by Bayes rule, the prior distribution of all possible hypothetical means is uniform. It follows that the probability Q of any theoretically true mean falling into a tail beyond a null hypothesis would be equal to that tails area as a proportion of the whole. It also follows that the P value (the probability of the observed mean or something more extreme conditional on the null hypothesis) is equal to Q. Replication involves doing two independent studies, thus doubling the variance for the combined posterior probability distribution. So, if the original effect size was 1.96, the number of observations was 100, the SEM was 1 and the original P value was 0.025, the theoretical probability of a replicating study getting a P value of up to 0.025 again is only 0.283. By applying this double variance to achieve a power of 80%, the required number of observations is doubled compared to conventional approaches. If some replicating study is to achieve a P value of up to 0.025 yet again with a probability of 0.8, then this requires 3 times as many observations in the power calculation. This might explain the replication crisis.</summary></entry><entry><title type="html">Comparison of methods for mediation analysis with multiple correlated mediators</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Comparisonofmethodsformediationanalysiswithmultiplecorrelatedmediators.html" rel="alternate" type="text/html" title="Comparison of methods for mediation analysis with multiple correlated mediators" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Comparisonofmethodsformediationanalysiswithmultiplecorrelatedmediators</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Comparisonofmethodsformediationanalysiswithmultiplecorrelatedmediators.html">&lt;p&gt;Various methods have emerged for conducting mediation analyses with multiple correlated mediators, each with distinct strengths and limitations. However, a comparative evaluation of these methods is lacking, providing the motivation for this paper. This study examines six mediation analysis methods for multiple correlated mediators that provide insights to the contributors for health disparities. We assessed the performance of each method in identifying joint or path-specific mediation effects in the context of binary outcome variables varying mediator types and levels of residual correlation between mediators. Through comprehensive simulations, the performance of six methods in estimating joint and/or path-specific mediation effects was assessed rigorously using a variety of metrics including bias, mean squared error, coverage and width of the 95$\%$ confidence intervals. Subsequently, these methods were applied to the REasons for Geographic And Racial Differences in Stroke (REGARDS) study, where differing conclusions were obtained depending on the mediation method employed. This evaluation provides valuable guidance for researchers grappling with complex multi-mediator scenarios, enabling them to select an optimal mediation method for their research question and dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16174&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mary Appah, D. Leann Long, George Howard, Melissa J. Smith</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Various methods have emerged for conducting mediation analyses with multiple correlated mediators, each with distinct strengths and limitations. However, a comparative evaluation of these methods is lacking, providing the motivation for this paper. This study examines six mediation analysis methods for multiple correlated mediators that provide insights to the contributors for health disparities. We assessed the performance of each method in identifying joint or path-specific mediation effects in the context of binary outcome variables varying mediator types and levels of residual correlation between mediators. Through comprehensive simulations, the performance of six methods in estimating joint and/or path-specific mediation effects was assessed rigorously using a variety of metrics including bias, mean squared error, coverage and width of the 95$\%$ confidence intervals. Subsequently, these methods were applied to the REasons for Geographic And Racial Differences in Stroke (REGARDS) study, where differing conclusions were obtained depending on the mediation method employed. This evaluation provides valuable guidance for researchers grappling with complex multi-mediator scenarios, enabling them to select an optimal mediation method for their research question and dataset.</summary></entry><entry><title type="html">Conditional Bayesian Quadrature</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ConditionalBayesianQuadrature.html" rel="alternate" type="text/html" title="Conditional Bayesian Quadrature" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ConditionalBayesianQuadrature</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ConditionalBayesianQuadrature.html">&lt;p&gt;We propose a novel approach for estimating conditional or parametric expectations in the setting where obtaining samples or evaluating integrands is costly. Through the framework of probabilistic numerical methods (such as Bayesian quadrature), our novel approach allows to incorporates prior information about the integrands especially the prior smoothness knowledge about the integrands and the conditional expectation. As a result, our approach provides a way of quantifying uncertainty and leads to a fast convergence rate, which is confirmed both theoretically and empirically on challenging tasks in Bayesian sensitivity analysis, computational finance and decision making under uncertainty.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16530&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zonghao Chen, Masha Naslidnyk, Arthur Gretton, François-Xavier Briol</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">We propose a novel approach for estimating conditional or parametric expectations in the setting where obtaining samples or evaluating integrands is costly. Through the framework of probabilistic numerical methods (such as Bayesian quadrature), our novel approach allows to incorporates prior information about the integrands especially the prior smoothness knowledge about the integrands and the conditional expectation. As a result, our approach provides a way of quantifying uncertainty and leads to a fast convergence rate, which is confirmed both theoretically and empirically on challenging tasks in Bayesian sensitivity analysis, computational finance and decision making under uncertainty.</summary></entry><entry><title type="html">Conformal time series decomposition with component-wise exchangeability</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Conformaltimeseriesdecompositionwithcomponentwiseexchangeability.html" rel="alternate" type="text/html" title="Conformal time series decomposition with component-wise exchangeability" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Conformaltimeseriesdecompositionwithcomponentwiseexchangeability</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Conformaltimeseriesdecompositionwithcomponentwiseexchangeability.html">&lt;p&gt;Conformal prediction offers a practical framework for distribution-free uncertainty quantification, providing finite-sample coverage guarantees under relatively mild assumptions on data exchangeability. However, these assumptions cease to hold for time series due to their temporally correlated nature. In this work, we present a novel use of conformal prediction for time series forecasting that incorporates time series decomposition. This approach allows us to model different temporal components individually. By applying specific conformal algorithms to each component and then merging the obtained prediction intervals, we customize our methods to account for the different exchangeability regimes underlying each component. Our decomposition-based approach is thoroughly discussed and empirically evaluated on synthetic and real-world data. We find that the method provides promising results on well-structured time series, but can be limited by factors such as the decomposition step for more complex data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16766&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Derck W. E. Prinzhorn, Thijmen Nijdam, Putri A. van der Linden, Alexander Timans</name></author><category term="stat.ML," /><category term="stat.AP" /><summary type="html">Conformal prediction offers a practical framework for distribution-free uncertainty quantification, providing finite-sample coverage guarantees under relatively mild assumptions on data exchangeability. However, these assumptions cease to hold for time series due to their temporally correlated nature. In this work, we present a novel use of conformal prediction for time series forecasting that incorporates time series decomposition. This approach allows us to model different temporal components individually. By applying specific conformal algorithms to each component and then merging the obtained prediction intervals, we customize our methods to account for the different exchangeability regimes underlying each component. Our decomposition-based approach is thoroughly discussed and empirically evaluated on synthetic and real-world data. We find that the method provides promising results on well-structured time series, but can be limited by factors such as the decomposition step for more complex data.</summary></entry><entry><title type="html">Convergence rates of non-stationary and deep Gaussian process regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ConvergenceratesofnonstationaryanddeepGaussianprocessregression.html" rel="alternate" type="text/html" title="Convergence rates of non-stationary and deep Gaussian process regression" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ConvergenceratesofnonstationaryanddeepGaussianprocessregression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ConvergenceratesofnonstationaryanddeepGaussianprocessregression.html">&lt;p&gt;The focus of this work is the convergence of non-stationary and deep Gaussian process regression. More precisely, we follow a Bayesian approach to regression or interpolation, where the prior placed on the unknown function $f$ is a non-stationary or deep Gaussian process, and we derive convergence rates of the posterior mean to the true function $f$ in terms of the number of observed training points. In some cases, we also show convergence of the posterior variance to zero. The only assumption imposed on the function $f$ is that it is an element of a certain reproducing kernel Hilbert space, which we in particular cases show to be norm-equivalent to a Sobolev space. Our analysis includes the case of estimated hyper-parameters in the covariance kernels employed, both in an empirical Bayes’ setting and the particular hierarchical setting constructed through deep Gaussian processes. We consider the settings of noise-free or noisy observations on deterministic or random training points. We establish general assumptions sufficient for the convergence of deep Gaussian process regression, along with explicit examples demonstrating the fulfilment of these assumptions. Specifically, our examples require that the H&quot;older or Sobolev norms of the penultimate layer are bounded almost surely.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.07320&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Conor Moriarty-Osborne, Aretha L. Teckentrup</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">The focus of this work is the convergence of non-stationary and deep Gaussian process regression. More precisely, we follow a Bayesian approach to regression or interpolation, where the prior placed on the unknown function $f$ is a non-stationary or deep Gaussian process, and we derive convergence rates of the posterior mean to the true function $f$ in terms of the number of observed training points. In some cases, we also show convergence of the posterior variance to zero. The only assumption imposed on the function $f$ is that it is an element of a certain reproducing kernel Hilbert space, which we in particular cases show to be norm-equivalent to a Sobolev space. Our analysis includes the case of estimated hyper-parameters in the covariance kernels employed, both in an empirical Bayes’ setting and the particular hierarchical setting constructed through deep Gaussian processes. We consider the settings of noise-free or noisy observations on deterministic or random training points. We establish general assumptions sufficient for the convergence of deep Gaussian process regression, along with explicit examples demonstrating the fulfilment of these assumptions. Specifically, our examples require that the H&quot;older or Sobolev norms of the penultimate layer are bounded almost surely.</summary></entry><entry><title type="html">Deep Optimal Experimental Design for Parameter Estimation Problems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/DeepOptimalExperimentalDesignforParameterEstimationProblems.html" rel="alternate" type="text/html" title="Deep Optimal Experimental Design for Parameter Estimation Problems" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/DeepOptimalExperimentalDesignforParameterEstimationProblems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/DeepOptimalExperimentalDesignforParameterEstimationProblems.html">&lt;p&gt;Optimal experimental design is a well studied field in applied science and engineering. Techniques for estimating such a design are commonly used within the framework of parameter estimation. Nonetheless, in recent years parameter estimation techniques are changing rapidly with the introduction of deep learning techniques to replace traditional estimation methods. This in turn requires the adaptation of optimal experimental design that is associated with these new techniques. In this paper we investigate a new experimental design methodology that uses deep learning. We show that the training of a network as a Likelihood Free Estimator can be used to significantly simplify the design process and circumvent the need for the computationally expensive bi-level optimization problem that is inherent in optimal experimental design for non-linear systems. Furthermore, deep design improves the quality of the recovery process for parameter estimation problems. As proof of concept we apply our methodology to two different systems of Ordinary Differential Equations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14003&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Md Shahriar Rahim Siddiqui, Arman Rahmim, Eldad Haber</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Optimal experimental design is a well studied field in applied science and engineering. Techniques for estimating such a design are commonly used within the framework of parameter estimation. Nonetheless, in recent years parameter estimation techniques are changing rapidly with the introduction of deep learning techniques to replace traditional estimation methods. This in turn requires the adaptation of optimal experimental design that is associated with these new techniques. In this paper we investigate a new experimental design methodology that uses deep learning. We show that the training of a network as a Likelihood Free Estimator can be used to significantly simplify the design process and circumvent the need for the computationally expensive bi-level optimization problem that is inherent in optimal experimental design for non-linear systems. Furthermore, deep design improves the quality of the recovery process for parameter estimation problems. As proof of concept we apply our methodology to two different systems of Ordinary Differential Equations.</summary></entry><entry><title type="html">Discovering influential text using convolutional neural networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Discoveringinfluentialtextusingconvolutionalneuralnetworks.html" rel="alternate" type="text/html" title="Discovering influential text using convolutional neural networks" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Discoveringinfluentialtextusingconvolutionalneuralnetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Discoveringinfluentialtextusingconvolutionalneuralnetworks.html">&lt;p&gt;Experimental methods for estimating the impacts of text on human evaluation have been widely used in the social sciences. However, researchers in experimental settings are usually limited to testing a small number of pre-specified text treatments. While efforts to mine unstructured texts for features that causally affect outcomes have been ongoing in recent years, these models have primarily focused on the topics or specific words of text, which may not always be the mechanism of the effect. We connect these efforts with NLP interpretability techniques and present a method for flexibly discovering clusters of similar text phrases that are predictive of human reactions to texts using convolutional neural networks. When used in an experimental setting, this method can identify text treatments and their effects under certain assumptions. We apply the method to two datasets. The first enables direct validation of the model’s ability to detect phrases known to cause the outcome. The second demonstrates its ability to flexibly discover text treatments with varying textual structures. In both cases, the model learns a greater variety of text treatments compared to benchmark methods, and these text features quantitatively meet or exceed the ability of benchmark methods to predict the outcome.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10086&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Megan Ayers, Luke Sanford, Margaret Roberts, Eddie Yang</name></author><category term="stat.ME" /><summary type="html">Experimental methods for estimating the impacts of text on human evaluation have been widely used in the social sciences. However, researchers in experimental settings are usually limited to testing a small number of pre-specified text treatments. While efforts to mine unstructured texts for features that causally affect outcomes have been ongoing in recent years, these models have primarily focused on the topics or specific words of text, which may not always be the mechanism of the effect. We connect these efforts with NLP interpretability techniques and present a method for flexibly discovering clusters of similar text phrases that are predictive of human reactions to texts using convolutional neural networks. When used in an experimental setting, this method can identify text treatments and their effects under certain assumptions. We apply the method to two datasets. The first enables direct validation of the model’s ability to detect phrases known to cause the outcome. The second demonstrates its ability to flexibly discover text treatments with varying textual structures. In both cases, the model learns a greater variety of text treatments compared to benchmark methods, and these text features quantitatively meet or exceed the ability of benchmark methods to predict the outcome.</summary></entry><entry><title type="html">Distance-based Chatterjee correlation: a new generalized robust measure of directed association for multivariate real and complex-valued data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/DistancebasedChatterjeecorrelationanewgeneralizedrobustmeasureofdirectedassociationformultivariaterealandcomplexvalueddata.html" rel="alternate" type="text/html" title="Distance-based Chatterjee correlation: a new generalized robust measure of directed association for multivariate real and complex-valued data" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/DistancebasedChatterjeecorrelationanewgeneralizedrobustmeasureofdirectedassociationformultivariaterealandcomplexvalueddata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/DistancebasedChatterjeecorrelationanewgeneralizedrobustmeasureofdirectedassociationformultivariaterealandcomplexvalueddata.html">&lt;p&gt;Building upon the Chatterjee correlation (2021: J. Am. Stat. Assoc. 116, p2009) for two real-valued variables, this study introduces a generalized measure of directed association between two vector variables, real or complex-valued, and of possibly different dimensions. The new measure is denoted as the “distance-based Chatterjee correlation”, owing to the use here of the “distance transformed data” defined in Szekely et al (2007: Ann. Statist. 35, p2769) for the distance correlation. A main property of the new measure, inherited from the original Chatterjee correlation, is its predictive and asymmetric nature: it measures how well one variable can be predicted by the other, asymmetrically. This allows for inferring the causal direction of the association, by using the method of Blobaum et al (2019: PeerJ Comput. Sci. 1, e169). Since the original Chatterjee correlation is based on ranks, it is not available for complex variables, nor for general multivariate data. The novelty of our work is the extension to multivariate real and complex-valued pairs of vectors, offering a robust measure of directed association in a completely non-parametric setting. Informally, the intuitive assumption used here is that distance correlation is mathematically equivalent to Pearson’s correlation when applied to “distance transformed” data. The next logical step is to compute Chatterjee’s correlation on the same “distance transformed” data, thereby extending the analysis to multivariate vectors of real and complex valued data. As a bonus, the new measure here is robust to outliers, which is not true for the distance correlation of Szekely et al. Additionally, this approach allows for inference regarding the causal direction of the association between the variables.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16458&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roberto D. Pascual-Marqui, Kieko Kochi, Toshihiko Kinoshita</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Building upon the Chatterjee correlation (2021: J. Am. Stat. Assoc. 116, p2009) for two real-valued variables, this study introduces a generalized measure of directed association between two vector variables, real or complex-valued, and of possibly different dimensions. The new measure is denoted as the “distance-based Chatterjee correlation”, owing to the use here of the “distance transformed data” defined in Szekely et al (2007: Ann. Statist. 35, p2769) for the distance correlation. A main property of the new measure, inherited from the original Chatterjee correlation, is its predictive and asymmetric nature: it measures how well one variable can be predicted by the other, asymmetrically. This allows for inferring the causal direction of the association, by using the method of Blobaum et al (2019: PeerJ Comput. Sci. 1, e169). Since the original Chatterjee correlation is based on ranks, it is not available for complex variables, nor for general multivariate data. The novelty of our work is the extension to multivariate real and complex-valued pairs of vectors, offering a robust measure of directed association in a completely non-parametric setting. Informally, the intuitive assumption used here is that distance correlation is mathematically equivalent to Pearson’s correlation when applied to “distance transformed” data. The next logical step is to compute Chatterjee’s correlation on the same “distance transformed” data, thereby extending the analysis to multivariate vectors of real and complex valued data. As a bonus, the new measure here is robust to outliers, which is not true for the distance correlation of Szekely et al. Additionally, this approach allows for inference regarding the causal direction of the association between the variables.</summary></entry><entry><title type="html">Distribution-Free Online Change Detection for Low-Rank Images</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/DistributionFreeOnlineChangeDetectionforLowRankImages.html" rel="alternate" type="text/html" title="Distribution-Free Online Change Detection for Low-Rank Images" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/DistributionFreeOnlineChangeDetectionforLowRankImages</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/DistributionFreeOnlineChangeDetectionforLowRankImages.html">&lt;p&gt;We present a distribution-free CUSUM procedure designed for online change detection in a time series of low-rank images, particularly when the change causes a mean shift. We represent images as matrix data and allow for temporal dependence, in addition to inherent spatial dependence, before and after the change. The marginal distributions are assumed to be general, not limited to any specific parametric distribution. We propose new monitoring statistics that utilize the low-rank structure of the in-control mean matrix. Additionally, we study the properties of the proposed detection procedure, assessing whether the monitoring statistics effectively capture a mean shift and evaluating the rate of increase in average run length relative to the control limit in both in-control and out-of-control cases. The effectiveness of our procedure is demonstrated through simulated and real data experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16136&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tingnan Gong, Seong-Hee Kim, Yao Xie</name></author><category term="stat.ME" /><summary type="html">We present a distribution-free CUSUM procedure designed for online change detection in a time series of low-rank images, particularly when the change causes a mean shift. We represent images as matrix data and allow for temporal dependence, in addition to inherent spatial dependence, before and after the change. The marginal distributions are assumed to be general, not limited to any specific parametric distribution. We propose new monitoring statistics that utilize the low-rank structure of the in-control mean matrix. Additionally, we study the properties of the proposed detection procedure, assessing whether the monitoring statistics effectively capture a mean shift and evaluating the rate of increase in average run length relative to the control limit in both in-control and out-of-control cases. The effectiveness of our procedure is demonstrated through simulated and real data experiments.</summary></entry><entry><title type="html">EFECT – A Method and Metric to Assess the Reproducibility of Stochastic Simulation Studies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/EFECTAMethodandMetrictoAssesstheReproducibilityofStochasticSimulationStudies.html" rel="alternate" type="text/html" title="EFECT – A Method and Metric to Assess the Reproducibility of Stochastic Simulation Studies" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/EFECTAMethodandMetrictoAssesstheReproducibilityofStochasticSimulationStudies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/EFECTAMethodandMetrictoAssesstheReproducibilityofStochasticSimulationStudies.html">&lt;p&gt;Reproducibility is a foundational standard for validating scientific claims in computational research. Stochastic computational models are employed across diverse fields such as systems biology, financial modelling and environmental sciences. Existing infrastructure and software tools support various aspects of reproducible model development, application, and dissemination, but do not adequately address independently reproducing simulation results that form the basis of scientific conclusions. To bridge this gap, we introduce the Empirical Characteristic Function Equality Convergence Test (EFECT), a data-driven method to quantify the reproducibility of stochastic simulation results. EFECT employs empirical characteristic functions to compare reported results with those independently generated by assessing distributional inequality, termed EFECT error, a metric to quantify the likelihood of equality. Additionally, we establish the EFECT convergence point, a metric for determining the required number of simulation runs to achieve an EFECT error value of a priori statistical significance, setting a reproducibility benchmark. EFECT supports all real-valued and bounded results irrespective of the model or method that produced them, and accommodates stochasticity from intrinsic model variability and random sampling of model inputs. We tested EFECT with stochastic differential equations, agent-based models, and Boolean networks, demonstrating its broad applicability and effectiveness. EFECT standardizes stochastic simulation reproducibility, establishing a workflow that guarantees reliable results, supporting a wide range of stakeholders, and thereby enhancing validation of stochastic simulation studies, across a model’s lifecycle. To promote future standardization efforts, we are developing open source software library libSSR in diverse programming languages for easy integration of EFECT.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16820&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>T. J. Sego, Matthias König, Luis L. Fonseca, Baylor Fain, Adam C. Knapp, Krishna Tiwari, Henning Hermjakob, Herbert M. Sauro, James A. Glazier, Reinhard C. Laubenbacher, Rahuman S. Malik-Sheriff</name></author><category term="stat.ME" /><summary type="html">Reproducibility is a foundational standard for validating scientific claims in computational research. Stochastic computational models are employed across diverse fields such as systems biology, financial modelling and environmental sciences. Existing infrastructure and software tools support various aspects of reproducible model development, application, and dissemination, but do not adequately address independently reproducing simulation results that form the basis of scientific conclusions. To bridge this gap, we introduce the Empirical Characteristic Function Equality Convergence Test (EFECT), a data-driven method to quantify the reproducibility of stochastic simulation results. EFECT employs empirical characteristic functions to compare reported results with those independently generated by assessing distributional inequality, termed EFECT error, a metric to quantify the likelihood of equality. Additionally, we establish the EFECT convergence point, a metric for determining the required number of simulation runs to achieve an EFECT error value of a priori statistical significance, setting a reproducibility benchmark. EFECT supports all real-valued and bounded results irrespective of the model or method that produced them, and accommodates stochasticity from intrinsic model variability and random sampling of model inputs. We tested EFECT with stochastic differential equations, agent-based models, and Boolean networks, demonstrating its broad applicability and effectiveness. EFECT standardizes stochastic simulation reproducibility, establishing a workflow that guarantees reliable results, supporting a wide range of stakeholders, and thereby enhancing validation of stochastic simulation studies, across a model’s lifecycle. To promote future standardization efforts, we are developing open source software library libSSR in diverse programming languages for easy integration of EFECT.</summary></entry><entry><title type="html">Efficient Multivariate Initial Sequence Estimators for MCMC</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/EfficientMultivariateInitialSequenceEstimatorsforMCMC.html" rel="alternate" type="text/html" title="Efficient Multivariate Initial Sequence Estimators for MCMC" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/EfficientMultivariateInitialSequenceEstimatorsforMCMC</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/EfficientMultivariateInitialSequenceEstimatorsforMCMC.html">&lt;p&gt;Estimating Monte Carlo error is critical to valid simulation results in Markov chain Monte Carlo (MCMC) and initial sequence estimators were one of the first methods introduced for this. Over the last few years, focus has been on multivariate assessment of simulation error, and many multivariate generalizations of univariate methods have been developed. The multivariate initial sequence estimator is known to exhibit superior finite-sample performance compared to its competitors. However, the multivariate initial sequence estimator can be prohibitively slow, limiting its widespread use. We provide an efficient alternative to the multivariate initial sequence estimator that inherits both its asymptotic properties as well as the finite-sample superior performance. The effectiveness of the proposed estimator is shown via some MCMC example implementations. Further, we also present univariate and multivariate initial sequence estimators for when parallel MCMC chains are run and demonstrate their effectiveness over popular alternative.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15874&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arka Banerjee, Dootika Vats</name></author><category term="stat.CO" /><summary type="html">Estimating Monte Carlo error is critical to valid simulation results in Markov chain Monte Carlo (MCMC) and initial sequence estimators were one of the first methods introduced for this. Over the last few years, focus has been on multivariate assessment of simulation error, and many multivariate generalizations of univariate methods have been developed. The multivariate initial sequence estimator is known to exhibit superior finite-sample performance compared to its competitors. However, the multivariate initial sequence estimator can be prohibitively slow, limiting its widespread use. We provide an efficient alternative to the multivariate initial sequence estimator that inherits both its asymptotic properties as well as the finite-sample superior performance. The effectiveness of the proposed estimator is shown via some MCMC example implementations. Further, we also present univariate and multivariate initial sequence estimators for when parallel MCMC chains are run and demonstrate their effectiveness over popular alternative.</summary></entry><entry><title type="html">Efficient estimation of longitudinal treatment effects using difference-in-differences and machine learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Efficientestimationoflongitudinaltreatmenteffectsusingdifferenceindifferencesandmachinelearning.html" rel="alternate" type="text/html" title="Efficient estimation of longitudinal treatment effects using difference-in-differences and machine learning" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Efficientestimationoflongitudinaltreatmenteffectsusingdifferenceindifferencesandmachinelearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Efficientestimationoflongitudinaltreatmenteffectsusingdifferenceindifferencesandmachinelearning.html">&lt;p&gt;Difference-in-differences is based on a parallel trends assumption, which states that changes over time in average potential outcomes are independent of treatment assignment, possibly conditional on covariates. With time-varying treatments, parallel trends assumptions can identify many types of parameters, but most work has focused on group-time average treatment effects and similar parameters conditional on the treatment trajectory. This paper focuses instead on identification and estimation of the intervention-specific mean - the mean potential outcome had everyone been exposed to a proposed intervention - which may be directly policy-relevant in some settings. Previous estimators for this parameter under parallel trends have relied on correctly-specified parametric models, which may be difficult to guarantee in applications. We develop multiply-robust and efficient estimators of the intervention-specific mean based on the efficient influence function, and derive conditions under which data-adaptive machine learning methods can be used to relax modeling assumptions. Our approach allows the parallel trends assumption to be conditional on the history of time-varying covariates, thus allowing for adjustment for time-varying covariates possibly impacted by prior treatments. Simulation results support the use of the proposed methods at modest sample sizes. As an example, we estimate the effect of a hypothetical federal minimum wage increase on self-rated health in the US.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16234&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nicholas Illenberger, Iván Díaz, Audrey Renson</name></author><category term="stat.ME" /><summary type="html">Difference-in-differences is based on a parallel trends assumption, which states that changes over time in average potential outcomes are independent of treatment assignment, possibly conditional on covariates. With time-varying treatments, parallel trends assumptions can identify many types of parameters, but most work has focused on group-time average treatment effects and similar parameters conditional on the treatment trajectory. This paper focuses instead on identification and estimation of the intervention-specific mean - the mean potential outcome had everyone been exposed to a proposed intervention - which may be directly policy-relevant in some settings. Previous estimators for this parameter under parallel trends have relied on correctly-specified parametric models, which may be difficult to guarantee in applications. We develop multiply-robust and efficient estimators of the intervention-specific mean based on the efficient influence function, and derive conditions under which data-adaptive machine learning methods can be used to relax modeling assumptions. Our approach allows the parallel trends assumption to be conditional on the history of time-varying covariates, thus allowing for adjustment for time-varying covariates possibly impacted by prior treatments. Simulation results support the use of the proposed methods at modest sample sizes. As an example, we estimate the effect of a hypothetical federal minimum wage increase on self-rated health in the US.</summary></entry><entry><title type="html">Exact and Approximate Conformal Inference for Multi-Output Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ExactandApproximateConformalInferenceforMultiOutputRegression.html" rel="alternate" type="text/html" title="Exact and Approximate Conformal Inference for Multi-Output Regression" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ExactandApproximateConformalInferenceforMultiOutputRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ExactandApproximateConformalInferenceforMultiOutputRegression.html">&lt;p&gt;It is common in machine learning to estimate a response $y$ given covariate information $x$. However, these predictions alone do not quantify any uncertainty associated with said predictions. One way to overcome this deficiency is with conformal inference methods, which construct a set containing the unobserved response $y$ with a prescribed probability. Unfortunately, even with a one-dimensional response, conformal inference is computationally expensive despite recent encouraging advances. In this paper, we explore multi-output regression, delivering exact derivations of conformal inference $p$-values when the predictive model can be described as a linear function of $y$. Additionally, we propose \texttt{unionCP} and a multivariate extension of \texttt{rootCP} as efficient ways of approximating the conformal prediction region for a wide array of multi-output predictors, both linear and nonlinear, while preserving computational advantages. We also provide both theoretical and empirical evidence of the effectiveness of these methods using both real-world and simulated data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.17405&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chancellor Johnstone, Eugene Ndiaye</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.OT," /><category term="stat.TH" /><summary type="html">It is common in machine learning to estimate a response $y$ given covariate information $x$. However, these predictions alone do not quantify any uncertainty associated with said predictions. One way to overcome this deficiency is with conformal inference methods, which construct a set containing the unobserved response $y$ with a prescribed probability. Unfortunately, even with a one-dimensional response, conformal inference is computationally expensive despite recent encouraging advances. In this paper, we explore multi-output regression, delivering exact derivations of conformal inference $p$-values when the predictive model can be described as a linear function of $y$. Additionally, we propose \texttt{unionCP} and a multivariate extension of \texttt{rootCP} as efficient ways of approximating the conformal prediction region for a wide array of multi-output predictors, both linear and nonlinear, while preserving computational advantages. We also provide both theoretical and empirical evidence of the effectiveness of these methods using both real-world and simulated data.</summary></entry><entry><title type="html">Exploring the difficulty of estimating win probability: a simulation study</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Exploringthedifficultyofestimatingwinprobabilityasimulationstudy.html" rel="alternate" type="text/html" title="Exploring the difficulty of estimating win probability: a simulation study" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Exploringthedifficultyofestimatingwinprobabilityasimulationstudy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Exploringthedifficultyofestimatingwinprobabilityasimulationstudy.html">&lt;p&gt;Estimating win probability is one of the classic modeling tasks of sports analytics. Many widely used win probability estimators are statistical win probability models, which fit the relationship between a binary win/loss outcome variable and certain game-state variables using data-driven regression or machine learning approaches. To illustrate just how difficult it is to accurately fit a statistical win probability model from noisy and highly correlated observational data, in this paper we conduct a simulation study. We create a simplified random walk version of football in which true win probability at each game-state is known, and we see how well a model recovers it. We find that the dependence structure of observational play-by-play data substantially inflates the bias and variance of estimators and lowers the effective sample size. This makes it essential to quantify uncertainty in win probability estimates, but typical bootstrapped confidence intervals are too narrow and don’t achieve nominal coverage. Hence, we introduce a novel method, the fractional bootstrap, to calibrate these intervals to achieve adequate coverage.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16171&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ryan S. Brill, Ronald Yurko, Abraham J. Wyner</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Estimating win probability is one of the classic modeling tasks of sports analytics. Many widely used win probability estimators are statistical win probability models, which fit the relationship between a binary win/loss outcome variable and certain game-state variables using data-driven regression or machine learning approaches. To illustrate just how difficult it is to accurately fit a statistical win probability model from noisy and highly correlated observational data, in this paper we conduct a simulation study. We create a simplified random walk version of football in which true win probability at each game-state is known, and we see how well a model recovers it. We find that the dependence structure of observational play-by-play data substantially inflates the bias and variance of estimators and lowers the effective sample size. This makes it essential to quantify uncertainty in win probability estimates, but typical bootstrapped confidence intervals are too narrow and don’t achieve nominal coverage. Hence, we introduce a novel method, the fractional bootstrap, to calibrate these intervals to achieve adequate coverage.</summary></entry><entry><title type="html">F-FOMAML: GNN-Enhanced Meta-Learning for Peak Period Demand Forecasting with Proxy Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/FFOMAMLGNNEnhancedMetaLearningforPeakPeriodDemandForecastingwithProxyData.html" rel="alternate" type="text/html" title="F-FOMAML: GNN-Enhanced Meta-Learning for Peak Period Demand Forecasting with Proxy Data" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/FFOMAMLGNNEnhancedMetaLearningforPeakPeriodDemandForecastingwithProxyData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/FFOMAMLGNNEnhancedMetaLearningforPeakPeriodDemandForecastingwithProxyData.html">&lt;p&gt;Demand prediction is a crucial task for e-commerce and physical retail businesses, especially during high-stake sales events. However, the limited availability of historical data from these peak periods poses a significant challenge for traditional forecasting methods. In this paper, we propose a novel approach that leverages strategically chosen proxy data reflective of potential sales patterns from similar entities during non-peak periods, enriched by features learned from a graph neural networks (GNNs)-based forecasting model, to predict demand during peak events. We formulate the demand prediction as a meta-learning problem and develop the Feature-based First-Order Model-Agnostic Meta-Learning (F-FOMAML) algorithm that leverages proxy data from non-peak periods and GNN-generated relational metadata to learn feature-specific layer parameters, thereby adapting to demand forecasts for peak events. Theoretically, we show that by considering domain similarities through task-specific metadata, our model achieves improved generalization, where the excess risk decreases as the number of training tasks increases. Empirical evaluations on large-scale industrial datasets demonstrate the superiority of our approach. Compared to existing state-of-the-art models, our method demonstrates a notable improvement in demand prediction accuracy, reducing the Mean Absolute Error by 26.24% on an internal vending machine dataset and by 1.04% on the publicly accessible JD.com dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16221&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zexing Xu, Linjun Zhang, Sitan Yang, Rasoul Etesami, Hanghang Tong, Huan Zhang, Jiawei Han</name></author><category term="stat.ME" /><summary type="html">Demand prediction is a crucial task for e-commerce and physical retail businesses, especially during high-stake sales events. However, the limited availability of historical data from these peak periods poses a significant challenge for traditional forecasting methods. In this paper, we propose a novel approach that leverages strategically chosen proxy data reflective of potential sales patterns from similar entities during non-peak periods, enriched by features learned from a graph neural networks (GNNs)-based forecasting model, to predict demand during peak events. We formulate the demand prediction as a meta-learning problem and develop the Feature-based First-Order Model-Agnostic Meta-Learning (F-FOMAML) algorithm that leverages proxy data from non-peak periods and GNN-generated relational metadata to learn feature-specific layer parameters, thereby adapting to demand forecasts for peak events. Theoretically, we show that by considering domain similarities through task-specific metadata, our model achieves improved generalization, where the excess risk decreases as the number of training tasks increases. Empirical evaluations on large-scale industrial datasets demonstrate the superiority of our approach. Compared to existing state-of-the-art models, our method demonstrates a notable improvement in demand prediction accuracy, reducing the Mean Absolute Error by 26.24% on an internal vending machine dataset and by 1.04% on the publicly accessible JD.com dataset.</summary></entry><entry><title type="html">Followers do not dictate the virality of news outlets on social media</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Followersdonotdictatetheviralityofnewsoutletsonsocialmedia.html" rel="alternate" type="text/html" title="Followers do not dictate the virality of news outlets on social media" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Followersdonotdictatetheviralityofnewsoutletsonsocialmedia</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Followersdonotdictatetheviralityofnewsoutletsonsocialmedia.html">&lt;p&gt;Initially conceived for entertainment, social media platforms have profoundly transformed the dissemination of information and consequently reshaped the dynamics of agenda-setting. In this scenario, understanding the factors that capture audience attention and drive viral content is crucial. Employing Gibrat’s Law, which posits that an entity’s growth rate is unrelated to its size, we examine the engagement growth dynamics of news outlets on social media. Our analysis encloses the Facebook historical data of over a thousand news outlets, encompassing approximately 57 million posts in four European languages from 2008 to the end of 2022. We discover universal growth dynamics according to which news virality is independent of the traditional size or engagement with the outlet. Moreover, our analysis reveals a significant long-term impact of news source reliability on engagement growth, with engagement induced by unreliable sources decreasing over time. We conclude the paper by presenting a statistical model replicating the observed growth dynamics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.17890&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Sangiorgio, Matteo Cinelli, Roy Cerqueti, Walter Quattrociocchi</name></author><category term="stat.AP" /><summary type="html">Initially conceived for entertainment, social media platforms have profoundly transformed the dissemination of information and consequently reshaped the dynamics of agenda-setting. In this scenario, understanding the factors that capture audience attention and drive viral content is crucial. Employing Gibrat’s Law, which posits that an entity’s growth rate is unrelated to its size, we examine the engagement growth dynamics of news outlets on social media. Our analysis encloses the Facebook historical data of over a thousand news outlets, encompassing approximately 57 million posts in four European languages from 2008 to the end of 2022. We discover universal growth dynamics according to which news virality is independent of the traditional size or engagement with the outlet. Moreover, our analysis reveals a significant long-term impact of news source reliability on engagement growth, with engagement induced by unreliable sources decreasing over time. We conclude the paper by presenting a statistical model replicating the observed growth dynamics.</summary></entry><entry><title type="html">Genealogical processes of non-neutral population models under rapid mutation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Genealogicalprocessesofnonneutralpopulationmodelsunderrapidmutation.html" rel="alternate" type="text/html" title="Genealogical processes of non-neutral population models under rapid mutation" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Genealogicalprocessesofnonneutralpopulationmodelsunderrapidmutation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Genealogicalprocessesofnonneutralpopulationmodelsunderrapidmutation.html">&lt;p&gt;We show that genealogical trees arising from a broad class of non-neutral models of population evolution converge to the Kingman coalescent under a suitable rescaling of time. As well as non-neutral biological evolution, our results apply to genetic algorithms encompassing the prominent class of sequential Monte Carlo (SMC) methods. The time rescaling we need differs slightly from that used in classical results for convergence to the Kingman coalescent, which has implications for the performance of different resampling schemes in SMC algorithms. In addition, our work substantially simplifies earlier proofs of convergence to the Kingman coalescent, and corrects an error common to several earlier results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16465&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jere Koskela, Paul A. Jenkins, Adam M. Johansen, Dario Spano</name></author><category term="stat.CO" /><summary type="html">We show that genealogical trees arising from a broad class of non-neutral models of population evolution converge to the Kingman coalescent under a suitable rescaling of time. As well as non-neutral biological evolution, our results apply to genetic algorithms encompassing the prominent class of sequential Monte Carlo (SMC) methods. The time rescaling we need differs slightly from that used in classical results for convergence to the Kingman coalescent, which has implications for the performance of different resampling schemes in SMC algorithms. In addition, our work substantially simplifies earlier proofs of convergence to the Kingman coalescent, and corrects an error common to several earlier results.</summary></entry><entry><title type="html">Generative Model for Change Point Detection in Dynamic Graphs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/GenerativeModelforChangePointDetectioninDynamicGraphs.html" rel="alternate" type="text/html" title="Generative Model for Change Point Detection in Dynamic Graphs" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/GenerativeModelforChangePointDetectioninDynamicGraphs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/GenerativeModelforChangePointDetectioninDynamicGraphs.html">&lt;p&gt;This paper proposes a generative model to detect change points in time series of graphs. The proposed framework consists of learnable prior distributions for low-dimensional graph representations and of a decoder that can generate graphs from the latent representations. The informative prior distributions in the latent spaces are learned from the observed data as empirical Bayes, and the expressive power of generative model is exploited to assist multiple change point detection. Specifically, the model parameters are learned via maximum approximate likelihood, with a Group Fused Lasso regularization on the prior parameters. The optimization problem is then solved via Alternating Direction Method of Multipliers (ADMM), and Langevin Dynamics are recruited for posterior inference. Experiments in both simulated and real data demonstrate the ability of the generative model in supporting change point detection with good performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.04719&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yik Lun Kei, Jialiang Li, Hangjian Li, Yanzhen Chen, Oscar Hernan Madrid Padilla</name></author><category term="stat.ME" /><summary type="html">This paper proposes a generative model to detect change points in time series of graphs. The proposed framework consists of learnable prior distributions for low-dimensional graph representations and of a decoder that can generate graphs from the latent representations. The informative prior distributions in the latent spaces are learned from the observed data as empirical Bayes, and the expressive power of generative model is exploited to assist multiple change point detection. Specifically, the model parameters are learned via maximum approximate likelihood, with a Group Fused Lasso regularization on the prior parameters. The optimization problem is then solved via Alternating Direction Method of Multipliers (ADMM), and Langevin Dynamics are recruited for posterior inference. Experiments in both simulated and real data demonstrate the ability of the generative model in supporting change point detection with good performance.</summary></entry><entry><title type="html">Graphical copula GARCH modeling with dynamic conditional dependence</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/GraphicalcopulaGARCHmodelingwithdynamicconditionaldependence.html" rel="alternate" type="text/html" title="Graphical copula GARCH modeling with dynamic conditional dependence" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/GraphicalcopulaGARCHmodelingwithdynamicconditionaldependence</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/GraphicalcopulaGARCHmodelingwithdynamicconditionaldependence.html">&lt;p&gt;Modeling returns on large portfolios is a challenging problem as the number of parameters in the covariance matrix grows as the square of the size of the portfolio. Traditional correlation models, for example, the dynamic conditional correlation (DCC)-GARCH model, often ignore the nonlinear dependencies in the tail of the return distribution. In this paper, we aim to develop a framework to model the nonlinear dependencies dynamically, namely the graphical copula GARCH (GC-GARCH) model. Motivated from the capital asset pricing model, to allow modeling of large portfolios, the number of parameters can be greatly reduced by introducing conditional independence among stocks given some risk factors. The joint distribution of the risk factors is factorized using a directed acyclic graph (DAG) with pair-copula construction (PCC) to enhance the modeling of the tails of the return distribution while offering the flexibility of having complex dependent structures. The DAG induces topological orders to the risk factors, which can be regarded as a list of directions of the flow of information. The conditional distributions among stock returns are also modeled using PCC. Dynamic conditional dependence structures are incorporated to allow the parameters in the copulas to be time-varying. Three-stage estimation is used to estimate parameters in the marginal distributions, the risk factor copulas, and the stock copulas. The simulation study shows that the proposed estimation procedure can estimate the parameters and the underlying DAG structure accurately. In the investment experiment of the empirical study, we demonstrate that the GC-GARCH model produces more precise conditional value-at-risk prediction and considerably higher cumulative portfolio returns than the DCC-GARCH model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15582&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lupe Shun Hin Chan, Amanda Man Ying Chu, Mike Ka Pui So</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Modeling returns on large portfolios is a challenging problem as the number of parameters in the covariance matrix grows as the square of the size of the portfolio. Traditional correlation models, for example, the dynamic conditional correlation (DCC)-GARCH model, often ignore the nonlinear dependencies in the tail of the return distribution. In this paper, we aim to develop a framework to model the nonlinear dependencies dynamically, namely the graphical copula GARCH (GC-GARCH) model. Motivated from the capital asset pricing model, to allow modeling of large portfolios, the number of parameters can be greatly reduced by introducing conditional independence among stocks given some risk factors. The joint distribution of the risk factors is factorized using a directed acyclic graph (DAG) with pair-copula construction (PCC) to enhance the modeling of the tails of the return distribution while offering the flexibility of having complex dependent structures. The DAG induces topological orders to the risk factors, which can be regarded as a list of directions of the flow of information. The conditional distributions among stock returns are also modeled using PCC. Dynamic conditional dependence structures are incorporated to allow the parameters in the copulas to be time-varying. Three-stage estimation is used to estimate parameters in the marginal distributions, the risk factor copulas, and the stock copulas. The simulation study shows that the proposed estimation procedure can estimate the parameters and the underlying DAG structure accurately. In the investment experiment of the empirical study, we demonstrate that the GC-GARCH model produces more precise conditional value-at-risk prediction and considerably higher cumulative portfolio returns than the DCC-GARCH model.</summary></entry><entry><title type="html">Hedging in Sequential Experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/HedginginSequentialExperiments.html" rel="alternate" type="text/html" title="Hedging in Sequential Experiments" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/HedginginSequentialExperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/HedginginSequentialExperiments.html">&lt;p&gt;Experimentation involves risk. The investigator expends time and money in the pursuit of data that supports a hypothesis. In the end, the investigator may find that all of these costs were for naught and the data fail to reject the null. Furthermore, the investigator may not be able to test other hypotheses with the same data set in order to avoid false positives due to p-hacking. Therefore, there is a need for a mechanism for investigators to hedge the risk of financial and statistical bankruptcy in the business of experimentation.
  In this work, we build on the game-theoretic statistics framework to enable an investigator to hedge their bets against the null hypothesis and thus avoid ruin. First, we describe a method by which the investigator’s test martingale wealth process can be capitalized by solving for the risk-neutral price. Then, we show that a portfolio that comprises the risky test martingale and a risk-free process is still a test martingale which enables the investigator to select a particular risk-return position using Markowitz portfolio theory. Finally, we show that a function that is derivative of the test martingale process can be constructed and used as a hedging instrument by the investigator or as a speculative instrument by a risk-seeking investor who wants to participate in the potential returns of the uncertain experiment wealth process. Together, these instruments enable an investigator to hedge the risk of ruin and they enable a investigator to efficiently hedge experimental risk.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15867&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thomas Cook, Patrick Flaherty</name></author><category term="stat.ME" /><summary type="html">Experimentation involves risk. The investigator expends time and money in the pursuit of data that supports a hypothesis. In the end, the investigator may find that all of these costs were for naught and the data fail to reject the null. Furthermore, the investigator may not be able to test other hypotheses with the same data set in order to avoid false positives due to p-hacking. Therefore, there is a need for a mechanism for investigators to hedge the risk of financial and statistical bankruptcy in the business of experimentation. In this work, we build on the game-theoretic statistics framework to enable an investigator to hedge their bets against the null hypothesis and thus avoid ruin. First, we describe a method by which the investigator’s test martingale wealth process can be capitalized by solving for the risk-neutral price. Then, we show that a portfolio that comprises the risky test martingale and a risk-free process is still a test martingale which enables the investigator to select a particular risk-return position using Markowitz portfolio theory. Finally, we show that a function that is derivative of the test martingale process can be constructed and used as a hedging instrument by the investigator or as a speculative instrument by a risk-seeking investor who wants to participate in the potential returns of the uncertain experiment wealth process. Together, these instruments enable an investigator to hedge the risk of ruin and they enable a investigator to efficiently hedge experimental risk.</summary></entry><entry><title type="html">Heterogeneous peer effects of college roommates on academic performance</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Heterogeneouspeereffectsofcollegeroommatesonacademicperformance.html" rel="alternate" type="text/html" title="Heterogeneous peer effects of college roommates on academic performance" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Heterogeneouspeereffectsofcollegeroommatesonacademicperformance</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Heterogeneouspeereffectsofcollegeroommatesonacademicperformance.html">&lt;p&gt;Understanding how student peers influence learning outcomes is crucial for effective education management in complex social systems. The complexities of peer selection and evolving peer relationships, however, pose challenges for identifying peer effects using static observational data. Here we use both null-model and regression approaches to examine peer effects using longitudinal data from 5,272 undergraduates, where roommate assignments are plausibly random upon enrollment and roommate relationships persist until graduation. Specifically, we construct a roommate null model by randomly shuffling students among dorm rooms and introduce an assimilation metric to quantify similarities in roommate academic performance. We find significantly larger assimilation in actual data than in the roommate null model, suggesting roommate peer effects, whereby roommates have more similar performance than expected by chance alone. Moreover, assimilation exhibits an overall increasing trend over time, suggesting that peer effects become stronger the longer roommates live together. Our regression analysis further reveals the moderating role of peer heterogeneity. In particular, when roommates perform similarly, the positive relationship between a student’s future performance and their roommates’ average prior performance is more pronounced, and their ordinal rank in the dorm room has an independent effect. Our findings contribute to understanding the role of college roommates in influencing student academic performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15439&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yi Cao, Tao Zhou, Jian Gao</name></author><category term="stat.AP" /><summary type="html">Understanding how student peers influence learning outcomes is crucial for effective education management in complex social systems. The complexities of peer selection and evolving peer relationships, however, pose challenges for identifying peer effects using static observational data. Here we use both null-model and regression approaches to examine peer effects using longitudinal data from 5,272 undergraduates, where roommate assignments are plausibly random upon enrollment and roommate relationships persist until graduation. Specifically, we construct a roommate null model by randomly shuffling students among dorm rooms and introduce an assimilation metric to quantify similarities in roommate academic performance. We find significantly larger assimilation in actual data than in the roommate null model, suggesting roommate peer effects, whereby roommates have more similar performance than expected by chance alone. Moreover, assimilation exhibits an overall increasing trend over time, suggesting that peer effects become stronger the longer roommates live together. Our regression analysis further reveals the moderating role of peer heterogeneity. In particular, when roommates perform similarly, the positive relationship between a student’s future performance and their roommates’ average prior performance is more pronounced, and their ordinal rank in the dorm room has an independent effect. Our findings contribute to understanding the role of college roommates in influencing student academic performance.</summary></entry><entry><title type="html">How big does a population need to be before demographers can ignore individual-level randomness in demographic events?</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Howbigdoesapopulationneedtobebeforedemographerscanignoreindividuallevelrandomnessindemographicevents.html" rel="alternate" type="text/html" title="How big does a population need to be before demographers can ignore individual-level randomness in demographic events?" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Howbigdoesapopulationneedtobebeforedemographerscanignoreindividuallevelrandomnessindemographicevents</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Howbigdoesapopulationneedtobebeforedemographerscanignoreindividuallevelrandomnessindemographicevents.html">&lt;p&gt;When studying a national-level population, demographers can safely ignore the effect of individual-level randomness on age-sex structure. When studying a single community, or group of communities, however, the potential importance of individual-level randomness is less clear. We seek to measure the effect of individual-level randomness in births and deaths on standard summary indicators of age-sex structure, for populations of different sizes, focusing on on demographic conditions typical of historical populations. We conduct a microsimulation experiment where we simulate events and age-sex structure under a range of settings for demographic rates and population size. The experiment results suggest that individual-level randomness strongly affects age-sex structure for populations of about 100, but has a much smaller effect on populations of 1,000, and a negligible effect on populations of 10,000. Our conclusion is that analyses of age-sex structure in historical populations with sizes on the order 100 must account for individual-level randomness in demographic events. Analyses of populations with sizes on the order of 1,000 may need to make some allowance for individual-level variation, but other issues, such as measurement error, probably deserve more attention. Analyses of populations of 10,000 can safely ignore individual-level variation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15514&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>John Bryant , Tahu Kukutai , Junni L. Zhang</name></author><category term="stat.ME" /><summary type="html">When studying a national-level population, demographers can safely ignore the effect of individual-level randomness on age-sex structure. When studying a single community, or group of communities, however, the potential importance of individual-level randomness is less clear. We seek to measure the effect of individual-level randomness in births and deaths on standard summary indicators of age-sex structure, for populations of different sizes, focusing on on demographic conditions typical of historical populations. We conduct a microsimulation experiment where we simulate events and age-sex structure under a range of settings for demographic rates and population size. The experiment results suggest that individual-level randomness strongly affects age-sex structure for populations of about 100, but has a much smaller effect on populations of 1,000, and a negligible effect on populations of 10,000. Our conclusion is that analyses of age-sex structure in historical populations with sizes on the order 100 must account for individual-level randomness in demographic events. Analyses of populations with sizes on the order of 1,000 may need to make some allowance for individual-level variation, but other issues, such as measurement error, probably deserve more attention. Analyses of populations of 10,000 can safely ignore individual-level variation.</summary></entry><entry><title type="html">Informative Simultaneous Confidence Intervals for Graphical Test Procedures</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/InformativeSimultaneousConfidenceIntervalsforGraphicalTestProcedures.html" rel="alternate" type="text/html" title="Informative Simultaneous Confidence Intervals for Graphical Test Procedures" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/InformativeSimultaneousConfidenceIntervalsforGraphicalTestProcedures</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/InformativeSimultaneousConfidenceIntervalsforGraphicalTestProcedures.html">&lt;p&gt;Simultaneous confidence intervals (SCIs) that are compatible with a given closed test procedure are often non-informative. More precisely, for a one-sided null hypothesis, the bound of the SCI can stick to the border of the null hypothesis, irrespective of how far the point estimate deviates from the null hypothesis. This has been illustrated for the Bonferroni-Holm and fall-back procedures, for which alternative SCIs have been suggested, that are free of this deficiency. These informative SCIs are not fully compatible with the initial multiple test, but are close to it and hence provide similar power advantages. They provide a multiple hypothesis test with strong family-wise error rate control that can be used in replacement of the initial multiple test. The current paper extends previous work for informative SCIs to graphical test procedures. The information gained from the newly suggested SCIs is shown to be always increasing with increasing evidence against a null hypothesis. The new SCIs provide a compromise between information gain and the goal to reject as many hypotheses as possible. The SCIs are defined via a family of dual graphs and the projection method. A simple iterative algorithm for the computation of the intervals is provided. A simulation study illustrates the results for a complex graphical test procedure.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.13719&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Werner Brannath, Liane Kluge, Martin Scharpenberg</name></author><category term="stat.ME" /><summary type="html">Simultaneous confidence intervals (SCIs) that are compatible with a given closed test procedure are often non-informative. More precisely, for a one-sided null hypothesis, the bound of the SCI can stick to the border of the null hypothesis, irrespective of how far the point estimate deviates from the null hypothesis. This has been illustrated for the Bonferroni-Holm and fall-back procedures, for which alternative SCIs have been suggested, that are free of this deficiency. These informative SCIs are not fully compatible with the initial multiple test, but are close to it and hence provide similar power advantages. They provide a multiple hypothesis test with strong family-wise error rate control that can be used in replacement of the initial multiple test. The current paper extends previous work for informative SCIs to graphical test procedures. The information gained from the newly suggested SCIs is shown to be always increasing with increasing evidence against a null hypothesis. The new SCIs provide a compromise between information gain and the goal to reject as many hypotheses as possible. The SCIs are defined via a family of dual graphs and the projection method. A simple iterative algorithm for the computation of the intervals is provided. A simulation study illustrates the results for a complex graphical test procedure.</summary></entry><entry><title type="html">Learning When the Concept Shifts: Confounding, Invariance, and Dimension Reduction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/LearningWhentheConceptShiftsConfoundingInvarianceandDimensionReduction.html" rel="alternate" type="text/html" title="Learning When the Concept Shifts: Confounding, Invariance, and Dimension Reduction" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/LearningWhentheConceptShiftsConfoundingInvarianceandDimensionReduction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/LearningWhentheConceptShiftsConfoundingInvarianceandDimensionReduction.html">&lt;p&gt;Practitioners often deploy a learned prediction model in a new environment where the joint distribution of covariate and response has shifted. In observational data, the distribution shift is often driven by unobserved confounding factors lurking in the environment, with the underlying mechanism unknown. Confounding can obfuscate the definition of the best prediction model (concept shift) and shift covariates to domains yet unseen (covariate shift). Therefore, a model maximizing prediction accuracy in the source environment could suffer a significant accuracy drop in the target environment. This motivates us to study the domain adaptation problem with observational data: given labeled covariate and response pairs from a source environment, and unlabeled covariates from a target environment, how can one predict the missing target response reliably? We root the adaptation problem in a linear structural causal model to address endogeneity and unobserved confounding. We study the necessity and benefit of leveraging exogenous, invariant covariate representations to cure concept shifts and improve target prediction. This further motivates a new representation learning method for adaptation that optimizes for a lower-dimensional linear subspace and, subsequently, a prediction model confined to that subspace. The procedure operates on a non-convex objective-that naturally interpolates between predictability and stability/invariance-constrained on the Stiefel manifold. We study the optimization landscape and prove that, when the regularization is sufficient, nearly all local optima align with an invariant linear subspace resilient to both concept and covariate shift. In terms of predictability, we show a model that uses the learned lower-dimensional subspace can incur a nearly ideal gap between target and source risk. Three real-world data sets are investigated to validate our method and theory.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15904&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kulunu Dharmakeerthi, YoonHaeng Hur, Tengyuan Liang</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Practitioners often deploy a learned prediction model in a new environment where the joint distribution of covariate and response has shifted. In observational data, the distribution shift is often driven by unobserved confounding factors lurking in the environment, with the underlying mechanism unknown. Confounding can obfuscate the definition of the best prediction model (concept shift) and shift covariates to domains yet unseen (covariate shift). Therefore, a model maximizing prediction accuracy in the source environment could suffer a significant accuracy drop in the target environment. This motivates us to study the domain adaptation problem with observational data: given labeled covariate and response pairs from a source environment, and unlabeled covariates from a target environment, how can one predict the missing target response reliably? We root the adaptation problem in a linear structural causal model to address endogeneity and unobserved confounding. We study the necessity and benefit of leveraging exogenous, invariant covariate representations to cure concept shifts and improve target prediction. This further motivates a new representation learning method for adaptation that optimizes for a lower-dimensional linear subspace and, subsequently, a prediction model confined to that subspace. The procedure operates on a non-convex objective-that naturally interpolates between predictability and stability/invariance-constrained on the Stiefel manifold. We study the optimization landscape and prove that, when the regularization is sufficient, nearly all local optima align with an invariant linear subspace resilient to both concept and covariate shift. In terms of predictability, we show a model that uses the learned lower-dimensional subspace can incur a nearly ideal gap between target and source risk. Three real-world data sets are investigated to validate our method and theory.</summary></entry><entry><title type="html">METRIK: Measurement-Efficient Randomized Controlled Trials using Transformers with Input Masking</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/METRIKMeasurementEfficientRandomizedControlledTrialsusingTransformerswithInputMasking.html" rel="alternate" type="text/html" title="METRIK: Measurement-Efficient Randomized Controlled Trials using Transformers with Input Masking" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/METRIKMeasurementEfficientRandomizedControlledTrialsusingTransformerswithInputMasking</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/METRIKMeasurementEfficientRandomizedControlledTrialsusingTransformerswithInputMasking.html">&lt;p&gt;Clinical randomized controlled trials (RCTs) collect hundreds of measurements spanning various metric types (e.g., laboratory tests, cognitive/motor assessments, etc.) across 100s-1000s of subjects to evaluate the effect of a treatment, but do so at the cost of significant trial expense. To reduce the number of measurements, trial protocols can be revised to remove metrics extraneous to the study’s objective, but doing so requires additional human labor and limits the set of hypotheses that can be studied with the collected data. In contrast, a planned missing design (PMD) can reduce the amount of data collected without removing any metric by imputing the unsampled data. Standard PMDs randomly sample data to leverage statistical properties of imputation algorithms, but are ad hoc, hence suboptimal. Methods that learn PMDs produce more sample-efficient PMDs, but are not suitable for RCTs because they require ample prior data (150+ subjects) to model the data distribution. Therefore, we introduce a framework called Measurement EfficienT Randomized Controlled Trials using Transformers with Input MasKing (METRIK), which, for the first time, calculates a PMD specific to the RCT from a modest amount of prior data (e.g., 60 subjects). Specifically, METRIK models the PMD as a learnable input masking layer that is optimized with a state-of-the-art imputer based on the Transformer architecture. METRIK implements a novel sampling and selection algorithm to generate a PMD that satisfies the trial designer’s objective, i.e., whether to maximize sampling efficiency or imputation performance for a given sampling budget. Evaluated across five real-world clinical RCT datasets, METRIK increases the sampling efficiency of and imputation performance under the generated PMD by leveraging correlations over time and across metrics, thereby removing the need to manually remove metrics from the RCT.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16351&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sayeri Lala , Niraj K. Jha</name></author><category term="stat.ME" /><summary type="html">Clinical randomized controlled trials (RCTs) collect hundreds of measurements spanning various metric types (e.g., laboratory tests, cognitive/motor assessments, etc.) across 100s-1000s of subjects to evaluate the effect of a treatment, but do so at the cost of significant trial expense. To reduce the number of measurements, trial protocols can be revised to remove metrics extraneous to the study’s objective, but doing so requires additional human labor and limits the set of hypotheses that can be studied with the collected data. In contrast, a planned missing design (PMD) can reduce the amount of data collected without removing any metric by imputing the unsampled data. Standard PMDs randomly sample data to leverage statistical properties of imputation algorithms, but are ad hoc, hence suboptimal. Methods that learn PMDs produce more sample-efficient PMDs, but are not suitable for RCTs because they require ample prior data (150+ subjects) to model the data distribution. Therefore, we introduce a framework called Measurement EfficienT Randomized Controlled Trials using Transformers with Input MasKing (METRIK), which, for the first time, calculates a PMD specific to the RCT from a modest amount of prior data (e.g., 60 subjects). Specifically, METRIK models the PMD as a learnable input masking layer that is optimized with a state-of-the-art imputer based on the Transformer architecture. METRIK implements a novel sampling and selection algorithm to generate a PMD that satisfies the trial designer’s objective, i.e., whether to maximize sampling efficiency or imputation performance for a given sampling budget. Evaluated across five real-world clinical RCT datasets, METRIK increases the sampling efficiency of and imputation performance under the generated PMD by leveraging correlations over time and across metrics, thereby removing the need to manually remove metrics from the RCT.</summary></entry><entry><title type="html">Meta-experiments: Improving experimentation through experimentation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/MetaexperimentsImprovingexperimentationthroughexperimentation.html" rel="alternate" type="text/html" title="Meta-experiments: Improving experimentation through experimentation" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/MetaexperimentsImprovingexperimentationthroughexperimentation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/MetaexperimentsImprovingexperimentationthroughexperimentation.html">&lt;p&gt;A/B testing is widexly used in the industry to optimize customer facing websites. Many companies employ experimentation specialists to facilitate and improve the process of A/B testing. Here, we present the application of A/B testing to this improvement effort itself, by running experiments on the experimentation process, which we call ‘meta-experiments’. We discuss the challenges of this approach using the example of one of our meta-experiments, which helped experimenters to run more sufficiently powered A/B tests. We also point out the benefits of ‘dog fooding’ for the experimentation specialists when running their own experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16629&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Melanie J. I. Müller</name></author><category term="stat.AP" /><summary type="html">A/B testing is widexly used in the industry to optimize customer facing websites. Many companies employ experimentation specialists to facilitate and improve the process of A/B testing. Here, we present the application of A/B testing to this improvement effort itself, by running experiments on the experimentation process, which we call ‘meta-experiments’. We discuss the challenges of this approach using the example of one of our meta-experiments, which helped experimenters to run more sufficiently powered A/B tests. We also point out the benefits of ‘dog fooding’ for the experimentation specialists when running their own experiments.</summary></entry><entry><title type="html">Mixture of Directed Graphical Models for Discrete Spatial Random Fields</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/MixtureofDirectedGraphicalModelsforDiscreteSpatialRandomFields.html" rel="alternate" type="text/html" title="Mixture of Directed Graphical Models for Discrete Spatial Random Fields" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/MixtureofDirectedGraphicalModelsforDiscreteSpatialRandomFields</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/MixtureofDirectedGraphicalModelsforDiscreteSpatialRandomFields.html">&lt;p&gt;Current approaches for modeling discrete-valued outcomes associated with spatially-dependent areal units incur computational and theoretical challenges, especially in the Bayesian setting when full posterior inference is desired. As an alternative, we propose a novel statistical modeling framework for this data setting, namely a mixture of directed graphical models (MDGMs). The components of the mixture, directed graphical models, can be represented by directed acyclic graphs (DAGs) and are computationally quick to evaluate. The DAGs representing the mixture components are selected to correspond to an undirected graphical representation of an assumed spatial contiguity/dependence structure of the areal units, which underlies the specification of traditional modeling approaches for discrete spatial processes such as Markov random fields (MRFs). We introduce the concept of compatibility to show how an undirected graph can be used as a template for the structural dependencies between areal units to create sets of DAGs which, as a collection, preserve the structural dependencies represented in the template undirected graph. We then introduce three classes of compatible DAGs and corresponding algorithms for fitting MDGMs based on these classes. In addition, we compare MDGMs to MRFs and a popular Bayesian MRF model approximation used in high-dimensional settings in a series of simulations and an analysis of ecometrics data collected as part of the Adolescent Health and Development in Context Study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15700&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>J. Brandon Carter, Catherine A. Calder</name></author><category term="stat.ME" /><summary type="html">Current approaches for modeling discrete-valued outcomes associated with spatially-dependent areal units incur computational and theoretical challenges, especially in the Bayesian setting when full posterior inference is desired. As an alternative, we propose a novel statistical modeling framework for this data setting, namely a mixture of directed graphical models (MDGMs). The components of the mixture, directed graphical models, can be represented by directed acyclic graphs (DAGs) and are computationally quick to evaluate. The DAGs representing the mixture components are selected to correspond to an undirected graphical representation of an assumed spatial contiguity/dependence structure of the areal units, which underlies the specification of traditional modeling approaches for discrete spatial processes such as Markov random fields (MRFs). We introduce the concept of compatibility to show how an undirected graph can be used as a template for the structural dependencies between areal units to create sets of DAGs which, as a collection, preserve the structural dependencies represented in the template undirected graph. We then introduce three classes of compatible DAGs and corresponding algorithms for fitting MDGMs based on these classes. In addition, we compare MDGMs to MRFs and a popular Bayesian MRF model approximation used in high-dimensional settings in a series of simulations and an analysis of ecometrics data collected as part of the Adolescent Health and Development in Context Study.</summary></entry><entry><title type="html">Modern approaches for evaluating treatment effect heterogeneity from clinical trials and observational data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Modernapproachesforevaluatingtreatmenteffectheterogeneityfromclinicaltrialsandobservationaldata.html" rel="alternate" type="text/html" title="Modern approaches for evaluating treatment effect heterogeneity from clinical trials and observational data" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Modernapproachesforevaluatingtreatmenteffectheterogeneityfromclinicaltrialsandobservationaldata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Modernapproachesforevaluatingtreatmenteffectheterogeneityfromclinicaltrialsandobservationaldata.html">&lt;p&gt;In this paper we review recent advances in statistical methods for the evaluation of the heterogeneity of treatment effects (HTE), including subgroup identification and estimation of individualized treatment regimens, from randomized clinical trials and observational studies. We identify several types of approaches using the features introduced in Lipkovich, Dmitrienko and D’Agostino (2017) that distinguish the recommended principled methods from basic methods for HTE evaluation that typically rely on rules of thumb and general guidelines (the methods are often referred to as common practices). We discuss the advantages and disadvantages of various principled methods as well as common measures for evaluating their performance. We use simulated data and a case study based on a historical clinical trial to illustrate several new approaches to HTE evaluation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.14889&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ilya Lipkovich, David Svensson, Bohdana Ratitch, Alex Dmitrienko</name></author><category term="stat.ME" /><summary type="html">In this paper we review recent advances in statistical methods for the evaluation of the heterogeneity of treatment effects (HTE), including subgroup identification and estimation of individualized treatment regimens, from randomized clinical trials and observational studies. We identify several types of approaches using the features introduced in Lipkovich, Dmitrienko and D’Agostino (2017) that distinguish the recommended principled methods from basic methods for HTE evaluation that typically rely on rules of thumb and general guidelines (the methods are often referred to as common practices). We discuss the advantages and disadvantages of various principled methods as well as common measures for evaluating their performance. We use simulated data and a case study based on a historical clinical trial to illustrate several new approaches to HTE evaluation.</summary></entry><entry><title type="html">Multistep Criticality Search and Power Shaping in Microreactors with Reinforcement Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/MultistepCriticalitySearchandPowerShapinginMicroreactorswithReinforcementLearning.html" rel="alternate" type="text/html" title="Multistep Criticality Search and Power Shaping in Microreactors with Reinforcement Learning" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/MultistepCriticalitySearchandPowerShapinginMicroreactorswithReinforcementLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/MultistepCriticalitySearchandPowerShapinginMicroreactorswithReinforcementLearning.html">&lt;p&gt;Reducing operation and maintenance costs is a key objective for advanced reactors in general and microreactors in particular. To achieve this reduction, developing robust autonomous control algorithms is essential to ensure safe and autonomous reactor operation. Recently, artificial intelligence and machine learning algorithms, specifically reinforcement learning (RL) algorithms, have seen rapid increased application to control problems, such as plasma control in fusion tokamaks and building energy management. In this work, we introduce the use of RL for intelligent control in nuclear microreactors. The RL agent is trained using proximal policy optimization (PPO) and advantage actor-critic (A2C), cutting-edge deep RL techniques, based on a high-fidelity simulation of a microreactor design inspired by the Westinghouse eVinci\textsuperscript{TM} design. We utilized a Serpent model to generate data on drum positions, core criticality, and core power distribution for training a feedforward neural network surrogate model. This surrogate model was then used to guide a PPO and A2C control policies in determining the optimal drum position across various reactor burnup states, ensuring critical core conditions and symmetrical power distribution across all six core portions. The results demonstrate the excellent performance of PPO in identifying optimal drum positions, achieving a hextant power tilt ratio of approximately 1.002 (within the limit of $&amp;lt;$ 1.02) and maintaining criticality within a 10 pcm range. A2C did not provide as competitive of a performance as PPO in terms of performance metrics for all burnup steps considered in the cycle. Additionally, the results highlight the capability of well-trained RL control policies to quickly identify control actions, suggesting a promising approach for enabling real-time autonomous control through digital twins.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15931&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Majdi I. Radaideh, Leo Tunkle, Dean Price, Kamal Abdulraheem, Linyu Lin, Moutaz Elias</name></author><category term="stat.AP" /><summary type="html">Reducing operation and maintenance costs is a key objective for advanced reactors in general and microreactors in particular. To achieve this reduction, developing robust autonomous control algorithms is essential to ensure safe and autonomous reactor operation. Recently, artificial intelligence and machine learning algorithms, specifically reinforcement learning (RL) algorithms, have seen rapid increased application to control problems, such as plasma control in fusion tokamaks and building energy management. In this work, we introduce the use of RL for intelligent control in nuclear microreactors. The RL agent is trained using proximal policy optimization (PPO) and advantage actor-critic (A2C), cutting-edge deep RL techniques, based on a high-fidelity simulation of a microreactor design inspired by the Westinghouse eVinci\textsuperscript{TM} design. We utilized a Serpent model to generate data on drum positions, core criticality, and core power distribution for training a feedforward neural network surrogate model. This surrogate model was then used to guide a PPO and A2C control policies in determining the optimal drum position across various reactor burnup states, ensuring critical core conditions and symmetrical power distribution across all six core portions. The results demonstrate the excellent performance of PPO in identifying optimal drum positions, achieving a hextant power tilt ratio of approximately 1.002 (within the limit of $&amp;lt;$ 1.02) and maintaining criticality within a 10 pcm range. A2C did not provide as competitive of a performance as PPO in terms of performance metrics for all burnup steps considered in the cycle. Additionally, the results highlight the capability of well-trained RL control policies to quickly identify control actions, suggesting a promising approach for enabling real-time autonomous control through digital twins.</summary></entry><entry><title type="html">Nonparametric FBST for Validating Linear Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/NonparametricFBSTforValidatingLinearModels.html" rel="alternate" type="text/html" title="Nonparametric FBST for Validating Linear Models" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/NonparametricFBSTforValidatingLinearModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/NonparametricFBSTforValidatingLinearModels.html">&lt;p&gt;The Full Bayesian Significance Test (FBST) possesses many desirable aspects, such as not requiring a non-zero prior probability for hypotheses while also producing a measure of evidence for $H_0$. Still, few attempts have been made to bring the FBST to nonparametric settings, with the main drawback being the need to obtain the highest posterior density (HPD) in a function space. In this work, we use Gaussian processes to provide an analytically tractable FBST for hypotheses of the type \(H_0: g(\boldsymbol{x}) = \boldsymbol{b}(\boldsymbol{x})\boldsymbol{\beta}, \quad \forall \boldsymbol{x} \in \mathcal{X}, \quad \boldsymbol{\beta} \in \mathbb{R}^k,\) where $g(\cdot)$ is the regression function, $\boldsymbol{b}(\cdot)$ is a vector of linearly independent linear functions – such as $\boldsymbol{b}(\boldsymbol{x}) = \boldsymbol{x}’$ – and $\mathcal{X}$ is the covariates’ domain. We also make use of pragmatic hypotheses to verify if the adherence of linear models may be approximately instead of exactly true, allowing for the inclusion of valuable information such as measurement errors and utility judgments. This contribution extends the theory of the FBST, allowing its application in nonparametric settings and providing a procedure that easily tests if linear models are adequate for the data and that can automatically perform variable selection.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15608&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rodrigo F. L. Lassance, Julio M. Stern, Rafael B. Stern</name></author><category term="stat.ME" /><summary type="html">The Full Bayesian Significance Test (FBST) possesses many desirable aspects, such as not requiring a non-zero prior probability for hypotheses while also producing a measure of evidence for $H_0$. Still, few attempts have been made to bring the FBST to nonparametric settings, with the main drawback being the need to obtain the highest posterior density (HPD) in a function space. In this work, we use Gaussian processes to provide an analytically tractable FBST for hypotheses of the type \(H_0: g(\boldsymbol{x}) = \boldsymbol{b}(\boldsymbol{x})\boldsymbol{\beta}, \quad \forall \boldsymbol{x} \in \mathcal{X}, \quad \boldsymbol{\beta} \in \mathbb{R}^k,\) where $g(\cdot)$ is the regression function, $\boldsymbol{b}(\cdot)$ is a vector of linearly independent linear functions – such as $\boldsymbol{b}(\boldsymbol{x}) = \boldsymbol{x}’$ – and $\mathcal{X}$ is the covariates’ domain. We also make use of pragmatic hypotheses to verify if the adherence of linear models may be approximately instead of exactly true, allowing for the inclusion of valuable information such as measurement errors and utility judgments. This contribution extends the theory of the FBST, allowing its application in nonparametric settings and providing a procedure that easily tests if linear models are adequate for the data and that can automatically perform variable selection.</summary></entry><entry><title type="html">On the extensions of the Chatterjee-Spearman test</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/OntheextensionsoftheChatterjeeSpearmantest.html" rel="alternate" type="text/html" title="On the extensions of the Chatterjee-Spearman test" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/OntheextensionsoftheChatterjeeSpearmantest</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/OntheextensionsoftheChatterjeeSpearmantest.html">&lt;p&gt;Chatterjee (2021) introduced a novel independence test that is rank-based, asymptotically normal and consistent against all alternatives. One limitation of Chatterjee’s test is its low statistical power for detecting monotonic relationships. To address this limitation, in our previous work (Zhang, 2024, Commun. Stat. - Theory Methods), we proposed to combine Chatterjee’s and Spearman’s correlations into a max-type test and established the asymptotic joint normality. This work examines three key extensions of the combined test. First, motivated by its original asymmetric form, we extend the Chatterjee-Spearman test to a symmetric version, and derive the asymptotic null distribution of the symmetrized statistic. Second, we investigate the relationships between Chatterjee’s correlation and other popular rank correlations, including Kendall’s tau and quadrant correlation. We demonstrate that, under independence, Chatterjee’s correlation and any of these rank correlations are asymptotically joint normal and independent. Simulation studies demonstrate that the Chatterjee-Kendall test has better power than the Chatterjee-Spearman test. Finally, we explore two possible extensions to the multivariate case. These extensions expand the applicability of the rank-based combined tests to a broader range of scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16859&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Qingyang Zhang</name></author><category term="stat.ME" /><summary type="html">Chatterjee (2021) introduced a novel independence test that is rank-based, asymptotically normal and consistent against all alternatives. One limitation of Chatterjee’s test is its low statistical power for detecting monotonic relationships. To address this limitation, in our previous work (Zhang, 2024, Commun. Stat. - Theory Methods), we proposed to combine Chatterjee’s and Spearman’s correlations into a max-type test and established the asymptotic joint normality. This work examines three key extensions of the combined test. First, motivated by its original asymmetric form, we extend the Chatterjee-Spearman test to a symmetric version, and derive the asymptotic null distribution of the symmetrized statistic. Second, we investigate the relationships between Chatterjee’s correlation and other popular rank correlations, including Kendall’s tau and quadrant correlation. We demonstrate that, under independence, Chatterjee’s correlation and any of these rank correlations are asymptotically joint normal and independent. Simulation studies demonstrate that the Chatterjee-Kendall test has better power than the Chatterjee-Spearman test. Finally, we explore two possible extensions to the multivariate case. These extensions expand the applicability of the rank-based combined tests to a broader range of scenarios.</summary></entry><entry><title type="html">On the use of splines for representing ordered factors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Ontheuseofsplinesforrepresentingorderedfactors.html" rel="alternate" type="text/html" title="On the use of splines for representing ordered factors" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Ontheuseofsplinesforrepresentingorderedfactors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Ontheuseofsplinesforrepresentingorderedfactors.html">&lt;p&gt;In the context of regression-type statistical models, the inclusion of some ordered factors among the explanatory variables requires the conversion of qualitative levels to numeric components of the linear predictor. The present note represent a follow-up of a methodology proposed by Azzalini (2023} for constructing numeric scores assigned to the factors levels. The aim of the present supplement it to allow additional flexibility of the mapping from ordered levels and numeric scores.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15933&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Adelchi Azzalini</name></author><category term="stat.ME" /><summary type="html">In the context of regression-type statistical models, the inclusion of some ordered factors among the explanatory variables requires the conversion of qualitative levels to numeric components of the linear predictor. The present note represent a follow-up of a methodology proposed by Azzalini (2023} for constructing numeric scores assigned to the factors levels. The aim of the present supplement it to allow additional flexibility of the mapping from ordered levels and numeric scores.</summary></entry><entry><title type="html">Position: Benchmarking is Limited in Reinforcement Learning Research</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/PositionBenchmarkingisLimitedinReinforcementLearningResearch.html" rel="alternate" type="text/html" title="Position: Benchmarking is Limited in Reinforcement Learning Research" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/PositionBenchmarkingisLimitedinReinforcementLearningResearch</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/PositionBenchmarkingisLimitedinReinforcementLearningResearch.html">&lt;p&gt;Novel reinforcement learning algorithms, or improvements on existing ones, are commonly justified by evaluating their performance on benchmark environments and are compared to an ever-changing set of standard algorithms. However, despite numerous calls for improvements, experimental practices continue to produce misleading or unsupported claims. One reason for the ongoing substandard practices is that conducting rigorous benchmarking experiments requires substantial computational time. This work investigates the sources of increased computation costs in rigorous experiment designs. We show that conducting rigorous performance benchmarks will likely have computational costs that are often prohibitive. As a result, we argue for using an additional experimentation paradigm to overcome the limitations of benchmarking.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16241&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Scott M. Jordan, Adam White, Bruno Castro da Silva, Martha White, Philip S. Thomas</name></author><category term="stat.ME" /><summary type="html">Novel reinforcement learning algorithms, or improvements on existing ones, are commonly justified by evaluating their performance on benchmark environments and are compared to an ever-changing set of standard algorithms. However, despite numerous calls for improvements, experimental practices continue to produce misleading or unsupported claims. One reason for the ongoing substandard practices is that conducting rigorous benchmarking experiments requires substantial computational time. This work investigates the sources of increased computation costs in rigorous experiment designs. We show that conducting rigorous performance benchmarks will likely have computational costs that are often prohibitive. As a result, we argue for using an additional experimentation paradigm to overcome the limitations of benchmarking.</summary></entry><entry><title type="html">Practical privacy metrics for synthetic data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Practicalprivacymetricsforsyntheticdata.html" rel="alternate" type="text/html" title="Practical privacy metrics for synthetic data" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Practicalprivacymetricsforsyntheticdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Practicalprivacymetricsforsyntheticdata.html">&lt;p&gt;This paper explains how the synthpop package for R has been extended to include functions to calculate measures of identity and attribute disclosure risk for synthetic data that measure risks for the records used to create the synthetic data. The basic function, disclosure, calculates identity disclosure for a set of quasi-identifiers (keys) and attribute disclosure for one variable specified as a target from the same set of keys. The second function, disclosure.summary, is a wrapper for the first and presents summary results for a set of targets. This short paper explains the measures of disclosure risk and documents how they are calculated. We recommend two measures: $RepU$ (replicated uniques) for identity disclosure and $DiSCO$ (Disclosive in Synthetic Correct Original) for attribute disclosure. Both are expressed a \% of the original records and each can be compared to similar measures calculated from the original data. Experience with using the functions on real data found that some apparent disclosures could be identified as coming from relationships in the data that would be expected to be known to anyone familiar with its features. We flag cases when this seems to have occurred and provide means of excluding them.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16826&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Gillian M Raab, Beata Nowok, Chris Dibben</name></author><category term="stat.AP" /><summary type="html">This paper explains how the synthpop package for R has been extended to include functions to calculate measures of identity and attribute disclosure risk for synthetic data that measure risks for the records used to create the synthetic data. The basic function, disclosure, calculates identity disclosure for a set of quasi-identifiers (keys) and attribute disclosure for one variable specified as a target from the same set of keys. The second function, disclosure.summary, is a wrapper for the first and presents summary results for a set of targets. This short paper explains the measures of disclosure risk and documents how they are calculated. We recommend two measures: $RepU$ (replicated uniques) for identity disclosure and $DiSCO$ (Disclosive in Synthetic Correct Original) for attribute disclosure. Both are expressed a \% of the original records and each can be compared to similar measures calculated from the original data. Experience with using the functions on real data found that some apparent disclosures could be identified as coming from relationships in the data that would be expected to be known to anyone familiar with its features. We flag cases when this seems to have occurred and provide means of excluding them.</summary></entry><entry><title type="html">Prediction of causal genes at GWAS loci with pleiotropic gene regulatory effects using sets of correlated instrumental variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/PredictionofcausalgenesatGWASlociwithpleiotropicgeneregulatoryeffectsusingsetsofcorrelatedinstrumentalvariables.html" rel="alternate" type="text/html" title="Prediction of causal genes at GWAS loci with pleiotropic gene regulatory effects using sets of correlated instrumental variables" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/PredictionofcausalgenesatGWASlociwithpleiotropicgeneregulatoryeffectsusingsetsofcorrelatedinstrumentalvariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/PredictionofcausalgenesatGWASlociwithpleiotropicgeneregulatoryeffectsusingsetsofcorrelatedinstrumentalvariables.html">&lt;p&gt;Multivariate Mendelian randomization (MVMR) is a statistical technique that uses sets of genetic instruments to estimate the direct causal effects of multiple exposures on an outcome of interest. At genomic loci with pleiotropic gene regulatory effects, that is, loci where the same genetic variants are associated to multiple nearby genes, MVMR can potentially be used to predict candidate causal genes. However, consensus in the field dictates that the genetic instruments in MVMR must be independent, which is usually not possible when considering a group of candidate genes from the same locus. We used causal inference theory to show that MVMR with correlated instruments satisfies the instrumental set condition. This is a classical result by Brito and Pearl (2002) for structural equation models that guarantees the identifiability of causal effects in situations where multiple exposures collectively, but not individually, separate a set of instrumental variables from an outcome variable. Extensive simulations confirmed the validity and usefulness of these theoretical results even at modest sample sizes. Importantly, the causal effect estimates remain unbiased and their variance small when instruments are highly correlated. We applied MVMR with correlated instrumental variable sets at risk loci from genome-wide association studies (GWAS) for coronary artery disease using eQTL data from the STARNET study. Our method predicts causal genes at twelve loci, each associated with multiple colocated genes in multiple tissues. However, the extensive degree of regulatory pleiotropy across tissues and the limited number of causal variants in each locus still require that MVMR is run on a tissue-by-tissue basis, and testing all gene-tissue pairs at a given locus in a single model to predict causal gene-tissue combinations remains infeasible.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.06261&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mariyam Khan, Adriaan-Alexander Ludl, Sean Bankier, Johan Bjorkegren, Tom Michoel</name></author><category term="stat.ME" /><summary type="html">Multivariate Mendelian randomization (MVMR) is a statistical technique that uses sets of genetic instruments to estimate the direct causal effects of multiple exposures on an outcome of interest. At genomic loci with pleiotropic gene regulatory effects, that is, loci where the same genetic variants are associated to multiple nearby genes, MVMR can potentially be used to predict candidate causal genes. However, consensus in the field dictates that the genetic instruments in MVMR must be independent, which is usually not possible when considering a group of candidate genes from the same locus. We used causal inference theory to show that MVMR with correlated instruments satisfies the instrumental set condition. This is a classical result by Brito and Pearl (2002) for structural equation models that guarantees the identifiability of causal effects in situations where multiple exposures collectively, but not individually, separate a set of instrumental variables from an outcome variable. Extensive simulations confirmed the validity and usefulness of these theoretical results even at modest sample sizes. Importantly, the causal effect estimates remain unbiased and their variance small when instruments are highly correlated. We applied MVMR with correlated instrumental variable sets at risk loci from genome-wide association studies (GWAS) for coronary artery disease using eQTL data from the STARNET study. Our method predicts causal genes at twelve loci, each associated with multiple colocated genes in multiple tissues. However, the extensive degree of regulatory pleiotropy across tissues and the limited number of causal variants in each locus still require that MVMR is run on a tissue-by-tissue basis, and testing all gene-tissue pairs at a given locus in a single model to predict causal gene-tissue combinations remains infeasible.</summary></entry><entry><title type="html">Recursive variational Gaussian approximation with the Whittle likelihood for linear non-Gaussian state space models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/RecursivevariationalGaussianapproximationwiththeWhittlelikelihoodforlinearnonGaussianstatespacemodels.html" rel="alternate" type="text/html" title="Recursive variational Gaussian approximation with the Whittle likelihood for linear non-Gaussian state space models" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/RecursivevariationalGaussianapproximationwiththeWhittlelikelihoodforlinearnonGaussianstatespacemodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/RecursivevariationalGaussianapproximationwiththeWhittlelikelihoodforlinearnonGaussianstatespacemodels.html">&lt;p&gt;Parameter inference for linear and non-Gaussian state space models is challenging because the likelihood function contains an intractable integral over the latent state variables. Exact inference using Markov chain Monte Carlo is computationally expensive, particularly for long time series data. Variational Bayes methods are useful when exact inference is infeasible. These methods approximate the posterior density of the parameters by a simple and tractable distribution found through optimisation. In this paper, we propose a novel sequential variational Bayes approach that makes use of the Whittle likelihood for computationally efficient parameter inference in this class of state space models. Our algorithm, which we call Recursive Variational Gaussian Approximation with the Whittle Likelihood (R-VGA-Whittle), updates the variational parameters by processing data in the frequency domain. At each iteration, R-VGA-Whittle requires the gradient and Hessian of the Whittle log-likelihood, which are available in closed form for a wide class of models. Through several examples using a linear Gaussian state space model and a univariate/bivariate non-Gaussian stochastic volatility model, we show that R-VGA-Whittle provides good approximations to posterior distributions of the parameters and is very computationally efficient when compared to asymptotically exact methods such as Hamiltonian Monte Carlo.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15998&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bao Anh Vu, David Gunawan, Andrew Zammit-Mangion</name></author><category term="stat.CO" /><summary type="html">Parameter inference for linear and non-Gaussian state space models is challenging because the likelihood function contains an intractable integral over the latent state variables. Exact inference using Markov chain Monte Carlo is computationally expensive, particularly for long time series data. Variational Bayes methods are useful when exact inference is infeasible. These methods approximate the posterior density of the parameters by a simple and tractable distribution found through optimisation. In this paper, we propose a novel sequential variational Bayes approach that makes use of the Whittle likelihood for computationally efficient parameter inference in this class of state space models. Our algorithm, which we call Recursive Variational Gaussian Approximation with the Whittle Likelihood (R-VGA-Whittle), updates the variational parameters by processing data in the frequency domain. At each iteration, R-VGA-Whittle requires the gradient and Hessian of the Whittle log-likelihood, which are available in closed form for a wide class of models. Through several examples using a linear Gaussian state space model and a univariate/bivariate non-Gaussian stochastic volatility model, we show that R-VGA-Whittle provides good approximations to posterior distributions of the parameters and is very computationally efficient when compared to asymptotically exact methods such as Hamiltonian Monte Carlo.</summary></entry><entry><title type="html">Reinterpreting Economic Complexity: A co-clustering approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ReinterpretingEconomicComplexityAcoclusteringapproach.html" rel="alternate" type="text/html" title="Reinterpreting Economic Complexity: A co-clustering approach" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ReinterpretingEconomicComplexityAcoclusteringapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ReinterpretingEconomicComplexityAcoclusteringapproach.html">&lt;p&gt;Economic growth results from countries’ accumulation of organizational and technological capabilities. The Economic and Product Complexity Indices, introduced as an attempt to measure these capabilities from a country’s basket of exported products, have become popular to study economic development, the geography of innovation, and industrial policies. Despite this reception, the interpretation of these indicators proved difficult. Although the original Method of Reflections suggested a direct interconnection between country and product metrics, it has been proved that the Economic and Product Complexity Indices result from a spectral clustering algorithm that separately groups similar countries or similar products, respectively. This recent approach to economic and product complexity conflicts with the original one and treats separately countries and products. However, building on previous interpretations of the indices and the recent evolution in spectral clustering, we show that these indices simultaneously identify two co-clusters of similar countries and products. This viewpoint reconciles the spectral clustering interpretation of the indices with the original Method of Reflections interpretation. By proving the often neglected intimate relationship between country and product complexity, this approach emphasizes the role of a selected set of products in determining economic development while extending the range of applications of these indicators in economics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16199&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carlo Bottai, Jacopo Di Iorio, Martina Iori</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Economic growth results from countries’ accumulation of organizational and technological capabilities. The Economic and Product Complexity Indices, introduced as an attempt to measure these capabilities from a country’s basket of exported products, have become popular to study economic development, the geography of innovation, and industrial policies. Despite this reception, the interpretation of these indicators proved difficult. Although the original Method of Reflections suggested a direct interconnection between country and product metrics, it has been proved that the Economic and Product Complexity Indices result from a spectral clustering algorithm that separately groups similar countries or similar products, respectively. This recent approach to economic and product complexity conflicts with the original one and treats separately countries and products. However, building on previous interpretations of the indices and the recent evolution in spectral clustering, we show that these indices simultaneously identify two co-clusters of similar countries and products. This viewpoint reconciles the spectral clustering interpretation of the indices with the original Method of Reflections interpretation. By proving the often neglected intimate relationship between country and product complexity, this approach emphasizes the role of a selected set of products in determining economic development while extending the range of applications of these indicators in economics.</summary></entry><entry><title type="html">Retention in STEM: Factors Influencing Student Persistence and Employment</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/RetentioninSTEMFactorsInfluencingStudentPersistenceandEmployment.html" rel="alternate" type="text/html" title="Retention in STEM: Factors Influencing Student Persistence and Employment" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/RetentioninSTEMFactorsInfluencingStudentPersistenceandEmployment</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/RetentioninSTEMFactorsInfluencingStudentPersistenceandEmployment.html">&lt;p&gt;This study utilizes data from the Baccalaureate and Beyond Longitudinal Study to explore factors associated with the likelihood of students’ employment in STEM fields one year after graduation. We examined various factors related to students’ individual characteristics (e.g., gender, race, and financial situation), institutional experiences (e.g., major, academic standing, research involvement, internships, extracurricular activities, and undergraduate practicum), and institutional and national trends. The results indicate lower STEM employment likelihood for minority groups and students with academic probation. The findings also highlight the positive impact of undergraduate practicum and job relevance to major on STEM employment likelihood. On the contrary, career services were negatively associated with the likelihood of students’ STEM occupation choice, suggesting potential shortcomings in STEM job preparation within these services. The study provides valuable insights and actionable recommendations for policymakers and educators seeking to increase diversity and inclusion in STEM fields, suggesting the need for more efficient and tailored educational interventions and curriculum development.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.14142&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Linli Zhou, Damji Heo Stratton, Xin Li</name></author><category term="stat.AP" /><summary type="html">This study utilizes data from the Baccalaureate and Beyond Longitudinal Study to explore factors associated with the likelihood of students’ employment in STEM fields one year after graduation. We examined various factors related to students’ individual characteristics (e.g., gender, race, and financial situation), institutional experiences (e.g., major, academic standing, research involvement, internships, extracurricular activities, and undergraduate practicum), and institutional and national trends. The results indicate lower STEM employment likelihood for minority groups and students with academic probation. The findings also highlight the positive impact of undergraduate practicum and job relevance to major on STEM employment likelihood. On the contrary, career services were negatively associated with the likelihood of students’ STEM occupation choice, suggesting potential shortcomings in STEM job preparation within these services. The study provides valuable insights and actionable recommendations for policymakers and educators seeking to increase diversity and inclusion in STEM fields, suggesting the need for more efficient and tailored educational interventions and curriculum development.</summary></entry><entry><title type="html">Re-thinking Spatial Confounding in Spatial Linear Mixed Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/RethinkingSpatialConfoundinginSpatialLinearMixedModels.html" rel="alternate" type="text/html" title="Re-thinking Spatial Confounding in Spatial Linear Mixed Models" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/RethinkingSpatialConfoundinginSpatialLinearMixedModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/RethinkingSpatialConfoundinginSpatialLinearMixedModels.html">&lt;p&gt;In the last two decades, considerable research has been devoted to a phenomenon known as spatial confounding. Spatial confounding is thought to occur when there is multicollinearity between a covariate and the random effect in a spatial regression model. This multicollinearity is considered highly problematic when the inferential goal is estimating regression coefficients and various methodologies have been proposed to attempt to alleviate it. Recently, it has become apparent that many of these methodologies are flawed, yet the field continues to expand. In this paper, we offer a novel perspective of synthesizing the work in the field of spatial confounding. We propose that at least two distinct phenomena are currently conflated with the term spatial confounding. We refer to these as the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;analysis model&apos;&apos; and the&lt;/code&gt;data generation’’ types of spatial confounding. We show that these two issues can lead to contradicting conclusions about whether spatial confounding exists and whether methods to alleviate it will improve inference. Our results also illustrate that in most cases, traditional spatial linear mixed models do help to improve inference on regression coefficients. Drawing on the insights gained, we offer a path forward for research in spatial confounding.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.05743&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kori Khan, Candace Berrett</name></author><category term="stat.ME" /><summary type="html">In the last two decades, considerable research has been devoted to a phenomenon known as spatial confounding. Spatial confounding is thought to occur when there is multicollinearity between a covariate and the random effect in a spatial regression model. This multicollinearity is considered highly problematic when the inferential goal is estimating regression coefficients and various methodologies have been proposed to attempt to alleviate it. Recently, it has become apparent that many of these methodologies are flawed, yet the field continues to expand. In this paper, we offer a novel perspective of synthesizing the work in the field of spatial confounding. We propose that at least two distinct phenomena are currently conflated with the term spatial confounding. We refer to these as the analysis model&apos;&apos; and thedata generation’’ types of spatial confounding. We show that these two issues can lead to contradicting conclusions about whether spatial confounding exists and whether methods to alleviate it will improve inference. Our results also illustrate that in most cases, traditional spatial linear mixed models do help to improve inference on regression coefficients. Drawing on the insights gained, we offer a path forward for research in spatial confounding.</summary></entry><entry><title type="html">Scalable Composite Likelihood Estimation of Probit Models with Crossed Random Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ScalableCompositeLikelihoodEstimationofProbitModelswithCrossedRandomEffects.html" rel="alternate" type="text/html" title="Scalable Composite Likelihood Estimation of Probit Models with Crossed Random Effects" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ScalableCompositeLikelihoodEstimationofProbitModelswithCrossedRandomEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ScalableCompositeLikelihoodEstimationofProbitModelswithCrossedRandomEffects.html">&lt;p&gt;Crossed random effects structures arise in many scientific contexts. They raise severe computational problems with likelihood computations scaling like $N^{3/2}$ or worse for $N$ data points. In this paper we develop a new composite likelihood approach for crossed random effects probit models. For data arranged in R rows and C columns, the likelihood function includes a very difficult R + C dimensional integral. The composite likelihood we develop uses the marginal distribution of the response along with two hierarchical models. The cost is reduced to $\mathcal{O}(N)$ and it can be computed with $R + C$ one dimensional integrals. We find that the commonly used Laplace approximation has a cost that grows superlinearly. We get consistent estimates of the probit slope and variance components from our composite likelihood algorithm. We also show how to estimate the covariance of the estimated regression coefficients. The algorithm scales readily to a data set of five million observations from Stitch Fix with $R + C &amp;gt; 700{,}000$.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.15681&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ruggero Bellio, Swarnadip Ghosh, Art B. Owen, Cristiano Varin</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Crossed random effects structures arise in many scientific contexts. They raise severe computational problems with likelihood computations scaling like $N^{3/2}$ or worse for $N$ data points. In this paper we develop a new composite likelihood approach for crossed random effects probit models. For data arranged in R rows and C columns, the likelihood function includes a very difficult R + C dimensional integral. The composite likelihood we develop uses the marginal distribution of the response along with two hierarchical models. The cost is reduced to $\mathcal{O}(N)$ and it can be computed with $R + C$ one dimensional integrals. We find that the commonly used Laplace approximation has a cost that grows superlinearly. We get consistent estimates of the probit slope and variance components from our composite likelihood algorithm. We also show how to estimate the covariance of the estimated regression coefficients. The algorithm scales readily to a data set of five million observations from Stitch Fix with $R + C &amp;gt; 700{,}000$.</summary></entry><entry><title type="html">Semiparametric Estimation of Treatment Effects in Observational Studies with Heterogeneous Partial Interference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SemiparametricEstimationofTreatmentEffectsinObservationalStudieswithHeterogeneousPartialInterference.html" rel="alternate" type="text/html" title="Semiparametric Estimation of Treatment Effects in Observational Studies with Heterogeneous Partial Interference" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SemiparametricEstimationofTreatmentEffectsinObservationalStudieswithHeterogeneousPartialInterference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SemiparametricEstimationofTreatmentEffectsinObservationalStudieswithHeterogeneousPartialInterference.html">&lt;p&gt;In many observational studies in social science and medicine, subjects or units are connected, and one unit’s treatment and attributes may affect another’s treatment and outcome, violating the stable unit treatment value assumption (SUTVA) and resulting in interference. To enable feasible estimation and inference, many previous works assume exchangeability of interfering units (neighbors). However, in many applications with distinctive units, interference is heterogeneous and needs to be modeled explicitly. In this paper, we focus on the partial interference setting, and only restrict units to be exchangeable conditional on observable characteristics. Under this framework, we propose generalized augmented inverse propensity weighted (AIPW) estimators for general causal estimands that include heterogeneous direct and spillover effects. We show that they are semiparametric efficient and robust to heterogeneous interference as well as model misspecifications. We apply our methods to the Add Health dataset to study the direct effects of alcohol consumption on academic performance and the spillover effects of parental incarceration on adolescent well-being.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2107.12420&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhaonan Qu, Ruoxuan Xiong, Jizhou Liu, Guido Imbens</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In many observational studies in social science and medicine, subjects or units are connected, and one unit’s treatment and attributes may affect another’s treatment and outcome, violating the stable unit treatment value assumption (SUTVA) and resulting in interference. To enable feasible estimation and inference, many previous works assume exchangeability of interfering units (neighbors). However, in many applications with distinctive units, interference is heterogeneous and needs to be modeled explicitly. In this paper, we focus on the partial interference setting, and only restrict units to be exchangeable conditional on observable characteristics. Under this framework, we propose generalized augmented inverse propensity weighted (AIPW) estimators for general causal estimands that include heterogeneous direct and spillover effects. We show that they are semiparametric efficient and robust to heterogeneous interference as well as model misspecifications. We apply our methods to the Add Health dataset to study the direct effects of alcohol consumption on academic performance and the spillover effects of parental incarceration on adolescent well-being.</summary></entry><entry><title type="html">Sensitivity Analysis of Inverse Probability Weighting Estimators of Causal Effects in Observational Studies with Multivalued Treatments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SensitivityAnalysisofInverseProbabilityWeightingEstimatorsofCausalEffectsinObservationalStudieswithMultivaluedTreatments.html" rel="alternate" type="text/html" title="Sensitivity Analysis of Inverse Probability Weighting Estimators of Causal Effects in Observational Studies with Multivalued Treatments" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SensitivityAnalysisofInverseProbabilityWeightingEstimatorsofCausalEffectsinObservationalStudieswithMultivaluedTreatments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SensitivityAnalysisofInverseProbabilityWeightingEstimatorsofCausalEffectsinObservationalStudieswithMultivaluedTreatments.html">&lt;p&gt;One of the fundamental challenges in drawing causal inferences from observational studies is that the assumption of no unmeasured confounding is not testable from observed data. Therefore, assessing sensitivity to this assumption’s violation is important to obtain valid causal conclusions in observational studies. Although several sensitivity analysis frameworks are available in the casual inference literature, very few of them are applicable to observational studies with multivalued treatments. To address this issue, we propose a sensitivity analysis framework for performing sensitivity analysis in multivalued treatment settings. Within this framework, a general class of additive causal estimands has been proposed. We demonstrate that the estimation of the causal estimands under the proposed sensitivity model can be performed very efficiently. Simulation results show that the proposed framework performs well in terms of bias of the point estimates and coverage of the confidence intervals when there is sufficient overlap in the covariate distributions. We illustrate the application of our proposed method by conducting an observational study that estimates the causal effect of fish consumption on blood mercury levels.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.15986&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Md Abdul Basit, Mahbub A. H. M. Latif, Abdus S Wahed</name></author><category term="stat.ME" /><summary type="html">One of the fundamental challenges in drawing causal inferences from observational studies is that the assumption of no unmeasured confounding is not testable from observed data. Therefore, assessing sensitivity to this assumption’s violation is important to obtain valid causal conclusions in observational studies. Although several sensitivity analysis frameworks are available in the casual inference literature, very few of them are applicable to observational studies with multivalued treatments. To address this issue, we propose a sensitivity analysis framework for performing sensitivity analysis in multivalued treatment settings. Within this framework, a general class of additive causal estimands has been proposed. We demonstrate that the estimation of the causal estimands under the proposed sensitivity model can be performed very efficiently. Simulation results show that the proposed framework performs well in terms of bias of the point estimates and coverage of the confidence intervals when there is sufficient overlap in the covariate distributions. We illustrate the application of our proposed method by conducting an observational study that estimates the causal effect of fish consumption on blood mercury levels.</summary></entry><entry><title type="html">Similarity-based Random Partition Distribution for Clustering Functional Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SimilaritybasedRandomPartitionDistributionforClusteringFunctionalData.html" rel="alternate" type="text/html" title="Similarity-based Random Partition Distribution for Clustering Functional Data" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SimilaritybasedRandomPartitionDistributionforClusteringFunctionalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SimilaritybasedRandomPartitionDistributionforClusteringFunctionalData.html">&lt;p&gt;Random partition distribution is a crucial tool for model-based clustering. This study advances the field of random partition in the context of functional spatial data, focusing on the challenges posed by hourly population data across various regions and dates. We propose an extended generalized Dirichlet process, named the similarity-based generalized Dirichlet process (SGDP), to address the limitations of simple random partition distributions (e.g., those induced by the Dirichlet process), such as an overabundance of clusters. This model prevents excess cluster production as well as incorporates pairwise similarity information to ensure accurate and meaningful grouping. The theoretical properties of the SGDP are studied. Then, SGDP-based random partition is applied to a real-world dataset of hourly population flow in $500\text{m}^2$ meshes in the central part of Tokyo. In this empirical context, our method excels at detecting meaningful patterns in the data while accounting for spatial nuances. The results underscore the adaptability and utility of the method, showcasing its prowess in revealing intricate spatiotemporal dynamics. The proposed SGDP will significantly contribute to urban planning, transportation, and policy-making and will be a helpful tool for understanding population dynamics and their implications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.01704&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tomoya Wakayama, Shonosuke Sugasawa, Genya Kobayashi</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Random partition distribution is a crucial tool for model-based clustering. This study advances the field of random partition in the context of functional spatial data, focusing on the challenges posed by hourly population data across various regions and dates. We propose an extended generalized Dirichlet process, named the similarity-based generalized Dirichlet process (SGDP), to address the limitations of simple random partition distributions (e.g., those induced by the Dirichlet process), such as an overabundance of clusters. This model prevents excess cluster production as well as incorporates pairwise similarity information to ensure accurate and meaningful grouping. The theoretical properties of the SGDP are studied. Then, SGDP-based random partition is applied to a real-world dataset of hourly population flow in $500\text{m}^2$ meshes in the central part of Tokyo. In this empirical context, our method excels at detecting meaningful patterns in the data while accounting for spatial nuances. The results underscore the adaptability and utility of the method, showcasing its prowess in revealing intricate spatiotemporal dynamics. The proposed SGDP will significantly contribute to urban planning, transportation, and policy-making and will be a helpful tool for understanding population dynamics and their implications.</summary></entry><entry><title type="html">Simultaneous computation of Kendall’s tau and its jackknife variance</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SimultaneouscomputationofKendallstauanditsjackknifevariance.html" rel="alternate" type="text/html" title="Simultaneous computation of Kendall’s tau and its jackknife variance" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SimultaneouscomputationofKendallstauanditsjackknifevariance</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SimultaneouscomputationofKendallstauanditsjackknifevariance.html">&lt;p&gt;We present efficient algorithms for simultaneously computing Kendall’s tau and the jackknife estimator of its variance. For the classical pairwise tau, we describe a modification of Knight’s algorithm (originally designed to compute only tau) that does so while preserving its $O(n \log_2 n)$ runtime in the number of observations $n$. We also introduce a novel algorithm computing a multivariate extension of tau and its jackknife variance in $O(n \log_2^p n)$ time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2206.04019&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Samuel Perreault</name></author><category term="stat.CO" /><summary type="html">We present efficient algorithms for simultaneously computing Kendall’s tau and the jackknife estimator of its variance. For the classical pairwise tau, we describe a modification of Knight’s algorithm (originally designed to compute only tau) that does so while preserving its $O(n \log_2 n)$ runtime in the number of observations $n$. We also introduce a novel algorithm computing a multivariate extension of tau and its jackknife variance in $O(n \log_2^p n)$ time.</summary></entry><entry><title type="html">Sparse Bayesian multidimensional scaling(s)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SparseBayesianmultidimensionalscalings.html" rel="alternate" type="text/html" title="Sparse Bayesian multidimensional scaling(s)" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SparseBayesianmultidimensionalscalings</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SparseBayesianmultidimensionalscalings.html">&lt;p&gt;Bayesian multidimensional scaling (BMDS) is a probabilistic dimension reduction tool that allows one to model and visualize data consisting of dissimilarities between pairs of objects. Although BMDS has proven useful within, e.g., Bayesian phylogenetic inference, its likelihood and gradient calculations require a burdensome order of $N^2$ floating-point operations, where $N$ is the number of data points. Thus, BMDS becomes impractical as $N$ grows large. We propose and compare two sparse versions of BMDS (sBMDS) that apply log-likelihood and gradient computations to subsets of the observed dissimilarity matrix data. Landmark sBMDS (L-sBMDS) extracts columns, while banded sBMDS (B-sBMDS) extracts diagonals of the data. These sparse variants let one specify a time complexity between $N^2$ and $N$. Under simplified settings, we prove posterior consistency for subsampled distance matrices. Through simulations, we examine the accuracy and computational efficiency across all models using both the Metropolis-Hastings and Hamiltonian Monte Carlo algorithms. We observe approximately 3-fold, 10-fold and 40-fold speedups with negligible loss of accuracy, when applying the sBMDS likelihoods and gradients to 500, 1,000 and 5,000 data points with 50 bands (landmarks); these speedups only increase with the size of data considered. Finally, we apply the sBMDS variants to the phylogeographic modeling of multiple influenza subtypes to better understand how these strains spread through global air transportation networks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15573&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ami Sheth, Aaron Smith, Andrew J. Holbrook</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Bayesian multidimensional scaling (BMDS) is a probabilistic dimension reduction tool that allows one to model and visualize data consisting of dissimilarities between pairs of objects. Although BMDS has proven useful within, e.g., Bayesian phylogenetic inference, its likelihood and gradient calculations require a burdensome order of $N^2$ floating-point operations, where $N$ is the number of data points. Thus, BMDS becomes impractical as $N$ grows large. We propose and compare two sparse versions of BMDS (sBMDS) that apply log-likelihood and gradient computations to subsets of the observed dissimilarity matrix data. Landmark sBMDS (L-sBMDS) extracts columns, while banded sBMDS (B-sBMDS) extracts diagonals of the data. These sparse variants let one specify a time complexity between $N^2$ and $N$. Under simplified settings, we prove posterior consistency for subsampled distance matrices. Through simulations, we examine the accuracy and computational efficiency across all models using both the Metropolis-Hastings and Hamiltonian Monte Carlo algorithms. We observe approximately 3-fold, 10-fold and 40-fold speedups with negligible loss of accuracy, when applying the sBMDS likelihoods and gradients to 500, 1,000 and 5,000 data points with 50 bands (landmarks); these speedups only increase with the size of data considered. Finally, we apply the sBMDS variants to the phylogeographic modeling of multiple influenza subtypes to better understand how these strains spread through global air transportation networks.</summary></entry><entry><title type="html">Spatially Structured Regression for Non-conformable Spaces: Integrating Pathology Imaging and Genomics Data in Cancer</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SpatiallyStructuredRegressionforNonconformableSpacesIntegratingPathologyImagingandGenomicsDatainCancer.html" rel="alternate" type="text/html" title="Spatially Structured Regression for Non-conformable Spaces: Integrating Pathology Imaging and Genomics Data in Cancer" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SpatiallyStructuredRegressionforNonconformableSpacesIntegratingPathologyImagingandGenomicsDatainCancer</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/SpatiallyStructuredRegressionforNonconformableSpacesIntegratingPathologyImagingandGenomicsDatainCancer.html">&lt;p&gt;The spatial composition and cellular heterogeneity of the tumor microenvironment plays a critical role in cancer development and progression. High-definition pathology imaging of tumor biopsies provide a high-resolution view of the spatial organization of different types of cells. This allows for systematic assessment of intra- and inter-patient spatial cellular interactions and heterogeneity by integrating accompanying patient-level genomics data. However, joint modeling across tumor biopsies presents unique challenges due to non-conformability (lack of a common spatial domain across biopsies) as well as high-dimensionality. To address this problem, we propose the Dual random effect and main effect selection model for Spatially structured regression model (DreameSpase). DreameSpase employs a Bayesian variable selection framework that facilitates the assessment of spatial heterogeneity with respect to covariates both within (through fixed effects) and between spaces (through spatial random effects) for non-conformable spatial domains. We demonstrate the efficacy of DreameSpase via simulations and integrative analyses of pathology imaging and gene expression data obtained from $335$ melanoma biopsies. Our findings confirm several existing relationships, e.g. neutrophil genes being associated with both inter- and intra-patient spatial heterogeneity, as well as discovering novel associations. We also provide freely available and computationally efficient software for implementing DreameSpase.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16721&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nathaniel Osher, Jian Kang, Arvind Rao, Veerabhadran Baladandayuthapani</name></author><category term="stat.AP" /><summary type="html">The spatial composition and cellular heterogeneity of the tumor microenvironment plays a critical role in cancer development and progression. High-definition pathology imaging of tumor biopsies provide a high-resolution view of the spatial organization of different types of cells. This allows for systematic assessment of intra- and inter-patient spatial cellular interactions and heterogeneity by integrating accompanying patient-level genomics data. However, joint modeling across tumor biopsies presents unique challenges due to non-conformability (lack of a common spatial domain across biopsies) as well as high-dimensionality. To address this problem, we propose the Dual random effect and main effect selection model for Spatially structured regression model (DreameSpase). DreameSpase employs a Bayesian variable selection framework that facilitates the assessment of spatial heterogeneity with respect to covariates both within (through fixed effects) and between spaces (through spatial random effects) for non-conformable spatial domains. We demonstrate the efficacy of DreameSpase via simulations and integrative analyses of pathology imaging and gene expression data obtained from $335$ melanoma biopsies. Our findings confirm several existing relationships, e.g. neutrophil genes being associated with both inter- and intra-patient spatial heterogeneity, as well as discovering novel associations. We also provide freely available and computationally efficient software for implementing DreameSpase.</summary></entry><entry><title type="html">Statistical Inference and A/B Testing in Fisher Markets and Paced Auctions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/StatisticalInferenceandABTestinginFisherMarketsandPacedAuctions.html" rel="alternate" type="text/html" title="Statistical Inference and A/B Testing in Fisher Markets and Paced Auctions" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/StatisticalInferenceandABTestinginFisherMarketsandPacedAuctions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/StatisticalInferenceandABTestinginFisherMarketsandPacedAuctions.html">&lt;p&gt;We initiate the study of statistical inference and A/B testing for two market equilibrium models: linear Fisher market (LFM) equilibrium and first-price pacing equilibrium (FPPE). LFM arises from fair resource allocation systems such as allocation of food to food banks and notification opportunities to different types of notifications. For LFM, we assume that the data observed is captured by the classical finite-dimensional Fisher market equilibrium, and its steady-state behavior is modeled by a continuous limit Fisher market. The second type of equilibrium we study, FPPE, arises from internet advertising where advertisers are constrained by budgets and advertising opportunities are sold via first-price auctions. For platforms that use pacing-based methods to smooth out the spending of advertisers, FPPE provides a hindsight-optimal configuration of the pacing method. We propose a statistical framework for the FPPE model, in which a continuous limit FPPE models the steady-state behavior of the auction platform, and a finite FPPE provides the data to estimate primitives of the limit FPPE. Both LFM and FPPE have an Eisenberg-Gale convex program characterization, the pillar upon which we derive our statistical theory. We start by deriving basic convergence results for the finite market to the limit market. We then derive asymptotic distributions, and construct confidence intervals. Furthermore, we establish the asymptotic local minimax optimality of estimation based on finite markets. We then show that the theory can be used for conducting statistically valid A/B testing on auction platforms. Synthetic and semi-synthetic experiments verify the validity and practicality of our theory.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15522&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luofeng Liao, Christian Kroer</name></author><category term="stat.AP," /><category term="stat.TH" /><summary type="html">We initiate the study of statistical inference and A/B testing for two market equilibrium models: linear Fisher market (LFM) equilibrium and first-price pacing equilibrium (FPPE). LFM arises from fair resource allocation systems such as allocation of food to food banks and notification opportunities to different types of notifications. For LFM, we assume that the data observed is captured by the classical finite-dimensional Fisher market equilibrium, and its steady-state behavior is modeled by a continuous limit Fisher market. The second type of equilibrium we study, FPPE, arises from internet advertising where advertisers are constrained by budgets and advertising opportunities are sold via first-price auctions. For platforms that use pacing-based methods to smooth out the spending of advertisers, FPPE provides a hindsight-optimal configuration of the pacing method. We propose a statistical framework for the FPPE model, in which a continuous limit FPPE models the steady-state behavior of the auction platform, and a finite FPPE provides the data to estimate primitives of the limit FPPE. Both LFM and FPPE have an Eisenberg-Gale convex program characterization, the pillar upon which we derive our statistical theory. We start by deriving basic convergence results for the finite market to the limit market. We then derive asymptotic distributions, and construct confidence intervals. Furthermore, we establish the asymptotic local minimax optimality of estimation based on finite markets. We then show that the theory can be used for conducting statistically valid A/B testing on auction platforms. Synthetic and semi-synthetic experiments verify the validity and practicality of our theory.</summary></entry><entry><title type="html">Statistical Inference for Bures-Wasserstein Flows</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/StatisticalInferenceforBuresWassersteinFlows.html" rel="alternate" type="text/html" title="Statistical Inference for Bures-Wasserstein Flows" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/StatisticalInferenceforBuresWassersteinFlows</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/StatisticalInferenceforBuresWassersteinFlows.html">&lt;p&gt;We develop a statistical framework for conducting inference on collections of time-varying covariance operators (covariance flows) over a general, possibly infinite dimensional, Hilbert space. We model the intrinsically non-linear structure of covariances by means of the Bures-Wasserstein metric geometry. We make use of the Riemmanian-like structure induced by this metric to define a notion of mean and covariance of a random flow, and develop an associated Karhunen-Lo`eve expansion. We then treat the problem of estimation and construction of functional principal components from a finite collection of covariance flows, observed fully or irregularly.
  Our theoretical results are motivated by modern problems in functional data analysis, where one observes operator-valued random processes – for instance when analysing dynamic functional connectivity and fMRI data, or when analysing multiple functional time series in the frequency domain. Nevertheless, our framework is also novel in the finite-dimensions (matrix case), and we demonstrate what simplifications can be afforded then. We illustrate our methodology by means of simulations and data analyses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.13764&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Leonardo V. Santoro, Victor M. Panaretos</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We develop a statistical framework for conducting inference on collections of time-varying covariance operators (covariance flows) over a general, possibly infinite dimensional, Hilbert space. We model the intrinsically non-linear structure of covariances by means of the Bures-Wasserstein metric geometry. We make use of the Riemmanian-like structure induced by this metric to define a notion of mean and covariance of a random flow, and develop an associated Karhunen-Lo`eve expansion. We then treat the problem of estimation and construction of functional principal components from a finite collection of covariance flows, observed fully or irregularly. Our theoretical results are motivated by modern problems in functional data analysis, where one observes operator-valued random processes – for instance when analysing dynamic functional connectivity and fMRI data, or when analysing multiple functional time series in the frequency domain. Nevertheless, our framework is also novel in the finite-dimensions (matrix case), and we demonstrate what simplifications can be afforded then. We illustrate our methodology by means of simulations and data analyses.</summary></entry><entry><title type="html">Statistical ranking with dynamic covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Statisticalrankingwithdynamiccovariates.html" rel="alternate" type="text/html" title="Statistical ranking with dynamic covariates" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Statisticalrankingwithdynamiccovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/Statisticalrankingwithdynamiccovariates.html">&lt;p&gt;We consider a covariate-assisted ranking model grounded in the Plackett–Luce framework. Unlike existing works focusing on pure covariates or individual effects with fixed covariates, our approach integrates individual effects with dynamic covariates. This added flexibility enhances realistic ranking yet poses significant challenges for analyzing the associated estimation procedures. This paper makes an initial attempt to address these challenges. We begin by discussing the sufficient and necessary condition for the model’s identifiability. We then introduce an efficient alternating maximization algorithm to compute the maximum likelihood estimator (MLE). Under suitable assumptions on the topology of comparison graphs and dynamic covariates, we establish a quantitative uniform consistency result for the MLE with convergence rates characterized by the asymptotic graph connectivity. The proposed graph topology assumption holds for several popular random graph models under optimal leading-order sparsity conditions. A comprehensive numerical study is conducted to corroborate our theoretical findings and demonstrate the application of the proposed model to real-world datasets, including horse racing and tennis competitions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16507&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Pinjun Dong, Ruijian Han, Binyan Jiang, Yiming Xu</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We consider a covariate-assisted ranking model grounded in the Plackett–Luce framework. Unlike existing works focusing on pure covariates or individual effects with fixed covariates, our approach integrates individual effects with dynamic covariates. This added flexibility enhances realistic ranking yet poses significant challenges for analyzing the associated estimation procedures. This paper makes an initial attempt to address these challenges. We begin by discussing the sufficient and necessary condition for the model’s identifiability. We then introduce an efficient alternating maximization algorithm to compute the maximum likelihood estimator (MLE). Under suitable assumptions on the topology of comparison graphs and dynamic covariates, we establish a quantitative uniform consistency result for the MLE with convergence rates characterized by the asymptotic graph connectivity. The proposed graph topology assumption holds for several popular random graph models under optimal leading-order sparsity conditions. A comprehensive numerical study is conducted to corroborate our theoretical findings and demonstrate the application of the proposed model to real-world datasets, including horse racing and tennis competitions.</summary></entry><entry><title type="html">Unveiling Activity-Travel Patterns through Topological Data Analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/UnveilingActivityTravelPatternsthroughTopologicalDataAnalysis.html" rel="alternate" type="text/html" title="Unveiling Activity-Travel Patterns through Topological Data Analysis" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/UnveilingActivityTravelPatternsthroughTopologicalDataAnalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/UnveilingActivityTravelPatternsthroughTopologicalDataAnalysis.html">&lt;p&gt;In the context of rapid urbanization, understanding the patterns of urban residents’ activities and mobility is crucial for optimizing transportation systems and enhancing urban management efficiency. This study addresses the limitations of traditional travel analysis methods in handling high-dimensional and large-scale spatiotemporal data by incorporating Topological Data Analysis (TDA) techniques, specifically using persistent homology. This method allows for the extraction of information from the topological structure of data, enabling the effective identification and analysis of complex spatiotemporal behavior patterns without reducing the data’s dimensionality. We utilized mobile signaling data from a community in Shenzhen, which includes detailed geographic and temporal information, providing an ideal sample for analyzing urban residents’ behavior patterns. Using our pattern mining framework, we successfully identified five main patterns of residents’ activities and travel, revealing daily behavioral habits and reflecting the activity heterogeneity among residents with different socio-economic attributes. These findings not only assist urban planners in better session design but also provide new characteristics for predictive mobility models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16742&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nuoxian Huang, Yulin Wu</name></author><category term="stat.AP" /><summary type="html">In the context of rapid urbanization, understanding the patterns of urban residents’ activities and mobility is crucial for optimizing transportation systems and enhancing urban management efficiency. This study addresses the limitations of traditional travel analysis methods in handling high-dimensional and large-scale spatiotemporal data by incorporating Topological Data Analysis (TDA) techniques, specifically using persistent homology. This method allows for the extraction of information from the topological structure of data, enabling the effective identification and analysis of complex spatiotemporal behavior patterns without reducing the data’s dimensionality. We utilized mobile signaling data from a community in Shenzhen, which includes detailed geographic and temporal information, providing an ideal sample for analyzing urban residents’ behavior patterns. Using our pattern mining framework, we successfully identified five main patterns of residents’ activities and travel, revealing daily behavioral habits and reflecting the activity heterogeneity among residents with different socio-economic attributes. These findings not only assist urban planners in better session design but also provide new characteristics for predictive mobility models.</summary></entry><entry><title type="html">VICatMix: variational Bayesian clustering and variable selection for discrete biomedical data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/VICatMixvariationalBayesianclusteringandvariableselectionfordiscretebiomedicaldata.html" rel="alternate" type="text/html" title="VICatMix: variational Bayesian clustering and variable selection for discrete biomedical data" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/VICatMixvariationalBayesianclusteringandvariableselectionfordiscretebiomedicaldata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/VICatMixvariationalBayesianclusteringandvariableselectionfordiscretebiomedicaldata.html">&lt;p&gt;Effective clustering of biomedical data is crucial in precision medicine, enabling accurate stratifiction of patients or samples. However, the growth in availability of high-dimensional categorical data, including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;omics data, necessitates computationally efficient clustering algorithms. We present VICatMix, a variational Bayesian finite mixture model designed for the clustering of categorical data. The use of variational inference (VI) in its training allows the model to outperform competitors in term of efficiency, while maintaining high accuracy. VICatMix furthermore performs variable selection, enhancing its performance on high-dimensional, noisy data. The proposed model incorporates summarisation and model averaging to mitigate poor local optima in VI, allowing for improved estimation of the true number of clusters simultaneously with feature saliency. We demonstrate the performance of VICatMix with both simulated and real-world data, including applications to datasets from The Cancer Genome Atlas (TCGA), showing its use in cancer subtyping and driver gene discovery. We demonstrate VICatMix&apos;s utility in integrative cluster analysis with different &lt;/code&gt;omics datasets, enabling the discovery of novel subtypes.
  \textbf{Availability:} VICatMix is freely available as an R package, incorporating C++ for faster computation, at \url{https://github.com/j-ackierao/VICatMix}.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16227&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Paul D. W. Kirk, Jackie Rao</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Effective clustering of biomedical data is crucial in precision medicine, enabling accurate stratifiction of patients or samples. However, the growth in availability of high-dimensional categorical data, including omics data, necessitates computationally efficient clustering algorithms. We present VICatMix, a variational Bayesian finite mixture model designed for the clustering of categorical data. The use of variational inference (VI) in its training allows the model to outperform competitors in term of efficiency, while maintaining high accuracy. VICatMix furthermore performs variable selection, enhancing its performance on high-dimensional, noisy data. The proposed model incorporates summarisation and model averaging to mitigate poor local optima in VI, allowing for improved estimation of the true number of clusters simultaneously with feature saliency. We demonstrate the performance of VICatMix with both simulated and real-world data, including applications to datasets from The Cancer Genome Atlas (TCGA), showing its use in cancer subtyping and driver gene discovery. We demonstrate VICatMix&apos;s utility in integrative cluster analysis with different omics datasets, enabling the discovery of novel subtypes. \textbf{Availability:} VICatMix is freely available as an R package, incorporating C++ for faster computation, at \url{https://github.com/j-ackierao/VICatMix}.</summary></entry><entry><title type="html">Valuation methods for professional sports clubs: A historical review, a model development, and the application to Japanese football clubs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ValuationmethodsforprofessionalsportsclubsAhistoricalreviewamodeldevelopmentandtheapplicationtoJapanesefootballclubs.html" rel="alternate" type="text/html" title="Valuation methods for professional sports clubs: A historical review, a model development, and the application to Japanese football clubs" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ValuationmethodsforprofessionalsportsclubsAhistoricalreviewamodeldevelopmentandtheapplicationtoJapanesefootballclubs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/ValuationmethodsforprofessionalsportsclubsAhistoricalreviewamodeldevelopmentandtheapplicationtoJapanesefootballclubs.html">&lt;p&gt;In the trend towards the globalization of football and the increasing commercialization of professional football clubs, a methodology for calculating the firm value of clubs in non-western countries has yet to be established. This study reviews the valuation methods for the club firm values in Europe and North America and how values are calculated at the time of changing ownership of Japanese clubs and develops regression models with higher explanatory power than before to estimate the more accurate firm value of Japanese football clubs. A review of the existing literature on methods for calculating the firm value of professional sports clubs in Europe and North America, as well as financial statements and registers relating to changes of ownership of Japanese clubs, was conducted. After that, multiple regression analyses were conducted using the firm value of European clubs as the explained variable. From the literature review and the Japanese case studies, it has become clear that European clubs’ standard valuation methods are based on revenue and other factors, while in Japan, valuation is based solely on the par value of stocks or net assets. Multiple regression analysis revealed that the firm value of European clubs over the past three years is best explained by revenue or player market value and the number of SNS followers. Two models with high explanatory power were developed. The estimated firm value using the revenue-based formula was higher than the one based on player market value. However, in the J.League, the former was more than three times higher than the latter, while the former was only 1.2 times higher for European clubs. The discrepancy relates to differences in European and J.League clubs’ revenues and asset structures. In either formula, the firm value of J.League clubs exceeded the actual transaction price when the change of ownership occurred in the past.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16773&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Masaaki Kimura, Zen Walsh, Takuo Inoue, Toshiya Takahashi, Hideki Koizumi</name></author><category term="stat.AP" /><summary type="html">In the trend towards the globalization of football and the increasing commercialization of professional football clubs, a methodology for calculating the firm value of clubs in non-western countries has yet to be established. This study reviews the valuation methods for the club firm values in Europe and North America and how values are calculated at the time of changing ownership of Japanese clubs and develops regression models with higher explanatory power than before to estimate the more accurate firm value of Japanese football clubs. A review of the existing literature on methods for calculating the firm value of professional sports clubs in Europe and North America, as well as financial statements and registers relating to changes of ownership of Japanese clubs, was conducted. After that, multiple regression analyses were conducted using the firm value of European clubs as the explained variable. From the literature review and the Japanese case studies, it has become clear that European clubs’ standard valuation methods are based on revenue and other factors, while in Japan, valuation is based solely on the par value of stocks or net assets. Multiple regression analysis revealed that the firm value of European clubs over the past three years is best explained by revenue or player market value and the number of SNS followers. Two models with high explanatory power were developed. The estimated firm value using the revenue-based formula was higher than the one based on player market value. However, in the J.League, the former was more than three times higher than the latter, while the former was only 1.2 times higher for European clubs. The discrepancy relates to differences in European and J.League clubs’ revenues and asset structures. In either formula, the firm value of J.League clubs exceeded the actual transaction price when the change of ownership occurred in the past.</summary></entry><entry><title type="html">What is Best for Students, Numerical Scores or Letter Grades?</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/WhatisBestforStudentsNumericalScoresorLetterGrades.html" rel="alternate" type="text/html" title="What is Best for Students, Numerical Scores or Letter Grades?" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/WhatisBestforStudentsNumericalScoresorLetterGrades</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/WhatisBestforStudentsNumericalScoresorLetterGrades.html">&lt;p&gt;We study letter grading schemes, which are routinely employed for evaluating student performance. Typically, a numerical score obtained via one or more evaluations is converted into a letter grade (e.g., A+, B-, etc.) by associating a disjoint interval of numerical scores to each letter grade.
  We propose the first model for studying the (de)motivational effects of such grading on the students and, consequently, on their performance in future evaluations. We use the model to compare uniform letter grading schemes, in which the range of scores is divided into equal-length parts that are mapped to the letter grades, to numerical scoring, in which the score is not converted to any letter grade (equivalently, every score is its own letter grade).
  Theoretically, we identify realistic conditions under which numerical scoring is better than any uniform letter grading scheme. Our experiments confirm that this holds under even weaker conditions, but also find cases where the converse occurs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15405&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Evi Micha, Shreyas Sekar, Nisarg Shah</name></author><category term="stat.AP" /><summary type="html">We study letter grading schemes, which are routinely employed for evaluating student performance. Typically, a numerical score obtained via one or more evaluations is converted into a letter grade (e.g., A+, B-, etc.) by associating a disjoint interval of numerical scores to each letter grade. We propose the first model for studying the (de)motivational effects of such grading on the students and, consequently, on their performance in future evaluations. We use the model to compare uniform letter grading schemes, in which the range of scores is divided into equal-length parts that are mapped to the letter grades, to numerical scoring, in which the score is not converted to any letter grade (equivalently, every score is its own letter grade). Theoretically, we identify realistic conditions under which numerical scoring is better than any uniform letter grading scheme. Our experiments confirm that this holds under even weaker conditions, but also find cases where the converse occurs.</summary></entry><entry><title type="html">YEAST: Yet Another Sequential Test</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/YEASTYetAnotherSequentialTest.html" rel="alternate" type="text/html" title="YEAST: Yet Another Sequential Test" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/YEASTYetAnotherSequentialTest</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/YEASTYetAnotherSequentialTest.html">&lt;p&gt;Large-scale randomised experiments have become a standard tool for developing products and improving user experience. To reduce losses from shipping harmful changes experimental results are, in practice, often checked repeatedly, which leads to inflated false alarm rates. To alleviate this problem, one can use sequential testing techniques as they control false discovery rates despite repeated checks. While multiple sequential testing methods exist in the literature, they either restrict the number of interim checks the experimenter can perform or have tuning parameters that require calibration. In this paper, we propose a novel sequential testing method that does not limit the number of interim checks and at the same time does not have any tuning parameters. The proposed method is new and does not stem from existing experiment monitoring procedures. It controls false discovery rates by ``inverting’’ a bound on the threshold crossing probability derived from a classical maximal inequality. We demonstrate both in simulations and using real-world data that the proposed method outperforms current state-of-the-art sequential tests for continuous test monitoring. In addition, we illustrate the method’s effectiveness with a real-world application on a major online fashion platform.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16523&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexey Kurennoy, Majed Dodin, Tural Gurbanov, Ana Peleteiro Ramallo</name></author><category term="stat.ME" /><summary type="html">Large-scale randomised experiments have become a standard tool for developing products and improving user experience. To reduce losses from shipping harmful changes experimental results are, in practice, often checked repeatedly, which leads to inflated false alarm rates. To alleviate this problem, one can use sequential testing techniques as they control false discovery rates despite repeated checks. While multiple sequential testing methods exist in the literature, they either restrict the number of interim checks the experimenter can perform or have tuning parameters that require calibration. In this paper, we propose a novel sequential testing method that does not limit the number of interim checks and at the same time does not have any tuning parameters. The proposed method is new and does not stem from existing experiment monitoring procedures. It controls false discovery rates by ``inverting’’ a bound on the threshold crossing probability derived from a classical maximal inequality. We demonstrate both in simulations and using real-world data that the proposed method outperforms current state-of-the-art sequential tests for continuous test monitoring. In addition, we illustrate the method’s effectiveness with a real-world application on a major online fashion platform.</summary></entry></feed>