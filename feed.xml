<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-14T07:14:43+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Robust Bayesian approach for reliability prognosis of one-shot devices under cumulative risk model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/ARobustBayesianapproachforreliabilityprognosisofoneshotdevicesundercumulativeriskmodel.html" rel="alternate" type="text/html" title="A Robust Bayesian approach for reliability prognosis of one-shot devices under cumulative risk model" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/ARobustBayesianapproachforreliabilityprognosisofoneshotdevicesundercumulativeriskmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/ARobustBayesianapproachforreliabilityprognosisofoneshotdevicesundercumulativeriskmodel.html">&lt;p&gt;The reliability prognosis of one-shot devices is drawing increasing attention because of their wide applicability. The present study aims to determine the lifetime prognosis of highly durable one-shot device units under a step-stress accelerated life testing (SSALT) experiment applying a cumulative risk model (CRM). In an SSALT experiment, CRM retains the continuity of hazard function by allowing the lag period before the effects of stress change emerge. In an analysis of such lifetime data, plentiful datasets might have outliers where conventional methods like maximum likelihood estimation or likelihood-based Bayesian estimation frequently fail. This work develops a robust estimation method based on density power divergence in classical and Bayesian frameworks. The hypothesis is tested by implementing the Bayes factor based on a robustified posterior. In Bayesian estimation, we exploit Hamiltonian Monte Carlo, which has certain advantages over the conventional Metropolis-Hastings algorithms. Further, the influence functions are examined to evaluate the robust behaviour of the estimators and the Bayes factor. Finally, the analytical development is validated through a simulation study and a real data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08867&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shanya Baghel, Shuvashree Mondal</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The reliability prognosis of one-shot devices is drawing increasing attention because of their wide applicability. The present study aims to determine the lifetime prognosis of highly durable one-shot device units under a step-stress accelerated life testing (SSALT) experiment applying a cumulative risk model (CRM). In an SSALT experiment, CRM retains the continuity of hazard function by allowing the lag period before the effects of stress change emerge. In an analysis of such lifetime data, plentiful datasets might have outliers where conventional methods like maximum likelihood estimation or likelihood-based Bayesian estimation frequently fail. This work develops a robust estimation method based on density power divergence in classical and Bayesian frameworks. The hypothesis is tested by implementing the Bayes factor based on a robustified posterior. In Bayesian estimation, we exploit Hamiltonian Monte Carlo, which has certain advantages over the conventional Metropolis-Hastings algorithms. Further, the influence functions are examined to evaluate the robust behaviour of the estimators and the Bayes factor. Finally, the analytical development is validated through a simulation study and a real data analysis.</summary></entry><entry><title type="html">A geometric approach to informed MCMC sampling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/AgeometricapproachtoinformedMCMCsampling.html" rel="alternate" type="text/html" title="A geometric approach to informed MCMC sampling" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/AgeometricapproachtoinformedMCMCsampling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/AgeometricapproachtoinformedMCMCsampling.html">&lt;p&gt;A Riemannian geometric framework for Markov chain Monte Carlo (MCMC) is developed where using the Fisher-Rao metric on the manifold of probability density functions (pdfs) informed proposal densities for Metropolis-Hastings (MH) algorithms are constructed. We exploit the square-root representation of pdfs under which the Fisher-Rao metric boils down to the standard $L^2$ metric on the positive orthant of the unit hypersphere. The square-root representation allows us to easily compute the geodesic distance between densities, resulting in a straightforward implementation of the proposed geometric MCMC methodology. Unlike the random walk MH that blindly proposes a candidate state using no information about the target, the geometric MH algorithms effectively move an uninformed base density (e.g., a random walk proposal density) towards different global/local approximations of the target density. We compare the proposed geometric MH algorithm with other MCMC algorithms for various Markov chain orderings, namely the covariance, efficiency, Peskun, and spectral gap orderings. The superior performance of the geometric algorithms over other MH algorithms like the random walk Metropolis, independent MH and variants of Metropolis adjusted Langevin algorithms is demonstrated in the context of various multimodal, nonlinear and high dimensional examples. In particular, we use extensive simulation and real data applications to compare these algorithms for analyzing mixture models, logistic regression models and ultra-high dimensional Bayesian variable selection models. A publicly available R package accompanies the article.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09010&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vivekananda Roy</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">A Riemannian geometric framework for Markov chain Monte Carlo (MCMC) is developed where using the Fisher-Rao metric on the manifold of probability density functions (pdfs) informed proposal densities for Metropolis-Hastings (MH) algorithms are constructed. We exploit the square-root representation of pdfs under which the Fisher-Rao metric boils down to the standard $L^2$ metric on the positive orthant of the unit hypersphere. The square-root representation allows us to easily compute the geodesic distance between densities, resulting in a straightforward implementation of the proposed geometric MCMC methodology. Unlike the random walk MH that blindly proposes a candidate state using no information about the target, the geometric MH algorithms effectively move an uninformed base density (e.g., a random walk proposal density) towards different global/local approximations of the target density. We compare the proposed geometric MH algorithm with other MCMC algorithms for various Markov chain orderings, namely the covariance, efficiency, Peskun, and spectral gap orderings. The superior performance of the geometric algorithms over other MH algorithms like the random walk Metropolis, independent MH and variants of Metropolis adjusted Langevin algorithms is demonstrated in the context of various multimodal, nonlinear and high dimensional examples. In particular, we use extensive simulation and real data applications to compare these algorithms for analyzing mixture models, logistic regression models and ultra-high dimensional Bayesian variable selection models. A publicly available R package accompanies the article.</summary></entry><entry><title type="html">An Empirical Bayes Approach for Estimating Skill Models for Professional Darts Players</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/AnEmpiricalBayesApproachforEstimatingSkillModelsforProfessionalDartsPlayers.html" rel="alternate" type="text/html" title="An Empirical Bayes Approach for Estimating Skill Models for Professional Darts Players" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/AnEmpiricalBayesApproachforEstimatingSkillModelsforProfessionalDartsPlayers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/AnEmpiricalBayesApproachforEstimatingSkillModelsforProfessionalDartsPlayers.html">&lt;p&gt;We perform an exploratory data analysis on a data-set for the top 16 professional darts players from the 2019 season. We use this data-set to fit player skill models which can then be used in dynamic zero-sum games (ZSGs) that model real-world matches between players. We propose an empirical Bayesian approach based on the Dirichlet-Multinomial (DM) model that overcomes limitations in the data. Specifically we introduce two DM-based skill models where the first model borrows strength from other darts players and the second model borrows strength from other regions of the dartboard. We find these DM-based models outperform simpler benchmark models with respect to Brier and Spherical scores, both of which are proper scoring rules. We also show in ZSGs settings that the difference between DM-based skill models and the simpler benchmark models is practically significant. Finally, we use our DM-model to analyze specific situations that arose in real-world darts matches during the 2019 season.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.10750&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Martin B. Haugh, Chun Wang</name></author><category term="stat.AP" /><summary type="html">We perform an exploratory data analysis on a data-set for the top 16 professional darts players from the 2019 season. We use this data-set to fit player skill models which can then be used in dynamic zero-sum games (ZSGs) that model real-world matches between players. We propose an empirical Bayesian approach based on the Dirichlet-Multinomial (DM) model that overcomes limitations in the data. Specifically we introduce two DM-based skill models where the first model borrows strength from other darts players and the second model borrows strength from other regions of the dartboard. We find these DM-based models outperform simpler benchmark models with respect to Brier and Spherical scores, both of which are proper scoring rules. We also show in ZSGs settings that the difference between DM-based skill models and the simpler benchmark models is practically significant. Finally, we use our DM-model to analyze specific situations that arose in real-world darts matches during the 2019 season.</summary></entry><entry><title type="html">Approximate Message Passing for orthogonally invariant ensembles: Multivariate non-linearities and spectral initialization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/ApproximateMessagePassingfororthogonallyinvariantensemblesMultivariatenonlinearitiesandspectralinitialization.html" rel="alternate" type="text/html" title="Approximate Message Passing for orthogonally invariant ensembles: Multivariate non-linearities and spectral initialization" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/ApproximateMessagePassingfororthogonallyinvariantensemblesMultivariatenonlinearitiesandspectralinitialization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/ApproximateMessagePassingfororthogonallyinvariantensemblesMultivariatenonlinearitiesandspectralinitialization.html">&lt;p&gt;We study a class of Approximate Message Passing (AMP) algorithms for symmetric and rectangular spiked random matrix models with orthogonally invariant noise. The AMP iterates have fixed dimension $K \geq 1$, a multivariate non-linearity is applied in each AMP iteration, and the algorithm is spectrally initialized with $K$ super-critical sample eigenvectors. We derive the forms of the Onsager debiasing coefficients and corresponding AMP state evolution, which depend on the free cumulants of the noise spectral distribution. This extends previous results for such models with $K=1$ and an independent initialization.
  Applying this approach to Bayesian principal components analysis, we introduce a Bayes-OAMP algorithm that uses as its non-linearity the posterior mean conditional on all preceding AMP iterates. We describe a practical implementation of this algorithm, where all debiasing and state evolution parameters are estimated from the observed data, and we illustrate the accuracy and stability of this approach in simulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.02318&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinyi Zhong, Tianhao Wang, Zhou Fan</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We study a class of Approximate Message Passing (AMP) algorithms for symmetric and rectangular spiked random matrix models with orthogonally invariant noise. The AMP iterates have fixed dimension $K \geq 1$, a multivariate non-linearity is applied in each AMP iteration, and the algorithm is spectrally initialized with $K$ super-critical sample eigenvectors. We derive the forms of the Onsager debiasing coefficients and corresponding AMP state evolution, which depend on the free cumulants of the noise spectral distribution. This extends previous results for such models with $K=1$ and an independent initialization. Applying this approach to Bayesian principal components analysis, we introduce a Bayes-OAMP algorithm that uses as its non-linearity the posterior mean conditional on all preceding AMP iterates. We describe a practical implementation of this algorithm, where all debiasing and state evolution parameters are estimated from the observed data, and we illustrate the accuracy and stability of this approach in simulations.</summary></entry><entry><title type="html">A procedure for multiple testing of partial conjunction hypotheses based on a hazard rate inequality</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/Aprocedureformultipletestingofpartialconjunctionhypothesesbasedonahazardrateinequality.html" rel="alternate" type="text/html" title="A procedure for multiple testing of partial conjunction hypotheses based on a hazard rate inequality" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/Aprocedureformultipletestingofpartialconjunctionhypothesesbasedonahazardrateinequality</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/Aprocedureformultipletestingofpartialconjunctionhypothesesbasedonahazardrateinequality.html">&lt;p&gt;The partial conjunction null hypothesis is tested in order to discover a signal that is present in multiple studies. The standard approach of carrying out a multiple test procedure on the partial conjunction (PC) $p$-values can be extremely conservative. We suggest alleviating this conservativeness, by eliminating many of the conservative PC $p$-values prior to the application of a multiple test procedure. This leads to the following two step procedure: first, select the set with PC $p$-values below a selection threshold; second, within the selected set only, apply a family-wise error rate or false discovery rate controlling procedure on the conditional PC $p$-values. The conditional PC $p$-values are valid if the null p-values are uniform and the combining method is Fisher. The proof of their validity is based on a novel inequality in hazard rate order of partial sums of order statistics which may be of independent interest. We also provide the conditions for which the false discovery rate controlling procedures considered will be below the nominal level. We demonstrate the potential usefulness of our novel method, CoFilter (conditional testing after filtering), for analyzing multiple genome wide association studies of Crohn’s disease.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.06692&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thorsten Dickhaus, Ruth Heller, Anh-Tuan Hoang, Yosef Rinott</name></author><category term="stat.ME" /><summary type="html">The partial conjunction null hypothesis is tested in order to discover a signal that is present in multiple studies. The standard approach of carrying out a multiple test procedure on the partial conjunction (PC) $p$-values can be extremely conservative. We suggest alleviating this conservativeness, by eliminating many of the conservative PC $p$-values prior to the application of a multiple test procedure. This leads to the following two step procedure: first, select the set with PC $p$-values below a selection threshold; second, within the selected set only, apply a family-wise error rate or false discovery rate controlling procedure on the conditional PC $p$-values. The conditional PC $p$-values are valid if the null p-values are uniform and the combining method is Fisher. The proof of their validity is based on a novel inequality in hazard rate order of partial sums of order statistics which may be of independent interest. We also provide the conditions for which the false discovery rate controlling procedures considered will be below the nominal level. We demonstrate the potential usefulness of our novel method, CoFilter (conditional testing after filtering), for analyzing multiple genome wide association studies of Crohn’s disease.</summary></entry><entry><title type="html">Bayesian Structural Model Updating with Multimodal Variational Autoencoder</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/BayesianStructuralModelUpdatingwithMultimodalVariationalAutoencoder.html" rel="alternate" type="text/html" title="Bayesian Structural Model Updating with Multimodal Variational Autoencoder" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/BayesianStructuralModelUpdatingwithMultimodalVariationalAutoencoder</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/BayesianStructuralModelUpdatingwithMultimodalVariationalAutoencoder.html">&lt;p&gt;This paper presents a novel framework for Bayesian structural model updating and proposes a method that utilizes the surrogate unimodal encoders of a multimodal variational autoencoder. This method facilitates an efficient nonparametric estimation of the likelihood describing the observed data. It is particularly suitable for high-dimensional correlated simultaneous observations applicable to various dynamic analysis models. The proposed approach is benchmarked using a numerical model of a single-story frame building with acceleration and dynamic strain measurements.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09051&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tatsuya Itoi, Kazuho Amishiki, Sangwon Lee, Taro Yaoyama</name></author><category term="stat.ML," /><category term="stat.AP" /><summary type="html">This paper presents a novel framework for Bayesian structural model updating and proposes a method that utilizes the surrogate unimodal encoders of a multimodal variational autoencoder. This method facilitates an efficient nonparametric estimation of the likelihood describing the observed data. It is particularly suitable for high-dimensional correlated simultaneous observations applicable to various dynamic analysis models. The proposed approach is benchmarked using a numerical model of a single-story frame building with acceleration and dynamic strain measurements.</summary></entry><entry><title type="html">Causal Inference on Missing Exposure via Robust Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/CausalInferenceonMissingExposureviaRobustEstimation.html" rel="alternate" type="text/html" title="Causal Inference on Missing Exposure via Robust Estimation" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/CausalInferenceonMissingExposureviaRobustEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/CausalInferenceonMissingExposureviaRobustEstimation.html">&lt;p&gt;How to deal with missing data in observational studies is a common concern for causal inference. When the covariates are missing at random (MAR), multiple approaches have been provided to help solve the issue. However, if the exposure is MAR, few approaches are available and careful adjustments on both missingness and confounding issues are required to ensure a consistent estimate of the true causal effect on the response. In this article, a new inverse probability weighting (IPW) estimator based on weighted estimating equations (WEE) is proposed to incorporate weights from both the missingness and propensity score (PS) models, which can reduce the joint effect of extreme weights in finite samples. Additionally, we develop a triple robust (TR) estimator via WEE to further protect against the misspecification of the missingness model. The asymptotic properties of WEE estimators are proved using properties of estimating equations. Based on the simulation studies, WEE methods outperform others including imputation-based approaches in terms of bias and variability. Finally, an application study is conducted to identify the causal effect of the presence of cardiovascular disease on mortality for COVID-19 patients.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08668&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuliang Shi, Yeying Zhu, Joel A. Dubin</name></author><category term="stat.ME" /><summary type="html">How to deal with missing data in observational studies is a common concern for causal inference. When the covariates are missing at random (MAR), multiple approaches have been provided to help solve the issue. However, if the exposure is MAR, few approaches are available and careful adjustments on both missingness and confounding issues are required to ensure a consistent estimate of the true causal effect on the response. In this article, a new inverse probability weighting (IPW) estimator based on weighted estimating equations (WEE) is proposed to incorporate weights from both the missingness and propensity score (PS) models, which can reduce the joint effect of extreme weights in finite samples. Additionally, we develop a triple robust (TR) estimator via WEE to further protect against the misspecification of the missingness model. The asymptotic properties of WEE estimators are proved using properties of estimating equations. Based on the simulation studies, WEE methods outperform others including imputation-based approaches in terms of bias and variability. Finally, an application study is conducted to identify the causal effect of the presence of cardiovascular disease on mortality for COVID-19 patients.</summary></entry><entry><title type="html">Conformal prediction with local weights: randomization enables local guarantees</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/Conformalpredictionwithlocalweightsrandomizationenableslocalguarantees.html" rel="alternate" type="text/html" title="Conformal prediction with local weights: randomization enables local guarantees" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/Conformalpredictionwithlocalweightsrandomizationenableslocalguarantees</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/Conformalpredictionwithlocalweightsrandomizationenableslocalguarantees.html">&lt;p&gt;In this work, we consider the problem of building distribution-free prediction intervals with finite-sample conditional coverage guarantees. Conformal prediction (CP) is an increasingly popular framework for building prediction intervals with distribution-free guarantees, but these guarantees only ensure marginal coverage: the probability of coverage is averaged over a random draw of both the training and test data, meaning that there might be substantial undercoverage within certain subpopulations. Instead, ideally, we would want to have local coverage guarantees that hold for each possible value of the test point’s features. While the impossibility of achieving pointwise local coverage is well established in the literature, many variants of conformal prediction algorithm show favorable local coverage properties empirically. Relaxing the definition of local coverage can allow for a theoretical understanding of this empirical phenomenon. We aim to bridge this gap between theoretical validation and empirical performance by proving achievable and interpretable guarantees for a relaxed notion of local coverage. Building on the localized CP method of Guan (2023) and the weighted CP framework of Tibshirani et al. (2019), we propose a new method, randomly-localized conformal prediction (RLCP), which returns prediction intervals that are not only marginally valid but also achieve a relaxed local coverage guarantee and guarantees under covariate shift. Through a series of simulations and real data experiments, we validate these coverage guarantees of RLCP while comparing it with the other local conformal prediction methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.07850&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rohan Hore, Rina Foygel Barber</name></author><category term="stat.ME" /><summary type="html">In this work, we consider the problem of building distribution-free prediction intervals with finite-sample conditional coverage guarantees. Conformal prediction (CP) is an increasingly popular framework for building prediction intervals with distribution-free guarantees, but these guarantees only ensure marginal coverage: the probability of coverage is averaged over a random draw of both the training and test data, meaning that there might be substantial undercoverage within certain subpopulations. Instead, ideally, we would want to have local coverage guarantees that hold for each possible value of the test point’s features. While the impossibility of achieving pointwise local coverage is well established in the literature, many variants of conformal prediction algorithm show favorable local coverage properties empirically. Relaxing the definition of local coverage can allow for a theoretical understanding of this empirical phenomenon. We aim to bridge this gap between theoretical validation and empirical performance by proving achievable and interpretable guarantees for a relaxed notion of local coverage. Building on the localized CP method of Guan (2023) and the weighted CP framework of Tibshirani et al. (2019), we propose a new method, randomly-localized conformal prediction (RLCP), which returns prediction intervals that are not only marginally valid but also achieve a relaxed local coverage guarantee and guarantees under covariate shift. Through a series of simulations and real data experiments, we validate these coverage guarantees of RLCP while comparing it with the other local conformal prediction methods.</summary></entry><entry><title type="html">Coordinated Trading Strategies for Battery Storage in Reserve and Spot Markets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/CoordinatedTradingStrategiesforBatteryStorageinReserveandSpotMarkets.html" rel="alternate" type="text/html" title="Coordinated Trading Strategies for Battery Storage in Reserve and Spot Markets" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/CoordinatedTradingStrategiesforBatteryStorageinReserveandSpotMarkets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/CoordinatedTradingStrategiesforBatteryStorageinReserveandSpotMarkets.html">&lt;p&gt;Quantity and price risks are key uncertainties market participants face in electricity markets with increased volatility, for instance, due to high shares of renewables. From day ahead until real-time, there is a large variation in the best available information, leading to price changes that flexible assets, such as battery storage, can exploit economically. This study contributes to understanding how coordinated bidding strategies can enhance multi-market trading and large-scale energy storage integration. Our findings shed light on the complexities arising from interdependencies and the high-dimensional nature of the problem. We show how stochastic dual dynamic programming is a suitable solution technique for such an environment. We include the three markets of the frequency containment reserve, day-ahead, and intraday in stochastic modelling and develop a multi-stage stochastic program. Prices are represented in a multidimensional Markov Chain, following the scheduling of the markets and allowing for time-dependent randomness. Using the example of a battery storage in the German energy sector, we provide valuable insights into the technical aspects of our method and the economic feasibility of battery storage operation. We find that capacity reservation in the frequency containment reserve dominates over the battery’s cycling in spot markets at the given resolution on prices in 2022. In an adjusted price environment, we find that coordination can yield an additional value of up to 12.5%.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08390&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Paul E. Seifert, Emil Kraft, Steffen Bakker, Stein-Erik Fleten</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Quantity and price risks are key uncertainties market participants face in electricity markets with increased volatility, for instance, due to high shares of renewables. From day ahead until real-time, there is a large variation in the best available information, leading to price changes that flexible assets, such as battery storage, can exploit economically. This study contributes to understanding how coordinated bidding strategies can enhance multi-market trading and large-scale energy storage integration. Our findings shed light on the complexities arising from interdependencies and the high-dimensional nature of the problem. We show how stochastic dual dynamic programming is a suitable solution technique for such an environment. We include the three markets of the frequency containment reserve, day-ahead, and intraday in stochastic modelling and develop a multi-stage stochastic program. Prices are represented in a multidimensional Markov Chain, following the scheduling of the markets and allowing for time-dependent randomness. Using the example of a battery storage in the German energy sector, we provide valuable insights into the technical aspects of our method and the economic feasibility of battery storage operation. We find that capacity reservation in the frequency containment reserve dominates over the battery’s cycling in spot markets at the given resolution on prices in 2022. In an adjusted price environment, we find that coordination can yield an additional value of up to 12.5%.</summary></entry><entry><title type="html">Covariate Selection for Optimizing Balance with Covariate-Adjusted Response-Adaptive Randomization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/CovariateSelectionforOptimizingBalancewithCovariateAdjustedResponseAdaptiveRandomization.html" rel="alternate" type="text/html" title="Covariate Selection for Optimizing Balance with Covariate-Adjusted Response-Adaptive Randomization" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/CovariateSelectionforOptimizingBalancewithCovariateAdjustedResponseAdaptiveRandomization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/CovariateSelectionforOptimizingBalancewithCovariateAdjustedResponseAdaptiveRandomization.html">&lt;p&gt;Balancing influential covariates is crucial for valid treatment comparisons in clinical studies. While covariate-adaptive randomization is commonly used to achieve balance, its performance can be inadequate when the number of baseline covariates is large. It is therefore essential to identify the influential factors associated with the outcome and ensure balance among these critical covariates. In this article, we propose a novel covariate-adjusted response-adaptive randomization that integrates the patients’ responses and covariates information to select sequentially significant covariates and maintain their balance. We establish theoretically the consistency of our covariate selection method and demonstrate that the improved covariate balancing, as evidenced by a faster convergence rate of the imbalance measure, leads to higher efficiency in estimating treatment effects. Furthermore, we provide extensive numerical and empirical studies to illustrate the benefits of our proposed method across various settings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08968&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ziqing Guo, Yang Liu, Lucy Xia</name></author><category term="stat.ME" /><summary type="html">Balancing influential covariates is crucial for valid treatment comparisons in clinical studies. While covariate-adaptive randomization is commonly used to achieve balance, its performance can be inadequate when the number of baseline covariates is large. It is therefore essential to identify the influential factors associated with the outcome and ensure balance among these critical covariates. In this article, we propose a novel covariate-adjusted response-adaptive randomization that integrates the patients’ responses and covariates information to select sequentially significant covariates and maintain their balance. We establish theoretically the consistency of our covariate selection method and demonstrate that the improved covariate balancing, as evidenced by a faster convergence rate of the imbalance measure, leads to higher efficiency in estimating treatment effects. Furthermore, we provide extensive numerical and empirical studies to illustrate the benefits of our proposed method across various settings.</summary></entry><entry><title type="html">Covariate balancing with measurement error</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/Covariatebalancingwithmeasurementerror.html" rel="alternate" type="text/html" title="Covariate balancing with measurement error" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/Covariatebalancingwithmeasurementerror</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/Covariatebalancingwithmeasurementerror.html">&lt;p&gt;In recent years, there is a growing body of causal inference literature focusing on covariate balancing methods. These methods eliminate observed confounding by equalizing covariate moments between the treated and control groups. The validity of covariate balancing relies on an implicit assumption that all covariates are accurately measured, which is frequently violated in observational studies. Nevertheless, the impact of measurement error on covariate balancing is unclear, and there is no existing work on balancing mismeasured covariates adequately. In this article, we show that naively ignoring measurement error reversely increases the magnitude of covariate imbalance and induces bias to treatment effect estimation. We then propose a class of measurement error correction strategies for the existing covariate balancing methods. Theoretically, we show that these strategies successfully recover balance for all covariates, and eliminate bias of treatment effect estimation. We assess the proposed correction methods in simulation studies and real data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09163&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xialing Wen, Ying Yan</name></author><category term="stat.ME" /><summary type="html">In recent years, there is a growing body of causal inference literature focusing on covariate balancing methods. These methods eliminate observed confounding by equalizing covariate moments between the treated and control groups. The validity of covariate balancing relies on an implicit assumption that all covariates are accurately measured, which is frequently violated in observational studies. Nevertheless, the impact of measurement error on covariate balancing is unclear, and there is no existing work on balancing mismeasured covariates adequately. In this article, we show that naively ignoring measurement error reversely increases the magnitude of covariate imbalance and induces bias to treatment effect estimation. We then propose a class of measurement error correction strategies for the existing covariate balancing methods. Theoretically, we show that these strategies successfully recover balance for all covariates, and eliminate bias of treatment effect estimation. We assess the proposed correction methods in simulation studies and real data analysis.</summary></entry><entry><title type="html">Efficient and Globally Robust Causal Excursion Effect Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/EfficientandGloballyRobustCausalExcursionEffectEstimation.html" rel="alternate" type="text/html" title="Efficient and Globally Robust Causal Excursion Effect Estimation" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/EfficientandGloballyRobustCausalExcursionEffectEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/EfficientandGloballyRobustCausalExcursionEffectEstimation.html">&lt;p&gt;Causal excursion effect (CEE) characterizes the effect of an intervention under policies that deviate from the experimental policy. It is widely used to study the effect of time-varying interventions that have the potential to be frequently adaptive, such as those delivered through smartphones. We study the semiparametric efficient estimation of CEE and we derive a semiparametric efficiency bound for CEE with identity or log link functions under working assumptions, in the context of micro-randomized trials. We propose a class of two-stage estimators that achieve the efficiency bound and are robust to misspecified nuisance models. In deriving the asymptotic property of the estimators, we establish a general theory for globally robust Z-estimators with either cross-fitted or non-cross-fitted nuisance parameters. We demonstrate substantial efficiency gain of the proposed estimator compared to existing ones through simulations and a real data application using the Drink Less micro-randomized trial.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.16529&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhaoxi Cheng, Lauren Bell, Tianchen Qian</name></author><category term="stat.ME" /><summary type="html">Causal excursion effect (CEE) characterizes the effect of an intervention under policies that deviate from the experimental policy. It is widely used to study the effect of time-varying interventions that have the potential to be frequently adaptive, such as those delivered through smartphones. We study the semiparametric efficient estimation of CEE and we derive a semiparametric efficiency bound for CEE with identity or log link functions under working assumptions, in the context of micro-randomized trials. We propose a class of two-stage estimators that achieve the efficiency bound and are robust to misspecified nuisance models. In deriving the asymptotic property of the estimators, we establish a general theory for globally robust Z-estimators with either cross-fitted or non-cross-fitted nuisance parameters. We demonstrate substantial efficiency gain of the proposed estimator compared to existing ones through simulations and a real data application using the Drink Less micro-randomized trial.</summary></entry><entry><title type="html">Empirical Evidence That There Is No Such Thing As A Validated Prediction Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/EmpiricalEvidenceThatThereIsNoSuchThingAsAValidatedPredictionModel.html" rel="alternate" type="text/html" title="Empirical Evidence That There Is No Such Thing As A Validated Prediction Model" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/EmpiricalEvidenceThatThereIsNoSuchThingAsAValidatedPredictionModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/EmpiricalEvidenceThatThereIsNoSuchThingAsAValidatedPredictionModel.html">&lt;p&gt;Background: External validations are essential to assess clinical prediction models (CPMs) before deployment. Apart from model misspecification, differences in patient population and other factors influence a model’s AUC (c-statistic). We aimed to quantify variation in AUCs across external validation studies and adjust expectations of a model’s performance in a new setting.
  Methods: The Tufts-PACE CPM Registry contains CPMs for cardiovascular disease prognosis. We analyzed the AUCs of 469 CPMs with a total of 1,603 external validations. For each CPM, we performed a random effects meta-analysis to estimate the between-study standard deviation $\tau$ among the AUCs. Since the majority of these meta-analyses has only a handful of validations, this leads to very poor estimates of $\tau$. So, we estimated a log normal distribution of $\tau$ across all CPMs and used this as an empirical prior. We compared this empirical Bayesian approach with frequentist meta-analyses using cross-validation.
  Results: The 469 CPMs had a median of 2 external validations (IQR: [1-3]). The estimated distribution of $\tau$ had a mean of 0.055 and a standard deviation of 0.015. If $\tau$ = 0.05, the 95% prediction interval for the AUC in a new setting is at least +/- 0.1, regardless of the number of validations. Frequentist methods underestimate the uncertainty about the AUC in a new setting. Accounting for $\tau$ in a Bayesian approach achieved near nominal coverage.
  Conclusion: Due to large heterogeneity among the validated AUC values of a CPM, there is great irreducible uncertainty in predicting the AUC in a new setting. This uncertainty is underestimated by existing methods. The proposed empirical Bayes approach addresses this problem which merits wide application in judging the validity of prediction models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08628&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Florian D. van Leeuwen, Ewout W. Steyerberg, David van Klaveren, Ben Wessler, David M. Kent, Erik W. van Zwet</name></author><category term="stat.ME" /><summary type="html">Background: External validations are essential to assess clinical prediction models (CPMs) before deployment. Apart from model misspecification, differences in patient population and other factors influence a model’s AUC (c-statistic). We aimed to quantify variation in AUCs across external validation studies and adjust expectations of a model’s performance in a new setting. Methods: The Tufts-PACE CPM Registry contains CPMs for cardiovascular disease prognosis. We analyzed the AUCs of 469 CPMs with a total of 1,603 external validations. For each CPM, we performed a random effects meta-analysis to estimate the between-study standard deviation $\tau$ among the AUCs. Since the majority of these meta-analyses has only a handful of validations, this leads to very poor estimates of $\tau$. So, we estimated a log normal distribution of $\tau$ across all CPMs and used this as an empirical prior. We compared this empirical Bayesian approach with frequentist meta-analyses using cross-validation. Results: The 469 CPMs had a median of 2 external validations (IQR: [1-3]). The estimated distribution of $\tau$ had a mean of 0.055 and a standard deviation of 0.015. If $\tau$ = 0.05, the 95% prediction interval for the AUC in a new setting is at least +/- 0.1, regardless of the number of validations. Frequentist methods underestimate the uncertainty about the AUC in a new setting. Accounting for $\tau$ in a Bayesian approach achieved near nominal coverage. Conclusion: Due to large heterogeneity among the validated AUC values of a CPM, there is great irreducible uncertainty in predicting the AUC in a new setting. This uncertainty is underestimated by existing methods. The proposed empirical Bayes approach addresses this problem which merits wide application in judging the validity of prediction models.</summary></entry><entry><title type="html">Empirical Networks are Sparse: Enhancing Multi-Edge Models with Zero-Inflation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/EmpiricalNetworksareSparseEnhancingMultiEdgeModelswithZeroInflation.html" rel="alternate" type="text/html" title="Empirical Networks are Sparse: Enhancing Multi-Edge Models with Zero-Inflation" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/EmpiricalNetworksareSparseEnhancingMultiEdgeModelswithZeroInflation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/EmpiricalNetworksareSparseEnhancingMultiEdgeModelswithZeroInflation.html">&lt;p&gt;Real-world networks are sparse. As we show in this article, even when a large number of interactions is observed most node pairs remain disconnected. We demonstrate that classical multi-edge network models, such as the $G(N,p)$, configuration models, and stochastic block models, fail to accurately capture this phenomenon. To mitigate this issue, zero-inflation must be integrated into these traditional models. Through zero-inflation, we incorporate a mechanism that accounts for the excess number of zeroes (disconnected pairs) observed in empirical data. By performing an analysis on all the datasets from the Sociopatterns repository, we illustrate how zero-inflated models more accurately reflect the sparsity and heavy-tailed edge count distributions observed in empirical data. Our findings underscore that failing to account for these ubiquitous properties in real-world networks inadvertently leads to biased models which do not accurately represent complex systems and their dynamics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09169&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Giona Casiraghi, Georges Andres</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Real-world networks are sparse. As we show in this article, even when a large number of interactions is observed most node pairs remain disconnected. We demonstrate that classical multi-edge network models, such as the $G(N,p)$, configuration models, and stochastic block models, fail to accurately capture this phenomenon. To mitigate this issue, zero-inflation must be integrated into these traditional models. Through zero-inflation, we incorporate a mechanism that accounts for the excess number of zeroes (disconnected pairs) observed in empirical data. By performing an analysis on all the datasets from the Sociopatterns repository, we illustrate how zero-inflated models more accurately reflect the sparsity and heavy-tailed edge count distributions observed in empirical data. Our findings underscore that failing to account for these ubiquitous properties in real-world networks inadvertently leads to biased models which do not accurately represent complex systems and their dynamics.</summary></entry><entry><title type="html">General Bayesian Predictive Synthesis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/GeneralBayesianPredictiveSynthesis.html" rel="alternate" type="text/html" title="General Bayesian Predictive Synthesis" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/GeneralBayesianPredictiveSynthesis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/GeneralBayesianPredictiveSynthesis.html">&lt;p&gt;This study investigates Bayesian ensemble learning for improving the quality of decision-making. We consider a decision-maker who selects an action from a set of candidates based on a policy trained using observations. In our setting, we assume the existence of experts who provide predictive distributions based on their own policies. Our goal is to integrate these predictive distributions within the Bayesian framework. Our proposed method, which we refer to as General Bayesian Predictive Synthesis (GBPS), is characterized by a loss minimization framework and does not rely on parameter estimation, unlike existing studies. Inspired by Bayesian predictive synthesis and general Bayes frameworks, we evaluate the performance of our proposed method through simulation studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09254&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Masahiro Kato</name></author><category term="stat.ME" /><summary type="html">This study investigates Bayesian ensemble learning for improving the quality of decision-making. We consider a decision-maker who selects an action from a set of candidates based on a policy trained using observations. In our setting, we assume the existence of experts who provide predictive distributions based on their own policies. Our goal is to integrate these predictive distributions within the Bayesian framework. Our proposed method, which we refer to as General Bayesian Predictive Synthesis (GBPS), is characterized by a loss minimization framework and does not rely on parameter estimation, unlike existing studies. Inspired by Bayesian predictive synthesis and general Bayes frameworks, we evaluate the performance of our proposed method through simulation studies.</summary></entry><entry><title type="html">Generative vs. Discriminative modeling under the lens of uncertainty quantification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/GenerativevsDiscriminativemodelingunderthelensofuncertaintyquantification.html" rel="alternate" type="text/html" title="Generative vs. Discriminative modeling under the lens of uncertainty quantification" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/GenerativevsDiscriminativemodelingunderthelensofuncertaintyquantification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/GenerativevsDiscriminativemodelingunderthelensofuncertaintyquantification.html">&lt;p&gt;Learning a parametric model from a given dataset indeed enables to capture intrinsic dependencies between random variables via a parametric conditional probability distribution and in turn predict the value of a label variable given observed variables. In this paper, we undertake a comparative analysis of generative and discriminative approaches which differ in their construction and the structure of the underlying inference problem. Our objective is to compare the ability of both approaches to leverage information from various sources in an epistemic uncertainty aware inference via the posterior predictive distribution. We assess the role of a prior distribution, explicit in the generative case and implicit in the discriminative case, leading to a discussion about discriminative models suffering from imbalanced dataset. We next examine the double role played by the observed variables in the generative case, and discuss the compatibility of both approaches with semi-supervised learning. We also provide with practical insights and we examine how the modeling choice impacts the sampling from the posterior predictive distribution. With regard to this, we propose a general sampling scheme enabling supervised learning for both approaches, as well as semi-supervised learning when compatible with the considered modeling approach. Throughout this paper, we illustrate our arguments and conclusions using the example of affine regression, and validate our comparative analysis through classification simulations using neural network based models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09172&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elouan Argouarc&apos;h, François Desbouvries, Eric Barat, Eiji Kawasaki</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Learning a parametric model from a given dataset indeed enables to capture intrinsic dependencies between random variables via a parametric conditional probability distribution and in turn predict the value of a label variable given observed variables. In this paper, we undertake a comparative analysis of generative and discriminative approaches which differ in their construction and the structure of the underlying inference problem. Our objective is to compare the ability of both approaches to leverage information from various sources in an epistemic uncertainty aware inference via the posterior predictive distribution. We assess the role of a prior distribution, explicit in the generative case and implicit in the discriminative case, leading to a discussion about discriminative models suffering from imbalanced dataset. We next examine the double role played by the observed variables in the generative case, and discuss the compatibility of both approaches with semi-supervised learning. We also provide with practical insights and we examine how the modeling choice impacts the sampling from the posterior predictive distribution. With regard to this, we propose a general sampling scheme enabling supervised learning for both approaches, as well as semi-supervised learning when compatible with the considered modeling approach. Throughout this paper, we illustrate our arguments and conclusions using the example of affine regression, and validate our comparative analysis through classification simulations using neural network based models.</summary></entry><entry><title type="html">How &amp;amp; Why To Use Audience Segmentation to Maximize (Listener) Demand Across Digital Music Portfolio</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/HowWhyToUseAudienceSegmentationtoMaximizeListenerDemandAcrossDigitalMusicPortfolio.html" rel="alternate" type="text/html" title="How &amp;amp; Why To Use Audience Segmentation to Maximize (Listener) Demand Across Digital Music Portfolio" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/HowWhyToUseAudienceSegmentationtoMaximizeListenerDemandAcrossDigitalMusicPortfolio</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/HowWhyToUseAudienceSegmentationtoMaximizeListenerDemandAcrossDigitalMusicPortfolio.html">&lt;p&gt;Digital delivery of songs has radically changed the way people can enjoy music, the sort of music available for listening, and the manner by which rights holders are compensated for their contributions to songs. Listeners enjoy an unlimited potpourri of sounds, uniquely free of incremental acquisition or switching costs which have been replaced by subscription or rentier fees. This regime shift has revealed listening patterns governed by affinity, boredom, attention budget, etc.: instantaneous, dynamic, organic or programmatic song selection. This regime shift in demand availability – with the commensurate translation of revenue implications – deprecates current orthodoxy for content curation. The impulse to point-of-sale model is insufficient in a regime where demand revenue is proportional to demand affinity and each are strongly dependent time series processes. We explore strategies &amp;amp; implications – which are generalizable to any media rights holding firm – from a prediction &amp;amp; optimization point of view for two straightforward demand models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09226&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kobi Abayomi</name></author><category term="stat.AP" /><summary type="html">Digital delivery of songs has radically changed the way people can enjoy music, the sort of music available for listening, and the manner by which rights holders are compensated for their contributions to songs. Listeners enjoy an unlimited potpourri of sounds, uniquely free of incremental acquisition or switching costs which have been replaced by subscription or rentier fees. This regime shift has revealed listening patterns governed by affinity, boredom, attention budget, etc.: instantaneous, dynamic, organic or programmatic song selection. This regime shift in demand availability – with the commensurate translation of revenue implications – deprecates current orthodoxy for content curation. The impulse to point-of-sale model is insufficient in a regime where demand revenue is proportional to demand affinity and each are strongly dependent time series processes. We explore strategies &amp;amp; implications – which are generalizable to any media rights holding firm – from a prediction &amp;amp; optimization point of view for two straightforward demand models.</summary></entry><entry><title type="html">Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/IdentificationbynonGaussianityinstructuralthresholdandsmoothtransitionvectorautoregressivemodels.html" rel="alternate" type="text/html" title="Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/IdentificationbynonGaussianityinstructuralthresholdandsmoothtransitionvectorautoregressivemodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/IdentificationbynonGaussianityinstructuralthresholdandsmoothtransitionvectorautoregressivemodels.html">&lt;p&gt;Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian. We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes. We also discuss labelling of the shocks, maximum likelihood estimation of the parameters, and stationarity the model. The introduced methods are implemented to the accompanying R package sstvars. Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy. In a structural logistic smooth transition vector autoregressive model consisting of two regimes, we find that a positive climate policy uncertainty shock decreases production in times of low economic policy uncertainty but slightly increases it in times of high economic policy uncertainty.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19707&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Savi Virolainen</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian. We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes. We also discuss labelling of the shocks, maximum likelihood estimation of the parameters, and stationarity the model. The introduced methods are implemented to the accompanying R package sstvars. Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy. In a structural logistic smooth transition vector autoregressive model consisting of two regimes, we find that a positive climate policy uncertainty shock decreases production in times of low economic policy uncertainty but slightly increases it in times of high economic policy uncertainty.</summary></entry><entry><title type="html">Improved methods for empirical Bayes multivariate multiple testing and effect size estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/ImprovedmethodsforempiricalBayesmultivariatemultipletestingandeffectsizeestimation.html" rel="alternate" type="text/html" title="Improved methods for empirical Bayes multivariate multiple testing and effect size estimation" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/ImprovedmethodsforempiricalBayesmultivariatemultipletestingandeffectsizeestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/ImprovedmethodsforempiricalBayesmultivariatemultipletestingandeffectsizeestimation.html">&lt;p&gt;Estimating the sharing of genetic effects across different conditions is important to many statistical analyses of genomic data. The patterns of sharing arising from these data are often highly heterogeneous. To flexibly model these heterogeneous sharing patterns, Urbut et al. (2019) proposed the multivariate adaptive shrinkage (MASH) method to jointly analyze genetic effects across multiple conditions. However, multivariate analyses using MASH (as well as other multivariate analyses) require good estimates of the sharing patterns, and estimating these patterns efficiently and accurately remains challenging. Here we describe new empirical Bayes methods that provide improvements in speed and accuracy over existing methods. The two key ideas are: (1) adaptive regularization to improve accuracy in settings with many conditions; (2) improving the speed of the model fitting algorithms by exploiting analytical results on covariance estimation. In simulations, we show that the new methods provide better model fits, better out-of-sample performance, and improved power and accuracy in detecting the true underlying signals. In an analysis of eQTLs in 49 human tissues, our new analysis pipeline achieves better model fits and better out-of-sample performance than the existing MASH analysis pipeline. We have implemented the new methods, which we call ``Ultimate Deconvolution’’, in an R package, udr, available on GitHub.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08784&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yunqi Yang, Peter Carbonetto, David Gerard, Matthew Stephens</name></author><category term="stat.ME" /><summary type="html">Estimating the sharing of genetic effects across different conditions is important to many statistical analyses of genomic data. The patterns of sharing arising from these data are often highly heterogeneous. To flexibly model these heterogeneous sharing patterns, Urbut et al. (2019) proposed the multivariate adaptive shrinkage (MASH) method to jointly analyze genetic effects across multiple conditions. However, multivariate analyses using MASH (as well as other multivariate analyses) require good estimates of the sharing patterns, and estimating these patterns efficiently and accurately remains challenging. Here we describe new empirical Bayes methods that provide improvements in speed and accuracy over existing methods. The two key ideas are: (1) adaptive regularization to improve accuracy in settings with many conditions; (2) improving the speed of the model fitting algorithms by exploiting analytical results on covariance estimation. In simulations, we show that the new methods provide better model fits, better out-of-sample performance, and improved power and accuracy in detecting the true underlying signals. In an analysis of eQTLs in 49 human tissues, our new analysis pipeline achieves better model fits and better out-of-sample performance than the existing MASH analysis pipeline. We have implemented the new methods, which we call ``Ultimate Deconvolution’’, in an R package, udr, available on GitHub.</summary></entry><entry><title type="html">Interventional Causal Discovery in a Mixture of DAGs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/InterventionalCausalDiscoveryinaMixtureofDAGs.html" rel="alternate" type="text/html" title="Interventional Causal Discovery in a Mixture of DAGs" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/InterventionalCausalDiscoveryinaMixtureofDAGs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/InterventionalCausalDiscoveryinaMixtureofDAGs.html">&lt;p&gt;Causal interactions among a group of variables are often modeled by a single causal graph. In some domains, however, these interactions are best described by multiple co-existing causal graphs, e.g., in dynamical systems or genomics. This paper addresses the hitherto unknown role of interventions in learning causal interactions among variables governed by a mixture of causal systems, each modeled by one directed acyclic graph (DAG). Causal discovery from mixtures is fundamentally more challenging than single-DAG causal discovery. Two major difficulties stem from (i) inherent uncertainty about the skeletons of the component DAGs that constitute the mixture and (ii) possibly cyclic relationships across these component DAGs. This paper addresses these challenges and aims to identify edges that exist in at least one component DAG of the mixture, referred to as true edges. First, it establishes matching necessary and sufficient conditions on the size of interventions required to identify the true edges. Next, guided by the necessity results, an adaptive algorithm is designed that learns all true edges using ${\cal O}(n^2)$ interventions, where $n$ is the number of nodes. Remarkably, the size of the interventions is optimal if the underlying mixture model does not contain cycles across its components. More generally, the gap between the intervention size used by the algorithm and the optimal size is quantified. It is shown to be bounded by the cyclic complexity number of the mixture model, defined as the size of the minimal intervention that can break the cycles in the mixture, which is upper bounded by the number of cycles among the ancestors of a node.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08666&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Burak Var{\i}c{\i}, Dmitriy Katz-Rogozhnikov, Dennis Wei, Prasanna Sattigeri, Ali Tajer</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Causal interactions among a group of variables are often modeled by a single causal graph. In some domains, however, these interactions are best described by multiple co-existing causal graphs, e.g., in dynamical systems or genomics. This paper addresses the hitherto unknown role of interventions in learning causal interactions among variables governed by a mixture of causal systems, each modeled by one directed acyclic graph (DAG). Causal discovery from mixtures is fundamentally more challenging than single-DAG causal discovery. Two major difficulties stem from (i) inherent uncertainty about the skeletons of the component DAGs that constitute the mixture and (ii) possibly cyclic relationships across these component DAGs. This paper addresses these challenges and aims to identify edges that exist in at least one component DAG of the mixture, referred to as true edges. First, it establishes matching necessary and sufficient conditions on the size of interventions required to identify the true edges. Next, guided by the necessity results, an adaptive algorithm is designed that learns all true edges using ${\cal O}(n^2)$ interventions, where $n$ is the number of nodes. Remarkably, the size of the interventions is optimal if the underlying mixture model does not contain cycles across its components. More generally, the gap between the intervention size used by the algorithm and the optimal size is quantified. It is shown to be bounded by the cyclic complexity number of the mixture model, defined as the size of the minimal intervention that can break the cycles in the mixture, which is upper bounded by the number of cycles among the ancestors of a node.</summary></entry><entry><title type="html">Introducing Diminutive Causal Structure into Graph Representation Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/IntroducingDiminutiveCausalStructureintoGraphRepresentationLearning.html" rel="alternate" type="text/html" title="Introducing Diminutive Causal Structure into Graph Representation Learning" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/IntroducingDiminutiveCausalStructureintoGraphRepresentationLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/IntroducingDiminutiveCausalStructureintoGraphRepresentationLearning.html">&lt;p&gt;When engaging in end-to-end graph representation learning with Graph Neural Networks (GNNs), the intricate causal relationships and rules inherent in graph data pose a formidable challenge for the model in accurately capturing authentic data relationships. A proposed mitigating strategy involves the direct integration of rules or relationships corresponding to the graph data into the model. However, within the domain of graph representation learning, the inherent complexity of graph data obstructs the derivation of a comprehensive causal structure that encapsulates universal rules or relationships governing the entire dataset. Instead, only specialized diminutive causal structures, delineating specific causal relationships within constrained subsets of graph data, emerge as discernible. Motivated by empirical insights, it is observed that GNN models exhibit a tendency to converge towards such specialized causal structures during the training process. Consequently, we posit that the introduction of these specific causal structures is advantageous for the training of GNN models. Building upon this proposition, we introduce a novel method that enables GNN models to glean insights from these specialized diminutive causal structures, thereby enhancing overall performance. Our method specifically extracts causal knowledge from the model representation of these diminutive causal structures and incorporates interchange intervention to optimize the learning process. Theoretical analysis serves to corroborate the efficacy of our proposed method. Furthermore, empirical experiments consistently demonstrate significant performance improvements across diverse datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08709&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hang Gao, Peng Qiao, Yifan Jin, Fengge Wu, Jiangmeng Li, Changwen Zheng</name></author><category term="stat.ME" /><summary type="html">When engaging in end-to-end graph representation learning with Graph Neural Networks (GNNs), the intricate causal relationships and rules inherent in graph data pose a formidable challenge for the model in accurately capturing authentic data relationships. A proposed mitigating strategy involves the direct integration of rules or relationships corresponding to the graph data into the model. However, within the domain of graph representation learning, the inherent complexity of graph data obstructs the derivation of a comprehensive causal structure that encapsulates universal rules or relationships governing the entire dataset. Instead, only specialized diminutive causal structures, delineating specific causal relationships within constrained subsets of graph data, emerge as discernible. Motivated by empirical insights, it is observed that GNN models exhibit a tendency to converge towards such specialized causal structures during the training process. Consequently, we posit that the introduction of these specific causal structures is advantageous for the training of GNN models. Building upon this proposition, we introduce a novel method that enables GNN models to glean insights from these specialized diminutive causal structures, thereby enhancing overall performance. Our method specifically extracts causal knowledge from the model representation of these diminutive causal structures and incorporates interchange intervention to optimize the learning process. Theoretical analysis serves to corroborate the efficacy of our proposed method. Furthermore, empirical experiments consistently demonstrate significant performance improvements across diverse datasets.</summary></entry><entry><title type="html">Learning High-dimensional Latent Variable Models via Doubly Stochastic Optimisation by Unadjusted Langevin</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/LearningHighdimensionalLatentVariableModelsviaDoublyStochasticOptimisationbyUnadjustedLangevin.html" rel="alternate" type="text/html" title="Learning High-dimensional Latent Variable Models via Doubly Stochastic Optimisation by Unadjusted Langevin" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/LearningHighdimensionalLatentVariableModelsviaDoublyStochasticOptimisationbyUnadjustedLangevin</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/LearningHighdimensionalLatentVariableModelsviaDoublyStochasticOptimisationbyUnadjustedLangevin.html">&lt;p&gt;Latent variable models are widely used in social and behavioural sciences, such as education, psychology, and political science. In recent years, high-dimensional latent variable models have become increasingly common for analysing large and complex data. Estimating high-dimensional latent variable models using marginal maximum likelihood is computationally demanding due to the complexity of integrals involved. To address this challenge, stochastic optimisation, which combines stochastic approximation and sampling techniques, has been shown to be effective. This method iterates between two steps – (1) sampling the latent variables from their posterior distribution based on the current parameter estimate, and (2) updating the fixed parameters using an approximate stochastic gradient constructed from the latent variable samples. In this paper, we propose a computationally more efficient stochastic optimisation algorithm. This improvement is achieved through the use of a minibatch of observations when sampling latent variables and constructing stochastic gradients, and an unadjusted Langevin sampler that utilises the gradient of the negative complete-data log-likelihood to sample latent variables. Theoretical results are established for the proposed algorithm, showing that the iterative parameter update converges to the marginal maximum likelihood estimate as the number of iterations goes to infinity. Furthermore, the proposed algorithm is shown to scale well to high-dimensional settings through simulation studies and a personality test application with 30,000 respondents, 300 items, and 30 latent dimensions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09311&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Motonori Oka, Yunxiao Chen, Irini Mounstaki</name></author><category term="stat.CO" /><summary type="html">Latent variable models are widely used in social and behavioural sciences, such as education, psychology, and political science. In recent years, high-dimensional latent variable models have become increasingly common for analysing large and complex data. Estimating high-dimensional latent variable models using marginal maximum likelihood is computationally demanding due to the complexity of integrals involved. To address this challenge, stochastic optimisation, which combines stochastic approximation and sampling techniques, has been shown to be effective. This method iterates between two steps – (1) sampling the latent variables from their posterior distribution based on the current parameter estimate, and (2) updating the fixed parameters using an approximate stochastic gradient constructed from the latent variable samples. In this paper, we propose a computationally more efficient stochastic optimisation algorithm. This improvement is achieved through the use of a minibatch of observations when sampling latent variables and constructing stochastic gradients, and an unadjusted Langevin sampler that utilises the gradient of the negative complete-data log-likelihood to sample latent variables. Theoretical results are established for the proposed algorithm, showing that the iterative parameter update converges to the marginal maximum likelihood estimate as the number of iterations goes to infinity. Furthermore, the proposed algorithm is shown to scale well to high-dimensional settings through simulation studies and a personality test application with 30,000 respondents, 300 items, and 30 latent dimensions.</summary></entry><entry><title type="html">Learning Joint and Individual Structure in Network Data with Covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/LearningJointandIndividualStructureinNetworkDatawithCovariates.html" rel="alternate" type="text/html" title="Learning Joint and Individual Structure in Network Data with Covariates" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/LearningJointandIndividualStructureinNetworkDatawithCovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/LearningJointandIndividualStructureinNetworkDatawithCovariates.html">&lt;p&gt;Datasets consisting of a network and covariates associated with its vertices have become ubiquitous. One problem pertaining to this type of data is to identify information unique to the network, information unique to the vertex covariates and information that is shared between the network and the vertex covariates. Existing techniques for network data and vertex covariates focus on capturing structure that is shared but are usually not able to differentiate structure that is unique to each dataset. This work formulates a low-rank model that simultaneously captures joint and individual information in network data with vertex covariates. A two-step estimation procedure is proposed, composed of an efficient spectral method followed by a refinement optimization step. Theoretically, we show that the spectral method is able to consistently recover the joint and individual components under a general signal-plus-noise model.
  Simulations and real data examples demonstrate the ability of the methods to recover accurate and interpretable components. In particular, the application of the methodology to a food trade network between countries with economic, developmental and geographical country-level indicators as covariates yields joint and individual factors that explain the trading patterns.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08776&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carson James, Dongbang Yuan, Irina Gaynanova, Jesús Arroyo</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Datasets consisting of a network and covariates associated with its vertices have become ubiquitous. One problem pertaining to this type of data is to identify information unique to the network, information unique to the vertex covariates and information that is shared between the network and the vertex covariates. Existing techniques for network data and vertex covariates focus on capturing structure that is shared but are usually not able to differentiate structure that is unique to each dataset. This work formulates a low-rank model that simultaneously captures joint and individual information in network data with vertex covariates. A two-step estimation procedure is proposed, composed of an efficient spectral method followed by a refinement optimization step. Theoretically, we show that the spectral method is able to consistently recover the joint and individual components under a general signal-plus-noise model. Simulations and real data examples demonstrate the ability of the methods to recover accurate and interpretable components. In particular, the application of the methodology to a food trade network between countries with economic, developmental and geographical country-level indicators as covariates yields joint and individual factors that explain the trading patterns.</summary></entry><entry><title type="html">Learning Metrics that Maximise Power for Accelerated A/B-Tests</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/LearningMetricsthatMaximisePowerforAcceleratedABTests.html" rel="alternate" type="text/html" title="Learning Metrics that Maximise Power for Accelerated A/B-Tests" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/LearningMetricsthatMaximisePowerforAcceleratedABTests</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/LearningMetricsthatMaximisePowerforAcceleratedABTests.html">&lt;p&gt;Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent.
  We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs. Empirical results show that we are able to increase statistical power by up to 78% when using our learnt metrics stand-alone, and by up to 210% when used in tandem with the North Star. Alternatively, we can obtain constant statistical power at a sample size that is down to 12% of what the North Star requires, significantly reducing the cost of experimentation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.03915&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Olivier Jeunen, Aleksei Ustimenko</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Online controlled experiments are a crucial tool to allow for confident decision-making in technology companies. A North Star metric is defined (such as long-term revenue or user retention), and system variants that statistically significantly improve on this metric in an A/B-test can be considered superior. North Star metrics are typically delayed and insensitive. As a result, the cost of experimentation is high: experiments need to run for a long time, and even then, type-II errors (i.e. false negatives) are prevalent. We propose to tackle this by learning metrics from short-term signals that directly maximise the statistical power they harness with respect to the North Star. We show that existing approaches are prone to overfitting, in that higher average metric sensitivity does not imply improved type-II errors, and propose to instead minimise the $p$-values a metric would have produced on a log of past experiments. We collect such datasets from two social media applications with over 160 million Monthly Active Users each, totalling over 153 A/B-pairs. Empirical results show that we are able to increase statistical power by up to 78% when using our learnt metrics stand-alone, and by up to 210% when used in tandem with the North Star. Alternatively, we can obtain constant statistical power at a sample size that is down to 12% of what the North Star requires, significantly reducing the cost of experimentation.</summary></entry><entry><title type="html">Oblivious subspace embeddings for compressed Tucker decompositions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/OblivioussubspaceembeddingsforcompressedTuckerdecompositions.html" rel="alternate" type="text/html" title="Oblivious subspace embeddings for compressed Tucker decompositions" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/OblivioussubspaceembeddingsforcompressedTuckerdecompositions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/OblivioussubspaceembeddingsforcompressedTuckerdecompositions.html">&lt;p&gt;Emphasis in the tensor literature on random embeddings (tools for low-distortion dimension reduction) for the canonical polyadic (CP) tensor decomposition has left analogous results for the more expressive Tucker decomposition comparatively lacking. This work establishes general Johnson-Lindenstrauss (JL) type guarantees for the estimation of Tucker decompositions when an oblivious random embedding is applied along each mode. When these embeddings are drawn from a JL-optimal family, the decomposition can be estimated within $\varepsilon$ relative error under restrictions on the embedding dimension that are in line with recent CP results. We implement a higher-order orthogonal iteration (HOOI) decomposition algorithm with random embeddings to demonstrate the practical benefits of this approach and its potential to improve the accessibility of otherwise prohibitive tensor analyses. On moderately large face image and fMRI neuroimaging datasets, empirical results show that substantial dimension reduction is possible with minimal increase in reconstruction error relative to traditional HOOI ($\leq$5% larger error, 50%-60% lower computation time for large models with 50% dimension reduction along each mode). Especially for large tensors, our method outperforms traditional higher-order singular value decomposition (HOSVD) and recently proposed TensorSketch methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09387&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matthew Pietrosanu, Bei Jiang, Linglong Kong</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Emphasis in the tensor literature on random embeddings (tools for low-distortion dimension reduction) for the canonical polyadic (CP) tensor decomposition has left analogous results for the more expressive Tucker decomposition comparatively lacking. This work establishes general Johnson-Lindenstrauss (JL) type guarantees for the estimation of Tucker decompositions when an oblivious random embedding is applied along each mode. When these embeddings are drawn from a JL-optimal family, the decomposition can be estimated within $\varepsilon$ relative error under restrictions on the embedding dimension that are in line with recent CP results. We implement a higher-order orthogonal iteration (HOOI) decomposition algorithm with random embeddings to demonstrate the practical benefits of this approach and its potential to improve the accessibility of otherwise prohibitive tensor analyses. On moderately large face image and fMRI neuroimaging datasets, empirical results show that substantial dimension reduction is possible with minimal increase in reconstruction error relative to traditional HOOI ($\leq$5% larger error, 50%-60% lower computation time for large models with 50% dimension reduction along each mode). Especially for large tensors, our method outperforms traditional higher-order singular value decomposition (HOSVD) and recently proposed TensorSketch methods.</summary></entry><entry><title type="html">Orthogonalized Estimation of Difference of $Q$-functions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/OrthogonalizedEstimationofDifferenceofQfunctions.html" rel="alternate" type="text/html" title="Orthogonalized Estimation of Difference of $Q$-functions" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/OrthogonalizedEstimationofDifferenceofQfunctions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/OrthogonalizedEstimationofDifferenceofQfunctions.html">&lt;p&gt;Offline reinforcement learning is important in many settings with available observational data but the inability to deploy new policies online due to safety, cost, and other concerns. Many recent advances in causal inference and machine learning target estimation of causal contrast functions such as CATE, which is sufficient for optimizing decisions and can adapt to potentially smoother structure. We develop a dynamic generalization of the R-learner (Nie and Wager 2021, Lewis and Syrgkanis 2021) for estimating and optimizing the difference of $Q^\pi$-functions, $Q^\pi(s,1)-Q^\pi(s,0)$ (which can be used to optimize multiple-valued actions). We leverage orthogonal estimation to improve convergence rates in the presence of slower nuisance estimation rates and prove consistency of policy optimization under a margin condition. The method can leverage black-box nuisance estimators of the $Q$-function and behavior policy to target estimation of a more structured $Q$-function contrast.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08697&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Angela Zhou</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Offline reinforcement learning is important in many settings with available observational data but the inability to deploy new policies online due to safety, cost, and other concerns. Many recent advances in causal inference and machine learning target estimation of causal contrast functions such as CATE, which is sufficient for optimizing decisions and can adapt to potentially smoother structure. We develop a dynamic generalization of the R-learner (Nie and Wager 2021, Lewis and Syrgkanis 2021) for estimating and optimizing the difference of $Q^\pi$-functions, $Q^\pi(s,1)-Q^\pi(s,0)$ (which can be used to optimize multiple-valued actions). We leverage orthogonal estimation to improve convergence rates in the presence of slower nuisance estimation rates and prove consistency of policy optimization under a margin condition. The method can leverage black-box nuisance estimators of the $Q$-function and behavior policy to target estimation of a more structured $Q$-function contrast.</summary></entry><entry><title type="html">Relational event models with global covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/Relationaleventmodelswithglobalcovariates.html" rel="alternate" type="text/html" title="Relational event models with global covariates" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/Relationaleventmodelswithglobalcovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/Relationaleventmodelswithglobalcovariates.html">&lt;p&gt;Traditional inference in relational event models from dynamic network data involves only dyadic and node-specific variables, as anything that is global, i.e. constant across dyads, drops out of the partial likelihood. We address this with the use of nested case-control sampling on a time-shifted version of the event process. This leads to a partial likelihood of a degenerate logistic additive model, enabling efficient estimation of global and non-global covariate effects. The method’s effectiveness is demonstrated through a simulation study. An application to bike sharing data reveals significant influences of global covariates like weather and time of day on bike-sharing dynamics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09055&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Melania Lembo, Rūta Juozaitienė, Veronica Vinciotti, Ernst C. Wit</name></author><category term="stat.ME" /><summary type="html">Traditional inference in relational event models from dynamic network data involves only dyadic and node-specific variables, as anything that is global, i.e. constant across dyads, drops out of the partial likelihood. We address this with the use of nested case-control sampling on a time-shifted version of the event process. This leads to a partial likelihood of a degenerate logistic additive model, enabling efficient estimation of global and non-global covariate effects. The method’s effectiveness is demonstrated through a simulation study. An application to bike sharing data reveals significant influences of global covariates like weather and time of day on bike-sharing dynamics.</summary></entry><entry><title type="html">Robust estimations from distribution structures: I. Mean</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/RobustestimationsfromdistributionstructuresIMean.html" rel="alternate" type="text/html" title="Robust estimations from distribution structures: I. Mean" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/RobustestimationsfromdistributionstructuresIMean</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/RobustestimationsfromdistributionstructuresIMean.html">&lt;p&gt;As the most fundamental problem in statistics, robust location estimation has many prominent solutions, such as the trimmed mean, Winsorized mean, Hodges Lehmann estimator, Huber M estimator, and median of means. Recent studies suggest that their maximum biases concerning the mean can be quite different, but the underlying mechanisms largely remain unclear. This study exploited a semiparametric method to classify distributions by the asymptotic orderliness of quantile combinations with varying breakdown points, showing their interrelations and connections to parametric distributions. Further deductions explain why the Winsorized mean typically has smaller biases compared to the trimmed mean; two sequences of semiparametric robust mean estimators emerge, particularly highlighting the superiority of the median Hodges Lehmann mean. This article sheds light on the understanding of the common nature of probability distributions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.12110&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tuobang Li</name></author><category term="stat.AP," /><category term="stat.CO," /><category term="stat.ME," /><category term="stat.OT," /><category term="stat.TH" /><summary type="html">As the most fundamental problem in statistics, robust location estimation has many prominent solutions, such as the trimmed mean, Winsorized mean, Hodges Lehmann estimator, Huber M estimator, and median of means. Recent studies suggest that their maximum biases concerning the mean can be quite different, but the underlying mechanisms largely remain unclear. This study exploited a semiparametric method to classify distributions by the asymptotic orderliness of quantile combinations with varying breakdown points, showing their interrelations and connections to parametric distributions. Further deductions explain why the Winsorized mean typically has smaller biases compared to the trimmed mean; two sequences of semiparametric robust mean estimators emerge, particularly highlighting the superiority of the median Hodges Lehmann mean. This article sheds light on the understanding of the common nature of probability distributions.</summary></entry><entry><title type="html">Robust estimations from distribution structures: V. Non-asymptotic</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/RobustestimationsfromdistributionstructuresVNonasymptotic.html" rel="alternate" type="text/html" title="Robust estimations from distribution structures: V. Non-asymptotic" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/RobustestimationsfromdistributionstructuresVNonasymptotic</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/RobustestimationsfromdistributionstructuresVNonasymptotic.html">&lt;p&gt;Due to the complexity of order statistics, the finite sample behaviour of robust statistics is generally not analytically solvable. While the Monte Carlo method can provide approximate solutions, its convergence rate is typically very slow, making the computational cost to achieve the desired accuracy unaffordable for ordinary users. In this paper, we propose an approach analogous to the Fourier transformation to decompose the finite sample structure of the uniform distribution. By obtaining sets of sequences that are consistent with parametric distributions for the first four sample moments, we can approximate the finite sample behavior of other estimators with significantly reduced computational costs. This article reveals the underlying structure of randomness and presents a novel approach to integrate multiple assumptions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.18951&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tuobang Li</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO," /><category term="stat.OT" /><summary type="html">Due to the complexity of order statistics, the finite sample behaviour of robust statistics is generally not analytically solvable. While the Monte Carlo method can provide approximate solutions, its convergence rate is typically very slow, making the computational cost to achieve the desired accuracy unaffordable for ordinary users. In this paper, we propose an approach analogous to the Fourier transformation to decompose the finite sample structure of the uniform distribution. By obtaining sets of sequences that are consistent with parametric distributions for the first four sample moments, we can approximate the finite sample behavior of other estimators with significantly reduced computational costs. This article reveals the underlying structure of randomness and presents a novel approach to integrate multiple assumptions.</summary></entry><entry><title type="html">Variational Bayes Inference for Spatial Error Models with Missing Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/VariationalBayesInferenceforSpatialErrorModelswithMissingData.html" rel="alternate" type="text/html" title="Variational Bayes Inference for Spatial Error Models with Missing Data" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/VariationalBayesInferenceforSpatialErrorModelswithMissingData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/VariationalBayesInferenceforSpatialErrorModelswithMissingData.html">&lt;p&gt;The spatial error model (SEM) is a type of simultaneous autoregressive (SAR) model for analysing spatially correlated data. Markov chain Monte Carlo (MCMC) is one of the most widely used Bayesian methods for estimating SEM, but it has significant limitations when it comes to handling missing data in the response variable due to its high computational cost. Variational Bayes (VB) approximation offers an alternative solution to this problem. Two VB-based algorithms employing Gaussian variational approximation with factor covariance structure are presented, joint VB (JVB) and hybrid VB (HVB), suitable for both missing at random and not at random inference. When dealing with many missing values, the JVB is inaccurate, and the standard HVB algorithm struggles to achieve accurate inferences. Our modified versions of HVB enable accurate inference within a reasonable computational time, thus improving its performance. The performance of the VB methods is evaluated using simulated and real datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08685&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anjana Wijayawardhana, David Gunawan, Thomas Suesse</name></author><category term="stat.ME" /><summary type="html">The spatial error model (SEM) is a type of simultaneous autoregressive (SAR) model for analysing spatially correlated data. Markov chain Monte Carlo (MCMC) is one of the most widely used Bayesian methods for estimating SEM, but it has significant limitations when it comes to handling missing data in the response variable due to its high computational cost. Variational Bayes (VB) approximation offers an alternative solution to this problem. Two VB-based algorithms employing Gaussian variational approximation with factor covariance structure are presented, joint VB (JVB) and hybrid VB (HVB), suitable for both missing at random and not at random inference. When dealing with many missing values, the JVB is inaccurate, and the standard HVB algorithm struggles to achieve accurate inferences. Our modified versions of HVB enable accurate inference within a reasonable computational time, thus improving its performance. The performance of the VB methods is evaluated using simulated and real datasets.</summary></entry><entry><title type="html">Volatility Forecasting Using Similarity-based Parameter Correction and Aggregated Shock Information</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/VolatilityForecastingUsingSimilaritybasedParameterCorrectionandAggregatedShockInformation.html" rel="alternate" type="text/html" title="Volatility Forecasting Using Similarity-based Parameter Correction and Aggregated Shock Information" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/VolatilityForecastingUsingSimilaritybasedParameterCorrectionandAggregatedShockInformation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/VolatilityForecastingUsingSimilaritybasedParameterCorrectionandAggregatedShockInformation.html">&lt;p&gt;We develop a procedure for forecasting the volatility of a time series immediately following a news shock. Adapting the similarity-based framework of Lin and Eck (2020), we exploit series that have experienced similar shocks. We aggregate their shock-induced excess volatilities by positing the shocks to be affine functions of exogenous covariates. The volatility shocks are modeled as random effects and estimated as fixed effects. The aggregation of these estimates is done in service of adjusting the $h$-step-ahead GARCH forecast of the time series under study by an additive term. The adjusted and unadjusted forecasts are evaluated using the unobservable but easily-estimated realized volatility (RV). A real-world application is provided, as are simulation results suggesting the conditions and hyperparameters under which our method thrives.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08738&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David Lundquist, Daniel Eck</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">We develop a procedure for forecasting the volatility of a time series immediately following a news shock. Adapting the similarity-based framework of Lin and Eck (2020), we exploit series that have experienced similar shocks. We aggregate their shock-induced excess volatilities by positing the shocks to be affine functions of exogenous covariates. The volatility shocks are modeled as random effects and estimated as fixed effects. The aggregation of these estimates is done in service of adjusting the $h$-step-ahead GARCH forecast of the time series under study by an additive term. The adjusted and unadjusted forecasts are evaluated using the unobservable but easily-estimated realized volatility (RV). A real-world application is provided, as are simulation results suggesting the conditions and hyperparameters under which our method thrives.</summary></entry><entry><title type="html">When Pearson $\chi^2$ and other divisible statistics are not goodness-of-fit tests</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/WhenPearsonchi2andotherdivisiblestatisticsarenotgoodnessoffittests.html" rel="alternate" type="text/html" title="When Pearson $\chi^2$ and other divisible statistics are not goodness-of-fit tests" /><published>2024-06-14T00:00:00+00:00</published><updated>2024-06-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/WhenPearsonchi2andotherdivisiblestatisticsarenotgoodnessoffittests</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/14/WhenPearsonchi2andotherdivisiblestatisticsarenotgoodnessoffittests.html">&lt;p&gt;Thousands of experiments are analyzed and papers are published each year involving the statistical analysis of grouped data. While this area of statistics is often perceived - somewhat naively - as saturated, several misconceptions still affect everyday practice, and new frontiers have so far remained unexplored. Researchers must be aware of the limitations affecting their analyses and what are the new possibilities in their hands.
  Motivated by this need, the article introduces a unifying approach to the analysis of grouped data which allows us to study the class of divisible statistics - that includes Pearson’s $\chi^2$, the likelihood ratio as special cases - with a fresh perspective. The contributions collected in this manuscript span from modeling and estimation to distribution-free goodness-of-fit tests.
  Perhaps the most surprising result presented here is that, in a sparse regime, all tests proposed in the literature are dominated by a class of weighted linear statistics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09195&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sara Algeri, Estate V. Khmaladze</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Thousands of experiments are analyzed and papers are published each year involving the statistical analysis of grouped data. While this area of statistics is often perceived - somewhat naively - as saturated, several misconceptions still affect everyday practice, and new frontiers have so far remained unexplored. Researchers must be aware of the limitations affecting their analyses and what are the new possibilities in their hands. Motivated by this need, the article introduces a unifying approach to the analysis of grouped data which allows us to study the class of divisible statistics - that includes Pearson’s $\chi^2$, the likelihood ratio as special cases - with a fresh perspective. The contributions collected in this manuscript span from modeling and estimation to distribution-free goodness-of-fit tests. Perhaps the most surprising result presented here is that, in a sparse regime, all tests proposed in the literature are dominated by a class of weighted linear statistics.</summary></entry></feed>