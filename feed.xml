<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-20T07:15:18+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Bayesian Convolutional Neural Network-based Generalized Linear Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/ABayesianConvolutionalNeuralNetworkbasedGeneralizedLinearModel.html" rel="alternate" type="text/html" title="A Bayesian Convolutional Neural Network-based Generalized Linear Model" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/ABayesianConvolutionalNeuralNetworkbasedGeneralizedLinearModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/ABayesianConvolutionalNeuralNetworkbasedGeneralizedLinearModel.html">&lt;p&gt;Convolutional neural networks (CNNs) provide flexible function approximations for a wide variety of applications when the input variables are in the form of images or spatial data. Although CNNs often outperform traditional statistical models in prediction accuracy, statistical inference, such as estimating the effects of covariates and quantifying the prediction uncertainty, is not trivial due to the highly complicated model structure and overparameterization. To address this challenge, we propose a new Bayesian approach by embedding CNNs within the generalized linear models (GLMs) framework. We use extracted nodes from the last hidden layer of CNN with Monte Carlo (MC) dropout as informative covariates in GLM. This improves accuracy in prediction and regression coefficient inference, allowing for the interpretation of coefficients and uncertainty quantification. By fitting ensemble GLMs across multiple realizations from MC dropout, we can account for uncertainties in extracting the features. We apply our methods to biological and epidemiological problems, which have both high-dimensional correlated inputs and vector covariates. Specifically, we consider malaria incidence data, brain tumor image data, and fMRI data. By extracting information from correlated inputs, the proposed method can provide an interpretable Bayesian analysis. The algorithm can be broadly applicable to image regressions or correlated data analysis by enabling accurate Bayesian inference quickly.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.09560&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yeseul Jeon, Won Chang, Seonghyun Jeong, Sanghoon Han, Jaewoo Park</name></author><category term="stat.ME" /><summary type="html">Convolutional neural networks (CNNs) provide flexible function approximations for a wide variety of applications when the input variables are in the form of images or spatial data. Although CNNs often outperform traditional statistical models in prediction accuracy, statistical inference, such as estimating the effects of covariates and quantifying the prediction uncertainty, is not trivial due to the highly complicated model structure and overparameterization. To address this challenge, we propose a new Bayesian approach by embedding CNNs within the generalized linear models (GLMs) framework. We use extracted nodes from the last hidden layer of CNN with Monte Carlo (MC) dropout as informative covariates in GLM. This improves accuracy in prediction and regression coefficient inference, allowing for the interpretation of coefficients and uncertainty quantification. By fitting ensemble GLMs across multiple realizations from MC dropout, we can account for uncertainties in extracting the features. We apply our methods to biological and epidemiological problems, which have both high-dimensional correlated inputs and vector covariates. Specifically, we consider malaria incidence data, brain tumor image data, and fMRI data. By extracting information from correlated inputs, the proposed method can provide an interpretable Bayesian analysis. The algorithm can be broadly applicable to image regressions or correlated data analysis by enabling accurate Bayesian inference quickly.</summary></entry><entry><title type="html">An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed Bandits</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/AnExperimentalDesignforAnytimeValidCausalInferenceonMultiArmedBandits.html" rel="alternate" type="text/html" title="An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed Bandits" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/AnExperimentalDesignforAnytimeValidCausalInferenceonMultiArmedBandits</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/AnExperimentalDesignforAnytimeValidCausalInferenceonMultiArmedBandits.html">&lt;p&gt;In multi-armed bandit (MAB) experiments, it is often advantageous to continuously produce inference on the average treatment effect (ATE) between arms as new data arrive and determine a data-driven stopping time for the experiment. We develop the Mixture Adaptive Design (MAD), a new experimental design for multi-armed bandit experiments that produces powerful and anytime-valid inference on the ATE for \emph{any} bandit algorithm of the experimenter’s choice, even those without probabilistic treatment assignment. Intuitively, the MAD “mixes” any bandit algorithm of the experimenter’s choice with a Bernoulli design through a tuning parameter $\delta_t$, where $\delta_t$ is a deterministic sequence that decreases the priority placed on the Bernoulli design as the sample size grows. We prove that for $\delta_t = \omega\left(t^{-1/4}\right)$, the MAD generates anytime-valid asymptotic confidence sequences that are guaranteed to shrink around the true ATE. Hence, the experimenter is guaranteed to detect a true non-zero treatment effect in finite time. Additionally, we prove that the regret of the MAD approaches that of its underlying bandit algorithm over time, and hence, incurs a relatively small loss in regret in return for powerful inferential guarantees. Finally, we conduct an extensive simulation study exhibiting that the MAD achieves finite-sample anytime validity and high power without significant losses in finite-sample reward.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.05794&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Biyonka Liang, Iavor Bojinov</name></author><category term="stat.ME" /><summary type="html">In multi-armed bandit (MAB) experiments, it is often advantageous to continuously produce inference on the average treatment effect (ATE) between arms as new data arrive and determine a data-driven stopping time for the experiment. We develop the Mixture Adaptive Design (MAD), a new experimental design for multi-armed bandit experiments that produces powerful and anytime-valid inference on the ATE for \emph{any} bandit algorithm of the experimenter’s choice, even those without probabilistic treatment assignment. Intuitively, the MAD “mixes” any bandit algorithm of the experimenter’s choice with a Bernoulli design through a tuning parameter $\delta_t$, where $\delta_t$ is a deterministic sequence that decreases the priority placed on the Bernoulli design as the sample size grows. We prove that for $\delta_t = \omega\left(t^{-1/4}\right)$, the MAD generates anytime-valid asymptotic confidence sequences that are guaranteed to shrink around the true ATE. Hence, the experimenter is guaranteed to detect a true non-zero treatment effect in finite time. Additionally, we prove that the regret of the MAD approaches that of its underlying bandit algorithm over time, and hence, incurs a relatively small loss in regret in return for powerful inferential guarantees. Finally, we conduct an extensive simulation study exhibiting that the MAD achieves finite-sample anytime validity and high power without significant losses in finite-sample reward.</summary></entry><entry><title type="html">An asymptotic Peskun ordering and its application to lifted samplers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/AnasymptoticPeskunorderinganditsapplicationtoliftedsamplers.html" rel="alternate" type="text/html" title="An asymptotic Peskun ordering and its application to lifted samplers" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/AnasymptoticPeskunorderinganditsapplicationtoliftedsamplers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/AnasymptoticPeskunorderinganditsapplicationtoliftedsamplers.html">&lt;p&gt;A Peskun ordering between two samplers, implying a dominance of one over the other, is known among the Markov chain Monte Carlo community for being a remarkably strong result. It is however also known for being a result that is notably difficult to establish. Indeed, one has to prove that the probability to reach a state $\mathbf{y}$ from a state $\mathbf{x}$, using a sampler, is greater than or equal to the probability using the other sampler, and this must hold for all pairs $(\mathbf{x}, \mathbf{y})$ such that $\mathbf{x} \neq \mathbf{y}$. We provide in this paper a weaker version that does not require an inequality between the probabilities for all these states: essentially, the dominance holds asymptotically, as a varying parameter grows without bound, as long as the states for which the probabilities are greater than or equal to belong to a mass-concentrating set. The weak ordering turns out to be useful to compare lifted samplers for partially-ordered discrete state-spaces with their Metropolis–Hastings counterparts. An analysis in great generality yields a qualitative conclusion: they asymptotically perform better in certain situations (and we are able to identify them), but not necessarily in others (and the reasons why are made clear). A quantitative study in a specific context of graphical-model simulation is also conducted.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.05492&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Philippe Gagnon, Florian Maire</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">A Peskun ordering between two samplers, implying a dominance of one over the other, is known among the Markov chain Monte Carlo community for being a remarkably strong result. It is however also known for being a result that is notably difficult to establish. Indeed, one has to prove that the probability to reach a state $\mathbf{y}$ from a state $\mathbf{x}$, using a sampler, is greater than or equal to the probability using the other sampler, and this must hold for all pairs $(\mathbf{x}, \mathbf{y})$ such that $\mathbf{x} \neq \mathbf{y}$. We provide in this paper a weaker version that does not require an inequality between the probabilities for all these states: essentially, the dominance holds asymptotically, as a varying parameter grows without bound, as long as the states for which the probabilities are greater than or equal to belong to a mass-concentrating set. The weak ordering turns out to be useful to compare lifted samplers for partially-ordered discrete state-spaces with their Metropolis–Hastings counterparts. An analysis in great generality yields a qualitative conclusion: they asymptotically perform better in certain situations (and we are able to identify them), but not necessarily in others (and the reasons why are made clear). A quantitative study in a specific context of graphical-model simulation is also conducted.</summary></entry><entry><title type="html">Asymptotic equivalence of Principal Components and Quasi Maximum Likelihood estimators in Large Approximate Factor Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/AsymptoticequivalenceofPrincipalComponentsandQuasiMaximumLikelihoodestimatorsinLargeApproximateFactorModels.html" rel="alternate" type="text/html" title="Asymptotic equivalence of Principal Components and Quasi Maximum Likelihood estimators in Large Approximate Factor Models" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/AsymptoticequivalenceofPrincipalComponentsandQuasiMaximumLikelihoodestimatorsinLargeApproximateFactorModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/AsymptoticequivalenceofPrincipalComponentsandQuasiMaximumLikelihoodestimatorsinLargeApproximateFactorModels.html">&lt;p&gt;We provide an alternative derivation of the asymptotic results for the Principal Components estimator of a large approximate factor model. Results are derived under a minimal set of assumptions and, in particular, we require only the existence of 4th order moments. A special focus is given to the time series setting, a case considered in almost all recent econometric applications of factor models. Hence, estimation is based on the classical $n\times n$ sample covariance matrix and not on a $T\times T$ covariance matrix often considered in the literature. Indeed, despite the two approaches being asymptotically equivalent, the former is more coherent with a time series setting and it immediately allows us to write more intuitive asymptotic expansions for the Principal Component estimators showing that they are equivalent to OLS as long as $\sqrt n/T\to 0$ and $\sqrt T/n\to 0$, that is the loadings are estimated in a time series regression as if the factors were known, while the factors are estimated in a cross-sectional regression as if the loadings were known. Finally, we give some alternative sets of primitive sufficient conditions for mean-squared consistency of the sample covariance matrix of the factors, of the idiosyncratic components, and of the observed time series, which is the starting point for Principal Component Analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.09864&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matteo Barigozzi</name></author><category term="stat.ME" /><summary type="html">We provide an alternative derivation of the asymptotic results for the Principal Components estimator of a large approximate factor model. Results are derived under a minimal set of assumptions and, in particular, we require only the existence of 4th order moments. A special focus is given to the time series setting, a case considered in almost all recent econometric applications of factor models. Hence, estimation is based on the classical $n\times n$ sample covariance matrix and not on a $T\times T$ covariance matrix often considered in the literature. Indeed, despite the two approaches being asymptotically equivalent, the former is more coherent with a time series setting and it immediately allows us to write more intuitive asymptotic expansions for the Principal Component estimators showing that they are equivalent to OLS as long as $\sqrt n/T\to 0$ and $\sqrt T/n\to 0$, that is the loadings are estimated in a time series regression as if the factors were known, while the factors are estimated in a cross-sectional regression as if the loadings were known. Finally, we give some alternative sets of primitive sufficient conditions for mean-squared consistency of the sample covariance matrix of the factors, of the idiosyncratic components, and of the observed time series, which is the starting point for Principal Component Analysis.</summary></entry><entry><title type="html">BOB: Bayesian Optimized Bootstrap for Uncertainty Quantification in Gaussian Mixture Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/BOBBayesianOptimizedBootstrapforUncertaintyQuantificationinGaussianMixtureModels.html" rel="alternate" type="text/html" title="BOB: Bayesian Optimized Bootstrap for Uncertainty Quantification in Gaussian Mixture Models" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/BOBBayesianOptimizedBootstrapforUncertaintyQuantificationinGaussianMixtureModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/BOBBayesianOptimizedBootstrapforUncertaintyQuantificationinGaussianMixtureModels.html">&lt;p&gt;A natural way to quantify uncertainties in Gaussian mixture models (GMMs) is through Bayesian methods. That said, sampling from the joint posterior distribution of GMMs via standard Markov chain Monte Carlo (MCMC) imposes several computational challenges, which have prevented a broader full Bayesian implementation of these models. A growing body of literature has introduced the Weighted Likelihood Bootstrap and the Weighted Bayesian Bootstrap as alternatives to MCMC sampling. The core idea of these methods is to repeatedly compute maximum a posteriori (MAP) estimates on many randomly weighted posterior densities. These MAP estimates then can be treated as approximate posterior draws. Nonetheless, a central question remains unanswered: How to select the random weights under arbitrary sample sizes. We, therefore, introduce the Bayesian Optimized Bootstrap (BOB), a computational method to automatically select these random weights by minimizing, through Bayesian Optimization, a black-box and noisy version of the reverse Kullback-Leibler (KL) divergence between the Bayesian posterior and an approximate posterior obtained via random weighting. Our proposed method outperforms competing approaches in recovering the Bayesian posterior, it provides a better uncertainty quantification, and it retains key asymptotic properties from existing methods. BOB’s performance is demonstrated through extensive simulations, along with real-world data analyses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.03644&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Santiago Marin, Bronwyn Loong, Anton H. Westveld</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">A natural way to quantify uncertainties in Gaussian mixture models (GMMs) is through Bayesian methods. That said, sampling from the joint posterior distribution of GMMs via standard Markov chain Monte Carlo (MCMC) imposes several computational challenges, which have prevented a broader full Bayesian implementation of these models. A growing body of literature has introduced the Weighted Likelihood Bootstrap and the Weighted Bayesian Bootstrap as alternatives to MCMC sampling. The core idea of these methods is to repeatedly compute maximum a posteriori (MAP) estimates on many randomly weighted posterior densities. These MAP estimates then can be treated as approximate posterior draws. Nonetheless, a central question remains unanswered: How to select the random weights under arbitrary sample sizes. We, therefore, introduce the Bayesian Optimized Bootstrap (BOB), a computational method to automatically select these random weights by minimizing, through Bayesian Optimization, a black-box and noisy version of the reverse Kullback-Leibler (KL) divergence between the Bayesian posterior and an approximate posterior obtained via random weighting. Our proposed method outperforms competing approaches in recovering the Bayesian posterior, it provides a better uncertainty quantification, and it retains key asymptotic properties from existing methods. BOB’s performance is demonstrated through extensive simulations, along with real-world data analyses.</summary></entry><entry><title type="html">Causal Discovery in Multivariate Extremes with a Hydrological Analysis of Swiss River Discharges</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/CausalDiscoveryinMultivariateExtremeswithaHydrologicalAnalysisofSwissRiverDischarges.html" rel="alternate" type="text/html" title="Causal Discovery in Multivariate Extremes with a Hydrological Analysis of Swiss River Discharges" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/CausalDiscoveryinMultivariateExtremeswithaHydrologicalAnalysisofSwissRiverDischarges</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/CausalDiscoveryinMultivariateExtremeswithaHydrologicalAnalysisofSwissRiverDischarges.html">&lt;p&gt;Causal asymmetry is based on the principle that an event is a cause only if its absence would not have been a cause. From there, uncovering causal effects becomes a matter of comparing a well-defined score in both directions. Motivated by studying causal effects at extreme levels of a multivariate random vector, we propose to construct a model-agnostic causal score relying solely on the assumption of the existence of a max-domain of attraction. Based on a representation of a Generalized Pareto random vector, we construct the causal score as the Wasserstein distance between the margins and a well-specified random variable. The proposed methodology is illustrated on a hydrologically simulated dataset of different characteristics of catchments in Switzerland: discharge, precipitation, and snowmelt.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10371&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Linda Mhalla, Valérie Chavez-Demoulin, Philippe Naveau</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Causal asymmetry is based on the principle that an event is a cause only if its absence would not have been a cause. From there, uncovering causal effects becomes a matter of comparing a well-defined score in both directions. Motivated by studying causal effects at extreme levels of a multivariate random vector, we propose to construct a model-agnostic causal score relying solely on the assumption of the existence of a max-domain of attraction. Based on a representation of a Generalized Pareto random vector, we construct the causal score as the Wasserstein distance between the margins and a well-specified random variable. The proposed methodology is illustrated on a hydrologically simulated dataset of different characteristics of catchments in Switzerland: discharge, precipitation, and snowmelt.</summary></entry><entry><title type="html">Causal inference approach to appraise long-term effects of maintenance policy on functional performance of asphalt pavements</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Causalinferenceapproachtoappraiselongtermeffectsofmaintenancepolicyonfunctionalperformanceofasphaltpavements.html" rel="alternate" type="text/html" title="Causal inference approach to appraise long-term effects of maintenance policy on functional performance of asphalt pavements" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Causalinferenceapproachtoappraiselongtermeffectsofmaintenancepolicyonfunctionalperformanceofasphaltpavements</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Causalinferenceapproachtoappraiselongtermeffectsofmaintenancepolicyonfunctionalperformanceofasphaltpavements.html">&lt;p&gt;Asphalt pavements as the most prevalent transportation infrastructure, are prone to serious traffic safety problems due to functional or structural damage caused by stresses or strains imposed through repeated traffic loads and continuous climatic cycles. The good quality or high serviceability of infrastructure networks is vital to the urbanization and industrial development of nations. In order to maintain good functional pavement performance and extend the service life of asphalt pavements, the long-term performance of pavements under maintenance policies needs to be evaluated and favorable options selected based on the condition of the pavement. A major challenge in evaluating maintenance policies is to produce valid treatments for the outcome assessment under the control of uncertainty of vehicle loads and the disturbance of freeze-thaw cycles in the climatic environment. In this study, a novel causal inference approach combining a classical causal structural model and a potential outcome model framework is proposed to appraise the long-term effects of four preventive maintenance treatments for longitudinal cracking over a 5-year period of upkeep. Three fundamental issues were brought to our attention: 1) detection of causal relationships prior to variables under environmental loading (identification of causal structure); 2) obtaining direct causal effects of treatment on outcomes excluding covariates (identification of causal effects); and 3) sensitivity analysis of causal relationships. The results show that the method can accurately evaluate the effect of preventive maintenance treatments and assess the maintenance time to cater well for the functional performance of different preventive maintenance approaches. This framework could help policymakers to develop appropriate maintenance strategies for pavements.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10329&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lingyun You, Nanning Guo, Zhengwu Long, Fusong Wang, Chundi Si, Aboelkasim Diab</name></author><category term="stat.AP" /><summary type="html">Asphalt pavements as the most prevalent transportation infrastructure, are prone to serious traffic safety problems due to functional or structural damage caused by stresses or strains imposed through repeated traffic loads and continuous climatic cycles. The good quality or high serviceability of infrastructure networks is vital to the urbanization and industrial development of nations. In order to maintain good functional pavement performance and extend the service life of asphalt pavements, the long-term performance of pavements under maintenance policies needs to be evaluated and favorable options selected based on the condition of the pavement. A major challenge in evaluating maintenance policies is to produce valid treatments for the outcome assessment under the control of uncertainty of vehicle loads and the disturbance of freeze-thaw cycles in the climatic environment. In this study, a novel causal inference approach combining a classical causal structural model and a potential outcome model framework is proposed to appraise the long-term effects of four preventive maintenance treatments for longitudinal cracking over a 5-year period of upkeep. Three fundamental issues were brought to our attention: 1) detection of causal relationships prior to variables under environmental loading (identification of causal structure); 2) obtaining direct causal effects of treatment on outcomes excluding covariates (identification of causal effects); and 3) sensitivity analysis of causal relationships. The results show that the method can accurately evaluate the effect of preventive maintenance treatments and assess the maintenance time to cater well for the functional performance of different preventive maintenance approaches. This framework could help policymakers to develop appropriate maintenance strategies for pavements.</summary></entry><entry><title type="html">Comparative evaluation of earthquake forecasting models: An application to Italy</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/ComparativeevaluationofearthquakeforecastingmodelsAnapplicationtoItaly.html" rel="alternate" type="text/html" title="Comparative evaluation of earthquake forecasting models: An application to Italy" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/ComparativeevaluationofearthquakeforecastingmodelsAnapplicationtoItaly</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/ComparativeevaluationofearthquakeforecastingmodelsAnapplicationtoItaly.html">&lt;p&gt;Testing earthquake forecasts is essential to obtain scientific information on forecasting models and sufficient credibility for societal usage. We aim at enhancing the testing phase proposed by the Collaboratory for the Study of Earthquake Predictability (CSEP, Schorlemmer et al., 2018) with new statistical methods supported by mathematical theory. To demonstrate their applicability, we evaluate three short-term forecasting models that were submitted to the CSEP Italy experiment, and two ensemble models thereof. The models produce weekly overlapping forecasts for the expected number of M4+ earthquakes in a collection of grid cells. We compare the models’ forecasts using consistent scoring functions for means or expectations, which are widely used and theoretically principled tools for forecast evaluation. We further discuss and demonstrate their connection to CSEP-style earthquake likelihood model testing. Then, using tools from isotonic regression, we investigate forecast reliability and apply score decompositions in terms of calibration and discrimination. Our results show where and how models outperform their competitors and reveal a substantial lack of calibration for various models. The proposed methods also apply to full-distribution (e.g., catalog-based) forecasts, without requiring Poisson distributions or making any other type of parametric assumption.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10712&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jonas R. Brehmer, Kristof Kraus, Tilmann Gneiting, Marcus Herrmann, Warner Marzocchi</name></author><category term="stat.AP" /><summary type="html">Testing earthquake forecasts is essential to obtain scientific information on forecasting models and sufficient credibility for societal usage. We aim at enhancing the testing phase proposed by the Collaboratory for the Study of Earthquake Predictability (CSEP, Schorlemmer et al., 2018) with new statistical methods supported by mathematical theory. To demonstrate their applicability, we evaluate three short-term forecasting models that were submitted to the CSEP Italy experiment, and two ensemble models thereof. The models produce weekly overlapping forecasts for the expected number of M4+ earthquakes in a collection of grid cells. We compare the models’ forecasts using consistent scoring functions for means or expectations, which are widely used and theoretically principled tools for forecast evaluation. We further discuss and demonstrate their connection to CSEP-style earthquake likelihood model testing. Then, using tools from isotonic regression, we investigate forecast reliability and apply score decompositions in terms of calibration and discrimination. Our results show where and how models outperform their competitors and reveal a substantial lack of calibration for various models. The proposed methods also apply to full-distribution (e.g., catalog-based) forecasts, without requiring Poisson distributions or making any other type of parametric assumption.</summary></entry><entry><title type="html">Differences in academic preparedness do not fully explain Black-White enrollment disparities in advanced high school coursework</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/DifferencesinacademicpreparednessdonotfullyexplainBlackWhiteenrollmentdisparitiesinadvancedhighschoolcoursework.html" rel="alternate" type="text/html" title="Differences in academic preparedness do not fully explain Black-White enrollment disparities in advanced high school coursework" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/DifferencesinacademicpreparednessdonotfullyexplainBlackWhiteenrollmentdisparitiesinadvancedhighschoolcoursework</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/DifferencesinacademicpreparednessdonotfullyexplainBlackWhiteenrollmentdisparitiesinadvancedhighschoolcoursework.html">&lt;p&gt;Whether racial disparities in enrollment in advanced high school coursework can be attributed to differences in prior academic preparation is a central question in sociological research and education policy. However, previous investigations face methodological limitations, for they compare race-specific enrollment rates of students after adjusting for characteristics only partially related to their academic preparedness for advanced coursework. Informed by a recently-developed statistical technique, we propose and estimate a novel measure of students’ academic preparedness and use administrative data from the New York City Department of Education to measure differences in AP mathematics enrollment rates among similarly prepared students of different races. We find that preexisting differences in academic preparation do not fully explain the under-representation of Black students relative to White students in AP mathematics. Our results imply that achieving equal opportunities for AP enrollment not only requires equalizing earlier academic experiences, but also addressing inequities that emerge from coursework placement processes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.15075&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>João M. Souto-Maior, Ravi Shroff</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Whether racial disparities in enrollment in advanced high school coursework can be attributed to differences in prior academic preparation is a central question in sociological research and education policy. However, previous investigations face methodological limitations, for they compare race-specific enrollment rates of students after adjusting for characteristics only partially related to their academic preparedness for advanced coursework. Informed by a recently-developed statistical technique, we propose and estimate a novel measure of students’ academic preparedness and use administrative data from the New York City Department of Education to measure differences in AP mathematics enrollment rates among similarly prepared students of different races. We find that preexisting differences in academic preparation do not fully explain the under-representation of Black students relative to White students in AP mathematics. Our results imply that achieving equal opportunities for AP enrollment not only requires equalizing earlier academic experiences, but also addressing inequities that emerge from coursework placement processes.</summary></entry><entry><title type="html">Differentially private projection-depth-based medians</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Differentiallyprivateprojectiondepthbasedmedians.html" rel="alternate" type="text/html" title="Differentially private projection-depth-based medians" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Differentiallyprivateprojectiondepthbasedmedians</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Differentiallyprivateprojectiondepthbasedmedians.html">&lt;p&gt;We develop $(\epsilon,\delta)$-differentially private projection-depth-based medians using the propose-test-release (PTR) and exponential mechanisms. Under general conditions on the input parameters and the population measure, (e.g. we do not assume any moment bounds), we quantify the probability the test in PTR fails, as well as the cost of privacy via finite sample deviation bounds. We then present a new definition of the finite sample breakdown point which applies to a mechanism, and present a lower bound on the finite sample breakdown point of the projection-depth-based median. We demonstrate our main results on the canonical projection-depth-based median, as well as on projection-depth-based medians derived from trimmed estimators. In the Gaussian setting, we show that the resulting deviation bound matches the known lower bound for private Gaussian mean estimation. In the Cauchy setting, we show that the “outlier error amplification” effect resulting from the heavy tails outweighs the cost of privacy. This result is then verified via numerical simulations. Additionally, we present results on general PTR mechanisms and a uniform concentration result on the projected spacings of order statistics, which may be of general interest.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.07792&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kelly Ramsay, Dylan Spicker</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We develop $(\epsilon,\delta)$-differentially private projection-depth-based medians using the propose-test-release (PTR) and exponential mechanisms. Under general conditions on the input parameters and the population measure, (e.g. we do not assume any moment bounds), we quantify the probability the test in PTR fails, as well as the cost of privacy via finite sample deviation bounds. We then present a new definition of the finite sample breakdown point which applies to a mechanism, and present a lower bound on the finite sample breakdown point of the projection-depth-based median. We demonstrate our main results on the canonical projection-depth-based median, as well as on projection-depth-based medians derived from trimmed estimators. In the Gaussian setting, we show that the resulting deviation bound matches the known lower bound for private Gaussian mean estimation. In the Cauchy setting, we show that the “outlier error amplification” effect resulting from the heavy tails outweighs the cost of privacy. This result is then verified via numerical simulations. Additionally, we present results on general PTR mechanisms and a uniform concentration result on the projected spacings of order statistics, which may be of general interest.</summary></entry><entry><title type="html">Efficient Sampling in Disease Surveillance through Subpopulations: Sampling Canaries in the Coal Mine</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/EfficientSamplinginDiseaseSurveillancethroughSubpopulationsSamplingCanariesintheCoalMine.html" rel="alternate" type="text/html" title="Efficient Sampling in Disease Surveillance through Subpopulations: Sampling Canaries in the Coal Mine" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/EfficientSamplinginDiseaseSurveillancethroughSubpopulationsSamplingCanariesintheCoalMine</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/EfficientSamplinginDiseaseSurveillancethroughSubpopulationsSamplingCanariesintheCoalMine.html">&lt;p&gt;We consider disease outbreak detection settings where the population under study consists of various subpopulations available for stratified surveillance. These subpopulations can for example be based on age cohorts, but may also correspond to other subgroups of the population under study such as international travellers. Rather than sampling uniformly over the entire population, one may elevate the effectiveness of the detection methodology by optimally choosing a subpopulation for sampling. We show (under some assumptions) the relative sampling efficiency between two subpopulations is inversely proportional to the ratio of their respective baseline disease risks. This leads to a considerable potential increase in sampling efficiency when sampling from the subpopulation with higher baseline disease risk, if the two subpopulation baseline risks differ strongly. Our mathematical results require a careful treatment of the power curves of exact binomial tests as a function of their sample size, which are erratic and non-monotonic due to the discreteness of the underlying distribution. Subpopulations with comparatively high baseline disease risk are typically in greater contact with health professionals, and thus when sampled for surveillance purposes this is typically motivated merely through a convenience argument. With this study, we aim to elevate the status of such “convenience surveillance” to optimal subpopulation surveillance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10742&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ivo V. Stoepker</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">We consider disease outbreak detection settings where the population under study consists of various subpopulations available for stratified surveillance. These subpopulations can for example be based on age cohorts, but may also correspond to other subgroups of the population under study such as international travellers. Rather than sampling uniformly over the entire population, one may elevate the effectiveness of the detection methodology by optimally choosing a subpopulation for sampling. We show (under some assumptions) the relative sampling efficiency between two subpopulations is inversely proportional to the ratio of their respective baseline disease risks. This leads to a considerable potential increase in sampling efficiency when sampling from the subpopulation with higher baseline disease risk, if the two subpopulation baseline risks differ strongly. Our mathematical results require a careful treatment of the power curves of exact binomial tests as a function of their sample size, which are erratic and non-monotonic due to the discreteness of the underlying distribution. Subpopulations with comparatively high baseline disease risk are typically in greater contact with health professionals, and thus when sampled for surveillance purposes this is typically motivated merely through a convenience argument. With this study, we aim to elevate the status of such “convenience surveillance” to optimal subpopulation surveillance.</summary></entry><entry><title type="html">Efficient estimation of target population treatment effect from multiple source trials under effect-measure transportability</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Efficientestimationoftargetpopulationtreatmenteffectfrommultiplesourcetrialsundereffectmeasuretransportability.html" rel="alternate" type="text/html" title="Efficient estimation of target population treatment effect from multiple source trials under effect-measure transportability" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Efficientestimationoftargetpopulationtreatmenteffectfrommultiplesourcetrialsundereffectmeasuretransportability</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Efficientestimationoftargetpopulationtreatmenteffectfrommultiplesourcetrialsundereffectmeasuretransportability.html">&lt;p&gt;When the marginal causal effect comparing the same treatment pair is available from multiple trials, we wish to transport all results to make inference on the target population effect. To account for the differences between populations, statistical analysis is often performed controlling for relevant variables. However, when transportability assumptions are placed on conditional causal effects, rather than the distribution of potential outcomes, we need to carefully choose these effect measures. In particular, we present identifiability results in two cases: target population average treatment effect for a continuous outcome and causal mean ratio for a positive outcome. We characterize the semiparametric efficiency bounds of the causal effects under the respective transportability assumptions and propose estimators that are doubly robust against model misspecifications. We highlight an important discussion on the tension between the non-collapsibility of conditional effects and the variational independence induced by transportability in the case of multiple source trials.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10769&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zehao Su, Helene C. W. Rytgaard, Henrik Ravn, Frank Eriksson</name></author><category term="stat.ME" /><summary type="html">When the marginal causal effect comparing the same treatment pair is available from multiple trials, we wish to transport all results to make inference on the target population effect. To account for the differences between populations, statistical analysis is often performed controlling for relevant variables. However, when transportability assumptions are placed on conditional causal effects, rather than the distribution of potential outcomes, we need to carefully choose these effect measures. In particular, we present identifiability results in two cases: target population average treatment effect for a continuous outcome and causal mean ratio for a positive outcome. We characterize the semiparametric efficiency bounds of the causal effects under the respective transportability assumptions and propose estimators that are doubly robust against model misspecifications. We highlight an important discussion on the tension between the non-collapsibility of conditional effects and the variational independence induced by transportability in the case of multiple source trials.</summary></entry><entry><title type="html">Fast and Scalable Inference for Spatial Extreme Value Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/FastandScalableInferenceforSpatialExtremeValueModels.html" rel="alternate" type="text/html" title="Fast and Scalable Inference for Spatial Extreme Value Models" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/FastandScalableInferenceforSpatialExtremeValueModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/FastandScalableInferenceforSpatialExtremeValueModels.html">&lt;p&gt;The generalized extreme value (GEV) distribution is a popular model for analyzing and forecasting extreme weather data. To increase prediction accuracy, spatial information is often pooled via a latent Gaussian process (GP) on the GEV parameters. Inference for GEV-GP models is typically carried out using Markov chain Monte Carlo (MCMC) methods, or using approximate inference methods such as the integrated nested Laplace approximation (INLA). However, MCMC becomes prohibitively slow as the number of spatial locations increases, whereas INLA is only applicable in practice to a limited subset of GEV-GP models. In this paper, we revisit the original Laplace approximation for fitting spatial GEV models. In combination with a popular sparsity-inducing spatial covariance approximation technique, we show through simulations that our approach accurately estimates the Bayesian predictive distribution of extreme weather events, is scalable to several thousand spatial locations, and is several orders of magnitude faster than MCMC. A case study in forecasting extreme snowfall across Canada is presented.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.07051&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Meixi Chen, Reza Ramezan, Martin Lysy</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">The generalized extreme value (GEV) distribution is a popular model for analyzing and forecasting extreme weather data. To increase prediction accuracy, spatial information is often pooled via a latent Gaussian process (GP) on the GEV parameters. Inference for GEV-GP models is typically carried out using Markov chain Monte Carlo (MCMC) methods, or using approximate inference methods such as the integrated nested Laplace approximation (INLA). However, MCMC becomes prohibitively slow as the number of spatial locations increases, whereas INLA is only applicable in practice to a limited subset of GEV-GP models. In this paper, we revisit the original Laplace approximation for fitting spatial GEV models. In combination with a popular sparsity-inducing spatial covariance approximation technique, we show through simulations that our approach accurately estimates the Bayesian predictive distribution of extreme weather events, is scalable to several thousand spatial locations, and is several orders of magnitude faster than MCMC. A case study in forecasting extreme snowfall across Canada is presented.</summary></entry><entry><title type="html">Forecasting with Hyper-Trees</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/ForecastingwithHyperTrees.html" rel="alternate" type="text/html" title="Forecasting with Hyper-Trees" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/ForecastingwithHyperTrees</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/ForecastingwithHyperTrees.html">&lt;p&gt;This paper introduces the concept of Hyper-Trees and offers a new direction in applying tree-based models to time series data. Unlike conventional applications of decision trees that forecast time series directly, Hyper-Trees are designed to learn the parameters of a target time series model. Our framework leverages the gradient-based nature of boosted trees, which allows us to extend the concept of Hyper-Networks to Hyper-Trees and to induce a time-series inductive bias to tree models. By relating the parameters of a target time series model to features, Hyper-Trees address the issue of parameter non-stationarity and enable tree-based forecasts to extend beyond their training range. With our research, we aim to explore the effectiveness of Hyper-Trees across various forecasting scenarios and to extend the application of gradient boosted decision trees outside their conventional use in time series modeling.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07836&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexander März, Kashif Rasul</name></author><category term="stat.ME" /><summary type="html">This paper introduces the concept of Hyper-Trees and offers a new direction in applying tree-based models to time series data. Unlike conventional applications of decision trees that forecast time series directly, Hyper-Trees are designed to learn the parameters of a target time series model. Our framework leverages the gradient-based nature of boosted trees, which allows us to extend the concept of Hyper-Networks to Hyper-Trees and to induce a time-series inductive bias to tree models. By relating the parameters of a target time series model to features, Hyper-Trees address the issue of parameter non-stationarity and enable tree-based forecasts to extend beyond their training range. With our research, we aim to explore the effectiveness of Hyper-Trees across various forecasting scenarios and to extend the application of gradient boosted decision trees outside their conventional use in time series modeling.</summary></entry><entry><title type="html">Geometric-Based Pruning Rules For Change Point Detection in Multiple Independent Time Series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/GeometricBasedPruningRulesForChangePointDetectioninMultipleIndependentTimeSeries.html" rel="alternate" type="text/html" title="Geometric-Based Pruning Rules For Change Point Detection in Multiple Independent Time Series" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/GeometricBasedPruningRulesForChangePointDetectioninMultipleIndependentTimeSeries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/GeometricBasedPruningRulesForChangePointDetectioninMultipleIndependentTimeSeries.html">&lt;p&gt;We consider the problem of detecting multiple changes in multiple independent time series. The search for the best segmentation can be expressed as a minimization problem over a given cost function. We focus on dynamic programming algorithms that solve this problem exactly. When the number of changes is proportional to data length, an inequality-based pruning rule encoded in the PELT algorithm leads to a linear time complexity. Another type of pruning, called functional pruning, gives a close-to-linear time complexity whatever the number of changes, but only for the analysis of univariate time series.
  We propose a few extensions of functional pruning for multiple independent time series based on the use of simple geometric shapes (balls and hyperrectangles). We focus on the Gaussian case, but some of our rules can be easily extended to the exponential family. In a simulation study we compare the computational efficiency of different geometric-based pruning rules. We show that for small dimensions (2, 3, 4) some of them ran significantly faster than inequality-based approaches in particular when the underlying number of changes is small compared to the data length.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.09555&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Liudmila Pishchagina, Guillem Rigaill, Vincent Runge</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">We consider the problem of detecting multiple changes in multiple independent time series. The search for the best segmentation can be expressed as a minimization problem over a given cost function. We focus on dynamic programming algorithms that solve this problem exactly. When the number of changes is proportional to data length, an inequality-based pruning rule encoded in the PELT algorithm leads to a linear time complexity. Another type of pruning, called functional pruning, gives a close-to-linear time complexity whatever the number of changes, but only for the analysis of univariate time series. We propose a few extensions of functional pruning for multiple independent time series based on the use of simple geometric shapes (balls and hyperrectangles). We focus on the Gaussian case, but some of our rules can be easily extended to the exponential family. In a simulation study we compare the computational efficiency of different geometric-based pruning rules. We show that for small dimensions (2, 3, 4) some of them ran significantly faster than inequality-based approaches in particular when the underlying number of changes is small compared to the data length.</summary></entry><entry><title type="html">Graph-Aligned Random Partition Model (GARP)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/GraphAlignedRandomPartitionModelGARP.html" rel="alternate" type="text/html" title="Graph-Aligned Random Partition Model (GARP)" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/GraphAlignedRandomPartitionModelGARP</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/GraphAlignedRandomPartitionModelGARP.html">&lt;p&gt;Bayesian nonparametric mixtures and random partition models are powerful tools for probabilistic clustering. However, standard independent mixture models can be restrictive in some applications such as inference on cell lineage due to the biological relations of the clusters. The increasing availability of large genomic data requires new statistical tools to perform model-based clustering and infer the relationship between homogeneous subgroups of units. Motivated by single-cell RNA applications we develop a novel dependent mixture model to jointly perform cluster analysis and align the clusters on a graph. Our flexible graph-aligned random partition model (GARP) exploits Gibbs-type priors as building blocks, allowing us to derive analytical results on the graph-aligned random partition’s probability mass function (pmf). We derive a generalization of the Chinese restaurant process from the pmf and a related efficient and neat MCMC algorithm to perform Bayesian inference. We perform posterior inference on real single-cell RNA data from mice stem cells. We further investigate the performance of our model in capturing the underlying clustering structure as well as the underlying graph by means of simulation studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.08485&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Giovanni Rebaudo, Peter Mueller</name></author><category term="stat.ME" /><summary type="html">Bayesian nonparametric mixtures and random partition models are powerful tools for probabilistic clustering. However, standard independent mixture models can be restrictive in some applications such as inference on cell lineage due to the biological relations of the clusters. The increasing availability of large genomic data requires new statistical tools to perform model-based clustering and infer the relationship between homogeneous subgroups of units. Motivated by single-cell RNA applications we develop a novel dependent mixture model to jointly perform cluster analysis and align the clusters on a graph. Our flexible graph-aligned random partition model (GARP) exploits Gibbs-type priors as building blocks, allowing us to derive analytical results on the graph-aligned random partition’s probability mass function (pmf). We derive a generalization of the Chinese restaurant process from the pmf and a related efficient and neat MCMC algorithm to perform Bayesian inference. We perform posterior inference on real single-cell RNA data from mice stem cells. We further investigate the performance of our model in capturing the underlying clustering structure as well as the underlying graph by means of simulation studies.</summary></entry><entry><title type="html">Hawkes Models And Their Applications</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/HawkesModelsAndTheirApplications.html" rel="alternate" type="text/html" title="Hawkes Models And Their Applications" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/HawkesModelsAndTheirApplications</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/HawkesModelsAndTheirApplications.html">&lt;p&gt;The Hawkes process is a model for counting the number of arrivals to a system which exhibits the self-exciting property - that one arrival creates a heightened chance of further arrivals in the near future. The model, and its generalizations, have been applied in a plethora of disparate domains, though two particularly developed applications are in seismology and in finance. As the original model is elegantly simple, generalizations have been proposed which: track marks for each arrival, are multivariate, have a spatial component, are driven by renewal processes, treat time as discrete, and so on. This paper creates a cohesive review of the traditional Hawkes model and the modern generalizations, providing details on their construction, simulation algorithms, and giving key references to the appropriate literature for a detailed treatment.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10527&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Patrick J. Laub, Young Lee, Philip K. Pollett, Thomas Taimre</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The Hawkes process is a model for counting the number of arrivals to a system which exhibits the self-exciting property - that one arrival creates a heightened chance of further arrivals in the near future. The model, and its generalizations, have been applied in a plethora of disparate domains, though two particularly developed applications are in seismology and in finance. As the original model is elegantly simple, generalizations have been proposed which: track marks for each arrival, are multivariate, have a spatial component, are driven by renewal processes, treat time as discrete, and so on. This paper creates a cohesive review of the traditional Hawkes model and the modern generalizations, providing details on their construction, simulation algorithms, and giving key references to the appropriate literature for a detailed treatment.</summary></entry><entry><title type="html">High-dimensional multiple imputation (HDMI) for partially observed confounders including natural language processing-derived auxiliary covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/HighdimensionalmultipleimputationHDMIforpartiallyobservedconfoundersincludingnaturallanguageprocessingderivedauxiliarycovariates.html" rel="alternate" type="text/html" title="High-dimensional multiple imputation (HDMI) for partially observed confounders including natural language processing-derived auxiliary covariates" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/HighdimensionalmultipleimputationHDMIforpartiallyobservedconfoundersincludingnaturallanguageprocessingderivedauxiliarycovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/HighdimensionalmultipleimputationHDMIforpartiallyobservedconfoundersincludingnaturallanguageprocessingderivedauxiliarycovariates.html">&lt;p&gt;Multiple imputation (MI) models can be improved by including auxiliary covariates (AC), but their performance in high-dimensional data is not well understood. We aimed to develop and compare high-dimensional MI (HDMI) approaches using structured and natural language processing (NLP)-derived AC in studies with partially observed confounders. We conducted a plasmode simulation study using data from opioid vs. non-steroidal anti-inflammatory drug (NSAID) initiators (X) with observed serum creatinine labs (Z2) and time-to-acute kidney injury as outcome. We simulated 100 cohorts with a null treatment effect, including X, Z2, atrial fibrillation (U), and 13 other investigator-derived confounders (Z1) in the outcome generation. We then imposed missingness (MZ2) on 50% of Z2 measurements as a function of Z2 and U and created different HDMI candidate AC using structured and NLP-derived features. We mimicked scenarios where U was unobserved by omitting it from all AC candidate sets. Using LASSO, we data-adaptively selected HDMI covariates associated with Z2 and MZ2 for MI, and with U to include in propensity score models. The treatment effect was estimated following propensity score matching in MI datasets and we benchmarked HDMI approaches against a baseline imputation and complete case analysis with Z1 only. HDMI using claims data showed the lowest bias (0.072). Combining claims and sentence embeddings led to an improvement in the efficiency displaying the lowest root-mean-squared-error (0.173) and coverage (94%). NLP-derived AC alone did not perform better than baseline MI. HDMI approaches may decrease bias in studies with partially observed confounders where missingness depends on unobserved factors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10925&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Janick Weberpals, Pamela A. Shaw, Kueiyu Joshua Lin, Richard Wyss, Joseph M Plasek, Li Zhou, Kerry Ngan, Thomas DeRamus, Sudha R. Raman, Bradley G. Hammill, Hana Lee, Sengwee Toh, John G. Connolly, Kimberly J. Dandreo, Fang Tian, Wei Liu, Jie Li, José J. Hernández-Muñoz, Sebastian Schneeweiss, Rishi J. Desai</name></author><category term="stat.ME" /><summary type="html">Multiple imputation (MI) models can be improved by including auxiliary covariates (AC), but their performance in high-dimensional data is not well understood. We aimed to develop and compare high-dimensional MI (HDMI) approaches using structured and natural language processing (NLP)-derived AC in studies with partially observed confounders. We conducted a plasmode simulation study using data from opioid vs. non-steroidal anti-inflammatory drug (NSAID) initiators (X) with observed serum creatinine labs (Z2) and time-to-acute kidney injury as outcome. We simulated 100 cohorts with a null treatment effect, including X, Z2, atrial fibrillation (U), and 13 other investigator-derived confounders (Z1) in the outcome generation. We then imposed missingness (MZ2) on 50% of Z2 measurements as a function of Z2 and U and created different HDMI candidate AC using structured and NLP-derived features. We mimicked scenarios where U was unobserved by omitting it from all AC candidate sets. Using LASSO, we data-adaptively selected HDMI covariates associated with Z2 and MZ2 for MI, and with U to include in propensity score models. The treatment effect was estimated following propensity score matching in MI datasets and we benchmarked HDMI approaches against a baseline imputation and complete case analysis with Z1 only. HDMI using claims data showed the lowest bias (0.072). Combining claims and sentence embeddings led to an improvement in the efficiency displaying the lowest root-mean-squared-error (0.173) and coverage (94%). NLP-derived AC alone did not perform better than baseline MI. HDMI approaches may decrease bias in studies with partially observed confounders where missingness depends on unobserved factors.</summary></entry><entry><title type="html">Integer Traffic Assignment Problem: Algorithms and Insights on Random Graphs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/IntegerTrafficAssignmentProblemAlgorithmsandInsightsonRandomGraphs.html" rel="alternate" type="text/html" title="Integer Traffic Assignment Problem: Algorithms and Insights on Random Graphs" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/IntegerTrafficAssignmentProblemAlgorithmsandInsightsonRandomGraphs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/IntegerTrafficAssignmentProblemAlgorithmsandInsightsonRandomGraphs.html">&lt;p&gt;Path optimization is a fundamental concern across various real-world scenarios, ranging from traffic congestion issues to efficient data routing over the internet. The Traffic Assignment Problem (TAP) is a classic continuous optimization problem in this field. This study considers the Integer Traffic Assignment Problem (ITAP), a discrete variant of TAP. ITAP involves determining optimal routes for commuters in a city represented by a graph, aiming to minimize congestion while adhering to integer flow constraints on paths. This restriction makes ITAP an NP-hard problem. While conventional TAP prioritizes repulsive interactions to minimize congestion, this work also explores the case of attractive interactions, related to minimizing the number of occupied edges. We present and evaluate multiple algorithms to address ITAP, including a message passing algorithm, a greedy approach, simulated annealing, and relaxation of ITAP to TAP. Inspired by studies of random ensembles in the large-size limit in statistical physics, comparisons between these algorithms are conducted on large sparse random regular graphs with a random set of origin-destination pairs. Our results indicate that while the simplest greedy algorithm performs competitively in the repulsive scenario, in the attractive case the message-passing-based algorithm and simulated annealing demonstrate superiority. We then investigate the relationship between TAP and ITAP in the repulsive case. We find that, as the number of paths increases, the solution of TAP converges toward that of ITAP, and we investigate the speed of this convergence. Depending on the number of paths, our analysis leads us to identify two scaling regimes: in one the average flow per edge is of order one, and in another the number of paths scales quadratically with the size of the graph, in which case the continuous relaxation solves the integer problem closely.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10763&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rayan Harfouche, Giovanni Piccioli, Lenka Zdeborová</name></author><category term="stat.CO" /><summary type="html">Path optimization is a fundamental concern across various real-world scenarios, ranging from traffic congestion issues to efficient data routing over the internet. The Traffic Assignment Problem (TAP) is a classic continuous optimization problem in this field. This study considers the Integer Traffic Assignment Problem (ITAP), a discrete variant of TAP. ITAP involves determining optimal routes for commuters in a city represented by a graph, aiming to minimize congestion while adhering to integer flow constraints on paths. This restriction makes ITAP an NP-hard problem. While conventional TAP prioritizes repulsive interactions to minimize congestion, this work also explores the case of attractive interactions, related to minimizing the number of occupied edges. We present and evaluate multiple algorithms to address ITAP, including a message passing algorithm, a greedy approach, simulated annealing, and relaxation of ITAP to TAP. Inspired by studies of random ensembles in the large-size limit in statistical physics, comparisons between these algorithms are conducted on large sparse random regular graphs with a random set of origin-destination pairs. Our results indicate that while the simplest greedy algorithm performs competitively in the repulsive scenario, in the attractive case the message-passing-based algorithm and simulated annealing demonstrate superiority. We then investigate the relationship between TAP and ITAP in the repulsive case. We find that, as the number of paths increases, the solution of TAP converges toward that of ITAP, and we investigate the speed of this convergence. Depending on the number of paths, our analysis leads us to identify two scaling regimes: in one the average flow per edge is of order one, and in another the number of paths scales quadratically with the size of the graph, in which case the continuous relaxation solves the integer problem closely.</summary></entry><entry><title type="html">Localised Natural Causal Learning Algorithms for Weak Consistency Conditions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/LocalisedNaturalCausalLearningAlgorithmsforWeakConsistencyConditions.html" rel="alternate" type="text/html" title="Localised Natural Causal Learning Algorithms for Weak Consistency Conditions" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/LocalisedNaturalCausalLearningAlgorithmsforWeakConsistencyConditions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/LocalisedNaturalCausalLearningAlgorithmsforWeakConsistencyConditions.html">&lt;p&gt;By relaxing conditions for natural structure learning algorithms, a family of constraint-based algorithms containing all exact structure learning algorithms under the faithfulness assumption, we define localised natural structure learning algorithms (LoNS). We also provide a set of necessary and sufficient assumptions for consistency of LoNS, which can be thought of as a strict relaxation of the restricted faithfulness assumption. We provide a practical LoNS algorithm that runs in exponential time, which is then compared with related existing structure learning algorithms, namely PC/SGS and the relatively recent Sparsest Permutation algorithm. Simulation studies are also provided.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.14775&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kai Z Teh, Kayvan Sadeghi, Terry Soo</name></author><category term="stat.ME" /><summary type="html">By relaxing conditions for natural structure learning algorithms, a family of constraint-based algorithms containing all exact structure learning algorithms under the faithfulness assumption, we define localised natural structure learning algorithms (LoNS). We also provide a set of necessary and sufficient assumptions for consistency of LoNS, which can be thought of as a strict relaxation of the restricted faithfulness assumption. We provide a practical LoNS algorithm that runs in exponential time, which is then compared with related existing structure learning algorithms, namely PC/SGS and the relatively recent Sparsest Permutation algorithm. Simulation studies are also provided.</summary></entry><entry><title type="html">Matrix Autoregressive Model with Vector Time Series Covariates for Spatio-Temporal Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/MatrixAutoregressiveModelwithVectorTimeSeriesCovariatesforSpatioTemporalData.html" rel="alternate" type="text/html" title="Matrix Autoregressive Model with Vector Time Series Covariates for Spatio-Temporal Data" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/MatrixAutoregressiveModelwithVectorTimeSeriesCovariatesforSpatioTemporalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/MatrixAutoregressiveModelwithVectorTimeSeriesCovariatesforSpatioTemporalData.html">&lt;p&gt;We develop a new methodology for forecasting matrix-valued time series with historical matrix data and auxiliary vector time series data. We focus on a time series of matrices defined on a static 2-D spatial grid and an auxiliary time series of non-spatial vectors. The proposed model, Matrix AutoRegression with Auxiliary Covariates (MARAC), contains an autoregressive component for the historical matrix predictors and an additive component that maps the auxiliary vector predictors to a matrix response via tensor-vector product. The autoregressive component adopts a bi-linear transformation framework following Chen et al. (2021), significantly reducing the number of parameters. The auxiliary component posits that the tensor coefficient, which maps non-spatial predictors to a spatial response, contains slices of spatially smooth matrix coefficients that are discrete evaluations of smooth functions from a Reproducible Kernel Hilbert Space (RKHS). We propose to estimate the model parameters under a penalized maximum likelihood estimation framework coupled with an alternating minimization algorithm. We establish the joint asymptotics of the autoregressive and tensor parameters under fixed and high-dimensional regimes. Extensive simulations and a geophysical application for forecasting the global Total Electron Content (TEC) are conducted to validate the performance of MARAC.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.15671&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hu Sun, Zuofeng Shang, Yang Chen</name></author><category term="stat.ME" /><summary type="html">We develop a new methodology for forecasting matrix-valued time series with historical matrix data and auxiliary vector time series data. We focus on a time series of matrices defined on a static 2-D spatial grid and an auxiliary time series of non-spatial vectors. The proposed model, Matrix AutoRegression with Auxiliary Covariates (MARAC), contains an autoregressive component for the historical matrix predictors and an additive component that maps the auxiliary vector predictors to a matrix response via tensor-vector product. The autoregressive component adopts a bi-linear transformation framework following Chen et al. (2021), significantly reducing the number of parameters. The auxiliary component posits that the tensor coefficient, which maps non-spatial predictors to a spatial response, contains slices of spatially smooth matrix coefficients that are discrete evaluations of smooth functions from a Reproducible Kernel Hilbert Space (RKHS). We propose to estimate the model parameters under a penalized maximum likelihood estimation framework coupled with an alternating minimization algorithm. We establish the joint asymptotics of the autoregressive and tensor parameters under fixed and high-dimensional regimes. Extensive simulations and a geophysical application for forecasting the global Total Electron Content (TEC) are conducted to validate the performance of MARAC.</summary></entry><entry><title type="html">Mediation Analysis with Mendelian Randomization and Efficient Multiple GWAS Integration</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/MediationAnalysiswithMendelianRandomizationandEfficientMultipleGWASIntegration.html" rel="alternate" type="text/html" title="Mediation Analysis with Mendelian Randomization and Efficient Multiple GWAS Integration" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/MediationAnalysiswithMendelianRandomizationandEfficientMultipleGWASIntegration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/MediationAnalysiswithMendelianRandomizationandEfficientMultipleGWASIntegration.html">&lt;p&gt;Mediation analysis is a powerful tool for studying causal pathways between exposure, mediator, and outcome variables of interest. While classical mediation analysis using observational data often requires strong and sometimes unrealistic assumptions, such as unconfoundedness, Mendelian Randomization (MR) avoids unmeasured confounding bias by employing genetic variations as instrumental variables. We develop a novel MR framework for mediation analysis with genome-wide associate study (GWAS) summary data, and provide solid statistical guarantees. Our framework employs carefully crafted estimating equations, allowing for different sets of genetic variations to instrument the exposure and the mediator, to efficiently integrate information stored in three independent GWAS. As part of this endeavor, we demonstrate that in mediation analysis, the challenge raised by instrument selection goes beyond the well-known winner’s curse issue, and therefore, addressing it requires special treatment. We then develop bias correction techniques to address the instrument selection issue and commonly encountered measurement error bias issue. Collectively, through our theoretical investigations, we show that our framework provides valid statistical inference for both direct and mediation effects with enhanced statistical efficiency compared to existing methods. We further illustrate the finite-sample performance of our approach through simulation experiments and a case study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.10563&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rita Qiuran Lyu, Chong Wu, Xinwei Ma, Jingshen Wang</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Mediation analysis is a powerful tool for studying causal pathways between exposure, mediator, and outcome variables of interest. While classical mediation analysis using observational data often requires strong and sometimes unrealistic assumptions, such as unconfoundedness, Mendelian Randomization (MR) avoids unmeasured confounding bias by employing genetic variations as instrumental variables. We develop a novel MR framework for mediation analysis with genome-wide associate study (GWAS) summary data, and provide solid statistical guarantees. Our framework employs carefully crafted estimating equations, allowing for different sets of genetic variations to instrument the exposure and the mediator, to efficiently integrate information stored in three independent GWAS. As part of this endeavor, we demonstrate that in mediation analysis, the challenge raised by instrument selection goes beyond the well-known winner’s curse issue, and therefore, addressing it requires special treatment. We then develop bias correction techniques to address the instrument selection issue and commonly encountered measurement error bias issue. Collectively, through our theoretical investigations, we show that our framework provides valid statistical inference for both direct and mediation effects with enhanced statistical efficiency compared to existing methods. We further illustrate the finite-sample performance of our approach through simulation experiments and a case study.</summary></entry><entry><title type="html">Model-based clustering in simple hypergraphs through a stochastic blockmodel</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Modelbasedclusteringinsimplehypergraphsthroughastochasticblockmodel.html" rel="alternate" type="text/html" title="Model-based clustering in simple hypergraphs through a stochastic blockmodel" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Modelbasedclusteringinsimplehypergraphsthroughastochasticblockmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Modelbasedclusteringinsimplehypergraphsthroughastochasticblockmodel.html">&lt;p&gt;We propose a model to address the overlooked problem of node clustering in simple hypergraphs. Simple hypergraphs are suitable when a node may not appear multiple times in the same hyperedge, such as in co-authorship datasets. Our model generalizes the stochastic blockmodel for graphs and assumes the existence of latent node groups and hyperedges are conditionally independent given these groups. We first establish the generic identifiability of the model parameters. We then develop a variational approximation Expectation-Maximization algorithm for parameter inference and node clustering, and derive a statistical criterion for model selection.
  To illustrate the performance of our R package HyperSBM, we compare it with other node clustering methods using synthetic data generated from the model, as well as from a line clustering experiment and a co-authorship dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.05983&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luca Brusa , Catherine Matias</name></author><category term="stat.ME" /><summary type="html">We propose a model to address the overlooked problem of node clustering in simple hypergraphs. Simple hypergraphs are suitable when a node may not appear multiple times in the same hyperedge, such as in co-authorship datasets. Our model generalizes the stochastic blockmodel for graphs and assumes the existence of latent node groups and hyperedges are conditionally independent given these groups. We first establish the generic identifiability of the model parameters. We then develop a variational approximation Expectation-Maximization algorithm for parameter inference and node clustering, and derive a statistical criterion for model selection. To illustrate the performance of our R package HyperSBM, we compare it with other node clustering methods using synthetic data generated from the model, as well as from a line clustering experiment and a co-authorship dataset.</summary></entry><entry><title type="html">Neural Optimization with Adaptive Heuristics for Intelligent Marketing System</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/NeuralOptimizationwithAdaptiveHeuristicsforIntelligentMarketingSystem.html" rel="alternate" type="text/html" title="Neural Optimization with Adaptive Heuristics for Intelligent Marketing System" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/NeuralOptimizationwithAdaptiveHeuristicsforIntelligentMarketingSystem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/NeuralOptimizationwithAdaptiveHeuristicsforIntelligentMarketingSystem.html">&lt;p&gt;Computational marketing has become increasingly important in today’s digital world, facing challenges such as massive heterogeneous data, multi-channel customer journeys, and limited marketing budgets. In this paper, we propose a general framework for marketing AI systems, the Neural Optimization with Adaptive Heuristics (NOAH) framework. NOAH is the first general framework for marketing optimization that considers both to-business (2B) and to-consumer (2C) products, as well as both owned and paid channels. We describe key modules of the NOAH framework, including prediction, optimization, and adaptive heuristics, providing examples for bidding and content optimization. We then detail the successful application of NOAH to LinkedIn’s email marketing system, showcasing significant wins over the legacy ranking system. Additionally, we share details and insights that are broadly useful, particularly on: (i) addressing delayed feedback with lifetime value, (ii) performing large-scale linear programming with randomization, (iii) improving retrieval with audience expansion, (iv) reducing signal dilution in targeting tests, and (v) handling zero-inflated heavy-tail metrics in statistical testing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10490&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Changshuai Wei, Benjamin Zelditch, Joyce Chen, Andre Assuncao Silva T Ribeiro, Jingyi Kenneth Tay, Borja Ocejo Elizondo, Keerthi Selvaraj, Aman Gupta, Licurgo Benemann De Almeida</name></author><category term="stat.ME" /><summary type="html">Computational marketing has become increasingly important in today’s digital world, facing challenges such as massive heterogeneous data, multi-channel customer journeys, and limited marketing budgets. In this paper, we propose a general framework for marketing AI systems, the Neural Optimization with Adaptive Heuristics (NOAH) framework. NOAH is the first general framework for marketing optimization that considers both to-business (2B) and to-consumer (2C) products, as well as both owned and paid channels. We describe key modules of the NOAH framework, including prediction, optimization, and adaptive heuristics, providing examples for bidding and content optimization. We then detail the successful application of NOAH to LinkedIn’s email marketing system, showcasing significant wins over the legacy ranking system. Additionally, we share details and insights that are broadly useful, particularly on: (i) addressing delayed feedback with lifetime value, (ii) performing large-scale linear programming with randomization, (iii) improving retrieval with audience expansion, (iv) reducing signal dilution in targeting tests, and (v) handling zero-inflated heavy-tail metrics in statistical testing.</summary></entry><entry><title type="html">Non trivial optimal sampling rate for estimating a Lipschitz-continuous function in presence of mean-reverting Ornstein-Uhlenbeck noise</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/NontrivialoptimalsamplingrateforestimatingaLipschitzcontinuousfunctioninpresenceofmeanrevertingOrnsteinUhlenbecknoise.html" rel="alternate" type="text/html" title="Non trivial optimal sampling rate for estimating a Lipschitz-continuous function in presence of mean-reverting Ornstein-Uhlenbeck noise" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/NontrivialoptimalsamplingrateforestimatingaLipschitzcontinuousfunctioninpresenceofmeanrevertingOrnsteinUhlenbecknoise</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/NontrivialoptimalsamplingrateforestimatingaLipschitzcontinuousfunctioninpresenceofmeanrevertingOrnsteinUhlenbecknoise.html">&lt;p&gt;We examine a mean-reverting Ornstein-Uhlenbeck process that perturbs an unknown Lipschitz-continuous drift and aim to estimate the drift’s value at a predetermined time horizon by sampling the path of the process. Due to the time varying nature of the drift we propose an estimation procedure that involves an online, time-varying optimization scheme implemented using a stochastic gradient ascent algorithm to maximize the log-likelihood of our observations. The objective of the paper is to investigate the optimal sample size/rate for achieving the minimum mean square distance between our estimator and the true value of the drift. In this setting we uncover a trade-off between the correlation of the observations, which increases with the sample size, and the dynamic nature of the unknown drift, which is weakened by increasing the frequency of observation. The mean square error is shown to be non monotonic in the sample size, attaining a global minimum whose precise description depends on the parameters that govern the model. In the static case, i.e. when the unknown drift is constant, our method outperforms the arithmetic mean of the observations in highly correlated regimes, despite the latter being a natural candidate estimator. We then compare our online estimator with the global maximum likelihood estimator.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10795&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Enrico Bernardi, Alberto Lanconelli, Christopher S. A. Lauria, Berk Tan Perçin</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We examine a mean-reverting Ornstein-Uhlenbeck process that perturbs an unknown Lipschitz-continuous drift and aim to estimate the drift’s value at a predetermined time horizon by sampling the path of the process. Due to the time varying nature of the drift we propose an estimation procedure that involves an online, time-varying optimization scheme implemented using a stochastic gradient ascent algorithm to maximize the log-likelihood of our observations. The objective of the paper is to investigate the optimal sample size/rate for achieving the minimum mean square distance between our estimator and the true value of the drift. In this setting we uncover a trade-off between the correlation of the observations, which increases with the sample size, and the dynamic nature of the unknown drift, which is weakened by increasing the frequency of observation. The mean square error is shown to be non monotonic in the sample size, attaining a global minimum whose precise description depends on the parameters that govern the model. In the static case, i.e. when the unknown drift is constant, our method outperforms the arithmetic mean of the observations in highly correlated regimes, despite the latter being a natural candidate estimator. We then compare our online estimator with the global maximum likelihood estimator.</summary></entry><entry><title type="html">Organizational Selection of Innovation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/OrganizationalSelectionofInnovation.html" rel="alternate" type="text/html" title="Organizational Selection of Innovation" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/OrganizationalSelectionofInnovation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/OrganizationalSelectionofInnovation.html">&lt;p&gt;Budgetary constraints force organizations to pursue only a subset of possible innovation projects. Identifying which subset is most promising is an error-prone exercise, and involving multiple decision makers may be prudent. This raises the question of how to most effectively aggregate their collective nous. Our model of organizational portfolio selection provides some first answers. We show that portfolio performance can vary widely. Delegating evaluation makes sense when organizations employ the relevant experts and can assign projects to them. In most other settings, aggregating the impressions of multiple agents leads to better performance than delegation. In particular, letting agents rank projects often outperforms alternative aggregation rules – including averaging agents’ project scores as well as counting their approval votes – especially when organizations have tight budgets and can select only a few project alternatives out of many.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09843&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lucas Böttcher, Ronald Klingebiel</name></author><category term="stat.AP" /><summary type="html">Budgetary constraints force organizations to pursue only a subset of possible innovation projects. Identifying which subset is most promising is an error-prone exercise, and involving multiple decision makers may be prudent. This raises the question of how to most effectively aggregate their collective nous. Our model of organizational portfolio selection provides some first answers. We show that portfolio performance can vary widely. Delegating evaluation makes sense when organizations employ the relevant experts and can assign projects to them. In most other settings, aggregating the impressions of multiple agents leads to better performance than delegation. In particular, letting agents rank projects often outperforms alternative aggregation rules – including averaging agents’ project scores as well as counting their approval votes – especially when organizations have tight budgets and can select only a few project alternatives out of many.</summary></entry><entry><title type="html">Prediction in Measurement Error Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/PredictioninMeasurementErrorModels.html" rel="alternate" type="text/html" title="Prediction in Measurement Error Models" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/PredictioninMeasurementErrorModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/PredictioninMeasurementErrorModels.html">&lt;p&gt;We study the well known difficult problem of prediction in measurement error models. By targeting directly at the prediction interval instead of the point prediction, we construct a prediction interval by providing estimators of both the center and the length of the interval which achieves a pre-determined prediction level. The constructing procedure requires a working model for the distribution of the variable prone to error. If the working model is correct, the prediction interval estimator obtains the smallest variability in terms of assessing the true center and length. If the working model is incorrect, the prediction interval estimation is still consistent. We further study how the length of the prediction interval depends on the choice of the true prediction interval center and provide guidance on obtaining minimal prediction interval length. Numerical experiments are conducted to illustrate the performance and we apply our method to predict concentration of Abeta1-12 in cerebrospinal fluid in an Alzheimer’s disease data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10461&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fei Jiang, Yanyuan Ma</name></author><category term="stat.ME" /><summary type="html">We study the well known difficult problem of prediction in measurement error models. By targeting directly at the prediction interval instead of the point prediction, we construct a prediction interval by providing estimators of both the center and the length of the interval which achieves a pre-determined prediction level. The constructing procedure requires a working model for the distribution of the variable prone to error. If the working model is correct, the prediction interval estimator obtains the smallest variability in terms of assessing the true center and length. If the working model is incorrect, the prediction interval estimation is still consistent. We further study how the length of the prediction interval depends on the choice of the true prediction interval center and provide guidance on obtaining minimal prediction interval length. Numerical experiments are conducted to illustrate the performance and we apply our method to predict concentration of Abeta1-12 in cerebrospinal fluid in an Alzheimer’s disease data.</summary></entry><entry><title type="html">Proximal indirect comparison</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Proximalindirectcomparison.html" rel="alternate" type="text/html" title="Proximal indirect comparison" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Proximalindirectcomparison</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/Proximalindirectcomparison.html">&lt;p&gt;We consider the problem of indirect comparison, where a treatment arm of interest is absent by design in the target randomized control trial (RCT) but available in a source RCT. The identifiability of the target population average treatment effect often relies on conditional transportability assumptions. However, it is a common concern whether all relevant effect modifiers are measured and controlled for. We highlight a new proximal identification result in the presence of shifted, unobserved effect modifiers based on proxies: an adjustment proxy in both RCTs and an additional reweighting proxy in the source RCT. We propose an estimator which is doubly-robust against misspecifications of the so-called bridge functions and asymptotically normal under mild consistency of the nuisance models. An alternative estimator is presented to accommodate missing outcomes in the source RCT, which we then apply to conduct a proximal indirect comparison analysis using two weight management trials.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10773&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zehao Su, Helene C. W. Rytgaard, Henrik Ravn, Frank Eriksson</name></author><category term="stat.ME" /><summary type="html">We consider the problem of indirect comparison, where a treatment arm of interest is absent by design in the target randomized control trial (RCT) but available in a source RCT. The identifiability of the target population average treatment effect often relies on conditional transportability assumptions. However, it is a common concern whether all relevant effect modifiers are measured and controlled for. We highlight a new proximal identification result in the presence of shifted, unobserved effect modifiers based on proxies: an adjustment proxy in both RCTs and an additional reweighting proxy in the source RCT. We propose an estimator which is doubly-robust against misspecifications of the so-called bridge functions and asymptotically normal under mild consistency of the nuisance models. An alternative estimator is presented to accommodate missing outcomes in the source RCT, which we then apply to conduct a proximal indirect comparison analysis using two weight management trials.</summary></entry><entry><title type="html">Rotation of the Globular Cluster Population of the Dark Matter Deficient Galaxy NGC 1052-DF4: Implication for the Total Mass</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/RotationoftheGlobularClusterPopulationoftheDarkMatterDeficientGalaxyNGC1052DF4ImplicationfortheTotalMass.html" rel="alternate" type="text/html" title="Rotation of the Globular Cluster Population of the Dark Matter Deficient Galaxy NGC 1052-DF4: Implication for the Total Mass" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/RotationoftheGlobularClusterPopulationoftheDarkMatterDeficientGalaxyNGC1052DF4ImplicationfortheTotalMass</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/RotationoftheGlobularClusterPopulationoftheDarkMatterDeficientGalaxyNGC1052DF4ImplicationfortheTotalMass.html">&lt;p&gt;We explore the globular cluster population of NGC 1052-DF4, a dark matter deficient galaxy, using Bayesian inference to search for the presence of rotation. The existence of such a rotating component is relevant to the estimation of the mass of the galaxy, and therefore the question of whether NGC 1052-DF4 is truly deficient of dark matter, similar to NGC 1052-DF2 another galaxy in the same group. The rotational characteristics of seven globular clusters in NGC 1052-DF4 were investigated, finding that a non-rotating kinematic model has a higher Bayesian evidence than a rotating model, by a factor of approximately 2.5. In addition, we find that under the assumption of rotation, its amplitude must be small. This distinct lack of rotation strengthens the case that, based on its intrinsic velocity dispersion, NGC 1052-DF4 is a truly dark matter deficient galaxy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10462&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuan ,  Li, Brendon J. Brewer, Geraint F. Lewis</name></author><category term="stat.AP" /><summary type="html">We explore the globular cluster population of NGC 1052-DF4, a dark matter deficient galaxy, using Bayesian inference to search for the presence of rotation. The existence of such a rotating component is relevant to the estimation of the mass of the galaxy, and therefore the question of whether NGC 1052-DF4 is truly deficient of dark matter, similar to NGC 1052-DF2 another galaxy in the same group. The rotational characteristics of seven globular clusters in NGC 1052-DF4 were investigated, finding that a non-rotating kinematic model has a higher Bayesian evidence than a rotating model, by a factor of approximately 2.5. In addition, we find that under the assumption of rotation, its amplitude must be small. This distinct lack of rotation strengthens the case that, based on its intrinsic velocity dispersion, NGC 1052-DF4 is a truly dark matter deficient galaxy.</summary></entry><entry><title type="html">Sparse and Orthogonal Low-rank Collective Matrix Factorization (solrCMF): Efficient data integration in flexible layouts</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/SparseandOrthogonalLowrankCollectiveMatrixFactorizationsolrCMFEfficientdataintegrationinflexiblelayouts.html" rel="alternate" type="text/html" title="Sparse and Orthogonal Low-rank Collective Matrix Factorization (solrCMF): Efficient data integration in flexible layouts" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/SparseandOrthogonalLowrankCollectiveMatrixFactorizationsolrCMFEfficientdataintegrationinflexiblelayouts</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/SparseandOrthogonalLowrankCollectiveMatrixFactorizationsolrCMFEfficientdataintegrationinflexiblelayouts.html">&lt;p&gt;Interest in unsupervised methods for joint analysis of heterogeneous data sources has risen in recent years. Low-rank latent factor models have proven to be an effective tool for data integration and have been extended to a large number of data source layouts. Of particular interest is the separation of variation present in data sources into shared and individual subspaces. In addition, interpretability of estimated latent factors is crucial to further understanding.
  We present sparse and orthogonal low-rank Collective Matrix Factorization (solrCMF) to estimate low-rank latent factor models for flexible data layouts. These encompass traditional multi-view (one group, multiple data types) and multi-grid (multiple groups, multiple data types) layouts, as well as augmented layouts, which allow the inclusion of side information between data types or groups. In addition, solrCMF allows tensor-like layouts (repeated layers), estimates interpretable factors, and determines variation structure among factors and data sources.
  Using a penalized optimization approach, we automatically separate variability into the globally and partially shared as well as individual components and estimate sparse representations of factors. To further increase interpretability of factors, we enforce orthogonality between them. Estimation is performed efficiently in a recent multi-block ADMM framework which we adapted to support embedded manifold constraints.
  The performance of solrCMF is demonstrated in simulation studies and compares favorably to existing methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10067&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Felix Held, Jacob Lindbäck, Rebecka Jörnsten</name></author><category term="stat.ME" /><summary type="html">Interest in unsupervised methods for joint analysis of heterogeneous data sources has risen in recent years. Low-rank latent factor models have proven to be an effective tool for data integration and have been extended to a large number of data source layouts. Of particular interest is the separation of variation present in data sources into shared and individual subspaces. In addition, interpretability of estimated latent factors is crucial to further understanding. We present sparse and orthogonal low-rank Collective Matrix Factorization (solrCMF) to estimate low-rank latent factor models for flexible data layouts. These encompass traditional multi-view (one group, multiple data types) and multi-grid (multiple groups, multiple data types) layouts, as well as augmented layouts, which allow the inclusion of side information between data types or groups. In addition, solrCMF allows tensor-like layouts (repeated layers), estimates interpretable factors, and determines variation structure among factors and data sources. Using a penalized optimization approach, we automatically separate variability into the globally and partially shared as well as individual components and estimate sparse representations of factors. To further increase interpretability of factors, we enforce orthogonality between them. Estimation is performed efficiently in a recent multi-block ADMM framework which we adapted to support embedded manifold constraints. The performance of solrCMF is demonstrated in simulation studies and compares favorably to existing methods.</summary></entry><entry><title type="html">Topological Data Analysis in smart manufacturing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/TopologicalDataAnalysisinsmartmanufacturing.html" rel="alternate" type="text/html" title="Topological Data Analysis in smart manufacturing" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/TopologicalDataAnalysisinsmartmanufacturing</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/TopologicalDataAnalysisinsmartmanufacturing.html">&lt;p&gt;Topological Data Analysis (TDA) is a discipline that applies algebraic topology techniques to analyze complex, multi-dimensional data. Although it is a relatively new field, TDA has been widely and successfully applied across various domains, such as medicine, materials science, and biology. This survey provides an overview of the state of the art of TDA within a dynamic and promising application area: industrial manufacturing and production, particularly within the Industry 4.0 context. We have conducted a rigorous and reproducible literature search focusing on TDA applications in industrial production and manufacturing settings. The identified works are categorized based on their application areas within the manufacturing process and the types of input data. We highlight the principal advantages of TDA tools in this context, address the challenges encountered and the future potential of the field. Furthermore, we identify TDA methods that are currently underexploited in specific industrial areas and discuss how their application could be beneficial, with the aim of stimulating further research in this field. This work seeks to bridge the theoretical advancements in TDA with the practical needs of industrial production. Our goal is to serve as a guide for practitioners and researchers applying TDA in industrial production and manufacturing systems. We advocate for the untapped potential of TDA in this domain and encourage continued exploration and research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.09319&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Martin Uray, Barbara Giunti, Michael Kerber, Stefan Huber</name></author><category term="stat.AP" /><summary type="html">Topological Data Analysis (TDA) is a discipline that applies algebraic topology techniques to analyze complex, multi-dimensional data. Although it is a relatively new field, TDA has been widely and successfully applied across various domains, such as medicine, materials science, and biology. This survey provides an overview of the state of the art of TDA within a dynamic and promising application area: industrial manufacturing and production, particularly within the Industry 4.0 context. We have conducted a rigorous and reproducible literature search focusing on TDA applications in industrial production and manufacturing settings. The identified works are categorized based on their application areas within the manufacturing process and the types of input data. We highlight the principal advantages of TDA tools in this context, address the challenges encountered and the future potential of the field. Furthermore, we identify TDA methods that are currently underexploited in specific industrial areas and discuss how their application could be beneficial, with the aim of stimulating further research in this field. This work seeks to bridge the theoretical advancements in TDA with the practical needs of industrial production. Our goal is to serve as a guide for practitioners and researchers applying TDA in industrial production and manufacturing systems. We advocate for the untapped potential of TDA in this domain and encourage continued exploration and research.</summary></entry><entry><title type="html">Uniform Pessimistic Risk and its Optimal Portfolio</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/UniformPessimisticRiskanditsOptimalPortfolio.html" rel="alternate" type="text/html" title="Uniform Pessimistic Risk and its Optimal Portfolio" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/UniformPessimisticRiskanditsOptimalPortfolio</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/UniformPessimisticRiskanditsOptimalPortfolio.html">&lt;p&gt;The optimal allocation of assets has been widely discussed with the theoretical analysis of risk measures, and pessimism is one of the most attractive approaches beyond the conventional optimal portfolio model. The $\alpha$-risk plays a crucial role in deriving a broad class of pessimistic optimal portfolios. However, estimating an optimal portfolio assessed by a pessimistic risk is still challenging due to the absence of a computationally tractable model. In this study, we propose an integral of $\alpha$-risk called the \textit{uniform pessimistic risk} and the computational algorithm to obtain an optimal portfolio based on the risk. Further, we investigate the theoretical properties of the proposed risk in view of three different approaches: multiple quantile regression, the proper scoring rule, and distributionally robust optimization. Real data analysis of three stock datasets (S\&amp;amp;P500, CSI500, KOSPI200) demonstrates the usefulness of the proposed risk and portfolio model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.07158&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sungchul Hong, Jong-June Jeon</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">The optimal allocation of assets has been widely discussed with the theoretical analysis of risk measures, and pessimism is one of the most attractive approaches beyond the conventional optimal portfolio model. The $\alpha$-risk plays a crucial role in deriving a broad class of pessimistic optimal portfolios. However, estimating an optimal portfolio assessed by a pessimistic risk is still challenging due to the absence of a computationally tractable model. In this study, we propose an integral of $\alpha$-risk called the \textit{uniform pessimistic risk} and the computational algorithm to obtain an optimal portfolio based on the risk. Further, we investigate the theoretical properties of the proposed risk in view of three different approaches: multiple quantile regression, the proper scoring rule, and distributionally robust optimization. Real data analysis of three stock datasets (S\&amp;amp;P500, CSI500, KOSPI200) demonstrates the usefulness of the proposed risk and portfolio model.</summary></entry><entry><title type="html">Utility-based optimization of Fujikawa’s basket trial design – Pre-specified protocol of a comparison study</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/UtilitybasedoptimizationofFujikawasbaskettrialdesignPrespecifiedprotocolofacomparisonstudy.html" rel="alternate" type="text/html" title="Utility-based optimization of Fujikawa’s basket trial design – Pre-specified protocol of a comparison study" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/UtilitybasedoptimizationofFujikawasbaskettrialdesignPrespecifiedprotocolofacomparisonstudy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/UtilitybasedoptimizationofFujikawasbaskettrialdesignPrespecifiedprotocolofacomparisonstudy.html">&lt;p&gt;Basket trial designs are a type of master protocol in which the same therapy is tested in several strata of the patient cohort. Many basket trial designs implement borrowing mechanisms. These allow sharing information between similar strata with the goal of increasing power in responsive strata while at the same time constraining type-I error inflation to a bearable threshold. These borrowing mechanisms can be tuned using numerical tuning parameters. The optimal choice of these tuning parameters is subject to research. In a comparison study using simulations and numerical calculations, we are planning to investigate the use of utility functions for quantifying the compromise between power and type-I error inflation and the use of numerical optimization algorithms for optimizing these functions. The present document is the protocol of this comparison study, defining each step of the study in accordance with the ADEMP scheme for pre-specification of simulation studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.02058&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lukas D Sauer, Alexander Ritz, Meinhard Kieser</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Basket trial designs are a type of master protocol in which the same therapy is tested in several strata of the patient cohort. Many basket trial designs implement borrowing mechanisms. These allow sharing information between similar strata with the goal of increasing power in responsive strata while at the same time constraining type-I error inflation to a bearable threshold. These borrowing mechanisms can be tuned using numerical tuning parameters. The optimal choice of these tuning parameters is subject to research. In a comparison study using simulations and numerical calculations, we are planning to investigate the use of utility functions for quantifying the compromise between power and type-I error inflation and the use of numerical optimization algorithms for optimizing these functions. The present document is the protocol of this comparison study, defining each step of the study in accordance with the ADEMP scheme for pre-specification of simulation studies.</summary></entry><entry><title type="html">$\ell_1$-Regularized Generalized Least Squares</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/ell1RegularizedGeneralizedLeastSquares.html" rel="alternate" type="text/html" title="$\ell_1$-Regularized Generalized Least Squares" /><published>2024-05-20T00:00:00+00:00</published><updated>2024-05-20T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/ell1RegularizedGeneralizedLeastSquares</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/20/ell1RegularizedGeneralizedLeastSquares.html">&lt;p&gt;In this paper we propose an $\ell_1$-regularized GLS estimator for high-dimensional regressions with potentially autocorrelated errors. We establish non-asymptotic oracle inequalities for estimation accuracy in a framework that allows for highly persistent autoregressive errors. In practice, the Whitening matrix required to implement the GLS is unkown, we present a feasible estimator for this matrix, derive consistency results and ultimately show how our proposed feasible GLS can recover closely the optimal performance (as if the errors were a white noise) of the LASSO. A simulation study verifies the performance of the proposed method, demonstrating that the penalized (feasible) GLS-LASSO estimator performs on par with the LASSO in the case of white noise errors, whilst outperforming it in terms of sign-recovery and estimation error when the errors exhibit significant correlation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10719&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kaveh S. Nobari, Alex Gibberd</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">In this paper we propose an $\ell_1$-regularized GLS estimator for high-dimensional regressions with potentially autocorrelated errors. We establish non-asymptotic oracle inequalities for estimation accuracy in a framework that allows for highly persistent autoregressive errors. In practice, the Whitening matrix required to implement the GLS is unkown, we present a feasible estimator for this matrix, derive consistency results and ultimately show how our proposed feasible GLS can recover closely the optimal performance (as if the errors were a white noise) of the LASSO. A simulation study verifies the performance of the proposed method, demonstrating that the penalized (feasible) GLS-LASSO estimator performs on par with the LASSO in the case of white noise errors, whilst outperforming it in terms of sign-recovery and estimation error when the errors exhibit significant correlation.</summary></entry></feed>