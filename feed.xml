<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-04-28T14:48:49+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Causal Analysis of CO2 Reduction Strategies in Electricity Markets Through Machine Learning-Driven Metalearners</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-Causal-Analysis-of-CO2-Reduction-Strategies-in-Electricity-Markets-Through-Machine-Learning-Driven-Metalearners.html" rel="alternate" type="text/html" title="A Causal Analysis of CO2 Reduction Strategies in Electricity Markets Through Machine Learning-Driven Metalearners" /><published>2024-04-27T00:00:00+00:00</published><updated>2024-04-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-Causal-Analysis-of-CO2-Reduction-Strategies-in-Electricity-Markets-Through-Machine-Learning-Driven-Metalearners</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-Causal-Analysis-of-CO2-Reduction-Strategies-in-Electricity-Markets-Through-Machine-Learning-Driven-Metalearners.html">&lt;p&gt;This study employs the Causal Machine Learning (CausalML) statistical method to analyze the influence of electricity pricing policies on carbon dioxide (CO2) levels in the household sector. Investigating the causality between potential outcomes and treatment effects, where changes in pricing policies are the treatment, our analysis challenges the conventional wisdom surrounding incentive-based electricity pricing. The study’s findings suggest that adopting such policies may inadvertently increase CO2 intensity. Additionally, we integrate a machine learning-based meta-algorithm, reflecting a contemporary statistical approach, to enhance the depth of our causal analysis. The study conducts a comparative analysis of learners X, T, S, and R to ascertain the optimal methods based on the defined question’s specified goals and contextual nuances. This research contributes valuable insights to the ongoing dialogue on sustainable development practices, emphasizing the importance of considering unintended consequences in policy formulation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.15499&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Iman Emtiazi Naeini, Zahra Saberi, Khadijeh Hassanzadeh</name></author><category term="stat.ME" /><summary type="html">This study employs the Causal Machine Learning (CausalML) statistical method to analyze the influence of electricity pricing policies on carbon dioxide (CO2) levels in the household sector. Investigating the causality between potential outcomes and treatment effects, where changes in pricing policies are the treatment, our analysis challenges the conventional wisdom surrounding incentive-based electricity pricing. The study’s findings suggest that adopting such policies may inadvertently increase CO2 intensity. Additionally, we integrate a machine learning-based meta-algorithm, reflecting a contemporary statistical approach, to enhance the depth of our causal analysis. The study conducts a comparative analysis of learners X, T, S, and R to ascertain the optimal methods based on the defined question’s specified goals and contextual nuances. This research contributes valuable insights to the ongoing dialogue on sustainable development practices, emphasizing the importance of considering unintended consequences in policy formulation.</summary></entry><entry><title type="html">A Generalized Logrank-type Test for Comparison of Treatment Regimes in Sequential Multiple Assignment Randomized Trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-Generalized-Logrank-type-Test-for-Comparison-of-Treatment-Regimes-in-Sequential-Multiple-Assignment-Randomized-Trials.html" rel="alternate" type="text/html" title="A Generalized Logrank-type Test for Comparison of Treatment Regimes in Sequential Multiple Assignment Randomized Trials" /><published>2024-04-27T00:00:00+00:00</published><updated>2024-04-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-Generalized-Logrank-type-Test-for-Comparison-of-Treatment-Regimes-in-Sequential-Multiple-Assignment-Randomized-Trials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-Generalized-Logrank-type-Test-for-Comparison-of-Treatment-Regimes-in-Sequential-Multiple-Assignment-Randomized-Trials.html">&lt;p&gt;The sequential multiple assignment randomized trial (SMART) is the
  ideal study design for the evaluation of multistage treatment
  regimes, which comprise sequential decision rules that recommend
  treatments for a patient at each of a series of decision points
  based on their evolving characteristics. A common goal is to
  compare the set of so-called embedded regimes represented in the
  design on the basis of a primary outcome of interest. In the study
  of chronic diseases and disorders, this outcome is often a time to
  an event, and a goal is to compare the distributions of the
  time-to-event outcome associated with each regime in the set. We
  present a general statistical framework in which we develop a
  logrank-type test for comparison of the survival distributions
  associated with regimes within a specified set based on the data
  from a SMART with an arbitrary number of stages that allows
  incorporation of covariate information to enhance efficiency and can
  also be used with data from an observational study. The framework
  provides clarification of the assumptions required to yield a
  principled test procedure, and the proposed test subsumes or offers
  an improved alternative to existing methods. We demonstrate
  performance of the methods in a suite of simulation
  studies. The methods are applied to a SMART in patients with acute
  promyelocytic leukemia.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.16813&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anastasios A. Tsiatis, Marie Davidian</name></author><category term="stat.ME" /><summary type="html">The sequential multiple assignment randomized trial (SMART) is the ideal study design for the evaluation of multistage treatment regimes, which comprise sequential decision rules that recommend treatments for a patient at each of a series of decision points based on their evolving characteristics. A common goal is to compare the set of so-called embedded regimes represented in the design on the basis of a primary outcome of interest. In the study of chronic diseases and disorders, this outcome is often a time to an event, and a goal is to compare the distributions of the time-to-event outcome associated with each regime in the set. We present a general statistical framework in which we develop a logrank-type test for comparison of the survival distributions associated with regimes within a specified set based on the data from a SMART with an arbitrary number of stages that allows incorporation of covariate information to enhance efficiency and can also be used with data from an observational study. The framework provides clarification of the assumptions required to yield a principled test procedure, and the proposed test subsumes or offers an improved alternative to existing methods. We demonstrate performance of the methods in a suite of simulation studies. The methods are applied to a SMART in patients with acute promyelocytic leukemia.</summary></entry><entry><title type="html">A Plot is Worth a Thousand Tests: Assessing Residual Diagnostics with the Lineup Protocol</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-Plot-is-Worth-a-Thousand-Tests-Assessing-Residual-Diagnostics-with-the-Lineup-Protocol.html" rel="alternate" type="text/html" title="A Plot is Worth a Thousand Tests: Assessing Residual Diagnostics with the Lineup Protocol" /><published>2024-04-27T00:00:00+00:00</published><updated>2024-04-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-Plot-is-Worth-a-Thousand-Tests:-Assessing-Residual-Diagnostics-with-the-Lineup-Protocol</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-Plot-is-Worth-a-Thousand-Tests-Assessing-Residual-Diagnostics-with-the-Lineup-Protocol.html">&lt;p&gt;Regression experts consistently recommend plotting residuals for model diagnosis, despite the availability of many numerical hypothesis test procedures designed to use residuals to assess problems with a model fit. Here we provide evidence for why this is good advice using data from a visual inference experiment. We show how conventional tests are too sensitive, which means that too often the conclusion would be that the model fit is inadequate. The experiment uses the lineup protocol which puts a residual plot in the context of null plots. This helps generate reliable and consistent reading of residual plots for better model diagnosis. It can also help in an obverse situation where a conventional test would fail to detect a problem with a model due to contaminated data. The lineup protocol also detects a range of departures from good residuals simultaneously. Supplemental materials for the article are available online.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.05964&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Weihao Li, Dianne Cook, Emi Tanaka, Susan VanderPlas</name></author><category term="stat.AP" /><summary type="html">Regression experts consistently recommend plotting residuals for model diagnosis, despite the availability of many numerical hypothesis test procedures designed to use residuals to assess problems with a model fit. Here we provide evidence for why this is good advice using data from a visual inference experiment. We show how conventional tests are too sensitive, which means that too often the conclusion would be that the model fit is inadequate. The experiment uses the lineup protocol which puts a residual plot in the context of null plots. This helps generate reliable and consistent reading of residual plots for better model diagnosis. It can also help in an obverse situation where a conventional test would fail to detect a problem with a model due to contaminated data. The lineup protocol also detects a range of departures from good residuals simultaneously. Supplemental materials for the article are available online.</summary></entry><entry><title type="html">A Poincaré Inequality and Consistency Results for Signal Sampling on Large Graphs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-Poincar%C3%A9-Inequality-and-Consistency-Results-for-Signal-Sampling-on-Large-Graphs.html" rel="alternate" type="text/html" title="A Poincaré Inequality and Consistency Results for Signal Sampling on Large Graphs" /><published>2024-04-27T00:00:00+00:00</published><updated>2024-04-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-Poincar%C3%A9-Inequality-and-Consistency-Results-for-Signal-Sampling-on-Large-Graphs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-Poincar%C3%A9-Inequality-and-Consistency-Results-for-Signal-Sampling-on-Large-Graphs.html">&lt;p&gt;Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit – the graphon. We prove a Poincar&apos;e inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting connections with spectral clustering and Gaussian elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related graphon signal sampling algorithm for large graphs, and demonstrate its good empirical performance on graph machine learning tasks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.10610&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thien Le, Luana Ruiz, Stefanie Jegelka</name></author><category term="stat.ML" /><summary type="html">Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit – the graphon. We prove a Poincar&apos;e inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting connections with spectral clustering and Gaussian elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related graphon signal sampling algorithm for large graphs, and demonstrate its good empirical performance on graph machine learning tasks.</summary></entry><entry><title type="html">A comparison of graphical methods in the case of the murder of Meredith Kercher</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-comparison-of-graphical-methods-in-the-case-of-the-murder-of-Meredith-Kercher.html" rel="alternate" type="text/html" title="A comparison of graphical methods in the case of the murder of Meredith Kercher" /><published>2024-04-27T00:00:00+00:00</published><updated>2024-04-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-comparison-of-graphical-methods-in-the-case-of-the-murder-of-Meredith-Kercher</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-comparison-of-graphical-methods-in-the-case-of-the-murder-of-Meredith-Kercher.html">&lt;p&gt;We compare three graphical methods for displaying evidence in a legal case: Wigmore Charts, Bayesian Networks and Chain Event Graphs. We find that these methods are aimed at three distinct audiences, respectively lawyers, forensic scientists and the police. The methods are illustrated using part of the evidence in the case of the murder of Meredith Kercher. More specifically, we focus on representing the list of propositions, evidence, testimony and facts given in the first trial against Raffaele Sollecito and Amanda Knox with these graphical methodologies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.16628&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>A. Philip Dawid, Francesco Dotto, Maxine Graves, Jay B. Kadane, Julia Mortera, Gail Robertson, Jim Q. Smith, Amy L. Wilson</name></author><category term="stat.AP" /><summary type="html">We compare three graphical methods for displaying evidence in a legal case: Wigmore Charts, Bayesian Networks and Chain Event Graphs. We find that these methods are aimed at three distinct audiences, respectively lawyers, forensic scientists and the police. The methods are illustrated using part of the evidence in the case of the murder of Meredith Kercher. More specifically, we focus on representing the list of propositions, evidence, testimony and facts given in the first trial against Raffaele Sollecito and Amanda Knox with these graphical methodologies.</summary></entry><entry><title type="html">A note on generalization bounds for losses with finite moments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-note-on-generalization-bounds-for-losses-with-finite-moments.html" rel="alternate" type="text/html" title="A note on generalization bounds for losses with finite moments" /><published>2024-04-27T00:00:00+00:00</published><updated>2024-04-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-note-on-generalization-bounds-for-losses-with-finite-moments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-note-on-generalization-bounds-for-losses-with-finite-moments.html">&lt;p&gt;This paper studies the truncation method from Alquier [1] to derive high-probability PAC-Bayes bounds for unbounded losses with heavy tails. Assuming that the $p$-th moment is bounded, the resulting bounds interpolate between a slow rate $1 / \sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p \to \infty$ and the loss is essentially bounded. Moreover, the paper derives a high-probability PAC-Bayes bound for losses with a bounded variance. This bound has an exponentially better dependence on the confidence parameter and the dependency measure than previous bounds in the literature. Finally, the paper extends all results to guarantees in expectation and single-draw PAC-Bayes. In order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded losses from [2] in these settings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.16681&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Borja Rodríguez-Gálvez, Omar Rivasplata, Ragnar Thobaben, Mikael Skoglund</name></author><category term="stat.ML" /><summary type="html">This paper studies the truncation method from Alquier [1] to derive high-probability PAC-Bayes bounds for unbounded losses with heavy tails. Assuming that the $p$-th moment is bounded, the resulting bounds interpolate between a slow rate $1 / \sqrt{n}$ when $p=2$, and a fast rate $1 / n$ when $p \to \infty$ and the loss is essentially bounded. Moreover, the paper derives a high-probability PAC-Bayes bound for losses with a bounded variance. This bound has an exponentially better dependence on the confidence parameter and the dependency measure than previous bounds in the literature. Finally, the paper extends all results to guarantees in expectation and single-draw PAC-Bayes. In order to so, it obtains analogues of the PAC-Bayes fast rate bound for bounded losses from [2] in these settings.</summary></entry><entry><title type="html">A short proof of the Dvoretzky–Kiefer–Wolfowitz–Massart inequality</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-short-proof-of-the-Dvoretzky-Kiefer-Wolfowitz-Massart-inequality.html" rel="alternate" type="text/html" title="A short proof of the Dvoretzky–Kiefer–Wolfowitz–Massart inequality" /><published>2024-04-27T00:00:00+00:00</published><updated>2024-04-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-short-proof-of-the-Dvoretzky--Kiefer--Wolfowitz--Massart-inequality</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-short-proof-of-the-Dvoretzky-Kiefer-Wolfowitz-Massart-inequality.html">&lt;p&gt;The Dvoretzky–Kiefer–Wolfowitz–Massart inequality gives a sub-Gaussian tail bound on the supremum norm distance between the empirical distribution function of a random sample and its population counterpart. We provide a short proof of a result that improves the existing bound in two respects. First, our one-sided bound holds without any restrictions on the failure probability, thereby verifying a conjecture of Birnbaum and McCarty (1958). Second, it is local in the sense that it holds uniformly over sub-intervals of the real line with an error rate that adapts to the behaviour of the population distribution function on the interval.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.16651&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Henry W J Reeve</name></author><category term="stat.TH" /><summary type="html">The Dvoretzky–Kiefer–Wolfowitz–Massart inequality gives a sub-Gaussian tail bound on the supremum norm distance between the empirical distribution function of a random sample and its population counterpart. We provide a short proof of a result that improves the existing bound in two respects. First, our one-sided bound holds without any restrictions on the failure probability, thereby verifying a conjecture of Birnbaum and McCarty (1958). Second, it is local in the sense that it holds uniformly over sub-intervals of the real line with an error rate that adapts to the behaviour of the population distribution function on the interval.</summary></entry><entry><title type="html">A tutorial on learning from preferences and choices with Gaussian Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-tutorial-on-learning-from-preferences-and-choices-with-Gaussian-Processes.html" rel="alternate" type="text/html" title="A tutorial on learning from preferences and choices with Gaussian Processes" /><published>2024-04-27T00:00:00+00:00</published><updated>2024-04-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-tutorial-on-learning-from-preferences-and-choices-with-Gaussian-Processes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/A-tutorial-on-learning-from-preferences-and-choices-with-Gaussian-Processes.html">&lt;p&gt;Preference modelling lies at the intersection of economics, decision theory, machine learning and statistics. By understanding individuals’ preferences and how they make choices, we can build products that closely match their expectations, paving the way for more efficient and personalised applications across a wide range of domains. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles (from economics and decision theory) into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This tutorial builds upon established research while simultaneously introducing some novel GP-based models to address specific gaps in the existing literature.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.11782&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alessio Benavoli, Dario Azzimonti</name></author><category term="stat.ML" /><summary type="html">Preference modelling lies at the intersection of economics, decision theory, machine learning and statistics. By understanding individuals’ preferences and how they make choices, we can build products that closely match their expectations, paving the way for more efficient and personalised applications across a wide range of domains. The objective of this tutorial is to present a cohesive and comprehensive framework for preference learning with Gaussian Processes (GPs), demonstrating how to seamlessly incorporate rationality principles (from economics and decision theory) into the learning process. By suitably tailoring the likelihood function, this framework enables the construction of preference learning models that encompass random utility models, limits of discernment, and scenarios with multiple conflicting utilities for both object- and label-preference. This tutorial builds upon established research while simultaneously introducing some novel GP-based models to address specific gaps in the existing literature.</summary></entry><entry><title type="html">Adaptive Frequency Bin Interval in FFT via Dense Sampling Factor $lpha$</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/Adaptive-Frequency-Bin-Interval-in-FFT-via-Dense-Sampling-Factor-$-alpha$.html" rel="alternate" type="text/html" title="Adaptive Frequency Bin Interval in FFT via Dense Sampling Factor $lpha$" /><published>2024-04-27T00:00:00+00:00</published><updated>2024-04-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/Adaptive-Frequency-Bin-Interval-in-FFT-via-Dense-Sampling-Factor-$%5Calpha$</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/Adaptive-Frequency-Bin-Interval-in-FFT-via-Dense-Sampling-Factor-$-alpha$.html">&lt;p&gt;The Fast Fourier Transform (FFT) is a fundamental tool for signal analysis, widely used across various fields. However, traditional FFT methods encounter challenges in adjusting the frequency bin interval, which may impede accurate spectral analysis. In this study, we propose a method for adjusting the frequency bin interval in FFT by introducing a parameter $\alpha$. We elucidate the underlying principles of the proposed method and discuss its potential applications across various contexts. Our findings suggest that the proposed method offers a promising approach to overcome the limitations of traditional FFT methods and enhance spectral analysis accuracy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.16665&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Haichao Xu</name></author><category term="stat.CO" /><summary type="html">The Fast Fourier Transform (FFT) is a fundamental tool for signal analysis, widely used across various fields. However, traditional FFT methods encounter challenges in adjusting the frequency bin interval, which may impede accurate spectral analysis. In this study, we propose a method for adjusting the frequency bin interval in FFT by introducing a parameter $\alpha$. We elucidate the underlying principles of the proposed method and discuss its potential applications across various contexts. Our findings suggest that the proposed method offers a promising approach to overcome the limitations of traditional FFT methods and enhance spectral analysis accuracy.</summary></entry><entry><title type="html">Agile gesture recognition for low-power applications: customisation for generalisation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/Agile-gesture-recognition-for-low-power-applications-customisation-for-generalisation.html" rel="alternate" type="text/html" title="Agile gesture recognition for low-power applications: customisation for generalisation" /><published>2024-04-27T00:00:00+00:00</published><updated>2024-04-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/Agile-gesture-recognition-for-low-power-applications:-customisation-for-generalisation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/27/Agile-gesture-recognition-for-low-power-applications-customisation-for-generalisation.html">&lt;p&gt;Automated hand gesture recognition has long been a focal point in the AI community. Traditionally, research in this field has predominantly focused on scenarios with access to a continuous flow of hand’s images. This focus has been driven by the widespread use of cameras and the abundant availability of image data. However, there is an increasing demand for gesture recognition technologies that operate on low-power sensor devices. This is due to the rising concerns for data leakage and end-user privacy, as well as the limited battery capacity and the computing power in low-cost devices. Moreover, the challenge in data collection for individually designed hardware also hinders the generalisation of a gesture recognition model.
  In this study, we unveil a novel methodology for pattern recognition systems using adaptive and agile error correction, designed to enhance the performance of legacy gesture recognition models on devices with limited battery capacity and computing power. This system comprises a compact Support Vector Machine as the base model for live gesture recognition. Additionally, it features an adaptive agile error corrector that employs few-shot learning within the feature space induced by high-dimensional kernel mappings. The error corrector can be customised for each user, allowing for dynamic adjustments to the gesture prediction based on their movement patterns while maintaining the agile performance of its base model on a low-cost and low-power micro-controller. This proposed system is distinguished by its compact size, rapid processing speed, and low power consumption, making it ideal for a wide range of embedded systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.15421&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ying Liu, Liucheng Guo, Valeri A. Makarovc, Alexander Gorbana, Evgeny Mirkesa, Ivan Y. Tyukin</name></author><category term="stat.AP" /><summary type="html">Automated hand gesture recognition has long been a focal point in the AI community. Traditionally, research in this field has predominantly focused on scenarios with access to a continuous flow of hand’s images. This focus has been driven by the widespread use of cameras and the abundant availability of image data. However, there is an increasing demand for gesture recognition technologies that operate on low-power sensor devices. This is due to the rising concerns for data leakage and end-user privacy, as well as the limited battery capacity and the computing power in low-cost devices. Moreover, the challenge in data collection for individually designed hardware also hinders the generalisation of a gesture recognition model. In this study, we unveil a novel methodology for pattern recognition systems using adaptive and agile error correction, designed to enhance the performance of legacy gesture recognition models on devices with limited battery capacity and computing power. This system comprises a compact Support Vector Machine as the base model for live gesture recognition. Additionally, it features an adaptive agile error corrector that employs few-shot learning within the feature space induced by high-dimensional kernel mappings. The error corrector can be customised for each user, allowing for dynamic adjustments to the gesture prediction based on their movement patterns while maintaining the agile performance of its base model on a low-cost and low-power micro-controller. This proposed system is distinguished by its compact size, rapid processing speed, and low power consumption, making it ideal for a wide range of embedded systems.</summary></entry></feed>