<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-21T07:13:52+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Bayesian Nonparametric Approach for Clustering Functional Trajectories over Time</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/ABayesianNonparametricApproachforClusteringFunctionalTrajectoriesoverTime.html" rel="alternate" type="text/html" title="A Bayesian Nonparametric Approach for Clustering Functional Trajectories over Time" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/ABayesianNonparametricApproachforClusteringFunctionalTrajectoriesoverTime</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/ABayesianNonparametricApproachforClusteringFunctionalTrajectoriesoverTime.html">&lt;p&gt;Functional concurrent, or varying-coefficient, regression models are commonly used in biomedical and clinical settings to investigate how the relation between an outcome and observed covariate varies as a function of another covariate. In this work, we propose a Bayesian nonparametric approach to investigate how clusters of these functional relations evolve over time. Our model clusters individual functional trajectories within and across time periods while flexibly accommodating the evolution of the partitions across time periods with covariates. Motivated by mobile health data collected in a novel, smartphone-based smoking cessation intervention study, we demonstrate how our proposed method can simultaneously cluster functional trajectories, accommodate temporal dependence, and provide insights into the transitions between functional clusters over time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11358&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mingrui Liang, Matthew D. Koslovsky, Emily T. Hebert, Darla E. Kendzor, Marina Vannucci</name></author><category term="stat.ME" /><summary type="html">Functional concurrent, or varying-coefficient, regression models are commonly used in biomedical and clinical settings to investigate how the relation between an outcome and observed covariate varies as a function of another covariate. In this work, we propose a Bayesian nonparametric approach to investigate how clusters of these functional relations evolve over time. Our model clusters individual functional trajectories within and across time periods while flexibly accommodating the evolution of the partitions across time periods with covariates. Motivated by mobile health data collected in a novel, smartphone-based smoking cessation intervention study, we demonstrate how our proposed method can simultaneously cluster functional trajectories, accommodate temporal dependence, and provide insights into the transitions between functional clusters over time.</summary></entry><entry><title type="html">A Randomized Permutation Whole-Model Test Heuristic for Self-Validated Ensemble Models (SVEM)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/ARandomizedPermutationWholeModelTestHeuristicforSelfValidatedEnsembleModelsSVEM.html" rel="alternate" type="text/html" title="A Randomized Permutation Whole-Model Test Heuristic for Self-Validated Ensemble Models (SVEM)" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/ARandomizedPermutationWholeModelTestHeuristicforSelfValidatedEnsembleModelsSVEM</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/ARandomizedPermutationWholeModelTestHeuristicforSelfValidatedEnsembleModelsSVEM.html">&lt;p&gt;We introduce a heuristic to test the significance of fit of Self-Validated Ensemble Models (SVEM) against the null hypothesis of a constant response. A SVEM model averages predictions from nBoot fits of a model, applied to fractionally weighted bootstraps of the target dataset. It tunes each fit on a validation copy of the training data, utilizing anti-correlated weights for training and validation. The proposed test computes SVEM predictions centered by the response column mean and normalized by the ensemble variability at each of nPoint points spaced throughout the factor space. A reference distribution is constructed by refitting the SVEM model to nPerm randomized permutations of the response column and recording the corresponding standardized predictions at the nPoint points. A reduced-rank singular value decomposition applied to the centered and scaled nPerm x nPoint reference matrix is used to calculate the Mahalanobis distance for each of the nPerm permutation results as well as the jackknife (holdout) Mahalanobis distance of the original response column. The process is repeated independently for each response in the experiment, producing a joint graphical summary. We present a simulation driven power analysis and discuss limitations of the test relating to model flexibility and design adequacy. The test maintains the nominal Type I error rate even when the base SVEM model contains more parameters than observations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11156&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrew T. Karl</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We introduce a heuristic to test the significance of fit of Self-Validated Ensemble Models (SVEM) against the null hypothesis of a constant response. A SVEM model averages predictions from nBoot fits of a model, applied to fractionally weighted bootstraps of the target dataset. It tunes each fit on a validation copy of the training data, utilizing anti-correlated weights for training and validation. The proposed test computes SVEM predictions centered by the response column mean and normalized by the ensemble variability at each of nPoint points spaced throughout the factor space. A reference distribution is constructed by refitting the SVEM model to nPerm randomized permutations of the response column and recording the corresponding standardized predictions at the nPoint points. A reduced-rank singular value decomposition applied to the centered and scaled nPerm x nPoint reference matrix is used to calculate the Mahalanobis distance for each of the nPerm permutation results as well as the jackknife (holdout) Mahalanobis distance of the original response column. The process is repeated independently for each response in the experiment, producing a joint graphical summary. We present a simulation driven power analysis and discuss limitations of the test relating to model flexibility and design adequacy. The test maintains the nominal Type I error rate even when the base SVEM model contains more parameters than observations.</summary></entry><entry><title type="html">A comparative study of augmented inverse propensity weighted estimators using outcome-adaptive lasso and other penalized regression methods</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Acomparativestudyofaugmentedinversepropensityweightedestimatorsusingoutcomeadaptivelassoandotherpenalizedregressionmethods.html" rel="alternate" type="text/html" title="A comparative study of augmented inverse propensity weighted estimators using outcome-adaptive lasso and other penalized regression methods" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Acomparativestudyofaugmentedinversepropensityweightedestimatorsusingoutcomeadaptivelassoandotherpenalizedregressionmethods</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Acomparativestudyofaugmentedinversepropensityweightedestimatorsusingoutcomeadaptivelassoandotherpenalizedregressionmethods.html">&lt;p&gt;Confounder selection may be efficiently conducted using penalized regression methods when causal effects are estimated from observational data with many variables. An outcome-adaptive lasso was proposed to build a model for the propensity score that can be employed in conjunction with other variable selection methods for the outcome model to apply the augmented inverse propensity weighted (AIPW) estimator. However, researchers may not know which method is optimal to use for outcome model when applying the AIPW estimator with the outcome-adaptive lasso. This study provided hints on readily implementable penalized regression methods that should be adopted for the outcome model as a counterpart of the outcome-adaptive lasso. We evaluated the bias and variance of the AIPW estimators using the propensity score (PS) model and an outcome model based on penalized regression methods under various conditions by analyzing a clinical trial example and numerical experiments; the estimates and standard errors of the AIPW estimators were almost identical in an example with over 5000 participants. The AIPW estimators using penalized regression methods with the oracle property performed well in terms of bias and variance in numerical experiments with smaller sample sizes. Meanwhile, the bias of the AIPW estimator using the ordinary lasso for the PS and outcome models was considerably larger.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11522&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wataru Hongo, Shuji Ando, Jun Tsuchida, Takashi Sozu</name></author><category term="stat.ME" /><summary type="html">Confounder selection may be efficiently conducted using penalized regression methods when causal effects are estimated from observational data with many variables. An outcome-adaptive lasso was proposed to build a model for the propensity score that can be employed in conjunction with other variable selection methods for the outcome model to apply the augmented inverse propensity weighted (AIPW) estimator. However, researchers may not know which method is optimal to use for outcome model when applying the AIPW estimator with the outcome-adaptive lasso. This study provided hints on readily implementable penalized regression methods that should be adopted for the outcome model as a counterpart of the outcome-adaptive lasso. We evaluated the bias and variance of the AIPW estimators using the propensity score (PS) model and an outcome model based on penalized regression methods under various conditions by analyzing a clinical trial example and numerical experiments; the estimates and standard errors of the AIPW estimators were almost identical in an example with over 5000 participants. The AIPW estimators using penalized regression methods with the oracle property performed well in terms of bias and variance in numerical experiments with smaller sample sizes. Meanwhile, the bias of the AIPW estimator using the ordinary lasso for the PS and outcome models was considerably larger.</summary></entry><entry><title type="html">Adaptive Online Experimental Design for Causal Discovery</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/AdaptiveOnlineExperimentalDesignforCausalDiscovery.html" rel="alternate" type="text/html" title="Adaptive Online Experimental Design for Causal Discovery" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/AdaptiveOnlineExperimentalDesignforCausalDiscovery</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/AdaptiveOnlineExperimentalDesignforCausalDiscovery.html">&lt;p&gt;Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11548&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Muhammad Qasim Elahi, Lai Wei, Murat Kocaoglu, Mahsa Ghasemi</name></author><category term="stat.AP" /><summary type="html">Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.</summary></entry><entry><title type="html">Analyze Additive and Interaction Effects via Collaborative Trees</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/AnalyzeAdditiveandInteractionEffectsviaCollaborativeTrees.html" rel="alternate" type="text/html" title="Analyze Additive and Interaction Effects via Collaborative Trees" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/AnalyzeAdditiveandInteractionEffectsviaCollaborativeTrees</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/AnalyzeAdditiveandInteractionEffectsviaCollaborativeTrees.html">&lt;p&gt;We present Collaborative Trees, a novel tree model designed for regression prediction, along with its bagging version, which aims to analyze complex statistical associations between features and uncover potential patterns inherent in the data. We decompose the mean decrease in impurity from the proposed tree model to analyze the additive and interaction effects of features on the response variable. Additionally, we introduce network diagrams to visually depict how each feature contributes additively to the response and how pairs of features contribute interaction effects. Through a detailed demonstration using an embryo growth dataset, we illustrate how the new statistical tools aid data analysis, both visually and numerically. Moreover, we delve into critical aspects of tree modeling, such as prediction performance, inference stability, and bias in feature importance measures, leveraging real datasets and simulation experiments for comprehensive discussions. On the theory side, we show that Collaborative Trees, built upon a ``sum of trees’’ approach with our own innovative tree model regularization, exhibit characteristics akin to matching pursuit, under the assumption of high-dimensional independent binary input features (or one-hot feature groups). This newfound link sheds light on the superior capability of our tree model in estimating additive effects of features, a crucial factor for accurate interaction effect estimation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11477&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chien-Ming Chi</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We present Collaborative Trees, a novel tree model designed for regression prediction, along with its bagging version, which aims to analyze complex statistical associations between features and uncover potential patterns inherent in the data. We decompose the mean decrease in impurity from the proposed tree model to analyze the additive and interaction effects of features on the response variable. Additionally, we introduce network diagrams to visually depict how each feature contributes additively to the response and how pairs of features contribute interaction effects. Through a detailed demonstration using an embryo growth dataset, we illustrate how the new statistical tools aid data analysis, both visually and numerically. Moreover, we delve into critical aspects of tree modeling, such as prediction performance, inference stability, and bias in feature importance measures, leveraging real datasets and simulation experiments for comprehensive discussions. On the theory side, we show that Collaborative Trees, built upon a ``sum of trees’’ approach with our own innovative tree model regularization, exhibit characteristics akin to matching pursuit, under the assumption of high-dimensional independent binary input features (or one-hot feature groups). This newfound link sheds light on the superior capability of our tree model in estimating additive effects of features, a crucial factor for accurate interaction effect estimation.</summary></entry><entry><title type="html">Approximation of bivariate densities with compositional splines</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Approximationofbivariatedensitieswithcompositionalsplines.html" rel="alternate" type="text/html" title="Approximation of bivariate densities with compositional splines" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Approximationofbivariatedensitieswithcompositionalsplines</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Approximationofbivariatedensitieswithcompositionalsplines.html">&lt;p&gt;Reliable estimation and approximation of probability density functions is fundamental for their further processing. However, their specific properties, i.e. scale invariance and relative scale, prevent the use of standard methods of spline approximation and have to be considered when building a suitable spline basis. Bayes Hilbert space methodology allows to account for these properties of densities and enables their conversion to a standard Lebesgue space of square integrable functions using the centered log-ratio transformation. As the transformed densities fulfill a zero integral constraint, the constraint should likewise be respected by any spline basis used. Bayes Hilbert space methodology also allows to decompose bivariate densities into their interactive and independent parts with univariate marginals. As this yields a useful framework for studying the dependence structure between random variables, a spline basis ideally should admit a corresponding decomposition. This paper proposes a new spline basis for (transformed) bivariate densities respecting the desired zero integral property. We show that there is a one-to-one correspondence of this basis to a corresponding basis in the Bayes Hilbert space of bivariate densities using tools of this methodology. Furthermore, the spline representation and the resulting decomposition into interactive and independent parts are derived. Finally, this novel spline representation is evaluated in a simulation study and applied to empirical geochemical data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11615&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Stanislav Škorňa, Jitka Machalová, Jana Burkotová, Karel Hron, Sonja Greven</name></author><category term="stat.ME" /><summary type="html">Reliable estimation and approximation of probability density functions is fundamental for their further processing. However, their specific properties, i.e. scale invariance and relative scale, prevent the use of standard methods of spline approximation and have to be considered when building a suitable spline basis. Bayes Hilbert space methodology allows to account for these properties of densities and enables their conversion to a standard Lebesgue space of square integrable functions using the centered log-ratio transformation. As the transformed densities fulfill a zero integral constraint, the constraint should likewise be respected by any spline basis used. Bayes Hilbert space methodology also allows to decompose bivariate densities into their interactive and independent parts with univariate marginals. As this yields a useful framework for studying the dependence structure between random variables, a spline basis ideally should admit a corresponding decomposition. This paper proposes a new spline basis for (transformed) bivariate densities respecting the desired zero integral property. We show that there is a one-to-one correspondence of this basis to a corresponding basis in the Bayes Hilbert space of bivariate densities using tools of this methodology. Furthermore, the spline representation and the resulting decomposition into interactive and independent parts are derived. Finally, this novel spline representation is evaluated in a simulation study and applied to empirical geochemical data.</summary></entry><entry><title type="html">Assessment of the quality of a prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Assessmentofthequalityofaprediction.html" rel="alternate" type="text/html" title="Assessment of the quality of a prediction" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Assessmentofthequalityofaprediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Assessmentofthequalityofaprediction.html">&lt;p&gt;Shannon defined the mutual information between two variables. We illustrate why the true mutual information between a variable and the predictions made by a prediction algorithm is not a suitable measure of prediction quality, but the apparent Shannon mutual information (ASI) is; indeed it is the unique prediction quality measure with either of two very different lists of desirable properties, as previously shown by de Finetti and other authors. However, estimating the uncertainty of the ASI is a difficult problem, because of long and non-symmetric heavy tails to the distribution of the individual values of $j(x,y)=\log\frac{Q_y(x)}{P(x)}$ We propose a Bayesian modelling method for the distribution of $j(x,y)$, from the posterior distribution of which the uncertainty in the ASI can be inferred. This method is based on Dirichlet-based mixtures of skew-Student distributions. We illustrate its use on data from a Bayesian model for prediction of the recurrence time of prostate cancer. We believe that this approach is generally appropriate for most problems, where it is infeasible to derive the explicit distribution of the samples of $j(x,y)$, though the precise modelling parameters may need adjustment to suit particular cases.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.15764&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roger Sewell, Elisabeth Crowe, Sharokh F. Shariat</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Shannon defined the mutual information between two variables. We illustrate why the true mutual information between a variable and the predictions made by a prediction algorithm is not a suitable measure of prediction quality, but the apparent Shannon mutual information (ASI) is; indeed it is the unique prediction quality measure with either of two very different lists of desirable properties, as previously shown by de Finetti and other authors. However, estimating the uncertainty of the ASI is a difficult problem, because of long and non-symmetric heavy tails to the distribution of the individual values of $j(x,y)=\log\frac{Q_y(x)}{P(x)}$ We propose a Bayesian modelling method for the distribution of $j(x,y)$, from the posterior distribution of which the uncertainty in the ASI can be inferred. This method is based on Dirichlet-based mixtures of skew-Student distributions. We illustrate its use on data from a Bayesian model for prediction of the recurrence time of prostate cancer. We believe that this approach is generally appropriate for most problems, where it is infeasible to derive the explicit distribution of the samples of $j(x,y)$, though the precise modelling parameters may need adjustment to suit particular cases.</summary></entry><entry><title type="html">Asymmetry models and separability for multi-way contingency tables with ordinal categories</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Asymmetrymodelsandseparabilityformultiwaycontingencytableswithordinalcategories.html" rel="alternate" type="text/html" title="Asymmetry models and separability for multi-way contingency tables with ordinal categories" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Asymmetrymodelsandseparabilityformultiwaycontingencytableswithordinalcategories</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Asymmetrymodelsandseparabilityformultiwaycontingencytableswithordinalcategories.html">&lt;p&gt;In this paper, we propose a model that indicates the asymmetry structure for cell probabilities in multivariate contingency tables with the same ordered categories. The proposed model is the closest to the symmetry model in terms of the $f$-divergence under certain conditions and incorporates various asymmetry models as special cases, including existing models. We elucidate the relationship between the proposed model and conventional models from several aspects of divergence in $f$-divergence. Furthermore, we provide theorems showing that the symmetry model can be decomposed into two or more models, each imposing less restrictive parameter constraints than the symmetry condition. We also discuss the properties of goodness-of-fit statistics, particularly focusing on the likelihood ratio test statistics and Wald test statistics. Finally, we summarize the proposed model and discuss some problems and future work.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12157&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hisaya Okahara, Kouji Tahata</name></author><category term="stat.ME" /><summary type="html">In this paper, we propose a model that indicates the asymmetry structure for cell probabilities in multivariate contingency tables with the same ordered categories. The proposed model is the closest to the symmetry model in terms of the $f$-divergence under certain conditions and incorporates various asymmetry models as special cases, including existing models. We elucidate the relationship between the proposed model and conventional models from several aspects of divergence in $f$-divergence. Furthermore, we provide theorems showing that the symmetry model can be decomposed into two or more models, each imposing less restrictive parameter constraints than the symmetry condition. We also discuss the properties of goodness-of-fit statistics, particularly focusing on the likelihood ratio test statistics and Wald test statistics. Finally, we summarize the proposed model and discuss some problems and future work.</summary></entry><entry><title type="html">Average treatment effect on the treated, under lack of positivity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Averagetreatmenteffectonthetreatedunderlackofpositivity.html" rel="alternate" type="text/html" title="Average treatment effect on the treated, under lack of positivity" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Averagetreatmenteffectonthetreatedunderlackofpositivity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Averagetreatmenteffectonthetreatedunderlackofpositivity.html">&lt;p&gt;The use of propensity score (PS) methods has become ubiquitous in causal inference. At the heart of these methods is the positivity assumption. Violation of the positivity assumption leads to the presence of extreme PS weights when estimating average causal effects of interest, such as the average treatment effect (ATE) or the average treatment effect on the treated (ATT), which renders invalid related statistical inference. To circumvent this issue, trimming or truncating the extreme estimated PSs have been widely used. However, these methods require that we specify a priori a threshold and sometimes an additional smoothing parameter. While there are a number of methods dealing with the lack of positivity when estimating ATE, surprisingly there is no much effort in the same issue for ATT. In this paper, we first review widely used methods, such as trimming and truncation in ATT. We emphasize the underlying intuition behind these methods to better understand their applications and highlight their main limitations. Then, we argue that the current methods simply target estimands that are scaled ATT (and thus move the goalpost to a different target of interest), where we specify the scale and the target populations. We further propose a PS weight-based alternative for the average causal effect on the treated, called overlap weighted average treatment effect on the treated (OWATT). The appeal of our proposed method lies in its ability to obtain similar or even better results than trimming and truncation while relaxing the constraint to choose a priori a threshold (or even specify a smoothing parameter). The performance of the proposed method is illustrated via a series of Monte Carlo simulations and a data analysis on racial disparities in health care expenditures.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.01334&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yi Liu, Huiyue Li, Yunji Zhou, Roland Matsouaka</name></author><category term="stat.ME" /><summary type="html">The use of propensity score (PS) methods has become ubiquitous in causal inference. At the heart of these methods is the positivity assumption. Violation of the positivity assumption leads to the presence of extreme PS weights when estimating average causal effects of interest, such as the average treatment effect (ATE) or the average treatment effect on the treated (ATT), which renders invalid related statistical inference. To circumvent this issue, trimming or truncating the extreme estimated PSs have been widely used. However, these methods require that we specify a priori a threshold and sometimes an additional smoothing parameter. While there are a number of methods dealing with the lack of positivity when estimating ATE, surprisingly there is no much effort in the same issue for ATT. In this paper, we first review widely used methods, such as trimming and truncation in ATT. We emphasize the underlying intuition behind these methods to better understand their applications and highlight their main limitations. Then, we argue that the current methods simply target estimands that are scaled ATT (and thus move the goalpost to a different target of interest), where we specify the scale and the target populations. We further propose a PS weight-based alternative for the average causal effect on the treated, called overlap weighted average treatment effect on the treated (OWATT). The appeal of our proposed method lies in its ability to obtain similar or even better results than trimming and truncation while relaxing the constraint to choose a priori a threshold (or even specify a smoothing parameter). The performance of the proposed method is illustrated via a series of Monte Carlo simulations and a data analysis on racial disparities in health care expenditures.</summary></entry><entry><title type="html">Causal Customer Churn Analysis with Low-rank Tensor Block Hazard Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/CausalCustomerChurnAnalysiswithLowrankTensorBlockHazardModel.html" rel="alternate" type="text/html" title="Causal Customer Churn Analysis with Low-rank Tensor Block Hazard Model" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/CausalCustomerChurnAnalysiswithLowrankTensorBlockHazardModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/CausalCustomerChurnAnalysiswithLowrankTensorBlockHazardModel.html">&lt;p&gt;This study introduces an innovative method for analyzing the impact of various interventions on customer churn, using the potential outcomes framework. We present a new causal model, the tensorized latent factor block hazard model, which incorporates tensor completion methods for a principled causal analysis of customer churn. A crucial element of our approach is the formulation of a 1-bit tensor completion for the parameter tensor. This captures hidden customer characteristics and temporal elements from churn records, effectively addressing the binary nature of churn data and its time-monotonic trends. Our model also uniquely categorizes interventions by their similar impacts, enhancing the precision and practicality of implementing customer retention strategies. For computational efficiency, we apply a projected gradient descent algorithm combined with spectral clustering. We lay down the theoretical groundwork for our model, including its non-asymptotic properties. The efficacy and superiority of our model are further validated through comprehensive experiments on both simulated and real-world applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11377&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chenyin Gao, Zhiming Zhang, Shu Yang</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">This study introduces an innovative method for analyzing the impact of various interventions on customer churn, using the potential outcomes framework. We present a new causal model, the tensorized latent factor block hazard model, which incorporates tensor completion methods for a principled causal analysis of customer churn. A crucial element of our approach is the formulation of a 1-bit tensor completion for the parameter tensor. This captures hidden customer characteristics and temporal elements from churn records, effectively addressing the binary nature of churn data and its time-monotonic trends. Our model also uniquely categorizes interventions by their similar impacts, enhancing the precision and practicality of implementing customer retention strategies. For computational efficiency, we apply a projected gradient descent algorithm combined with spectral clustering. We lay down the theoretical groundwork for our model, including its non-asymptotic properties. The efficacy and superiority of our model are further validated through comprehensive experiments on both simulated and real-world applications.</summary></entry><entry><title type="html">Comparing predictive ability in presence of instability over a very short time</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Comparingpredictiveabilityinpresenceofinstabilityoveraveryshorttime.html" rel="alternate" type="text/html" title="Comparing predictive ability in presence of instability over a very short time" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Comparingpredictiveabilityinpresenceofinstabilityoveraveryshorttime</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Comparingpredictiveabilityinpresenceofinstabilityoveraveryshorttime.html">&lt;p&gt;We consider forecast comparison in the presence of instability when this affects only a short period of time. We demonstrate that global tests do not perform well in this case, as they were not designed to capture very short-lived instabilities, and their power vanishes altogether when the magnitude of the shock is very large. We then discuss and propose approaches that are more suitable to detect such situations, such as nonparametric methods (S test or MAX procedure). We illustrate these results in different Monte Carlo exercises and in evaluating the nowcast of the quarterly US nominal GDP from the Survey of Professional Forecasters (SPF) against a naive benchmark of no growth, over the period that includes the GDP instability brought by the Covid-19 crisis. We recommend that the forecaster should not pool the sample, but exclude the short periods of high local instability from the evaluation exercise.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11954&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fabrizio Iacone, Luca Rossini, Andrea Viselli</name></author><category term="stat.AP" /><summary type="html">We consider forecast comparison in the presence of instability when this affects only a short period of time. We demonstrate that global tests do not perform well in this case, as they were not designed to capture very short-lived instabilities, and their power vanishes altogether when the magnitude of the shock is very large. We then discuss and propose approaches that are more suitable to detect such situations, such as nonparametric methods (S test or MAX procedure). We illustrate these results in different Monte Carlo exercises and in evaluating the nowcast of the quarterly US nominal GDP from the Survey of Professional Forecasters (SPF) against a naive benchmark of no growth, over the period that includes the GDP instability brought by the Covid-19 crisis. We recommend that the forecaster should not pool the sample, but exclude the short periods of high local instability from the evaluation exercise.</summary></entry><entry><title type="html">Delivery strategies to improve piglets exposure to oral antibiotics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Deliverystrategiestoimprovepigletsexposuretooralantibiotics.html" rel="alternate" type="text/html" title="Delivery strategies to improve piglets exposure to oral antibiotics" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Deliverystrategiestoimprovepigletsexposuretooralantibiotics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Deliverystrategiestoimprovepigletsexposuretooralantibiotics.html">&lt;p&gt;The widespread practice of delivering antibiotics through drinking water to livestock leads to considerable variability in exposure levels among animals, raising concerns regarding disease outbreaks and the emergence of antibiotic resistance. This variability is primarily driven by three pivotal factors: fluctuations in drug concentration within water pipes, variances in drinking behavior among animals, and differences in individual pharmacokinetic parameters. This article introduces an approach aimed at improving medication distribution by customizing it according to the drinking patterns of pigs, without escalating the medication dose. As examples, we demonstrate that incorporating the drinking behavior into the delivery of amoxicillin results in an increase in the percentage of piglets reaching an AUC/MIC ratio greater than 25h. Specifically, with Pasteurella multocida, the percentage rises from 30% to at least 60%, while with Actinobacillus pleuropneumoniae, it increases from 20% to more than 70%.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11004&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Noslen Hernández , Béatrice B. Roques , Marlène Z. Lacroix , Didier Concordet</name></author><category term="stat.AP" /><summary type="html">The widespread practice of delivering antibiotics through drinking water to livestock leads to considerable variability in exposure levels among animals, raising concerns regarding disease outbreaks and the emergence of antibiotic resistance. This variability is primarily driven by three pivotal factors: fluctuations in drug concentration within water pipes, variances in drinking behavior among animals, and differences in individual pharmacokinetic parameters. This article introduces an approach aimed at improving medication distribution by customizing it according to the drinking patterns of pigs, without escalating the medication dose. As examples, we demonstrate that incorporating the drinking behavior into the delivery of amoxicillin results in an increase in the percentage of piglets reaching an AUC/MIC ratio greater than 25h. Specifically, with Pasteurella multocida, the percentage rises from 30% to at least 60%, while with Actinobacillus pleuropneumoniae, it increases from 20% to more than 70%.</summary></entry><entry><title type="html">Derivative-informed neural operator acceleration of geometric MCMC for infinite-dimensional Bayesian inverse problems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/DerivativeinformedneuraloperatoraccelerationofgeometricMCMCforinfinitedimensionalBayesianinverseproblems.html" rel="alternate" type="text/html" title="Derivative-informed neural operator acceleration of geometric MCMC for infinite-dimensional Bayesian inverse problems" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/DerivativeinformedneuraloperatoraccelerationofgeometricMCMCforinfinitedimensionalBayesianinverseproblems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/DerivativeinformedneuraloperatoraccelerationofgeometricMCMCforinfinitedimensionalBayesianinverseproblems.html">&lt;p&gt;We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional Bayesian inverse problems (BIPs). While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires repeated computations of gradients and Hessians of the log-likelihood, which becomes prohibitive when the parameter-to-observable (PtO) map is defined through expensive-to-solve parametric partial differential equations (PDEs). We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal exploits fast surrogate predictions of the log-likelihood and, simultaneously, its gradient and Hessian. To achieve a substantial speedup, the surrogate must accurately approximate the PtO map and its Jacobian, which often demands a prohibitively large number of PtO map samples via conventional operator learning methods. In this work, we present an extension of derivative-informed operator learning [O’Leary-Roseberry et al., J. Comput. Phys., 496 (2024)] that uses joint samples of the PtO map and its Jacobian. This leads to derivative-informed neural operator (DINO) surrogates that accurately predict the observables and posterior local geometry at a significantly lower training cost than conventional methods. Cost and error analysis for reduced basis DINO surrogates are provided. Numerical studies demonstrate that DINO-driven MCMC generates effective posterior samples 3–9 times faster than geometric MCMC and 60–97 times faster than prior geometry-based MCMC. Furthermore, the training cost of DINO surrogates breaks even compared to geometric MCMC after just 10–25 effective posterior samples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.08220&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lianghao Cao, Thomas O&apos;Leary-Roseberry, Omar Ghattas</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional Bayesian inverse problems (BIPs). While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires repeated computations of gradients and Hessians of the log-likelihood, which becomes prohibitive when the parameter-to-observable (PtO) map is defined through expensive-to-solve parametric partial differential equations (PDEs). We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal exploits fast surrogate predictions of the log-likelihood and, simultaneously, its gradient and Hessian. To achieve a substantial speedup, the surrogate must accurately approximate the PtO map and its Jacobian, which often demands a prohibitively large number of PtO map samples via conventional operator learning methods. In this work, we present an extension of derivative-informed operator learning [O’Leary-Roseberry et al., J. Comput. Phys., 496 (2024)] that uses joint samples of the PtO map and its Jacobian. This leads to derivative-informed neural operator (DINO) surrogates that accurately predict the observables and posterior local geometry at a significantly lower training cost than conventional methods. Cost and error analysis for reduced basis DINO surrogates are provided. Numerical studies demonstrate that DINO-driven MCMC generates effective posterior samples 3–9 times faster than geometric MCMC and 60–97 times faster than prior geometry-based MCMC. Furthermore, the training cost of DINO surrogates breaks even compared to geometric MCMC after just 10–25 effective posterior samples.</summary></entry><entry><title type="html">Distributed Tensor Principal Component Analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/DistributedTensorPrincipalComponentAnalysis.html" rel="alternate" type="text/html" title="Distributed Tensor Principal Component Analysis" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/DistributedTensorPrincipalComponentAnalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/DistributedTensorPrincipalComponentAnalysis.html">&lt;p&gt;As tensors become widespread in modern data analysis, Tucker low-rank Principal Component Analysis (PCA) has become essential for dimensionality reduction and structural discovery in tensor datasets. Motivated by the common scenario where large-scale tensors are distributed across diverse geographic locations, this paper investigates tensor PCA within a distributed framework where direct data pooling is impractical.
  We offer a comprehensive analysis of three specific scenarios in distributed Tensor PCA: a homogeneous setting in which tensors at various locations are generated from a single noise-affected model; a heterogeneous setting where tensors at different locations come from distinct models but share some principal components, aiming to improve estimation across all locations; and a targeted heterogeneous setting, designed to boost estimation accuracy at a specific location with limited samples by utilizing transferred knowledge from other sites with ample data.
  We introduce novel estimation methods tailored to each scenario, establish statistical guarantees, and develop distributed inference techniques to construct confidence regions. Our theoretical findings demonstrate that these distributed methods achieve sharp rates of accuracy by efficiently aggregating shared information across different tensors, while maintaining reasonable communication costs. Empirical validation through simulations and real-world data applications highlights the advantages of our approaches, particularly in managing heterogeneous tensor data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11681&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elynn Chen, Xi Chen, Wenbo Jing, Yichen Zhang</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">As tensors become widespread in modern data analysis, Tucker low-rank Principal Component Analysis (PCA) has become essential for dimensionality reduction and structural discovery in tensor datasets. Motivated by the common scenario where large-scale tensors are distributed across diverse geographic locations, this paper investigates tensor PCA within a distributed framework where direct data pooling is impractical. We offer a comprehensive analysis of three specific scenarios in distributed Tensor PCA: a homogeneous setting in which tensors at various locations are generated from a single noise-affected model; a heterogeneous setting where tensors at different locations come from distinct models but share some principal components, aiming to improve estimation across all locations; and a targeted heterogeneous setting, designed to boost estimation accuracy at a specific location with limited samples by utilizing transferred knowledge from other sites with ample data. We introduce novel estimation methods tailored to each scenario, establish statistical guarantees, and develop distributed inference techniques to construct confidence regions. Our theoretical findings demonstrate that these distributed methods achieve sharp rates of accuracy by efficiently aggregating shared information across different tensors, while maintaining reasonable communication costs. Empirical validation through simulations and real-world data applications highlights the advantages of our approaches, particularly in managing heterogeneous tensor data.</summary></entry><entry><title type="html">Distribution-in-distribution-out Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/DistributionindistributionoutRegression.html" rel="alternate" type="text/html" title="Distribution-in-distribution-out Regression" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/DistributionindistributionoutRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/DistributionindistributionoutRegression.html">&lt;p&gt;Regression analysis with probability measures as input predictors and output response has recently drawn great attention. However, it is challenging to handle multiple input probability measures due to the non-flat Riemannian geometry of the Wasserstein space, hindering the definition of arithmetic operations, hence additive linear structure is not well-defined. In this work, a distribution-in-distribution-out regression model is proposed by introducing parallel transport to achieve provable commutativity and additivity of newly defined arithmetic operations in Wasserstein space. The appealing properties of the DIDO regression model can serve a foundation for model estimation, prediction, and inference. Specifically, the Fr&apos;echet least squares estimator is employed to obtain the best linear unbiased estimate, supported by the newly established Fr&apos;echet Gauss-Markov Theorem. Furthermore, we investigate a special case when predictors and response are all univariate Gaussian measures, leading to a simple close-form solution of linear model coefficients and $R^2$ metric. A simulation study and real case study in intraoperative cardiac output prediction are performed to evaluate the performance of the proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11626&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xiaoyu Chen , Mengfan Fu ,  Yujing ,  Huang, Xinwei Deng</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Regression analysis with probability measures as input predictors and output response has recently drawn great attention. However, it is challenging to handle multiple input probability measures due to the non-flat Riemannian geometry of the Wasserstein space, hindering the definition of arithmetic operations, hence additive linear structure is not well-defined. In this work, a distribution-in-distribution-out regression model is proposed by introducing parallel transport to achieve provable commutativity and additivity of newly defined arithmetic operations in Wasserstein space. The appealing properties of the DIDO regression model can serve a foundation for model estimation, prediction, and inference. Specifically, the Fr&apos;echet least squares estimator is employed to obtain the best linear unbiased estimate, supported by the newly established Fr&apos;echet Gauss-Markov Theorem. Furthermore, we investigate a special case when predictors and response are all univariate Gaussian measures, leading to a simple close-form solution of linear model coefficients and $R^2$ metric. A simulation study and real case study in intraoperative cardiac output prediction are performed to evaluate the performance of the proposed method.</summary></entry><entry><title type="html">Dynamic Contextual Pricing with Doubly Non-Parametric Random Utility Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/DynamicContextualPricingwithDoublyNonParametricRandomUtilityModels.html" rel="alternate" type="text/html" title="Dynamic Contextual Pricing with Doubly Non-Parametric Random Utility Models" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/DynamicContextualPricingwithDoublyNonParametricRandomUtilityModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/DynamicContextualPricingwithDoublyNonParametricRandomUtilityModels.html">&lt;p&gt;In the evolving landscape of digital commerce, adaptive dynamic pricing strategies are essential for gaining a competitive edge. This paper introduces novel {\em doubly nonparametric random utility models} that eschew traditional parametric assumptions used in estimating consumer demand’s mean utility function and noise distribution. Existing nonparametric methods like multi-scale {\em Distributional Nearest Neighbors (DNN and TDNN)}, initially designed for offline regression, face challenges in dynamic online pricing due to design limitations, such as the indirect observability of utility-related variables and the absence of uniform convergence guarantees. We address these challenges with innovative population equations that facilitate nonparametric estimation within decision-making frameworks and establish new analytical results on the uniform convergence rates of DNN and TDNN, enhancing their applicability in dynamic environments.
  Our theoretical analysis confirms that the statistical learning rates for the mean utility function and noise distribution are minimax optimal. We also derive a regret bound that illustrates the critical interaction between model dimensionality and noise distribution smoothness, deepening our understanding of dynamic pricing under varied market conditions. These contributions offer substantial theoretical insights and practical tools for implementing effective, data-driven pricing strategies, advancing the theoretical framework of pricing models and providing robust methodologies for navigating the complexities of modern markets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06866&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elynn Chen, Xi Chen, Lan Gao, Jiayu Li</name></author><category term="stat.ME" /><summary type="html">In the evolving landscape of digital commerce, adaptive dynamic pricing strategies are essential for gaining a competitive edge. This paper introduces novel {\em doubly nonparametric random utility models} that eschew traditional parametric assumptions used in estimating consumer demand’s mean utility function and noise distribution. Existing nonparametric methods like multi-scale {\em Distributional Nearest Neighbors (DNN and TDNN)}, initially designed for offline regression, face challenges in dynamic online pricing due to design limitations, such as the indirect observability of utility-related variables and the absence of uniform convergence guarantees. We address these challenges with innovative population equations that facilitate nonparametric estimation within decision-making frameworks and establish new analytical results on the uniform convergence rates of DNN and TDNN, enhancing their applicability in dynamic environments. Our theoretical analysis confirms that the statistical learning rates for the mean utility function and noise distribution are minimax optimal. We also derive a regret bound that illustrates the critical interaction between model dimensionality and noise distribution smoothness, deepening our understanding of dynamic pricing under varied market conditions. These contributions offer substantial theoretical insights and practical tools for implementing effective, data-driven pricing strategies, advancing the theoretical framework of pricing models and providing robust methodologies for navigating the complexities of modern markets.</summary></entry><entry><title type="html">Efficient Shapley Performance Attribution for Least-Squares Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/EfficientShapleyPerformanceAttributionforLeastSquaresRegression.html" rel="alternate" type="text/html" title="Efficient Shapley Performance Attribution for Least-Squares Regression" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/EfficientShapleyPerformanceAttributionforLeastSquaresRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/EfficientShapleyPerformanceAttributionforLeastSquaresRegression.html">&lt;p&gt;We consider the performance of a least-squares regression model, as judged by out-of-sample $R^2$. Shapley values give a fair attribution of the performance of a model to its input features, taking into account interdependencies between features. Evaluating the Shapley values exactly requires solving a number of regression problems that is exponential in the number of features, so a Monte Carlo-type approximation is typically used. We focus on the special case of least-squares regression models, where several tricks can be used to compute and evaluate regression models efficiently. These tricks give a substantial speed up, allowing many more Monte Carlo samples to be evaluated, achieving better accuracy. We refer to our method as least-squares Shapley performance attribution (LS-SPA), and describe our open-source implementation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.19245&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Logan Bell, Nikhil Devanathan, Stephen Boyd</name></author><category term="stat.CO" /><summary type="html">We consider the performance of a least-squares regression model, as judged by out-of-sample $R^2$. Shapley values give a fair attribution of the performance of a model to its input features, taking into account interdependencies between features. Evaluating the Shapley values exactly requires solving a number of regression problems that is exponential in the number of features, so a Monte Carlo-type approximation is typically used. We focus on the special case of least-squares regression models, where several tricks can be used to compute and evaluate regression models efficiently. These tricks give a substantial speed up, allowing many more Monte Carlo samples to be evaluated, achieving better accuracy. We refer to our method as least-squares Shapley performance attribution (LS-SPA), and describe our open-source implementation.</summary></entry><entry><title type="html">Estimating optimal tailored active surveillance strategy under interval censoring</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Estimatingoptimaltailoredactivesurveillancestrategyunderintervalcensoring.html" rel="alternate" type="text/html" title="Estimating optimal tailored active surveillance strategy under interval censoring" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Estimatingoptimaltailoredactivesurveillancestrategyunderintervalcensoring</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Estimatingoptimaltailoredactivesurveillancestrategyunderintervalcensoring.html">&lt;p&gt;Active surveillance (AS) using repeated biopsies to monitor disease progression has been a popular alternative to immediate surgical intervention in cancer care. However, a biopsy procedure is invasive and sometimes leads to severe side effects of infection and bleeding. To reduce the burden of repeated surveillance biopsies, biomarker-assistant decision rules are sought to replace the fix-for-all regimen with tailored biopsy intensity for individual patients. Constructing or evaluating such decision rules is challenging. The key AS outcome is often ascertained subject to interval censoring. Furthermore, patients will discontinue their participation in the AS study once they receive a positive surveillance biopsy. Thus, patient dropout is affected by the outcomes of these biopsies. In this work, we propose a nonparametric kernel-based method to estimate the true positive rates (TPRs) and true negative rates (TNRs) of a tailored AS strategy, accounting for interval censoring and immediate dropouts. Based on these estimates, we develop a weighted classification framework to estimate the optimal tailored AS strategy and further incorporate the cost-benefit ratio for cost-effectiveness in medical decision-making. Theoretically, we provide a uniform generalization error bound of the derived AS strategy accommodating all possible trade-offs between TPRs and TNRs. Simulation and application to a prostate cancer surveillance study show the superiority of the proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11720&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Muxuan Liang, Yingqi Zhao, Daniel W. Lin, Matthew Cooperberg, Yingye Zheng</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Active surveillance (AS) using repeated biopsies to monitor disease progression has been a popular alternative to immediate surgical intervention in cancer care. However, a biopsy procedure is invasive and sometimes leads to severe side effects of infection and bleeding. To reduce the burden of repeated surveillance biopsies, biomarker-assistant decision rules are sought to replace the fix-for-all regimen with tailored biopsy intensity for individual patients. Constructing or evaluating such decision rules is challenging. The key AS outcome is often ascertained subject to interval censoring. Furthermore, patients will discontinue their participation in the AS study once they receive a positive surveillance biopsy. Thus, patient dropout is affected by the outcomes of these biopsies. In this work, we propose a nonparametric kernel-based method to estimate the true positive rates (TPRs) and true negative rates (TNRs) of a tailored AS strategy, accounting for interval censoring and immediate dropouts. Based on these estimates, we develop a weighted classification framework to estimate the optimal tailored AS strategy and further incorporate the cost-benefit ratio for cost-effectiveness in medical decision-making. Theoretically, we provide a uniform generalization error bound of the derived AS strategy accommodating all possible trade-offs between TPRs and TNRs. Simulation and application to a prostate cancer surveillance study show the superiority of the proposed method.</summary></entry><entry><title type="html">Estimations of the Local Conditional Tail Average Treatment Effect</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/EstimationsoftheLocalConditionalTailAverageTreatmentEffect.html" rel="alternate" type="text/html" title="Estimations of the Local Conditional Tail Average Treatment Effect" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/EstimationsoftheLocalConditionalTailAverageTreatmentEffect</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/EstimationsoftheLocalConditionalTailAverageTreatmentEffect.html">&lt;p&gt;The conditional tail average treatment effect (CTATE) is defined as a difference between the conditional tail expectations of potential outcomes, which can capture heterogeneity and deliver aggregated local information on treatment effects over different quantile levels and is closely related to the notion of second-order stochastic dominance and the Lorenz curve. These properties render it a valuable tool for policy evaluation. In this paper, we study estimation of the CTATE locally for a group of compliers (local CTATE or LCTATE) under the two-sided noncompliance framework. We consider a semiparametric treatment effect framework under endogeneity for the LCTATE estimation using a newly introduced class of consistent loss functions jointly for the conditional tail expectation and quantile. We establish the asymptotic theory of our proposed LCTATE estimator and provide an efficient algorithm for its implementation. We then apply the method to evaluate the effects of participating in programs under the Job Training Partnership Act in the US.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2109.08793&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Le-Yu Chen, Yu-Min Yen</name></author><category term="stat.AP" /><summary type="html">The conditional tail average treatment effect (CTATE) is defined as a difference between the conditional tail expectations of potential outcomes, which can capture heterogeneity and deliver aggregated local information on treatment effects over different quantile levels and is closely related to the notion of second-order stochastic dominance and the Lorenz curve. These properties render it a valuable tool for policy evaluation. In this paper, we study estimation of the CTATE locally for a group of compliers (local CTATE or LCTATE) under the two-sided noncompliance framework. We consider a semiparametric treatment effect framework under endogeneity for the LCTATE estimation using a newly introduced class of consistent loss functions jointly for the conditional tail expectation and quantile. We establish the asymptotic theory of our proposed LCTATE estimator and provide an efficient algorithm for its implementation. We then apply the method to evaluate the effects of participating in programs under the Job Training Partnership Act in the US.</summary></entry><entry><title type="html">Euclidean mirrors and first-order changepoints in network time series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Euclideanmirrorsandfirstorderchangepointsinnetworktimeseries.html" rel="alternate" type="text/html" title="Euclidean mirrors and first-order changepoints in network time series" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Euclideanmirrorsandfirstorderchangepointsinnetworktimeseries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Euclideanmirrorsandfirstorderchangepointsinnetworktimeseries.html">&lt;p&gt;We describe a model for a network time series whose evolution is governed by an underlying stochastic process, known as the latent position process, in which network evolution can be represented in Euclidean space by a curve, called the Euclidean mirror. We define the notion of a first-order changepoint for a time series of networks, and construct a family of latent position process networks with underlying first-order changepoints. We prove that a spectral estimate of the associated Euclidean mirror localizes these changepoints, even when the graph distribution evolves continuously, but at a rate that changes. Simulated and real data examples on organoid networks show that this localization captures empirically significant shifts in network evolution.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11111&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tianyi Chen, Zachary Lubberts, Avanti Athreya, Youngser Park, Carey E. Priebe</name></author><category term="stat.ME" /><summary type="html">We describe a model for a network time series whose evolution is governed by an underlying stochastic process, known as the latent position process, in which network evolution can be represented in Euclidean space by a curve, called the Euclidean mirror. We define the notion of a first-order changepoint for a time series of networks, and construct a family of latent position process networks with underlying first-order changepoints. We prove that a spectral estimate of the associated Euclidean mirror localizes these changepoints, even when the graph distribution evolves continuously, but at a rate that changes. Simulated and real data examples on organoid networks show that this localization captures empirically significant shifts in network evolution.</summary></entry><entry><title type="html">General bounds on the quality of Bayesian coresets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/GeneralboundsonthequalityofBayesiancoresets.html" rel="alternate" type="text/html" title="General bounds on the quality of Bayesian coresets" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/GeneralboundsonthequalityofBayesiancoresets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/GeneralboundsonthequalityofBayesiancoresets.html">&lt;p&gt;Bayesian coresets speed up posterior inference in the large-scale data regime by approximating the full-data log-likelihood function with a surrogate log-likelihood based on a small, weighted subset of the data. But while Bayesian coresets and methods for construction are applicable in a wide range of models, existing theoretical analysis of the posterior inferential error incurred by coreset approximations only apply in restrictive settings – i.e., exponential family models, or models with strong log-concavity and smoothness assumptions. This work presents general upper and lower bounds on the Kullback-Leibler (KL) divergence of coreset approximations that reflect the full range of applicability of Bayesian coresets. The lower bounds require only mild model assumptions typical of Bayesian asymptotic analyses, while the upper bounds require the log-likelihood functions to satisfy a generalized subexponentiality criterion that is weaker than conditions used in earlier work. The lower bounds are applied to obtain fundamental limitations on the quality of coreset approximations, and to provide a theoretical explanation for the previously-observed poor empirical performance of importance sampling-based construction methods. The upper bounds are used to analyze the performance of recent subsample-optimize methods. The flexibility of the theory is demonstrated in validation experiments involving multimodal, unidentifiable, heavy-tailed Bayesian posterior distributions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11780&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Trevor Campbell</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Bayesian coresets speed up posterior inference in the large-scale data regime by approximating the full-data log-likelihood function with a surrogate log-likelihood based on a small, weighted subset of the data. But while Bayesian coresets and methods for construction are applicable in a wide range of models, existing theoretical analysis of the posterior inferential error incurred by coreset approximations only apply in restrictive settings – i.e., exponential family models, or models with strong log-concavity and smoothness assumptions. This work presents general upper and lower bounds on the Kullback-Leibler (KL) divergence of coreset approximations that reflect the full range of applicability of Bayesian coresets. The lower bounds require only mild model assumptions typical of Bayesian asymptotic analyses, while the upper bounds require the log-likelihood functions to satisfy a generalized subexponentiality criterion that is weaker than conditions used in earlier work. The lower bounds are applied to obtain fundamental limitations on the quality of coreset approximations, and to provide a theoretical explanation for the previously-observed poor empirical performance of importance sampling-based construction methods. The upper bounds are used to analyze the performance of recent subsample-optimize methods. The flexibility of the theory is demonstrated in validation experiments involving multimodal, unidentifiable, heavy-tailed Bayesian posterior distributions.</summary></entry><entry><title type="html">Generalized extremiles and risk measures of distorted random variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Generalizedextremilesandriskmeasuresofdistortedrandomvariables.html" rel="alternate" type="text/html" title="Generalized extremiles and risk measures of distorted random variables" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Generalizedextremilesandriskmeasuresofdistortedrandomvariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Generalizedextremilesandriskmeasuresofdistortedrandomvariables.html">&lt;p&gt;Quantiles, expectiles and extremiles can be seen as concepts defined via an optimization problem, where this optimization problem is driven by two important ingredients: the loss function as well as a distributional weight function. This leads to the formulation of a general class of functionals that contains next to the above concepts many interesting quantities, including also a subclass of distortion risks. The focus of the paper is on developing estimators for such functionals and to establish asymptotic consistency and asymptotic normality of these estimators. The advantage of the general framework is that it allows application to a very broad range of concepts, providing as such estimation tools and tools for statistical inference (for example for construction of confidence intervals) for all involved concepts. After developing the theory for the general functional we apply it to various settings, illustrating the broad applicability. In a real data example the developed tools are used in an analysis of natural disasters.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11248&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Dieter Debrauwer, Irène Gijbels, Klaus Herrmann</name></author><category term="stat.ME" /><summary type="html">Quantiles, expectiles and extremiles can be seen as concepts defined via an optimization problem, where this optimization problem is driven by two important ingredients: the loss function as well as a distributional weight function. This leads to the formulation of a general class of functionals that contains next to the above concepts many interesting quantities, including also a subclass of distortion risks. The focus of the paper is on developing estimators for such functionals and to establish asymptotic consistency and asymptotic normality of these estimators. The advantage of the general framework is that it allows application to a very broad range of concepts, providing as such estimation tools and tools for statistical inference (for example for construction of confidence intervals) for all involved concepts. After developing the theory for the general functional we apply it to various settings, illustrating the broad applicability. In a real data example the developed tools are used in an analysis of natural disasters.</summary></entry><entry><title type="html">Identification of Single-Treatment Effects in Factorial Experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/IdentificationofSingleTreatmentEffectsinFactorialExperiments.html" rel="alternate" type="text/html" title="Identification of Single-Treatment Effects in Factorial Experiments" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/IdentificationofSingleTreatmentEffectsinFactorialExperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/IdentificationofSingleTreatmentEffectsinFactorialExperiments.html">&lt;p&gt;Despite their cost, randomized controlled trials (RCTs) are widely regarded as gold-standard evidence in disciplines ranging from social science to medicine. In recent decades, researchers have increasingly sought to reduce the resource burden of repeated RCTs with factorial designs that simultaneously test multiple hypotheses, e.g. experiments that evaluate the effects of many medications or products simultaneously. Here I show that when multiple interventions are randomized in experiments, the effect any single intervention would have outside the experimental setting is not identified absent heroic assumptions, even if otherwise perfectly realistic conditions are achieved. This happens because single-treatment effects involve a counterfactual world with a single focal intervention, allowing other variables to take their natural values (which may be confounded or modified by the focal intervention). In contrast, observational studies and factorial experiments provide information about potential-outcome distributions with zero and multiple interventions, respectively. In this paper, I formalize sufficient conditions for the identifiability of those isolated quantities. I show that researchers who rely on this type of design have to justify either linearity of functional forms or – in the nonparametric case – specify with Directed Acyclic Graphs how variables are related in the real world. Finally, I develop nonparametric sharp bounds – i.e., maximally informative best-/worst-case estimates consistent with limited RCT data – that show when extrapolations about effect signs are empirically justified. These new results are illustrated with simulated data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09797&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Guilherme Duarte</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.OT" /><summary type="html">Despite their cost, randomized controlled trials (RCTs) are widely regarded as gold-standard evidence in disciplines ranging from social science to medicine. In recent decades, researchers have increasingly sought to reduce the resource burden of repeated RCTs with factorial designs that simultaneously test multiple hypotheses, e.g. experiments that evaluate the effects of many medications or products simultaneously. Here I show that when multiple interventions are randomized in experiments, the effect any single intervention would have outside the experimental setting is not identified absent heroic assumptions, even if otherwise perfectly realistic conditions are achieved. This happens because single-treatment effects involve a counterfactual world with a single focal intervention, allowing other variables to take their natural values (which may be confounded or modified by the focal intervention). In contrast, observational studies and factorial experiments provide information about potential-outcome distributions with zero and multiple interventions, respectively. In this paper, I formalize sufficient conditions for the identifiability of those isolated quantities. I show that researchers who rely on this type of design have to justify either linearity of functional forms or – in the nonparametric case – specify with Directed Acyclic Graphs how variables are related in the real world. Finally, I develop nonparametric sharp bounds – i.e., maximally informative best-/worst-case estimates consistent with limited RCT data – that show when extrapolations about effect signs are empirically justified. These new results are illustrated with simulated data.</summary></entry><entry><title type="html">Improving Ego-Cluster for Network Effect Measurement</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/ImprovingEgoClusterforNetworkEffectMeasurement.html" rel="alternate" type="text/html" title="Improving Ego-Cluster for Network Effect Measurement" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/ImprovingEgoClusterforNetworkEffectMeasurement</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/ImprovingEgoClusterforNetworkEffectMeasurement.html">&lt;p&gt;The network effect, wherein one user’s activity impacts another user, is common in social network platforms. Many new features in social networks are specifically designed to create a network effect, enhancing user engagement. For instance, content creators tend to produce more when their articles and posts receive positive feedback from followers. This paper discusses a new cluster-level experimentation methodology for measuring creator-side metrics in the context of A/B experiments. The methodology is designed to address cases where the experiment randomization unit and the metric measurement unit differ. It is a crucial part of LinkedIn’s overall strategy to foster a robust creator community and ecosystem. The method is developed based on widely-cited research at LinkedIn but significantly improves the efficiency and flexibility of the clustering algorithm. This improvement results in a stronger capability for measuring creator-side metrics and an increased velocity for creator-related experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.05945&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wentao Su, Weitao Duan</name></author><category term="stat.ME" /><summary type="html">The network effect, wherein one user’s activity impacts another user, is common in social network platforms. Many new features in social networks are specifically designed to create a network effect, enhancing user engagement. For instance, content creators tend to produce more when their articles and posts receive positive feedback from followers. This paper discusses a new cluster-level experimentation methodology for measuring creator-side metrics in the context of A/B experiments. The methodology is designed to address cases where the experiment randomization unit and the metric measurement unit differ. It is a crucial part of LinkedIn’s overall strategy to foster a robust creator community and ecosystem. The method is developed based on widely-cited research at LinkedIn but significantly improves the efficiency and flexibility of the clustering algorithm. This improvement results in a stronger capability for measuring creator-side metrics and an increased velocity for creator-related experiments.</summary></entry><entry><title type="html">Inference with non-differentiable surrogate loss in a general high-dimensional classification framework</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Inferencewithnondifferentiablesurrogatelossinageneralhighdimensionalclassificationframework.html" rel="alternate" type="text/html" title="Inference with non-differentiable surrogate loss in a general high-dimensional classification framework" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Inferencewithnondifferentiablesurrogatelossinageneralhighdimensionalclassificationframework</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Inferencewithnondifferentiablesurrogatelossinageneralhighdimensionalclassificationframework.html">&lt;p&gt;Penalized empirical risk minimization with a surrogate loss function is often used to derive a high-dimensional linear decision rule in classification problems. Although much of the literature focuses on the generalization error, there is a lack of valid inference procedures to identify the driving factors of the estimated decision rule, especially when the surrogate loss is non-differentiable. In this work, we propose a kernel-smoothed decorrelated score to construct hypothesis testing and interval estimations for the linear decision rule estimated using a piece-wise linear surrogate loss, which has a discontinuous gradient and non-regular Hessian. Specifically, we adopt kernel approximations to smooth the discontinuous gradient near discontinuity points and approximate the non-regular Hessian of the surrogate loss. In applications where additional nuisance parameters are involved, we propose a novel cross-fitted version to accommodate flexible nuisance estimates and kernel approximations. We establish the limiting distribution of the kernel-smoothed decorrelated score and its cross-fitted version in a high-dimensional setup. Simulation and real data analysis are conducted to demonstrate the validity and superiority of the proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11723&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Muxuan Liang, Yang Ning, Maureen A Smith, Ying-Qi Zhao</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Penalized empirical risk minimization with a surrogate loss function is often used to derive a high-dimensional linear decision rule in classification problems. Although much of the literature focuses on the generalization error, there is a lack of valid inference procedures to identify the driving factors of the estimated decision rule, especially when the surrogate loss is non-differentiable. In this work, we propose a kernel-smoothed decorrelated score to construct hypothesis testing and interval estimations for the linear decision rule estimated using a piece-wise linear surrogate loss, which has a discontinuous gradient and non-regular Hessian. Specifically, we adopt kernel approximations to smooth the discontinuous gradient near discontinuity points and approximate the non-regular Hessian of the surrogate loss. In applications where additional nuisance parameters are involved, we propose a novel cross-fitted version to accommodate flexible nuisance estimates and kernel approximations. We establish the limiting distribution of the kernel-smoothed decorrelated score and its cross-fitted version in a high-dimensional setup. Simulation and real data analysis are conducted to demonstrate the validity and superiority of the proposed method.</summary></entry><entry><title type="html">Linear Discriminant Regularized Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/LinearDiscriminantRegularizedRegression.html" rel="alternate" type="text/html" title="Linear Discriminant Regularized Regression" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/LinearDiscriminantRegularizedRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/LinearDiscriminantRegularizedRegression.html">&lt;p&gt;Linear Discriminant Analysis (LDA) is an important classification approach. Its simple linear form makes it easy to interpret and it is capable to handle multi-class responses. It is closely related to other classical multivariate statistical techniques, such as Fisher’s discriminant analysis, canonical correlation analysis and linear regression. In this paper we strengthen its connection to multivariate response regression by characterizing the explicit relationship between the discriminant directions and the regression coefficient matrix. This key characterization leads to a new regression-based multi-class classification procedure that is flexible enough to deploy any existing structured, regularized, and even non-parametric, regression methods. Moreover, our new formulation is amenable to analysis: we establish a general strategy of analyzing the excess misclassification risk of the proposed classifier for all aforementioned regression techniques. As applications, we provide complete theoretical guarantees for using the widely used $\ell_1$-regularization as well as for using the reduced-rank regression, neither of which has yet been fully analyzed in the LDA context. Our theoretical findings are corroborated by extensive simulation studies and real data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.14260&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xin Bing, Bingqing Li, Marten Wegkamp</name></author><category term="stat.ME" /><summary type="html">Linear Discriminant Analysis (LDA) is an important classification approach. Its simple linear form makes it easy to interpret and it is capable to handle multi-class responses. It is closely related to other classical multivariate statistical techniques, such as Fisher’s discriminant analysis, canonical correlation analysis and linear regression. In this paper we strengthen its connection to multivariate response regression by characterizing the explicit relationship between the discriminant directions and the regression coefficient matrix. This key characterization leads to a new regression-based multi-class classification procedure that is flexible enough to deploy any existing structured, regularized, and even non-parametric, regression methods. Moreover, our new formulation is amenable to analysis: we establish a general strategy of analyzing the excess misclassification risk of the proposed classifier for all aforementioned regression techniques. As applications, we provide complete theoretical guarantees for using the widely used $\ell_1$-regularization as well as for using the reduced-rank regression, neither of which has yet been fully analyzed in the LDA context. Our theoretical findings are corroborated by extensive simulation studies and real data analysis.</summary></entry><entry><title type="html">Morphological Prototyping for Unsupervised Slide Representation Learning in Computational Pathology</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/MorphologicalPrototypingforUnsupervisedSlideRepresentationLearninginComputationalPathology.html" rel="alternate" type="text/html" title="Morphological Prototyping for Unsupervised Slide Representation Learning in Computational Pathology" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/MorphologicalPrototypingforUnsupervisedSlideRepresentationLearninginComputationalPathology</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/MorphologicalPrototypingforUnsupervisedSlideRepresentationLearninginComputationalPathology.html">&lt;p&gt;Representation learning of pathology whole-slide images (WSIs) has been has primarily relied on weak supervision with Multiple Instance Learning (MIL). However, the slide representations resulting from this approach are highly tailored to specific clinical tasks, which limits their expressivity and generalization, particularly in scenarios with limited data. Instead, we hypothesize that morphological redundancy in tissue can be leveraged to build a task-agnostic slide representation in an unsupervised fashion. To this end, we introduce PANTHER, a prototype-based approach rooted in the Gaussian mixture model that summarizes the set of WSI patches into a much smaller set of morphological prototypes. Specifically, each patch is assumed to have been generated from a mixture distribution, where each mixture component represents a morphological exemplar. Utilizing the estimated mixture parameters, we then construct a compact slide representation that can be readily used for a wide range of downstream tasks. By performing an extensive evaluation of PANTHER on subtyping and survival tasks using 13 datasets, we show that 1) PANTHER outperforms or is on par with supervised MIL baselines and 2) the analysis of morphological prototypes brings new qualitative and quantitative insights into model interpretability.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11643&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrew H. Song, Richard J. Chen, Tong Ding, Drew F. K. Williamson, Guillaume Jaume, Faisal Mahmood</name></author><category term="stat.AP" /><summary type="html">Representation learning of pathology whole-slide images (WSIs) has been has primarily relied on weak supervision with Multiple Instance Learning (MIL). However, the slide representations resulting from this approach are highly tailored to specific clinical tasks, which limits their expressivity and generalization, particularly in scenarios with limited data. Instead, we hypothesize that morphological redundancy in tissue can be leveraged to build a task-agnostic slide representation in an unsupervised fashion. To this end, we introduce PANTHER, a prototype-based approach rooted in the Gaussian mixture model that summarizes the set of WSI patches into a much smaller set of morphological prototypes. Specifically, each patch is assumed to have been generated from a mixture distribution, where each mixture component represents a morphological exemplar. Utilizing the estimated mixture parameters, we then construct a compact slide representation that can be readily used for a wide range of downstream tasks. By performing an extensive evaluation of PANTHER on subtyping and survival tasks using 13 datasets, we show that 1) PANTHER outperforms or is on par with supervised MIL baselines and 2) the analysis of morphological prototypes brings new qualitative and quantitative insights into model interpretability.</summary></entry><entry><title type="html">Nearest Neighbors GParareal: Improving Scalability of Gaussian Processes for Parallel-in-Time Solvers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/NearestNeighborsGPararealImprovingScalabilityofGaussianProcessesforParallelinTimeSolvers.html" rel="alternate" type="text/html" title="Nearest Neighbors GParareal: Improving Scalability of Gaussian Processes for Parallel-in-Time Solvers" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/NearestNeighborsGPararealImprovingScalabilityofGaussianProcessesforParallelinTimeSolvers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/NearestNeighborsGPararealImprovingScalabilityofGaussianProcessesforParallelinTimeSolvers.html">&lt;p&gt;With the advent of supercomputers, multi-processor environments and parallel-in-time (PinT) algorithms offer ways to solve initial value problems for ordinary and partial differential equations (ODEs and PDEs) over long time intervals, a task often unfeasible with sequential solvers within realistic time frames. A recent approach, GParareal, combines Gaussian Processes with traditional PinT methodology (Parareal) to achieve faster parallel speed-ups. The method is known to outperform Parareal for low-dimensional ODEs and a limited number of computer cores. Here, we present Nearest Neighbors GParareal (nnGParareal), a novel data-enriched PinT integration algorithm. nnGParareal builds upon GParareal by improving its scalability properties for higher-dimensional systems and increased processor count. Through data reduction, the model complexity is reduced from cubic to log-linear in the sample size, yielding a fast and automated procedure to integrate initial value problems over long time intervals. First, we provide both an upper bound for the error and theoretical details on the speed-up benefits. Then, we empirically illustrate the superior performance of nnGParareal, compared to GParareal and Parareal, on nine different systems with unique features (e.g., stiff, chaotic, high-dimensional, or challenging-to-learn systems).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12182&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Guglielmo Gattiglio, Lyudmila Grigoryeva, Massimiliano Tamborrino</name></author><category term="stat.CO" /><summary type="html">With the advent of supercomputers, multi-processor environments and parallel-in-time (PinT) algorithms offer ways to solve initial value problems for ordinary and partial differential equations (ODEs and PDEs) over long time intervals, a task often unfeasible with sequential solvers within realistic time frames. A recent approach, GParareal, combines Gaussian Processes with traditional PinT methodology (Parareal) to achieve faster parallel speed-ups. The method is known to outperform Parareal for low-dimensional ODEs and a limited number of computer cores. Here, we present Nearest Neighbors GParareal (nnGParareal), a novel data-enriched PinT integration algorithm. nnGParareal builds upon GParareal by improving its scalability properties for higher-dimensional systems and increased processor count. Through data reduction, the model complexity is reduced from cubic to log-linear in the sample size, yielding a fast and automated procedure to integrate initial value problems over long time intervals. First, we provide both an upper bound for the error and theoretical details on the speed-up benefits. Then, we empirically illustrate the superior performance of nnGParareal, compared to GParareal and Parareal, on nine different systems with unique features (e.g., stiff, chaotic, high-dimensional, or challenging-to-learn systems).</summary></entry><entry><title type="html">Neural Optimization with Adaptive Heuristics for Intelligent Marketing System</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/NeuralOptimizationwithAdaptiveHeuristicsforIntelligentMarketingSystem.html" rel="alternate" type="text/html" title="Neural Optimization with Adaptive Heuristics for Intelligent Marketing System" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/NeuralOptimizationwithAdaptiveHeuristicsforIntelligentMarketingSystem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/NeuralOptimizationwithAdaptiveHeuristicsforIntelligentMarketingSystem.html">&lt;p&gt;Computational marketing has become increasingly important in today’s digital world, facing challenges such as massive heterogeneous data, multi-channel customer journeys, and limited marketing budgets. In this paper, we propose a general framework for marketing AI systems, the Neural Optimization with Adaptive Heuristics (NOAH) framework. NOAH is the first general framework for marketing optimization that considers both to-business (2B) and to-consumer (2C) products, as well as both owned and paid channels. We describe key modules of the NOAH framework, including prediction, optimization, and adaptive heuristics, providing examples for bidding and content optimization. We then detail the successful application of NOAH to LinkedIn’s email marketing system, showcasing significant wins over the legacy ranking system. Additionally, we share details and insights that are broadly useful, particularly on: (i) addressing delayed feedback with lifetime value, (ii) performing large-scale linear programming with randomization, (iii) improving retrieval with audience expansion, (iv) reducing signal dilution in targeting tests, and (v) handling zero-inflated heavy-tail metrics in statistical testing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10490&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Changshuai Wei, Benjamin Zelditch, Joyce Chen, Andre Assuncao Silva T Ribeiro, Jingyi Kenneth Tay, Borja Ocejo Elizondo, Keerthi Selvaraj, Aman Gupta, Licurgo Benemann De Almeida</name></author><category term="stat.ME" /><summary type="html">Computational marketing has become increasingly important in today’s digital world, facing challenges such as massive heterogeneous data, multi-channel customer journeys, and limited marketing budgets. In this paper, we propose a general framework for marketing AI systems, the Neural Optimization with Adaptive Heuristics (NOAH) framework. NOAH is the first general framework for marketing optimization that considers both to-business (2B) and to-consumer (2C) products, as well as both owned and paid channels. We describe key modules of the NOAH framework, including prediction, optimization, and adaptive heuristics, providing examples for bidding and content optimization. We then detail the successful application of NOAH to LinkedIn’s email marketing system, showcasing significant wins over the legacy ranking system. Additionally, we share details and insights that are broadly useful, particularly on: (i) addressing delayed feedback with lifetime value, (ii) performing large-scale linear programming with randomization, (iii) improving retrieval with audience expansion, (iv) reducing signal dilution in targeting tests, and (v) handling zero-inflated heavy-tail metrics in statistical testing.</summary></entry><entry><title type="html">Nonparametric Test for Volatility in Clustered Multiple Time Series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/NonparametricTestforVolatilityinClusteredMultipleTimeSeries.html" rel="alternate" type="text/html" title="Nonparametric Test for Volatility in Clustered Multiple Time Series" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/NonparametricTestforVolatilityinClusteredMultipleTimeSeries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/NonparametricTestforVolatilityinClusteredMultipleTimeSeries.html">&lt;p&gt;Contagion arising from clustering of multiple time series like those in the stock market indicators can further complicate the nature of volatility, rendering a parametric test (relying on asymptotic distribution) to suffer from issues on size and power. We propose a test on volatility based on the bootstrap method for multiple time series, intended to account for possible presence of contagion effect. While the test is fairly robust to distributional assumptions, it depends on the nature of volatility. The test is correctly sized even in cases where the time series are almost nonstationary. The test is also powerful specially when the time series are stationary in mean and that volatility are contained only in fewer clusters. We illustrate the method in global stock prices data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.14412&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Erniel B. Barrios, Paolo Victor T. Redondo</name></author><category term="stat.ME" /><summary type="html">Contagion arising from clustering of multiple time series like those in the stock market indicators can further complicate the nature of volatility, rendering a parametric test (relying on asymptotic distribution) to suffer from issues on size and power. We propose a test on volatility based on the bootstrap method for multiple time series, intended to account for possible presence of contagion effect. While the test is fairly robust to distributional assumptions, it depends on the nature of volatility. The test is correctly sized even in cases where the time series are almost nonstationary. The test is also powerful specially when the time series are stationary in mean and that volatility are contained only in fewer clusters. We illustrate the method in global stock prices data.</summary></entry><entry><title type="html">Nonparametric data segmentation in multivariate time series via joint characteristic functions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Nonparametricdatasegmentationinmultivariatetimeseriesviajointcharacteristicfunctions.html" rel="alternate" type="text/html" title="Nonparametric data segmentation in multivariate time series via joint characteristic functions" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Nonparametricdatasegmentationinmultivariatetimeseriesviajointcharacteristicfunctions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Nonparametricdatasegmentationinmultivariatetimeseriesviajointcharacteristicfunctions.html">&lt;p&gt;Modern time series data often exhibit complex dependence and structural changes which are not easily characterised by shifts in the mean or model parameters. We propose a nonparametric data segmentation methodology for multivariate time series termed NP-MOJO. By considering joint characteristic functions between the time series and its lagged values, NP-MOJO is able to detect change points in the marginal distribution, but also those in possibly non-linear serial dependence, all without the need to pre-specify the type of changes. We show the theoretical consistency of NP-MOJO in estimating the total number and the locations of the change points, and demonstrate the good performance of NP-MOJO against a variety of change point scenarios. We further demonstrate its usefulness in applications to seismology and economic time series.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.07581&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Euan T. McGonigle, Haeran Cho</name></author><category term="stat.ME" /><summary type="html">Modern time series data often exhibit complex dependence and structural changes which are not easily characterised by shifts in the mean or model parameters. We propose a nonparametric data segmentation methodology for multivariate time series termed NP-MOJO. By considering joint characteristic functions between the time series and its lagged values, NP-MOJO is able to detect change points in the marginal distribution, but also those in possibly non-linear serial dependence, all without the need to pre-specify the type of changes. We show the theoretical consistency of NP-MOJO in estimating the total number and the locations of the change points, and demonstrate the good performance of NP-MOJO against a variety of change point scenarios. We further demonstrate its usefulness in applications to seismology and economic time series.</summary></entry><entry><title type="html">On Generalized Transmuted Lifetime Distribution</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/OnGeneralizedTransmutedLifetimeDistribution.html" rel="alternate" type="text/html" title="On Generalized Transmuted Lifetime Distribution" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/OnGeneralizedTransmutedLifetimeDistribution</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/OnGeneralizedTransmutedLifetimeDistribution.html">&lt;p&gt;This article presents a new class of generalized transmuted lifetime distributions which includes a large number of lifetime distributions as sub-family. Several important mathematical quantities such as density function, distribution function, quantile function, moments, moment generating function, stress-strength reliability function, order statistics, R&apos;enyi and q-entropy, residual and reversed residual life function, and cumulative information generating function are obtained. The methods of maximum likelihood, ordinary least square, weighted least square, Cram&apos;er-von Mises, Anderson Darling, and Right-tail Anderson Darling are considered to estimate the model parameters in a general way. Further, a well-organized Monte Carlo simulation experiments have been performed to observe the behavior of the estimators. Finally, two real data have also been analyzed to demonstrate the effectiveness of the proposed distribution in real-life modeling.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11624&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alok Kumar Pandey, Alam Ali, Ashok Kumar Pathak</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This article presents a new class of generalized transmuted lifetime distributions which includes a large number of lifetime distributions as sub-family. Several important mathematical quantities such as density function, distribution function, quantile function, moments, moment generating function, stress-strength reliability function, order statistics, R&apos;enyi and q-entropy, residual and reversed residual life function, and cumulative information generating function are obtained. The methods of maximum likelihood, ordinary least square, weighted least square, Cram&apos;er-von Mises, Anderson Darling, and Right-tail Anderson Darling are considered to estimate the model parameters in a general way. Further, a well-organized Monte Carlo simulation experiments have been performed to observe the behavior of the estimators. Finally, two real data have also been analyzed to demonstrate the effectiveness of the proposed distribution in real-life modeling.</summary></entry><entry><title type="html">Performance Analysis of Monte Carlo Algorithms in Dense Subgraph Identification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/PerformanceAnalysisofMonteCarloAlgorithmsinDenseSubgraphIdentification.html" rel="alternate" type="text/html" title="Performance Analysis of Monte Carlo Algorithms in Dense Subgraph Identification" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/PerformanceAnalysisofMonteCarloAlgorithmsinDenseSubgraphIdentification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/PerformanceAnalysisofMonteCarloAlgorithmsinDenseSubgraphIdentification.html">&lt;p&gt;The exploration of network structures through the lens of graph theory has become a cornerstone in understanding complex systems across diverse fields. Identifying densely connected subgraphs within larger networks is crucial for uncovering functional modules in biological systems, cohesive groups within social networks, and critical paths in technological infrastructures. The most representative approach, the SM algorithm, cannot locate subgraphs with large sizes, therefore cannot identify dense subgraphs; while the SA algorithm previously used by researchers combines simulated annealing and efficient moves for the Markov chain. However, the global optima cannot be guaranteed to be located by the simulated annealing methods including SA unless a logarithmic cooling schedule is used. To this end, our study introduces and evaluates the performance of the Simulated Annealing Algorithm (SAA), which combines simulated annealing with the stochastic approximation Monte Carlo algorithm. The performance of SAA against two other numerical algorithms-SM and SA, is examined in the context of identifying these critical subgraph structures using simulated graphs with embeded cliques. We have found that SAA outperforms both SA and SM by 1) the number of iterations to find the densest subgraph and 2) the percentage of time the algorithm is able to find a clique after 10,000 iterations, and 3) computation time. The promising result of the SAA algorithm could offer a robust tool for dissecting complex systems and potentially transforming our approach to solving problems in interdisciplinary fields.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11688&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wanru Guo</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">The exploration of network structures through the lens of graph theory has become a cornerstone in understanding complex systems across diverse fields. Identifying densely connected subgraphs within larger networks is crucial for uncovering functional modules in biological systems, cohesive groups within social networks, and critical paths in technological infrastructures. The most representative approach, the SM algorithm, cannot locate subgraphs with large sizes, therefore cannot identify dense subgraphs; while the SA algorithm previously used by researchers combines simulated annealing and efficient moves for the Markov chain. However, the global optima cannot be guaranteed to be located by the simulated annealing methods including SA unless a logarithmic cooling schedule is used. To this end, our study introduces and evaluates the performance of the Simulated Annealing Algorithm (SAA), which combines simulated annealing with the stochastic approximation Monte Carlo algorithm. The performance of SAA against two other numerical algorithms-SM and SA, is examined in the context of identifying these critical subgraph structures using simulated graphs with embeded cliques. We have found that SAA outperforms both SA and SM by 1) the number of iterations to find the densest subgraph and 2) the percentage of time the algorithm is able to find a clique after 10,000 iterations, and 3) computation time. The promising result of the SAA algorithm could offer a robust tool for dissecting complex systems and potentially transforming our approach to solving problems in interdisciplinary fields.</summary></entry><entry><title type="html">Rate Optimality and Phase Transition for User-Level Local Differential Privacy</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/RateOptimalityandPhaseTransitionforUserLevelLocalDifferentialPrivacy.html" rel="alternate" type="text/html" title="Rate Optimality and Phase Transition for User-Level Local Differential Privacy" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/RateOptimalityandPhaseTransitionforUserLevelLocalDifferentialPrivacy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/RateOptimalityandPhaseTransitionforUserLevelLocalDifferentialPrivacy.html">&lt;p&gt;Most of the literature on differential privacy considers the item-level case where each user has a single observation, but a growing field of interest is that of user-level privacy where each of the $n$ users holds $T$ observations and wishes to maintain the privacy of their entire collection.
  In this paper, we derive a general minimax lower bound, which shows that, for locally private user-level estimation problems, the risk cannot, in general, be made to vanish for a fixed number of users even when each user holds an arbitrarily large number of observations. We then derive matching, up to logarithmic factors, lower and upper bounds for univariate and multidimensional mean estimation, sparse mean estimation and non-parametric density estimation. In particular, with other model parameters held fixed, we observe phase transition phenomena in the minimax rates as $T$ the number of observations each user holds varies.
  In the case of (non-sparse) mean estimation and density estimation, we see that, for $T$ below a phase transition boundary, the rate is the same as having $nT$ users in the item-level setting. Different behaviour is however observed in the case of $s$-sparse $d$-dimensional mean estimation, wherein consistent estimation is impossible when $d$ exceeds the number of observations in the item-level setting, but is possible in the user-level setting when $T \gtrsim s \log (d)$, up to logarithmic factors. This may be of independent interest for applications as an example of a high-dimensional problem that is feasible under local privacy constraints.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11923&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexander Kent, Thomas B. Berrett, Yi Yu</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Most of the literature on differential privacy considers the item-level case where each user has a single observation, but a growing field of interest is that of user-level privacy where each of the $n$ users holds $T$ observations and wishes to maintain the privacy of their entire collection. In this paper, we derive a general minimax lower bound, which shows that, for locally private user-level estimation problems, the risk cannot, in general, be made to vanish for a fixed number of users even when each user holds an arbitrarily large number of observations. We then derive matching, up to logarithmic factors, lower and upper bounds for univariate and multidimensional mean estimation, sparse mean estimation and non-parametric density estimation. In particular, with other model parameters held fixed, we observe phase transition phenomena in the minimax rates as $T$ the number of observations each user holds varies. In the case of (non-sparse) mean estimation and density estimation, we see that, for $T$ below a phase transition boundary, the rate is the same as having $nT$ users in the item-level setting. Different behaviour is however observed in the case of $s$-sparse $d$-dimensional mean estimation, wherein consistent estimation is impossible when $d$ exceeds the number of observations in the item-level setting, but is possible in the user-level setting when $T \gtrsim s \log (d)$, up to logarithmic factors. This may be of independent interest for applications as an example of a high-dimensional problem that is feasible under local privacy constraints.</summary></entry><entry><title type="html">Real Time Monitoring and Forecasting of COVID 19 Cases using an Adjusted Holt based Hybrid Model embedded with Wavelet based ANN</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/RealTimeMonitoringandForecastingofCOVID19CasesusinganAdjustedHoltbasedHybridModelembeddedwithWaveletbasedANN.html" rel="alternate" type="text/html" title="Real Time Monitoring and Forecasting of COVID 19 Cases using an Adjusted Holt based Hybrid Model embedded with Wavelet based ANN" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/RealTimeMonitoringandForecastingofCOVID19CasesusinganAdjustedHoltbasedHybridModelembeddedwithWaveletbasedANN</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/RealTimeMonitoringandForecastingofCOVID19CasesusinganAdjustedHoltbasedHybridModelembeddedwithWaveletbasedANN.html">&lt;p&gt;Since the inception of the SARS - CoV - 2 (COVID - 19) novel coronavirus, a lot of time and effort is being allocated to estimate the trajectory and possibly, forecast with a reasonable degree of accuracy, the number of cases, recoveries, and deaths due to the same. The model proposed in this paper is a mindful step in the same direction. The primary model in question is a Hybrid Holt’s Model embedded with a Wavelet-based ANN. To test its forecasting ability, we have compared three separate models, the first, being a simple ARIMA model, the second, also an ARIMA model with a wavelet-based function, and the third, being the proposed model. We have also compared the forecast accuracy of this model with that of a modern day Vanilla LSTM recurrent neural network model. We have tested the proposed model on the number of confirmed cases (daily) for the entire country as well as 6 hotspot states. We have also proposed a simple adjustment algorithm in addition to the hybrid model so that daily and/or weekly forecasts can be meted out, with respect to the entirety of the country, as well as a moving window performance metric based on out-of-sample forecasts. In order to have a more rounded approach to the analysis of COVID-19 dynamics, focus has also been given to the estimation of the Basic Reproduction Number, $R_0$ using a compartmental epidemiological model (SIR). Lastly, we have also given substantial attention to estimating the shelf-life of the proposed model. It is obvious yet noteworthy how an accurate model, in this regard, can ensure better allocation of healthcare resources, as well as, enable the government to take necessary measures ahead of time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11213&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Agniva Das, Kunnummal Muralidharan</name></author><category term="stat.AP," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">Since the inception of the SARS - CoV - 2 (COVID - 19) novel coronavirus, a lot of time and effort is being allocated to estimate the trajectory and possibly, forecast with a reasonable degree of accuracy, the number of cases, recoveries, and deaths due to the same. The model proposed in this paper is a mindful step in the same direction. The primary model in question is a Hybrid Holt’s Model embedded with a Wavelet-based ANN. To test its forecasting ability, we have compared three separate models, the first, being a simple ARIMA model, the second, also an ARIMA model with a wavelet-based function, and the third, being the proposed model. We have also compared the forecast accuracy of this model with that of a modern day Vanilla LSTM recurrent neural network model. We have tested the proposed model on the number of confirmed cases (daily) for the entire country as well as 6 hotspot states. We have also proposed a simple adjustment algorithm in addition to the hybrid model so that daily and/or weekly forecasts can be meted out, with respect to the entirety of the country, as well as a moving window performance metric based on out-of-sample forecasts. In order to have a more rounded approach to the analysis of COVID-19 dynamics, focus has also been given to the estimation of the Basic Reproduction Number, $R_0$ using a compartmental epidemiological model (SIR). Lastly, we have also given substantial attention to estimating the shelf-life of the proposed model. It is obvious yet noteworthy how an accurate model, in this regard, can ensure better allocation of healthcare resources, as well as, enable the government to take necessary measures ahead of time.</summary></entry><entry><title type="html">Relative Counterfactual Contrastive Learning for Mitigating Pretrained Stance Bias in Stance Detection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/RelativeCounterfactualContrastiveLearningforMitigatingPretrainedStanceBiasinStanceDetection.html" rel="alternate" type="text/html" title="Relative Counterfactual Contrastive Learning for Mitigating Pretrained Stance Bias in Stance Detection" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/RelativeCounterfactualContrastiveLearningforMitigatingPretrainedStanceBiasinStanceDetection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/RelativeCounterfactualContrastiveLearningforMitigatingPretrainedStanceBiasinStanceDetection.html">&lt;p&gt;Stance detection classifies stance relations (namely, Favor, Against, or Neither) between comments and targets. Pretrained language models (PLMs) are widely used to mine the stance relation to improve the performance of stance detection through pretrained knowledge. However, PLMs also embed ``bad’’ pretrained knowledge concerning stance into the extracted stance relation semantics, resulting in pretrained stance bias. It is not trivial to measure pretrained stance bias due to its weak quantifiability. In this paper, we propose Relative Counterfactual Contrastive Learning (RCCL), in which pretrained stance bias is mitigated as relative stance bias instead of absolute stance bias to overtake the difficulty of measuring bias. Firstly, we present a new structural causal model for characterizing complicated relationships among context, PLMs and stance relations to locate pretrained stance bias. Then, based on masked language model prediction, we present a target-aware relative stance sample generation method for obtaining relative bias. Finally, we use contrastive learning based on counterfactual theory to mitigate pretrained stance bias and preserve context stance relation. Experiments show that the proposed method is superior to stance detection and debiasing baselines.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10991&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiarui Zhang, Shaojuan Wu, Xiaowang Zhang, Zhiyong Feng</name></author><category term="stat.ME" /><summary type="html">Stance detection classifies stance relations (namely, Favor, Against, or Neither) between comments and targets. Pretrained language models (PLMs) are widely used to mine the stance relation to improve the performance of stance detection through pretrained knowledge. However, PLMs also embed ``bad’’ pretrained knowledge concerning stance into the extracted stance relation semantics, resulting in pretrained stance bias. It is not trivial to measure pretrained stance bias due to its weak quantifiability. In this paper, we propose Relative Counterfactual Contrastive Learning (RCCL), in which pretrained stance bias is mitigated as relative stance bias instead of absolute stance bias to overtake the difficulty of measuring bias. Firstly, we present a new structural causal model for characterizing complicated relationships among context, PLMs and stance relations to locate pretrained stance bias. Then, based on masked language model prediction, we present a target-aware relative stance sample generation method for obtaining relative bias. Finally, we use contrastive learning based on counterfactual theory to mitigate pretrained stance bias and preserve context stance relation. Experiments show that the proposed method is superior to stance detection and debiasing baselines.</summary></entry><entry><title type="html">Residual spectrum: Brain functional connectivity detection beyond coherence</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/ResidualspectrumBrainfunctionalconnectivitydetectionbeyondcoherence.html" rel="alternate" type="text/html" title="Residual spectrum: Brain functional connectivity detection beyond coherence" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/ResidualspectrumBrainfunctionalconnectivitydetectionbeyondcoherence</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/ResidualspectrumBrainfunctionalconnectivitydetectionbeyondcoherence.html">&lt;p&gt;Coherence is a widely used measure to assess linear relationships between time series. However, it fails to capture nonlinear dependencies. To overcome this limitation, this paper introduces the notion of residual spectral density as a higher-order extension of the squared coherence. The method is based on an orthogonal decomposition of time series regression models. We propose a test for the existence of the residual spectrum and derive its fundamental properties. A numerical study illustrates finite sample performance of the proposed method. An application of the method shows that the residual spectrum can effectively detect brain connectivity. Our study reveals a noteworthy contrast in connectivity patterns between schizophrenia patients and healthy individuals. Specifically, we observed that non-linear connectivity in schizophrenia patients surpasses that of healthy individuals, which stands in stark contrast to the established understanding that linear connectivity tends to be higher in healthy individuals. This finding sheds new light on the intricate dynamics of brain connectivity in schizophrenia.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.19461&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuichi Goto, Xuze Zhang, Benjamin Kedem, Shuo Chen</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Coherence is a widely used measure to assess linear relationships between time series. However, it fails to capture nonlinear dependencies. To overcome this limitation, this paper introduces the notion of residual spectral density as a higher-order extension of the squared coherence. The method is based on an orthogonal decomposition of time series regression models. We propose a test for the existence of the residual spectrum and derive its fundamental properties. A numerical study illustrates finite sample performance of the proposed method. An application of the method shows that the residual spectrum can effectively detect brain connectivity. Our study reveals a noteworthy contrast in connectivity patterns between schizophrenia patients and healthy individuals. Specifically, we observed that non-linear connectivity in schizophrenia patients surpasses that of healthy individuals, which stands in stark contrast to the established understanding that linear connectivity tends to be higher in healthy individuals. This finding sheds new light on the intricate dynamics of brain connectivity in schizophrenia.</summary></entry><entry><title type="html">Robust Design and Evaluation of Predictive Algorithms under Unobserved Confounding</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/RobustDesignandEvaluationofPredictiveAlgorithmsunderUnobservedConfounding.html" rel="alternate" type="text/html" title="Robust Design and Evaluation of Predictive Algorithms under Unobserved Confounding" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/RobustDesignandEvaluationofPredictiveAlgorithmsunderUnobservedConfounding</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/RobustDesignandEvaluationofPredictiveAlgorithmsunderUnobservedConfounding.html">&lt;p&gt;Predictive algorithms inform consequential decisions in settings where the outcome is selectively observed given choices made by human decision makers. We propose a unified framework for the robust design and evaluation of predictive algorithms in selectively observed data. We impose general assumptions on how much the outcome may vary on average between unselected and selected units conditional on observed covariates and identified nuisance parameters, formalizing popular empirical strategies for imputing missing data such as proxy outcomes and instrumental variables. We develop debiased machine learning estimators for the bounds on a large class of predictive performance estimands, such as the conditional likelihood of the outcome, a predictive algorithm’s mean square error, true/false positive rate, and many others, under these assumptions. In an administrative dataset from a large Australian financial institution, we illustrate how varying assumptions on unobserved confounding leads to meaningful changes in default risk predictions and evaluations of credit scores across sensitive groups.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.09844&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ashesh Rambachan, Amanda Coston, Edward Kennedy</name></author><category term="stat.ME" /><summary type="html">Predictive algorithms inform consequential decisions in settings where the outcome is selectively observed given choices made by human decision makers. We propose a unified framework for the robust design and evaluation of predictive algorithms in selectively observed data. We impose general assumptions on how much the outcome may vary on average between unselected and selected units conditional on observed covariates and identified nuisance parameters, formalizing popular empirical strategies for imputing missing data such as proxy outcomes and instrumental variables. We develop debiased machine learning estimators for the bounds on a large class of predictive performance estimands, such as the conditional likelihood of the outcome, a predictive algorithm’s mean square error, true/false positive rate, and many others, under these assumptions. In an administrative dataset from a large Australian financial institution, we illustrate how varying assumptions on unobserved confounding leads to meaningful changes in default risk predictions and evaluations of credit scores across sensitive groups.</summary></entry><entry><title type="html">Robust changepoint detection in the variability of multivariate functional data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Robustchangepointdetectioninthevariabilityofmultivariatefunctionaldata.html" rel="alternate" type="text/html" title="Robust changepoint detection in the variability of multivariate functional data" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Robustchangepointdetectioninthevariabilityofmultivariatefunctionaldata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Robustchangepointdetectioninthevariabilityofmultivariatefunctionaldata.html">&lt;p&gt;We consider the problem of robustly detecting changepoints in the variability of a sequence of independent multivariate functions. We develop a novel changepoint procedure, called the functional Kruskal–Wallis for covariance (FKWC) changepoint procedure, based on rank statistics and multivariate functional data depth. The FKWC changepoint procedure allows the user to test for at most one changepoint (AMOC) or an epidemic period, or to estimate the number and locations of an unknown amount of changepoints in the data. We show that when the ``signal-to-noise’’ ratio is bounded below, the changepoint estimates produced by the FKWC procedure attain the minimax localization rate for detecting general changes in distribution in the univariate setting (Theorem 1). We also provide the behavior of the proposed test statistics for the AMOC and epidemic setting under the null hypothesis (Theorem 2) and, as a simple consequence of our main result, these tests are consistent (Corollary 1). In simulation, we show that our method is particularly robust when compared to similar changepoint methods. We present an application of the FKWC procedure to intraday asset returns and f-MRI scans. As a by-product of Theorem 1, we provide a concentration result for integrated functional depth functions (Lemma 2), which may be of general interest.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.01611&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kelly Ramsay, Shoja&apos;eddin Chenouri</name></author><category term="stat.ME" /><summary type="html">We consider the problem of robustly detecting changepoints in the variability of a sequence of independent multivariate functions. We develop a novel changepoint procedure, called the functional Kruskal–Wallis for covariance (FKWC) changepoint procedure, based on rank statistics and multivariate functional data depth. The FKWC changepoint procedure allows the user to test for at most one changepoint (AMOC) or an epidemic period, or to estimate the number and locations of an unknown amount of changepoints in the data. We show that when the ``signal-to-noise’’ ratio is bounded below, the changepoint estimates produced by the FKWC procedure attain the minimax localization rate for detecting general changes in distribution in the univariate setting (Theorem 1). We also provide the behavior of the proposed test statistics for the AMOC and epidemic setting under the null hypothesis (Theorem 2) and, as a simple consequence of our main result, these tests are consistent (Corollary 1). In simulation, we show that our method is particularly robust when compared to similar changepoint methods. We present an application of the FKWC procedure to intraday asset returns and f-MRI scans. As a by-product of Theorem 1, we provide a concentration result for integrated functional depth functions (Lemma 2), which may be of general interest.</summary></entry><entry><title type="html">Structural Nested Mean Models Under Parallel Trends with Interference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/StructuralNestedMeanModelsUnderParallelTrendswithInterference.html" rel="alternate" type="text/html" title="Structural Nested Mean Models Under Parallel Trends with Interference" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/StructuralNestedMeanModelsUnderParallelTrendswithInterference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/StructuralNestedMeanModelsUnderParallelTrendswithInterference.html">&lt;p&gt;Despite the common occurrence of interference in Difference-in-Differences (DiD) applications, standard DiD methods rely on an assumption that interference is absent, and comparatively little work has considered how to accommodate and learn about spillover effects within a DiD framework. Here, we extend the so-called `DiD-SNMMs’ of Shahn et al (2022) to accommodate interference in a time-varying DiD setting. Doing so enables estimation of a richer set of effects than previous DiD approaches. For example, DiD-SNMMs do not assume the absence of spillover effects after direct exposures and can model how effects of direct or indirect (i.e. spillover) exposures depend on past and concurrent (direct or indirect) exposure and covariate history. We consider both cluster and network interference structures an illustrate the methodology in simulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11781&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zach Shahn, Paul Zivich, Audrey Renson</name></author><category term="stat.ME" /><summary type="html">Despite the common occurrence of interference in Difference-in-Differences (DiD) applications, standard DiD methods rely on an assumption that interference is absent, and comparatively little work has considered how to accommodate and learn about spillover effects within a DiD framework. Here, we extend the so-called `DiD-SNMMs’ of Shahn et al (2022) to accommodate interference in a time-varying DiD setting. Doing so enables estimation of a richer set of effects than previous DiD approaches. For example, DiD-SNMMs do not assume the absence of spillover effects after direct exposures and can model how effects of direct or indirect (i.e. spillover) exposures depend on past and concurrent (direct or indirect) exposure and covariate history. We consider both cluster and network interference structures an illustrate the methodology in simulations.</summary></entry><entry><title type="html">Temporal and spatial downscaling for solar radiation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Temporalandspatialdownscalingforsolarradiation.html" rel="alternate" type="text/html" title="Temporal and spatial downscaling for solar radiation" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Temporalandspatialdownscalingforsolarradiation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/Temporalandspatialdownscalingforsolarradiation.html">&lt;p&gt;Global and regional climate model projections are useful for gauging future patterns of climate variables, including solar radiation, but data from these models is often too coarse to assess local impacts. Within the context of solar radiation, the changing climate may have an effect on photovoltaic (PV) production, especially as the PV industry moves to extend plant lifetimes to 50 years. Predicting PV production while taking into account a changing climate requires data at a resolution that is useful for building PV plants. Although temporal and spatial downscaling of solar radiation data is widely studied, we present a novel method to downscale solar radiation data from daily averages to hourly profiles, while maintaining spatial correlation of parameters characterizing the diurnal profile of solar radiation. The method focuses on the use of a diurnal template which can be shifted and scaled according to the time or year and location and the use of thin plate splines for spatial downscaling. This analysis is applied to data from the National Solar Radiation Database housed at the National Renewable Energy Lab and a case study of the mentioned methods over several sub-regions of continental United States is presented.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11046&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maggie Bailey, Doug Nychka, Manajit Sengupta, Jaemo Yang, Soutir Bandyopadhyay</name></author><category term="stat.AP" /><summary type="html">Global and regional climate model projections are useful for gauging future patterns of climate variables, including solar radiation, but data from these models is often too coarse to assess local impacts. Within the context of solar radiation, the changing climate may have an effect on photovoltaic (PV) production, especially as the PV industry moves to extend plant lifetimes to 50 years. Predicting PV production while taking into account a changing climate requires data at a resolution that is useful for building PV plants. Although temporal and spatial downscaling of solar radiation data is widely studied, we present a novel method to downscale solar radiation data from daily averages to hourly profiles, while maintaining spatial correlation of parameters characterizing the diurnal profile of solar radiation. The method focuses on the use of a diurnal template which can be shifted and scaled according to the time or year and location and the use of thin plate splines for spatial downscaling. This analysis is applied to data from the National Solar Radiation Database housed at the National Renewable Energy Lab and a case study of the mentioned methods over several sub-regions of continental United States is presented.</summary></entry><entry><title type="html">Testing for Stationary or Persistent Coefficient Randomness in Predictive Regressions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/TestingforStationaryorPersistentCoefficientRandomnessinPredictiveRegressions.html" rel="alternate" type="text/html" title="Testing for Stationary or Persistent Coefficient Randomness in Predictive Regressions" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/TestingforStationaryorPersistentCoefficientRandomnessinPredictiveRegressions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/TestingforStationaryorPersistentCoefficientRandomnessinPredictiveRegressions.html">&lt;p&gt;This study considers tests for coefficient randomness in predictive regressions. Our focus is on how tests for coefficient randomness are influenced by the persistence of random coefficient. We show that when the random coefficient is stationary, or I(0), Nyblom’s (1989) LM test loses its optimality (in terms of power), which is established against the alternative of integrated, or I(1), random coefficient. We demonstrate this by constructing a test that is more powerful than the LM test when the random coefficient is stationary, although the test is dominated in terms of power by the LM test when the random coefficient is integrated. This implies that the best test for coefficient randomness differs from context to context, and the persistence of the random coefficient determines which test is the best one. We apply those tests to the U.S. stock returns data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.04926&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mikihito Nishi</name></author><category term="stat.ME" /><summary type="html">This study considers tests for coefficient randomness in predictive regressions. Our focus is on how tests for coefficient randomness are influenced by the persistence of random coefficient. We show that when the random coefficient is stationary, or I(0), Nyblom’s (1989) LM test loses its optimality (in terms of power), which is established against the alternative of integrated, or I(1), random coefficient. We demonstrate this by constructing a test that is more powerful than the LM test when the random coefficient is stationary, although the test is dominated in terms of power by the LM test when the random coefficient is integrated. This implies that the best test for coefficient randomness differs from context to context, and the persistence of the random coefficient determines which test is the best one. We apply those tests to the U.S. stock returns data.</summary></entry><entry><title type="html">Trajectory-Based Individualized Treatment Rules</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/TrajectoryBasedIndividualizedTreatmentRules.html" rel="alternate" type="text/html" title="Trajectory-Based Individualized Treatment Rules" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/TrajectoryBasedIndividualizedTreatmentRules</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/TrajectoryBasedIndividualizedTreatmentRules.html">&lt;p&gt;A core component of precision medicine research involves optimizing individualized treatment rules (ITRs) based on patient characteristics. Many studies used to estimate ITRs are longitudinal in nature, collecting outcomes over time. Yet, to date, methods developed to estimate ITRs often ignore the longitudinal structure of the data. Information available from the longitudinal nature of the data can be especially useful in mental health studies. Although treatment means might appear similar, understanding the trajectory of outcomes over time can reveal important differences between treatments and placebo effects. This longitudinal perspective is especially beneficial in mental health research, where subtle shifts in outcome patterns can hold significant implications. Despite numerous studies involving the collection of outcome data across various time points, most precision medicine methods used to develop ITRs overlook the information available from the longitudinal structure. The prevalence of missing data in such studies exacerbates the issue, as neglecting the longitudinal nature of the data can significantly impair the effectiveness of treatment rules. This paper develops a powerful longitudinal trajectory-based ITR construction method that incorporates baseline variables, via a single-index or biosignature, into the modeling of longitudinal outcomes. This trajectory-based ITR approach substantially minimizes the negative impact of missing data compared to more traditional ITR approaches. The approach is illustrated through simulation studies and a clinical trial for depression, contrasting it with more traditional ITRs that ignore longitudinal information.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09810&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lanqiu Yao, Thaddeus Tarpey</name></author><category term="stat.ME" /><summary type="html">A core component of precision medicine research involves optimizing individualized treatment rules (ITRs) based on patient characteristics. Many studies used to estimate ITRs are longitudinal in nature, collecting outcomes over time. Yet, to date, methods developed to estimate ITRs often ignore the longitudinal structure of the data. Information available from the longitudinal nature of the data can be especially useful in mental health studies. Although treatment means might appear similar, understanding the trajectory of outcomes over time can reveal important differences between treatments and placebo effects. This longitudinal perspective is especially beneficial in mental health research, where subtle shifts in outcome patterns can hold significant implications. Despite numerous studies involving the collection of outcome data across various time points, most precision medicine methods used to develop ITRs overlook the information available from the longitudinal structure. The prevalence of missing data in such studies exacerbates the issue, as neglecting the longitudinal nature of the data can significantly impair the effectiveness of treatment rules. This paper develops a powerful longitudinal trajectory-based ITR construction method that incorporates baseline variables, via a single-index or biosignature, into the modeling of longitudinal outcomes. This trajectory-based ITR approach substantially minimizes the negative impact of missing data compared to more traditional ITR approaches. The approach is illustrated through simulation studies and a clinical trial for depression, contrasting it with more traditional ITRs that ignore longitudinal information.</summary></entry><entry><title type="html">Uncover mortality patterns and hospital effects in COVID-19 heart failure patients: a novel Multilevel logistic cluster-weighted modeling approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/UncovermortalitypatternsandhospitaleffectsinCOVID19heartfailurepatientsanovelMultilevellogisticclusterweightedmodelingapproach.html" rel="alternate" type="text/html" title="Uncover mortality patterns and hospital effects in COVID-19 heart failure patients: a novel Multilevel logistic cluster-weighted modeling approach" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/UncovermortalitypatternsandhospitaleffectsinCOVID19heartfailurepatientsanovelMultilevellogisticclusterweightedmodelingapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/UncovermortalitypatternsandhospitaleffectsinCOVID19heartfailurepatientsanovelMultilevellogisticclusterweightedmodelingapproach.html">&lt;p&gt;Evaluating hospitals’ performance and its relation to patients’ characteristics is of utmost importance to ensure timely, effective, and optimal treatment. Such a matter is particularly relevant in areas and situations where the healthcare system must contend with an unexpected surge in hospitalizations, such as for heart failure patients in the Lombardy region of Italy during the COVID-19 pandemic. Motivated by this issue, the paper introduces a novel Multilevel Logistic Cluster-Weighted Model (ML-CWMd) for predicting 45-day mortality following hospitalization due to COVID-19. The methodology flexibly accommodates dependence patterns among continuous, categorical, and dichotomous variables; effectively accounting for hospital-specific effects in distinct patient subgroups showing different attributes. A tailored Expectation-Maximization algorithm is developed for parameter estimation, and extensive simulation studies are conducted to evaluate its performance against competing models. The novel approach is applied to administrative data from the Lombardy Region, aiming to profile heart failure patients hospitalized for COVID-19 and investigate the hospital-level impact on their overall mortality. A scenario analysis demonstrates the model’s efficacy in managing multiple sources of heterogeneity, thereby yielding promising results in aiding healthcare providers and policy-makers in the identification of patient-specific treatment pathways.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11239&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luca Caldera, Chiara Masci, Andrea Cappozzo, Marco Forlani, Barbara Antonelli, Olivia Leoni, Francesca Ieva</name></author><category term="stat.AP" /><summary type="html">Evaluating hospitals’ performance and its relation to patients’ characteristics is of utmost importance to ensure timely, effective, and optimal treatment. Such a matter is particularly relevant in areas and situations where the healthcare system must contend with an unexpected surge in hospitalizations, such as for heart failure patients in the Lombardy region of Italy during the COVID-19 pandemic. Motivated by this issue, the paper introduces a novel Multilevel Logistic Cluster-Weighted Model (ML-CWMd) for predicting 45-day mortality following hospitalization due to COVID-19. The methodology flexibly accommodates dependence patterns among continuous, categorical, and dichotomous variables; effectively accounting for hospital-specific effects in distinct patient subgroups showing different attributes. A tailored Expectation-Maximization algorithm is developed for parameter estimation, and extensive simulation studies are conducted to evaluate its performance against competing models. The novel approach is applied to administrative data from the Lombardy Region, aiming to profile heart failure patients hospitalized for COVID-19 and investigate the hospital-level impact on their overall mortality. A scenario analysis demonstrates the model’s efficacy in managing multiple sources of heterogeneity, thereby yielding promising results in aiding healthcare providers and policy-makers in the identification of patient-specific treatment pathways.</summary></entry><entry><title type="html">Uniform Ergodicity of Parallel Tempering With Efficient Local Exploration</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/UniformErgodicityofParallelTemperingWithEfficientLocalExploration.html" rel="alternate" type="text/html" title="Uniform Ergodicity of Parallel Tempering With Efficient Local Exploration" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/UniformErgodicityofParallelTemperingWithEfficientLocalExploration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/UniformErgodicityofParallelTemperingWithEfficientLocalExploration.html">&lt;p&gt;Non-reversible parallel tempering (NRPT) is an effective algorithm for sampling from target distributions with complex geometry, such as those arising from posterior distributions of weakly identifiable and high-dimensional Bayesian models. In this work we establish the uniform (geometric) ergodicity of NRPT under a model of efficient local exploration. The uniform ergodicity log rates are inversely proportional to an easily-estimable divergence, the global communication barrier (GCB), which was recently introduced in the literature. We obtain analogous ergodicity results for classical reversible parallel tempering, providing new evidence that NRPT dominates its reversible counterpart. Our results are based on an analysis of the hitting time of a continuous-time persistent random walk, which is also of independent interest. The rates that we obtain reflect real experiments well for distributions where global exploration is not possible without parallel tempering.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11384&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nikola Surjanovic, Saifuddin Syed, Alexandre Bouchard-Côté, Trevor Campbell</name></author><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Non-reversible parallel tempering (NRPT) is an effective algorithm for sampling from target distributions with complex geometry, such as those arising from posterior distributions of weakly identifiable and high-dimensional Bayesian models. In this work we establish the uniform (geometric) ergodicity of NRPT under a model of efficient local exploration. The uniform ergodicity log rates are inversely proportional to an easily-estimable divergence, the global communication barrier (GCB), which was recently introduced in the literature. We obtain analogous ergodicity results for classical reversible parallel tempering, providing new evidence that NRPT dominates its reversible counterpart. Our results are based on an analysis of the hitting time of a continuous-time persistent random walk, which is also of independent interest. The rates that we obtain reflect real experiments well for distributions where global exploration is not possible without parallel tempering.</summary></entry><entry><title type="html">What are You Weighting For? Improved Weights for Gaussian Mixture Filtering With Application to Cislunar Orbit Determination</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/WhatareYouWeightingForImprovedWeightsforGaussianMixtureFilteringWithApplicationtoCislunarOrbitDetermination.html" rel="alternate" type="text/html" title="What are You Weighting For? Improved Weights for Gaussian Mixture Filtering With Application to Cislunar Orbit Determination" /><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/WhatareYouWeightingForImprovedWeightsforGaussianMixtureFilteringWithApplicationtoCislunarOrbitDetermination</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/21/WhatareYouWeightingForImprovedWeightsforGaussianMixtureFilteringWithApplicationtoCislunarOrbitDetermination.html">&lt;p&gt;This work focuses on the critical aspect of accurate weight computation during the measurement incorporation phase of Gaussian mixture filters. The proposed novel approach computes weights by linearizing the measurement model about each component’s posterior estimate rather than the the prior, as traditionally done. This work proves equivalence with traditional methods for linear models, provides novel sigma-point extensions to the traditional and proposed methods, and empirically demonstrates improved performance in nonlinear cases. Two illustrative examples, the Avocado and a cislunar single target tracking scenario, serve to highlight the advantages of the new weight computation technique by analyzing filter accuracy and consistency through varying the number of Gaussian mixture components.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11081&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Dalton Durant, Andrey A. Popov, Renato Zanetti</name></author><category term="stat.ME" /><summary type="html">This work focuses on the critical aspect of accurate weight computation during the measurement incorporation phase of Gaussian mixture filters. The proposed novel approach computes weights by linearizing the measurement model about each component’s posterior estimate rather than the the prior, as traditionally done. This work proves equivalence with traditional methods for linear models, provides novel sigma-point extensions to the traditional and proposed methods, and empirically demonstrates improved performance in nonlinear cases. Two illustrative examples, the Avocado and a cislunar single target tracking scenario, serve to highlight the advantages of the new weight computation technique by analyzing filter accuracy and consistency through varying the number of Gaussian mixture components.</summary></entry></feed>