<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-04-29T14:42:58+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Novel Context driven Critical Integrative Levels (CIL) Approach: Advancing Human-Centric and Integrative Lighting Asset Management in Public Libraries with Practical Thresholds</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/ANovelContextdrivenCriticalIntegrativeLevelsCILApproachAdvancingHumanCentricandIntegrativeLightingAssetManagementinPublicLibrarieswithPracticalThresholds.html" rel="alternate" type="text/html" title="A Novel Context driven Critical Integrative Levels (CIL) Approach: Advancing Human-Centric and Integrative Lighting Asset Management in Public Libraries with Practical Thresholds" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/ANovelContextdrivenCriticalIntegrativeLevelsCILApproachAdvancingHumanCentricandIntegrativeLightingAssetManagementinPublicLibrarieswithPracticalThresholds</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/ANovelContextdrivenCriticalIntegrativeLevelsCILApproachAdvancingHumanCentricandIntegrativeLightingAssetManagementinPublicLibrarieswithPracticalThresholds.html">&lt;p&gt;This paper proposes the context driven Critical Integrative Levels (CIL), a novel approach to lighting asset management in public libraries that aligns with the transformative vision of human-centric and integrative lighting. This approach encompasses not only the visual aspects of lighting performance but also prioritizes the physiological and psychological well-being of library users. Incorporating a newly defined metric, Mean Time of Exposure (MTOE), the approach quantifies user-light interaction, enabling tailored lighting strategies that respond to diverse activities and needs in library spaces. Case studies demonstrate how the CIL matrix can be practically applied, offering significant improvements over conventional methods by focusing on optimized user experiences from both visual impacts and non-visual effects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17554&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jing Lin, Nina Mylly, Per Olof Hedekvist, Jingchun Shen</name></author><category term="stat.AP" /><summary type="html">This paper proposes the context driven Critical Integrative Levels (CIL), a novel approach to lighting asset management in public libraries that aligns with the transformative vision of human-centric and integrative lighting. This approach encompasses not only the visual aspects of lighting performance but also prioritizes the physiological and psychological well-being of library users. Incorporating a newly defined metric, Mean Time of Exposure (MTOE), the approach quantifies user-light interaction, enabling tailored lighting strategies that respond to diverse activities and needs in library spaces. Case studies demonstrate how the CIL matrix can be practically applied, offering significant improvements over conventional methods by focusing on optimized user experiences from both visual impacts and non-visual effects.</summary></entry><entry><title type="html">A Weibull Mixture Cure Frailty Model for High-dimensional Covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AWeibullMixtureCureFrailtyModelforHighdimensionalCovariates.html" rel="alternate" type="text/html" title="A Weibull Mixture Cure Frailty Model for High-dimensional Covariates" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AWeibullMixtureCureFrailtyModelforHighdimensionalCovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AWeibullMixtureCureFrailtyModelforHighdimensionalCovariates.html">&lt;p&gt;A novel mixture cure frailty model is introduced for handling censored survival data. Mixture cure models are preferable when the existence of a cured fraction among patients can be assumed. However, such models are heavily underexplored: frailty structures within cure models remain largely undeveloped, and furthermore, most existing methods do not work for high-dimensional datasets, when the number of predictors is significantly larger than the number of observations. In this study, we introduce a novel extension of the Weibull mixture cure model that incorporates a frailty component, employed to model an underlying latent population heterogeneity with respect to the outcome risk. Additionally, high-dimensional covariates are integrated into both the cure rate and survival part of the model, providing a comprehensive approach to employ the model in the context of high-dimensional omics data. We also perform variable selection via an adaptive elastic-net penalization, and propose a novel approach to inference using the expectation-maximization (EM) algorithm. Extensive simulation studies are conducted across various scenarios to demonstrate the performance of the model, and results indicate that our proposed method outperforms competitor models. We apply the novel approach to analyze RNAseq gene expression data from bulk breast cancer patients included in The Cancer Genome Atlas (TCGA) database. A set of prognostic biomarkers is then derived from selected genes, and subsequently validated via both functional enrichment analysis and comparison to the existing biological literature. Finally, a prognostic risk score index based on the identified biomarkers is proposed and validated by exploring the patients’ survival.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.06575&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fatih K{\i}z{\i}laslan, David Michael Swanson, Valeria Vitelli</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">A novel mixture cure frailty model is introduced for handling censored survival data. Mixture cure models are preferable when the existence of a cured fraction among patients can be assumed. However, such models are heavily underexplored: frailty structures within cure models remain largely undeveloped, and furthermore, most existing methods do not work for high-dimensional datasets, when the number of predictors is significantly larger than the number of observations. In this study, we introduce a novel extension of the Weibull mixture cure model that incorporates a frailty component, employed to model an underlying latent population heterogeneity with respect to the outcome risk. Additionally, high-dimensional covariates are integrated into both the cure rate and survival part of the model, providing a comprehensive approach to employ the model in the context of high-dimensional omics data. We also perform variable selection via an adaptive elastic-net penalization, and propose a novel approach to inference using the expectation-maximization (EM) algorithm. Extensive simulation studies are conducted across various scenarios to demonstrate the performance of the model, and results indicate that our proposed method outperforms competitor models. We apply the novel approach to analyze RNAseq gene expression data from bulk breast cancer patients included in The Cancer Genome Atlas (TCGA) database. A set of prognostic biomarkers is then derived from selected genes, and subsequently validated via both functional enrichment analysis and comparison to the existing biological literature. Finally, a prognostic risk score index based on the identified biomarkers is proposed and validated by exploring the patients’ survival.</summary></entry><entry><title type="html">A comparison of the discrimination performance of lasso and maximum likelihood estimation in logistic regression model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Acomparisonofthediscriminationperformanceoflassoandmaximumlikelihoodestimationinlogisticregressionmodel.html" rel="alternate" type="text/html" title="A comparison of the discrimination performance of lasso and maximum likelihood estimation in logistic regression model" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Acomparisonofthediscriminationperformanceoflassoandmaximumlikelihoodestimationinlogisticregressionmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Acomparisonofthediscriminationperformanceoflassoandmaximumlikelihoodestimationinlogisticregressionmodel.html">&lt;p&gt;Logistic regression is widely used in many areas of knowledge. Several works compare the performance of lasso and maximum likelihood estimation in logistic regression. However, part of these works do not perform simulation studies and the remaining ones do not consider scenarios in which the ratio of the number of covariates to sample size is high. In this work, we compare the discrimination performance of lasso and maximum likelihood estimation in logistic regression using simulation studies and applications. Variable selection is done both by lasso and by stepwise when maximum likelihood estimation is used. We consider a wide range of values for the ratio of the number of covariates to sample size. The main conclusion of the work is that lasso has a better discrimination performance than maximum likelihood estimation when the ratio of the number of covariates to sample size is high.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17482&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Gilberto P. Alcântara Junior, Gustavo H. A. Pereira</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Logistic regression is widely used in many areas of knowledge. Several works compare the performance of lasso and maximum likelihood estimation in logistic regression. However, part of these works do not perform simulation studies and the remaining ones do not consider scenarios in which the ratio of the number of covariates to sample size is high. In this work, we compare the discrimination performance of lasso and maximum likelihood estimation in logistic regression using simulation studies and applications. Variable selection is done both by lasso and by stepwise when maximum likelihood estimation is used. We consider a wide range of values for the ratio of the number of covariates to sample size. The main conclusion of the work is that lasso has a better discrimination performance than maximum likelihood estimation when the ratio of the number of covariates to sample size is high.</summary></entry><entry><title type="html">A comprehensive survey of the home advantage in American football</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AcomprehensivesurveyofthehomeadvantageinAmericanfootball.html" rel="alternate" type="text/html" title="A comprehensive survey of the home advantage in American football" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AcomprehensivesurveyofthehomeadvantageinAmericanfootball</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AcomprehensivesurveyofthehomeadvantageinAmericanfootball.html">&lt;p&gt;The existence and justification to the home advantage – the benefit a sports team receives when playing at home – has been studied across sport. The majority of research on this topic is limited to individual leagues in short time frames, which hinders extrapolation and a deeper understanding of possible causes. Using nearly two decades of data from the National Football League (NFL), the National Collegiate Athletic Association (NCAA), and high schools from across the United States, we provide a uniform approach to understanding the home advantage in American football. Our findings suggest home advantage is declining in the NFL and the highest levels of collegiate football, but not in amateur football. This increases the possibility that characteristics of the NCAA and NFL, such as travel improvements and instant replay, have helped level the playing field.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.16392&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luke S. Benz, Thompson J. Bliss, Michael J. Lopez</name></author><category term="stat.AP" /><summary type="html">The existence and justification to the home advantage – the benefit a sports team receives when playing at home – has been studied across sport. The majority of research on this topic is limited to individual leagues in short time frames, which hinders extrapolation and a deeper understanding of possible causes. Using nearly two decades of data from the National Football League (NFL), the National Collegiate Athletic Association (NCAA), and high schools from across the United States, we provide a uniform approach to understanding the home advantage in American football. Our findings suggest home advantage is declining in the NFL and the highest levels of collegiate football, but not in amateur football. This increases the possibility that characteristics of the NCAA and NFL, such as travel improvements and instant replay, have helped level the playing field.</summary></entry><entry><title type="html">An adaptive standardisation methodology for Day-Ahead electricity price forecasting</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AnadaptivestandardisationmethodologyforDayAheadelectricitypriceforecasting.html" rel="alternate" type="text/html" title="An adaptive standardisation methodology for Day-Ahead electricity price forecasting" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AnadaptivestandardisationmethodologyforDayAheadelectricitypriceforecasting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AnadaptivestandardisationmethodologyforDayAheadelectricitypriceforecasting.html">&lt;p&gt;The study of Day-Ahead prices in the electricity market is one of the most popular problems in time series forecasting. Previous research has focused on employing increasingly complex learning algorithms to capture the sophisticated dynamics of the market. However, there is a threshold where increased complexity fails to yield substantial improvements. In this work, we propose an alternative approach by introducing an adaptive standardisation to mitigate the effects of dataset shifts that commonly occur in the market. By doing so, learning algorithms can prioritize uncovering the true relationship between the target variable and the explanatory variables. We investigate five distinct markets, including two novel datasets, previously unexplored in the literature. These datasets provide a more realistic representation of the current market context, that conventional datasets do not show. The results demonstrate a significant improvement across all five markets using the widely accepted learning algorithms in the literature (LEAR and DNN). In particular, the combination of the proposed methodology with the methodology previously presented in the literature obtains the best results. This significant advancement unveils new lines of research in this field, highlighting the potential of adaptive transformations in enhancing the performance of forecasting models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.02610&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carlos Sebastián, Carlos E. González-Guillén, Jesús Juan</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">The study of Day-Ahead prices in the electricity market is one of the most popular problems in time series forecasting. Previous research has focused on employing increasingly complex learning algorithms to capture the sophisticated dynamics of the market. However, there is a threshold where increased complexity fails to yield substantial improvements. In this work, we propose an alternative approach by introducing an adaptive standardisation to mitigate the effects of dataset shifts that commonly occur in the market. By doing so, learning algorithms can prioritize uncovering the true relationship between the target variable and the explanatory variables. We investigate five distinct markets, including two novel datasets, previously unexplored in the literature. These datasets provide a more realistic representation of the current market context, that conventional datasets do not show. The results demonstrate a significant improvement across all five markets using the widely accepted learning algorithms in the literature (LEAR and DNN). In particular, the combination of the proposed methodology with the methodology previously presented in the literature obtains the best results. This significant advancement unveils new lines of research in this field, highlighting the potential of adaptive transformations in enhancing the performance of forecasting models.</summary></entry><entry><title type="html">Assigning Stationary Distributions to Sparse Stochastic Matrices</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AssigningStationaryDistributionstoSparseStochasticMatrices.html" rel="alternate" type="text/html" title="Assigning Stationary Distributions to Sparse Stochastic Matrices" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AssigningStationaryDistributionstoSparseStochasticMatrices</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AssigningStationaryDistributionstoSparseStochasticMatrices.html">&lt;p&gt;The target stationary distribution problem (TSDP) is the following: given an irreducible stochastic matrix $G$ and a target stationary distribution $\hat \mu$, construct a minimum norm perturbation, $\Delta$, such that $\hat G = G+\Delta$ is also stochastic and has the prescribed target stationary distribution, $\hat \mu$. In this paper, we revisit the TSDP under a constraint on the support of $\Delta$, that is, on the set of non-zero entries of $\Delta$. This is particularly meaningful in practice since one cannot typically modify all entries of $G$. We first show how to construct a feasible solution $\hat G$ that has essentially the same support as the matrix $G$. Then we show how to compute globally optimal and sparse solutions using the component-wise $\ell_1$ norm and linear optimization. We propose an efficient implementation that relies on a column-generation approach which allows us to solve sparse problems of size up to $10^5 \times 10^5$ in a few minutes. We illustrate the proposed algorithms with several numerical experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.16011&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nicolas Gillis, Paul Van Dooren</name></author><category term="stat.CO" /><summary type="html">The target stationary distribution problem (TSDP) is the following: given an irreducible stochastic matrix $G$ and a target stationary distribution $\hat \mu$, construct a minimum norm perturbation, $\Delta$, such that $\hat G = G+\Delta$ is also stochastic and has the prescribed target stationary distribution, $\hat \mu$. In this paper, we revisit the TSDP under a constraint on the support of $\Delta$, that is, on the set of non-zero entries of $\Delta$. This is particularly meaningful in practice since one cannot typically modify all entries of $G$. We first show how to construct a feasible solution $\hat G$ that has essentially the same support as the matrix $G$. Then we show how to compute globally optimal and sparse solutions using the component-wise $\ell_1$ norm and linear optimization. We propose an efficient implementation that relies on a column-generation approach which allows us to solve sparse problems of size up to $10^5 \times 10^5$ in a few minutes. We illustrate the proposed algorithms with several numerical experiments.</summary></entry><entry><title type="html">A unified framework for bounding causal effects on the always-survivor and other populations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Aunifiedframeworkforboundingcausaleffectsonthealwayssurvivorandotherpopulations.html" rel="alternate" type="text/html" title="A unified framework for bounding causal effects on the always-survivor and other populations" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Aunifiedframeworkforboundingcausaleffectsonthealwayssurvivorandotherpopulations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Aunifiedframeworkforboundingcausaleffectsonthealwayssurvivorandotherpopulations.html">&lt;p&gt;We investigate the bounding problem of causal effects in experimental studies in which the outcome is truncated by death, meaning that the subject dies before the outcome can be measured. Causal effects cannot be point identified without instruments and/or tight parametric assumptions but can be bounded under mild restrictions. Previous work on partial identification under the principal stratification framework has primarily focused on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;always-survivor&apos; subpopulation. In this paper, we present a novel nonparametric unified framework to provide sharp bounds on causal effects on discrete and continuous square-integrable outcomes. These bounds are derived on the &lt;/code&gt;always-survivor’, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;protected&apos;, and &lt;/code&gt;harmed’ subpopulations and on the entire population with/without assumptions of monotonicity and stochastic dominance. The main idea depends on rewriting the optimization problem in terms of the integrated tail probability expectation formula using a set of conditional probability distributions. The proposed procedure allows for settings with any type and number of covariates, and can be extended to incorporate average causal effects and complier average causal effects. Furthermore, we present several simulation studies conducted under various assumptions as well as the application of the proposed approach to a real dataset from the National Supported Work Demonstration.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.13398&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aixian Chen, Xia Cui, Guangren Yang</name></author><category term="stat.ME" /><summary type="html">We investigate the bounding problem of causal effects in experimental studies in which the outcome is truncated by death, meaning that the subject dies before the outcome can be measured. Causal effects cannot be point identified without instruments and/or tight parametric assumptions but can be bounded under mild restrictions. Previous work on partial identification under the principal stratification framework has primarily focused on the always-survivor&apos; subpopulation. In this paper, we present a novel nonparametric unified framework to provide sharp bounds on causal effects on discrete and continuous square-integrable outcomes. These bounds are derived on the always-survivor’, protected&apos;, and harmed’ subpopulations and on the entire population with/without assumptions of monotonicity and stochastic dominance. The main idea depends on rewriting the optimization problem in terms of the integrated tail probability expectation formula using a set of conditional probability distributions. The proposed procedure allows for settings with any type and number of covariates, and can be extended to incorporate average causal effects and complier average causal effects. Furthermore, we present several simulation studies conducted under various assumptions as well as the application of the proposed approach to a real dataset from the National Supported Work Demonstration.</summary></entry><entry><title type="html">Bayesian Federated Inference for Survival Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BayesianFederatedInferenceforSurvivalModels.html" rel="alternate" type="text/html" title="Bayesian Federated Inference for Survival Models" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BayesianFederatedInferenceforSurvivalModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BayesianFederatedInferenceforSurvivalModels.html">&lt;p&gt;In cancer research, overall survival and progression free survival are often analyzed with the Cox model. To estimate accurately the parameters in the model, sufficient data and, more importantly, sufficient events need to be observed. In practice, this is often a problem. Merging data sets from different medical centers may help, but this is not always possible due to strict privacy legislation and logistic difficulties. Recently, the Bayesian Federated Inference (BFI) strategy for generalized linear models was proposed. With this strategy the statistical analyses are performed in the local centers where the data were collected (or stored) and only the inference results are combined to a single estimated model; merging data is not necessary. The BFI methodology aims to compute from the separate inference results in the local centers what would have been obtained if the analysis had been based on the merged data sets. In this paper we generalize the BFI methodology as initially developed for generalized linear models to survival models. Simulation studies and real data analyses show excellent performance; i.e., the results obtained with the BFI methodology are very similar to the results obtained by analyzing the merged data. An R package for doing the analyses is available.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17464&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hassan Pazira, Emanuele Massa, Jetty AM Weijers, Anthony CC Coolen, Marianne A Jonker</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">In cancer research, overall survival and progression free survival are often analyzed with the Cox model. To estimate accurately the parameters in the model, sufficient data and, more importantly, sufficient events need to be observed. In practice, this is often a problem. Merging data sets from different medical centers may help, but this is not always possible due to strict privacy legislation and logistic difficulties. Recently, the Bayesian Federated Inference (BFI) strategy for generalized linear models was proposed. With this strategy the statistical analyses are performed in the local centers where the data were collected (or stored) and only the inference results are combined to a single estimated model; merging data is not necessary. The BFI methodology aims to compute from the separate inference results in the local centers what would have been obtained if the analysis had been based on the merged data sets. In this paper we generalize the BFI methodology as initially developed for generalized linear models to survival models. Simulation studies and real data analyses show excellent performance; i.e., the results obtained with the BFI methodology are very similar to the results obtained by analyzing the merged data. An R package for doing the analyses is available.</summary></entry><entry><title type="html">Bayesian Machine Learning meets Formal Methods: An application to spatio-temporal data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BayesianMachineLearningmeetsFormalMethodsAnapplicationtospatiotemporaldata.html" rel="alternate" type="text/html" title="Bayesian Machine Learning meets Formal Methods: An application to spatio-temporal data" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BayesianMachineLearningmeetsFormalMethodsAnapplicationtospatiotemporaldata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BayesianMachineLearningmeetsFormalMethodsAnapplicationtospatiotemporaldata.html">&lt;p&gt;We propose an interdisciplinary framework that combines Bayesian predictive inference, a well-established tool in Machine Learning, with Formal Methods rooted in the computer science community. Bayesian predictive inference allows for coherently incorporating uncertainty about unknown quantities by making use of methods or models that produce predictive distributions, which in turn inform decision problems. By formalizing these decision problems into properties with the help of spatio-temporal logic, we can formulate and predict how likely such properties are to be satisfied in the future at a certain location. Moreover, we can leverage our methodology to evaluate and compare models directly on their ability to predict the satisfaction of application-driven properties. The approach is illustrated in an urban mobility application, where the crowdedness in the center of Milan is proxied by aggregated mobile phone traffic data. We specify several desirable spatio-temporal properties related to city crowdedness such as a fault-tolerant network or the reachability of hospitals. After verifying these properties on draws from the posterior predictive distributions, we compare several spatio-temporal Bayesian models based on their overall and property-based predictive performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.01360&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Laura Vana, Ennio Visconti, Laura Nenzi, Annalisa Cadonna, Gregor Kastner</name></author><category term="stat.CO" /><summary type="html">We propose an interdisciplinary framework that combines Bayesian predictive inference, a well-established tool in Machine Learning, with Formal Methods rooted in the computer science community. Bayesian predictive inference allows for coherently incorporating uncertainty about unknown quantities by making use of methods or models that produce predictive distributions, which in turn inform decision problems. By formalizing these decision problems into properties with the help of spatio-temporal logic, we can formulate and predict how likely such properties are to be satisfied in the future at a certain location. Moreover, we can leverage our methodology to evaluate and compare models directly on their ability to predict the satisfaction of application-driven properties. The approach is illustrated in an urban mobility application, where the crowdedness in the center of Milan is proxied by aggregated mobile phone traffic data. We specify several desirable spatio-temporal properties related to city crowdedness such as a fault-tolerant network or the reachability of hospitals. After verifying these properties on draws from the posterior predictive distributions, we compare several spatio-temporal Bayesian models based on their overall and property-based predictive performance.</summary></entry><entry><title type="html">Boosting e-BH via conditional calibration</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BoostingeBHviaconditionalcalibration.html" rel="alternate" type="text/html" title="Boosting e-BH via conditional calibration" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BoostingeBHviaconditionalcalibration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BoostingeBHviaconditionalcalibration.html">&lt;p&gt;The e-BH procedure is an e-value-based multiple testing procedure that provably controls the false discovery rate (FDR) under any dependence structure between the e-values. Despite this appealing theoretical FDR control guarantee, the e-BH procedure often suffers from low power in practice. In this paper, we propose a general framework that boosts the power of e-BH without sacrificing its FDR control under arbitrary dependence. This is achieved by the technique of conditional calibration, where we take as input the e-values and calibrate them to be a set of “boosted e-values” that are guaranteed to be no less – and are often more – powerful than the original ones. Our general framework is explicitly instantiated in three classes of multiple testing problems: (1) testing under parametric models, (2) conditional independence testing under the model-X setting, and (3) model-free conformalized selection. Extensive numerical experiments show that our proposed method significantly improves the power of e-BH while continuing to control the FDR. We also demonstrate the effectiveness of our method through an application to an observational study dataset for identifying individuals whose counterfactuals satisfy certain properties.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17562&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Junu Lee, Zhimei Ren</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">The e-BH procedure is an e-value-based multiple testing procedure that provably controls the false discovery rate (FDR) under any dependence structure between the e-values. Despite this appealing theoretical FDR control guarantee, the e-BH procedure often suffers from low power in practice. In this paper, we propose a general framework that boosts the power of e-BH without sacrificing its FDR control under arbitrary dependence. This is achieved by the technique of conditional calibration, where we take as input the e-values and calibrate them to be a set of “boosted e-values” that are guaranteed to be no less – and are often more – powerful than the original ones. Our general framework is explicitly instantiated in three classes of multiple testing problems: (1) testing under parametric models, (2) conditional independence testing under the model-X setting, and (3) model-free conformalized selection. Extensive numerical experiments show that our proposed method significantly improves the power of e-BH while continuing to control the FDR. We also demonstrate the effectiveness of our method through an application to an observational study dataset for identifying individuals whose counterfactuals satisfy certain properties.</summary></entry><entry><title type="html">Bridging the Gap: Towards an Expanded Toolkit for ML-Supported Decision-Making in the Public Sector</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BridgingtheGapTowardsanExpandedToolkitforMLSupportedDecisionMakinginthePublicSector.html" rel="alternate" type="text/html" title="Bridging the Gap: Towards an Expanded Toolkit for ML-Supported Decision-Making in the Public Sector" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BridgingtheGapTowardsanExpandedToolkitforMLSupportedDecisionMakinginthePublicSector</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BridgingtheGapTowardsanExpandedToolkitforMLSupportedDecisionMakinginthePublicSector.html">&lt;p&gt;Machine Learning (ML) systems are becoming instrumental in the public sector, with applications spanning areas like criminal justice, social welfare, financial fraud detection, and public health. While these systems offer great potential benefits to institutional decision-making processes, such as improved efficiency and reliability, they still face the challenge of aligning nuanced policy objectives with the precise formalization requirements necessitated by ML models. In this paper, we aim to bridge the gap between ML model requirements and public sector decision-making by presenting a comprehensive overview of key technical challenges where disjunctions between policy goals and ML models commonly arise. We concentrate on pivotal points of the ML pipeline that connect the model to its operational environment, discussing the significance of representative training data and highlighting the importance of a model setup that facilitates effective decision-making. Additionally, we link these challenges with emerging methodological advancements, encompassing causal ML, domain adaptation, uncertainty quantification, and multi-objective optimization, illustrating the path forward for harmonizing ML and public sector objectives.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.19091&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Unai Fischer-Abaigar, Christoph Kern, Noam Barda, Frauke Kreuter</name></author><category term="stat.ME" /><summary type="html">Machine Learning (ML) systems are becoming instrumental in the public sector, with applications spanning areas like criminal justice, social welfare, financial fraud detection, and public health. While these systems offer great potential benefits to institutional decision-making processes, such as improved efficiency and reliability, they still face the challenge of aligning nuanced policy objectives with the precise formalization requirements necessitated by ML models. In this paper, we aim to bridge the gap between ML model requirements and public sector decision-making by presenting a comprehensive overview of key technical challenges where disjunctions between policy goals and ML models commonly arise. We concentrate on pivotal points of the ML pipeline that connect the model to its operational environment, discussing the significance of representative training data and highlighting the importance of a model setup that facilitates effective decision-making. Additionally, we link these challenges with emerging methodological advancements, encompassing causal ML, domain adaptation, uncertainty quantification, and multi-objective optimization, illustrating the path forward for harmonizing ML and public sector objectives.</summary></entry><entry><title type="html">Consistent information criteria for regularized regression and loss-based learning problems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Consistentinformationcriteriaforregularizedregressionandlossbasedlearningproblems.html" rel="alternate" type="text/html" title="Consistent information criteria for regularized regression and loss-based learning problems" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Consistentinformationcriteriaforregularizedregressionandlossbasedlearningproblems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Consistentinformationcriteriaforregularizedregressionandlossbasedlearningproblems.html">&lt;p&gt;Many problems in statistics and machine learning can be formulated as model selection problems, where the goal is to choose an optimal parsimonious model among a set of candidate models. It is typical to conduct model selection by penalizing the objective function via information criteria (IC), as with the pioneering work by Akaike and Schwarz. Via recent work, we propose a generalized IC framework to consistently estimate general loss-based learning problems. In this work, we propose a consistent estimation method for Generalized Linear Model (GLM) regressions by utilizing the recent IC developments. We advance the generalized IC framework by proposing model selection problems, where the model set consists of a potentially uncountable set of models. In addition to theoretical expositions, our proposal introduces a computational procedure for the implementation of our methods in the finite sample setting, which we demonstrate via an extensive simulation study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17181&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Qingyuan Zhang, Hien Duy Nguyen</name></author><category term="stat.ME" /><summary type="html">Many problems in statistics and machine learning can be formulated as model selection problems, where the goal is to choose an optimal parsimonious model among a set of candidate models. It is typical to conduct model selection by penalizing the objective function via information criteria (IC), as with the pioneering work by Akaike and Schwarz. Via recent work, we propose a generalized IC framework to consistently estimate general loss-based learning problems. In this work, we propose a consistent estimation method for Generalized Linear Model (GLM) regressions by utilizing the recent IC developments. We advance the generalized IC framework by proposing model selection problems, where the model set consists of a potentially uncountable set of models. In addition to theoretical expositions, our proposal introduces a computational procedure for the implementation of our methods in the finite sample setting, which we demonstrate via an extensive simulation study.</summary></entry><entry><title type="html">Correspondence analysis: handling cell-wise outliers via the reconstitution algorithm</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Correspondenceanalysishandlingcellwiseoutliersviathereconstitutionalgorithm.html" rel="alternate" type="text/html" title="Correspondence analysis: handling cell-wise outliers via the reconstitution algorithm" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Correspondenceanalysishandlingcellwiseoutliersviathereconstitutionalgorithm</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Correspondenceanalysishandlingcellwiseoutliersviathereconstitutionalgorithm.html">&lt;p&gt;Correspondence analysis (CA) is a popular technique to visualize the relationship between two categorical variables. CA uses the data from a two-way contingency table and is affected by the presence of outliers. The supplementary points method is a popular method to handle outliers. Its disadvantage is that the information from entire rows or columns is removed. However, outliers can be caused by cells only. In this paper, a reconstitution algorithm is introduced to cope with such cells. This algorithm can reduce the contribution of cells in CA instead of deleting entire rows or columns. Thus the remaining information in the row and column involved can be used in the analysis. The reconstitution algorithm is compared with two alternative methods for handling outliers, the supplementary points method and MacroPCA. It is shown that the proposed strategy works well.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17380&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Qianqian Qi, David J. Hessen, Aike N. Vonk, Peter G. M. van der Heijden</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Correspondence analysis (CA) is a popular technique to visualize the relationship between two categorical variables. CA uses the data from a two-way contingency table and is affected by the presence of outliers. The supplementary points method is a popular method to handle outliers. Its disadvantage is that the information from entire rows or columns is removed. However, outliers can be caused by cells only. In this paper, a reconstitution algorithm is introduced to cope with such cells. This algorithm can reduce the contribution of cells in CA instead of deleting entire rows or columns. Thus the remaining information in the row and column involved can be used in the analysis. The reconstitution algorithm is compared with two alternative methods for handling outliers, the supplementary points method and MacroPCA. It is shown that the proposed strategy works well.</summary></entry><entry><title type="html">Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation.html" rel="alternate" type="text/html" title="Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation.html">&lt;p&gt;There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using the inverse of probability weighting (IPW). However, due to the numerically unstable IPW weights, they suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator via weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Experimental results show that by effectively correcting the weight values, our method outperforms the existing ones, including traditional weighting schemes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17483&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yoichi Chikahara, Kansei Ushiyama</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using the inverse of probability weighting (IPW). However, due to the numerically unstable IPW weights, they suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator via weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Experimental results show that by effectively correcting the weight values, our method outperforms the existing ones, including traditional weighting schemes.</summary></entry><entry><title type="html">Enhancing Longitudinal Clinical Trial Efficiency with Digital Twins and Prognostic Covariate-Adjusted Mixed Models for Repeated Measures (PROCOVA-MMRM)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/EnhancingLongitudinalClinicalTrialEfficiencywithDigitalTwinsandPrognosticCovariateAdjustedMixedModelsforRepeatedMeasuresPROCOVAMMRM.html" rel="alternate" type="text/html" title="Enhancing Longitudinal Clinical Trial Efficiency with Digital Twins and Prognostic Covariate-Adjusted Mixed Models for Repeated Measures (PROCOVA-MMRM)" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/EnhancingLongitudinalClinicalTrialEfficiencywithDigitalTwinsandPrognosticCovariateAdjustedMixedModelsforRepeatedMeasuresPROCOVAMMRM</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/EnhancingLongitudinalClinicalTrialEfficiencywithDigitalTwinsandPrognosticCovariateAdjustedMixedModelsforRepeatedMeasuresPROCOVAMMRM.html">&lt;p&gt;Clinical trials are critical in advancing medical treatments but often suffer from immense time and financial burden. Advances in statistical methodologies and artificial intelligence (AI) present opportunities to address these inefficiencies. Here we introduce Prognostic Covariate-Adjusted Mixed Models for Repeated Measures (PROCOVA-MMRM) as an advantageous combination of prognostic covariate adjustment (PROCOVA) and Mixed Models for Repeated Measures (MMRM). PROCOVA-MMRM utilizes time-matched prognostic scores generated from AI models to enhance the precision of treatment effect estimators for longitudinal continuous outcomes, enabling reductions in sample size and enrollment times. We first provide a description of the background and implementation of PROCOVA-MMRM, followed by two case study reanalyses where we compare the performance of PROCOVA-MMRM versus the unadjusted MMRM. These reanalyses demonstrate significant improvements in statistical power and precision in clinical indications with unmet medical need, specifically Alzheimer’s Disease (AD) and Amyotrophic Lateral Sclerosis (ALS). We also explore the potential for sample size reduction with the prospective implementation of PROCOVA-MMRM, finding that the same or better results could have been achieved with fewer participants in these historical trials if the enhanced precision provided by PROCOVA-MMRM had been prospectively leveraged. We also confirm the robustness of the statistical properties of PROCOVA-MMRM in a variety of realistic simulation scenarios. Altogether, PROCOVA-MMRM represents a rigorous method of incorporating advances in the prediction of time-matched prognostic scores generated by AI into longitudinal analysis, potentially reducing both the cost and time required to bring new treatments to patients while adhering to regulatory standards.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17576&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jessica L. Ross , Arman Sabbaghi , Run Zhuang , Daniele Bertolini , the Alzheimer&apos;s Disease Cooperative Study , Alzheimer&apos;s Disease Neuroimaging Initiative , the Critical Path for Alzheimer&apos;s Disease Database , the European Prevention of Alzheimer&apos;s Disease ,  Consortium, the Pooled Resource Open-Access ALS Clinical Trials Consortium</name></author><category term="stat.AP" /><summary type="html">Clinical trials are critical in advancing medical treatments but often suffer from immense time and financial burden. Advances in statistical methodologies and artificial intelligence (AI) present opportunities to address these inefficiencies. Here we introduce Prognostic Covariate-Adjusted Mixed Models for Repeated Measures (PROCOVA-MMRM) as an advantageous combination of prognostic covariate adjustment (PROCOVA) and Mixed Models for Repeated Measures (MMRM). PROCOVA-MMRM utilizes time-matched prognostic scores generated from AI models to enhance the precision of treatment effect estimators for longitudinal continuous outcomes, enabling reductions in sample size and enrollment times. We first provide a description of the background and implementation of PROCOVA-MMRM, followed by two case study reanalyses where we compare the performance of PROCOVA-MMRM versus the unadjusted MMRM. These reanalyses demonstrate significant improvements in statistical power and precision in clinical indications with unmet medical need, specifically Alzheimer’s Disease (AD) and Amyotrophic Lateral Sclerosis (ALS). We also explore the potential for sample size reduction with the prospective implementation of PROCOVA-MMRM, finding that the same or better results could have been achieved with fewer participants in these historical trials if the enhanced precision provided by PROCOVA-MMRM had been prospectively leveraged. We also confirm the robustness of the statistical properties of PROCOVA-MMRM in a variety of realistic simulation scenarios. Altogether, PROCOVA-MMRM represents a rigorous method of incorporating advances in the prediction of time-matched prognostic scores generated by AI into longitudinal analysis, potentially reducing both the cost and time required to bring new treatments to patients while adhering to regulatory standards.</summary></entry><entry><title type="html">Full Characterization of Adaptively Strong Majority Voting in Crowdsourcing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/FullCharacterizationofAdaptivelyStrongMajorityVotinginCrowdsourcing.html" rel="alternate" type="text/html" title="Full Characterization of Adaptively Strong Majority Voting in Crowdsourcing" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/FullCharacterizationofAdaptivelyStrongMajorityVotinginCrowdsourcing</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/FullCharacterizationofAdaptivelyStrongMajorityVotinginCrowdsourcing.html">&lt;p&gt;In crowdsourcing, quality control is commonly achieved by having workers examine items and vote on their correctness. To minimize the impact of unreliable worker responses, a $\delta$-margin voting process is utilized, where additional votes are solicited until a predetermined threshold $\delta$ for agreement between workers is exceeded. The process is widely adopted but only as a heuristic. Our research presents a modeling approach using absorbing Markov chains to analyze the characteristics of this voting process that matter in crowdsourced processes. We provide closed-form equations for the quality of resulting consensus vote, the expected number of votes required for consensus, the variance of vote requirements, and other distribution moments. Our findings demonstrate how the threshold $\delta$ can be adjusted to achieve quality equivalence across voting processes that employ workers with varying accuracy levels. We also provide efficiency-equalizing payment rates for voting processes with different expected response accuracy levels. Additionally, our model considers items with varying degrees of difficulty and uncertainty about the difficulty of each example. Our simulations, using real-world crowdsourced vote data, validate the effectiveness of our theoretical model in characterizing the consensus aggregation process. The results of our study can be effectively employed in practical crowdsourcing applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.06390&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Margarita Boyarskaya, Panos Ipeirotis</name></author><category term="stat.AP" /><summary type="html">In crowdsourcing, quality control is commonly achieved by having workers examine items and vote on their correctness. To minimize the impact of unreliable worker responses, a $\delta$-margin voting process is utilized, where additional votes are solicited until a predetermined threshold $\delta$ for agreement between workers is exceeded. The process is widely adopted but only as a heuristic. Our research presents a modeling approach using absorbing Markov chains to analyze the characteristics of this voting process that matter in crowdsourced processes. We provide closed-form equations for the quality of resulting consensus vote, the expected number of votes required for consensus, the variance of vote requirements, and other distribution moments. Our findings demonstrate how the threshold $\delta$ can be adjusted to achieve quality equivalence across voting processes that employ workers with varying accuracy levels. We also provide efficiency-equalizing payment rates for voting processes with different expected response accuracy levels. Additionally, our model considers items with varying degrees of difficulty and uncertainty about the difficulty of each example. Our simulations, using real-world crowdsourced vote data, validate the effectiveness of our theoretical model in characterizing the consensus aggregation process. The results of our study can be effectively employed in practical crowdsourcing applications.</summary></entry><entry><title type="html">Lazy Data Practices Harm Fairness Research</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/LazyDataPracticesHarmFairnessResearch.html" rel="alternate" type="text/html" title="Lazy Data Practices Harm Fairness Research" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/LazyDataPracticesHarmFairnessResearch</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/LazyDataPracticesHarmFairnessResearch.html">&lt;p&gt;Data practices shape research and practice on fairness in machine learning (fair ML). Critical data studies offer important reflections and critiques for the responsible advancement of the field by highlighting shortcomings and proposing recommendations for improvement. In this work, we present a comprehensive analysis of fair ML datasets, demonstrating how unreflective yet common practices hinder the reach and reliability of algorithmic fairness findings. We systematically study protected information encoded in tabular datasets and their usage in 280 experiments across 142 publications.
  Our analyses identify three main areas of concern: (1) a \textbf{lack of representation for certain protected attributes} in both data and evaluations; (2) the widespread \textbf{exclusion of minorities} during data preprocessing; and (3) \textbf{opaque data processing} threatening the generalization of fairness research. By conducting exemplary analyses on the utilization of prominent datasets, we demonstrate how unreflective data decisions disproportionately affect minority groups, fairness metrics, and resultant model comparisons. Additionally, we identify supplementary factors such as limitations in publicly available data, privacy considerations, and a general lack of awareness, which exacerbate these challenges. To address these issues, we propose a set of recommendations for data usage in fairness research centered on transparency and responsible inclusion. This study underscores the need for a critical reevaluation of data practices in fair ML and offers directions to improve both the sourcing and usage of datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17293&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jan Simson, Alessandro Fabris, Christoph Kern</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Data practices shape research and practice on fairness in machine learning (fair ML). Critical data studies offer important reflections and critiques for the responsible advancement of the field by highlighting shortcomings and proposing recommendations for improvement. In this work, we present a comprehensive analysis of fair ML datasets, demonstrating how unreflective yet common practices hinder the reach and reliability of algorithmic fairness findings. We systematically study protected information encoded in tabular datasets and their usage in 280 experiments across 142 publications. Our analyses identify three main areas of concern: (1) a \textbf{lack of representation for certain protected attributes} in both data and evaluations; (2) the widespread \textbf{exclusion of minorities} during data preprocessing; and (3) \textbf{opaque data processing} threatening the generalization of fairness research. By conducting exemplary analyses on the utilization of prominent datasets, we demonstrate how unreflective data decisions disproportionately affect minority groups, fairness metrics, and resultant model comparisons. Additionally, we identify supplementary factors such as limitations in publicly available data, privacy considerations, and a general lack of awareness, which exacerbate these challenges. To address these issues, we propose a set of recommendations for data usage in fairness research centered on transparency and responsible inclusion. This study underscores the need for a critical reevaluation of data practices in fair ML and offers directions to improve both the sourcing and usage of datasets.</summary></entry><entry><title type="html">Leveraging Quadratic Polynomials in Python for Advanced Data Analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/LeveragingQuadraticPolynomialsinPythonforAdvancedDataAnalysis.html" rel="alternate" type="text/html" title="Leveraging Quadratic Polynomials in Python for Advanced Data Analysis" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/LeveragingQuadraticPolynomialsinPythonforAdvancedDataAnalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/LeveragingQuadraticPolynomialsinPythonforAdvancedDataAnalysis.html">&lt;p&gt;Objectives: This study aims to provide a comprehensive overview of the role of quadratic polynomials in data modeling and analysis, particularly in representing the curvature of natural phenomena. Methods: We begin with a fundamental explanation of quadratic polynomials and describe their general forms and theoretical significance. We then explored the application of these polynomials in regression analysis, detailing the process of fitting quadratic models to the data using Python libraries NumPy and Matplotlib. The methodology also included calculation of the coefficient of determination (R-squared) to evaluate the polynomial model fit. Results: Using practical examples accompanied by Python scripts, this study demonstrated the application of quadratic polynomials to analyze data patterns. These examples illustrate the utility of quadratic models in applied analytics. Conclusions: This study bridges the gap between theoretical mathematical concepts and practical data analysis, thereby enhancing the understanding and interpretation of the data patterns. Furthermore, its implementation in Python, released under MIT license, offers an accessible tool for public use.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.06133&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rostyslav Sipakov, Olena Voloshkina, Anastasiia Kovalova</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Objectives: This study aims to provide a comprehensive overview of the role of quadratic polynomials in data modeling and analysis, particularly in representing the curvature of natural phenomena. Methods: We begin with a fundamental explanation of quadratic polynomials and describe their general forms and theoretical significance. We then explored the application of these polynomials in regression analysis, detailing the process of fitting quadratic models to the data using Python libraries NumPy and Matplotlib. The methodology also included calculation of the coefficient of determination (R-squared) to evaluate the polynomial model fit. Results: Using practical examples accompanied by Python scripts, this study demonstrated the application of quadratic polynomials to analyze data patterns. These examples illustrate the utility of quadratic models in applied analytics. Conclusions: This study bridges the gap between theoretical mathematical concepts and practical data analysis, thereby enhancing the understanding and interpretation of the data patterns. Furthermore, its implementation in Python, released under MIT license, offers an accessible tool for public use.</summary></entry><entry><title type="html">Measurement and comparison of distributional shift with applications to ecology, economics, and image analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Measurementandcomparisonofdistributionalshiftwithapplicationstoecologyeconomicsandimageanalysis.html" rel="alternate" type="text/html" title="Measurement and comparison of distributional shift with applications to ecology, economics, and image analysis" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Measurementandcomparisonofdistributionalshiftwithapplicationstoecologyeconomicsandimageanalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Measurementandcomparisonofdistributionalshiftwithapplicationstoecologyeconomicsandimageanalysis.html">&lt;p&gt;The concept of shift is often invoked to describe directional differences in statistical moments but has not yet been established as a property of individual distributions. In the present study, we define distributional shift (DS) as the concentration of frequencies towards the lowest discrete class and derive its measurement from the sum of cumulative frequencies. We use empirical datasets to demonstrate DS as an advantageous measure of ecological rarity and as a generalisable measure of poverty and scarcity. We then define relative distributional shift (RDS) as the difference in DS between distributions, yielding a uniquely signed (i.e., directional) measure. Using simulated random sampling, we show that RDS is closely related to measures of distance, divergence, intersection, and probabilistic scoring. We apply RDS to image analysis by demonstrating its performance in the detection of light events, changes in complex patterns, patterns within visual noise, and colour shifts. Altogether, DS is an intuitive statistical property that underpins a uniquely useful comparative measure.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.11119&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kenneth J. Locey, Brian D. Stein</name></author><category term="stat.ME" /><summary type="html">The concept of shift is often invoked to describe directional differences in statistical moments but has not yet been established as a property of individual distributions. In the present study, we define distributional shift (DS) as the concentration of frequencies towards the lowest discrete class and derive its measurement from the sum of cumulative frequencies. We use empirical datasets to demonstrate DS as an advantageous measure of ecological rarity and as a generalisable measure of poverty and scarcity. We then define relative distributional shift (RDS) as the difference in DS between distributions, yielding a uniquely signed (i.e., directional) measure. Using simulated random sampling, we show that RDS is closely related to measures of distance, divergence, intersection, and probabilistic scoring. We apply RDS to image analysis by demonstrating its performance in the detection of light events, changes in complex patterns, patterns within visual noise, and colour shifts. Altogether, DS is an intuitive statistical property that underpins a uniquely useful comparative measure.</summary></entry><entry><title type="html">Natural Gradient Variational Bayes without Fisher Matrix Analytic Calculation and Its Inversion</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/NaturalGradientVariationalBayeswithoutFisherMatrixAnalyticCalculationandItsInversion.html" rel="alternate" type="text/html" title="Natural Gradient Variational Bayes without Fisher Matrix Analytic Calculation and Its Inversion" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/NaturalGradientVariationalBayeswithoutFisherMatrixAnalyticCalculationandItsInversion</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/NaturalGradientVariationalBayeswithoutFisherMatrixAnalyticCalculationandItsInversion.html">&lt;p&gt;This paper introduces a method for efficiently approximating the inverse of the Fisher information matrix, a crucial step in achieving effective variational Bayes inference. A notable aspect of our approach is the avoidance of analytically computing the Fisher information matrix and its explicit inversion. Instead, we introduce an iterative procedure for generating a sequence of matrices that converge to the inverse of Fisher information. The natural gradient variational Bayes algorithm without analytic expression of the Fisher matrix and its inversion is provably convergent and achieves a convergence rate of order O(log s/s), with s the number of iterations. We also obtain a central limit theorem for the iterates. Implementation of our method does not require storage of large matrices, and achieves a linear complexity in the number of variational parameters. Our algorithm exhibits versatility, making it applicable across a diverse array of variational Bayes domains, including Gaussian approximation and normalizing flow Variational Bayes. We offer a range of numerical examples to demonstrate the efficiency and reliability of the proposed variational Bayes method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.09633&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>A. Godichon-Baggioni, D. Nguyen, M-N Tran</name></author><category term="stat.ME" /><summary type="html">This paper introduces a method for efficiently approximating the inverse of the Fisher information matrix, a crucial step in achieving effective variational Bayes inference. A notable aspect of our approach is the avoidance of analytically computing the Fisher information matrix and its explicit inversion. Instead, we introduce an iterative procedure for generating a sequence of matrices that converge to the inverse of Fisher information. The natural gradient variational Bayes algorithm without analytic expression of the Fisher matrix and its inversion is provably convergent and achieves a convergence rate of order O(log s/s), with s the number of iterations. We also obtain a central limit theorem for the iterates. Implementation of our method does not require storage of large matrices, and achieves a linear complexity in the number of variational parameters. Our algorithm exhibits versatility, making it applicable across a diverse array of variational Bayes domains, including Gaussian approximation and normalizing flow Variational Bayes. We offer a range of numerical examples to demonstrate the efficiency and reliability of the proposed variational Bayes method.</summary></entry><entry><title type="html">Neyman Meets Causal Machine Learning: Experimental Evaluation of Individualized Treatment Rules</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/NeymanMeetsCausalMachineLearningExperimentalEvaluationofIndividualizedTreatmentRules.html" rel="alternate" type="text/html" title="Neyman Meets Causal Machine Learning: Experimental Evaluation of Individualized Treatment Rules" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/NeymanMeetsCausalMachineLearningExperimentalEvaluationofIndividualizedTreatmentRules</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/NeymanMeetsCausalMachineLearningExperimentalEvaluationofIndividualizedTreatmentRules.html">&lt;p&gt;A century ago, Neyman showed how to evaluate the efficacy of treatment using a randomized experiment under a minimal set of assumptions. This classical repeated sampling framework serves as a basis of routine experimental analyses conducted by today’s scientists across disciplines. In this paper, we demonstrate that Neyman’s methodology can also be used to experimentally evaluate the efficacy of individualized treatment rules (ITRs), which are derived by modern causal machine learning algorithms. In particular, we show how to account for additional uncertainty resulting from a training process based on cross-fitting. The primary advantage of Neyman’s approach is that it can be applied to any ITR regardless of the properties of machine learning algorithms that are used to derive the ITR. We also show, somewhat surprisingly, that for certain metrics, it is more efficient to conduct this ex-post experimental evaluation of an ITR than to conduct an ex-ante experimental evaluation that randomly assigns some units to the ITR. Our analysis demonstrates that Neyman’s repeated sampling framework is as relevant for causal inference today as it has been since its inception.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17019&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael Lingzhi Li, Kosuke Imai</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">A century ago, Neyman showed how to evaluate the efficacy of treatment using a randomized experiment under a minimal set of assumptions. This classical repeated sampling framework serves as a basis of routine experimental analyses conducted by today’s scientists across disciplines. In this paper, we demonstrate that Neyman’s methodology can also be used to experimentally evaluate the efficacy of individualized treatment rules (ITRs), which are derived by modern causal machine learning algorithms. In particular, we show how to account for additional uncertainty resulting from a training process based on cross-fitting. The primary advantage of Neyman’s approach is that it can be applied to any ITR regardless of the properties of machine learning algorithms that are used to derive the ITR. We also show, somewhat surprisingly, that for certain metrics, it is more efficient to conduct this ex-post experimental evaluation of an ITR than to conduct an ex-ante experimental evaluation that randomly assigns some units to the ITR. Our analysis demonstrates that Neyman’s repeated sampling framework is as relevant for causal inference today as it has been since its inception.</summary></entry><entry><title type="html">Notes on the Practical Application of Nested Sampling: MultiNest, (Non)convergence, and Rectification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/NotesonthePracticalApplicationofNestedSamplingMultiNestNonconvergenceandRectification.html" rel="alternate" type="text/html" title="Notes on the Practical Application of Nested Sampling: MultiNest, (Non)convergence, and Rectification" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/NotesonthePracticalApplicationofNestedSamplingMultiNestNonconvergenceandRectification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/NotesonthePracticalApplicationofNestedSamplingMultiNestNonconvergenceandRectification.html">&lt;p&gt;Nested sampling is a promising tool for Bayesian statistical analysis because it simultaneously performs parameter estimation and facilitates model comparison. MultiNest is one of the most popular nested sampling implementations, and has been applied to a wide variety of problems in the physical sciences. However, MultiNest results are frequently unreliable, and accompanying convergence tests are a necessary component of any analysis. Using simple, analytically tractable test problems, I illustrate how MultiNest (1) can produce systematically biased estimates of the Bayesian evidence, which are more significantly biased for problems of higher dimensionality; (2) can derive posterior estimates with errors on the order of $\sim100\%$; (3) is more likely to underestimate the width of a credible interval than to overestimate it - to a minor degree for smooth problems, but much more so when sampling noisy likelihoods. Nevertheless, I show how MultiNest can be used to jump-start Markov chain Monte Carlo sampling or more rigorous nested sampling techniques, potentially accelerating more trustworthy measurements of posterior distributions and Bayesian evidences, and overcoming the challenge of Markov chain Monte Carlo initialization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.16928&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexander J. Dittmann</name></author><category term="stat.CO" /><summary type="html">Nested sampling is a promising tool for Bayesian statistical analysis because it simultaneously performs parameter estimation and facilitates model comparison. MultiNest is one of the most popular nested sampling implementations, and has been applied to a wide variety of problems in the physical sciences. However, MultiNest results are frequently unreliable, and accompanying convergence tests are a necessary component of any analysis. Using simple, analytically tractable test problems, I illustrate how MultiNest (1) can produce systematically biased estimates of the Bayesian evidence, which are more significantly biased for problems of higher dimensionality; (2) can derive posterior estimates with errors on the order of $\sim100\%$; (3) is more likely to underestimate the width of a credible interval than to overestimate it - to a minor degree for smooth problems, but much more so when sampling noisy likelihoods. Nevertheless, I show how MultiNest can be used to jump-start Markov chain Monte Carlo sampling or more rigorous nested sampling techniques, potentially accelerating more trustworthy measurements of posterior distributions and Bayesian evidences, and overcoming the challenge of Markov chain Monte Carlo initialization.</summary></entry><entry><title type="html">On Elliptical and Inverse Elliptical Wishart distributions: Review, new results, and applications</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/OnEllipticalandInverseEllipticalWishartdistributionsReviewnewresultsandapplications.html" rel="alternate" type="text/html" title="On Elliptical and Inverse Elliptical Wishart distributions: Review, new results, and applications" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/OnEllipticalandInverseEllipticalWishartdistributionsReviewnewresultsandapplications</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/OnEllipticalandInverseEllipticalWishartdistributionsReviewnewresultsandapplications.html">&lt;p&gt;This paper deals with matrix-variate distributions, from Wishart to Inverse Elliptical Wishart distributions over the set of symmetric definite positive matrices. Similar to the multivariate scenario, (Inverse) Elliptical Wishart distributions form a vast and general family of distributions, encompassing, for instance, Wishart or $t$-Wishart ones. The first objective of this study is to present a unified overview of Wishart, Inverse Wishart, Elliptical Wishart, and Inverse Elliptical Wishart distributions through their fundamental properties. This involves leveraging the stochastic representation of these distributions to establish key statistical properties of the Normalized Wishart distribution. Subsequently, this enables the computation of expectations, variances, and Kronecker moments for Elliptical Wishart and Inverse Elliptical Wishart distributions. As an illustrative application, the practical utility of these generalized Elliptical Wishart distributions is demonstrated using a real electroencephalographic dataset. This showcases their effectiveness in accurately modeling heterogeneous data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17468&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Imen Ayadi, Florent Bouchard, Frédéric Pascal</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This paper deals with matrix-variate distributions, from Wishart to Inverse Elliptical Wishart distributions over the set of symmetric definite positive matrices. Similar to the multivariate scenario, (Inverse) Elliptical Wishart distributions form a vast and general family of distributions, encompassing, for instance, Wishart or $t$-Wishart ones. The first objective of this study is to present a unified overview of Wishart, Inverse Wishart, Elliptical Wishart, and Inverse Elliptical Wishart distributions through their fundamental properties. This involves leveraging the stochastic representation of these distributions to establish key statistical properties of the Normalized Wishart distribution. Subsequently, this enables the computation of expectations, variances, and Kronecker moments for Elliptical Wishart and Inverse Elliptical Wishart distributions. As an illustrative application, the practical utility of these generalized Elliptical Wishart distributions is demonstrated using a real electroencephalographic dataset. This showcases their effectiveness in accurately modeling heterogeneous data.</summary></entry><entry><title type="html">PHYSTAT Informal Review: Marginalizing versus Profiling of Nuisance Parameters</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/PHYSTATInformalReviewMarginalizingversusProfilingofNuisanceParameters.html" rel="alternate" type="text/html" title="PHYSTAT Informal Review: Marginalizing versus Profiling of Nuisance Parameters" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/PHYSTATInformalReviewMarginalizingversusProfilingofNuisanceParameters</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/PHYSTATInformalReviewMarginalizingversusProfilingofNuisanceParameters.html">&lt;p&gt;This is a writeup, with some elaboration, of the talks by the two authors (a physicist and a statistician) at the first PHYSTAT Informal review on January 24, 2024. We discuss Bayesian and frequentist approaches to dealing with nuisance parameters, in particular, integrated versus profiled likelihood methods. In regular models, with finitely many parameters and large sample sizes, the two approaches are asymptotically equivalent. But, outside this setting, the two methods can lead to different tests and confidence intervals. Assessing which approach is better generally requires comparing the power of the tests or the length of the confidence intervals. This analysis has to be conducted on a case-by-case basis. In the extreme case where the number of nuisance parameters is very large, possibly infinite, neither approach may be useful. Part I provides an informal history of usage in high energy particle physics, including a simple illustrative example. Part II includes an overview of some more recently developed methods in the statistics literature, including methods applicable when the use of the likelihood function is problematic.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17180&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Robert D. Cousins, Larry Wasserman</name></author><category term="stat.AP" /><summary type="html">This is a writeup, with some elaboration, of the talks by the two authors (a physicist and a statistician) at the first PHYSTAT Informal review on January 24, 2024. We discuss Bayesian and frequentist approaches to dealing with nuisance parameters, in particular, integrated versus profiled likelihood methods. In regular models, with finitely many parameters and large sample sizes, the two approaches are asymptotically equivalent. But, outside this setting, the two methods can lead to different tests and confidence intervals. Assessing which approach is better generally requires comparing the power of the tests or the length of the confidence intervals. This analysis has to be conducted on a case-by-case basis. In the extreme case where the number of nuisance parameters is very large, possibly infinite, neither approach may be useful. Part I provides an informal history of usage in high energy particle physics, including a simple illustrative example. Part II includes an overview of some more recently developed methods in the statistics literature, including methods applicable when the use of the likelihood function is problematic.</summary></entry><entry><title type="html">Predictive Modelling of Critical Variables for Improving HVOF Coating using Gamma Regression Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/PredictiveModellingofCriticalVariablesforImprovingHVOFCoatingusingGammaRegressionModels.html" rel="alternate" type="text/html" title="Predictive Modelling of Critical Variables for Improving HVOF Coating using Gamma Regression Models" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/PredictiveModellingofCriticalVariablesforImprovingHVOFCoatingusingGammaRegressionModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/PredictiveModellingofCriticalVariablesforImprovingHVOFCoatingusingGammaRegressionModels.html">&lt;p&gt;Thermal spray coating is a critical process in many industries, involving the application of coatings to surfaces to enhance their functionality. This paper proposes a framework for modelling and predicting critical target variables in thermal spray coating processes, based on the application of statistical design of experiments (DoE) and the modelling of the data using generalized linear models (GLMs) with a particular emphasis on gamma regression. Experimental data obtained from thermal spray coating trials are used to validate the presented approach, demonstrating that it is able to accurately model and predict critical target variables. As such, the framework has significant potential for the optimization of thermal spray coating processes, and can contribute to the development of more efficient and effective coating technologies in various industries.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.01194&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wolfgang Rannetbauer, Simon Hubmer, Carina Hambrock, Ronny Ramlau</name></author><category term="stat.AP" /><summary type="html">Thermal spray coating is a critical process in many industries, involving the application of coatings to surfaces to enhance their functionality. This paper proposes a framework for modelling and predicting critical target variables in thermal spray coating processes, based on the application of statistical design of experiments (DoE) and the modelling of the data using generalized linear models (GLMs) with a particular emphasis on gamma regression. Experimental data obtained from thermal spray coating trials are used to validate the presented approach, demonstrating that it is able to accurately model and predict critical target variables. As such, the framework has significant potential for the optimization of thermal spray coating processes, and can contribute to the development of more efficient and effective coating technologies in various industries.</summary></entry><entry><title type="html">Pseudo-Observations and Super Learner for the Estimation of the Restricted Mean Survival Time</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/PseudoObservationsandSuperLearnerfortheEstimationoftheRestrictedMeanSurvivalTime.html" rel="alternate" type="text/html" title="Pseudo-Observations and Super Learner for the Estimation of the Restricted Mean Survival Time" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/PseudoObservationsandSuperLearnerfortheEstimationoftheRestrictedMeanSurvivalTime</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/PseudoObservationsandSuperLearnerfortheEstimationoftheRestrictedMeanSurvivalTime.html">&lt;p&gt;In the context of right-censored data, we study the problem of predicting the restricted time to event based on a set of covariates. Under a quadratic loss, this problem is equivalent to estimating the conditional Restricted Mean Survival Time (RMST). To that aim, we propose a flexible and easy-to-use ensemble algorithm that combines pseudo-observations and super learner. The classical theoretical results of the super learner are extended to right-censored data, using a new definition of pseudo-observations, the so-called split pseudo-observations. Simulation studies indicate that the split pseudo-observations and the standard pseudo-observations are similar even for small sample sizes. The method is applied to maintenance and colon cancer datasets, showing the interest of the method in practice, as compared to other prediction methods. We complement the predictions obtained from our method with our RMST-adapted risk measure, prediction intervals and variable importance measures developed in a previous work.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17211&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ariane Cwiling , Vittorio Perduca , Olivier Bouaziz</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">In the context of right-censored data, we study the problem of predicting the restricted time to event based on a set of covariates. Under a quadratic loss, this problem is equivalent to estimating the conditional Restricted Mean Survival Time (RMST). To that aim, we propose a flexible and easy-to-use ensemble algorithm that combines pseudo-observations and super learner. The classical theoretical results of the super learner are extended to right-censored data, using a new definition of pseudo-observations, the so-called split pseudo-observations. Simulation studies indicate that the split pseudo-observations and the standard pseudo-observations are similar even for small sample sizes. The method is applied to maintenance and colon cancer datasets, showing the interest of the method in practice, as compared to other prediction methods. We complement the predictions obtained from our method with our RMST-adapted risk measure, prediction intervals and variable importance measures developed in a previous work.</summary></entry><entry><title type="html">Structured Conformal Inference for Matrix Completion with Applications to Group Recommender Systems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/StructuredConformalInferenceforMatrixCompletionwithApplicationstoGroupRecommenderSystems.html" rel="alternate" type="text/html" title="Structured Conformal Inference for Matrix Completion with Applications to Group Recommender Systems" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/StructuredConformalInferenceforMatrixCompletionwithApplicationstoGroupRecommenderSystems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/StructuredConformalInferenceforMatrixCompletionwithApplicationstoGroupRecommenderSystems.html">&lt;p&gt;We develop a conformal inference method to construct joint confidence regions for structured groups of missing entries within a sparsely observed matrix. This method is useful to provide reliable uncertainty estimation for group-level collaborative filtering; for example, it can be applied to help suggest a movie for a group of friends to watch together. Unlike standard conformal techniques, which make inferences for one individual at a time, our method achieves stronger group-level guarantees by carefully assembling a structured calibration data set mimicking the patterns expected among the test group of interest. We propose a generalized weighted conformalization framework to deal with the lack of exchangeability arising from such structured calibration, and in this process we introduce several innovations to overcome computational challenges. The practicality and effectiveness of our method are demonstrated through extensive numerical experiments and an analysis of the MovieLens 100K data set.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17561&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ziyi Liang, Tianmin Xie, Xin Tong, Matteo Sesia</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We develop a conformal inference method to construct joint confidence regions for structured groups of missing entries within a sparsely observed matrix. This method is useful to provide reliable uncertainty estimation for group-level collaborative filtering; for example, it can be applied to help suggest a movie for a group of friends to watch together. Unlike standard conformal techniques, which make inferences for one individual at a time, our method achieves stronger group-level guarantees by carefully assembling a structured calibration data set mimicking the patterns expected among the test group of interest. We propose a generalized weighted conformalization framework to deal with the lack of exchangeability arising from such structured calibration, and in this process we introduce several innovations to overcome computational challenges. The practicality and effectiveness of our method are demonstrated through extensive numerical experiments and an analysis of the MovieLens 100K data set.</summary></entry><entry><title type="html">The TruEnd-procedure: Treating trailing zero-valued balances in credit data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/TheTruEndprocedureTreatingtrailingzerovaluedbalancesincreditdata.html" rel="alternate" type="text/html" title="The TruEnd-procedure: Treating trailing zero-valued balances in credit data" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/TheTruEndprocedureTreatingtrailingzerovaluedbalancesincreditdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/TheTruEndprocedureTreatingtrailingzerovaluedbalancesincreditdata.html">&lt;p&gt;A novel procedure is presented for finding the true but latent endpoints within the repayment histories of individual loans. The monthly observations beyond these true endpoints are false, largely due to operational failures that delay account closure, thereby corrupting some loans in the dataset with `false’ observations. Detecting these false observations is difficult at scale since each affected loan history might have a different sequence of zero (or very small) month-end balances that persist towards the end. Identifying these trails of diminutive balances would require an exact definition of a “small balance”, which can be found using our so-called TruEnd-procedure. We demonstrate this procedure and isolate the ideal small-balance definition using residential mortgages from a large South African bank. Evidently, corrupted loans are remarkably prevalent and have excess histories that are surprisingly long, which ruin the timing of certain risk events and compromise any subsequent time-to-event model such as survival analysis. Excess histories can be discarded using the ideal small-balance definition, which demonstrably improves the accuracy of both the predicted timing and severity of risk events, without materially impacting the monetary value of the portfolio. The resulting estimates of credit losses are lower and less biased, which augurs well for raising accurate credit impairments under the IFRS 9 accounting standard. Our work therefore addresses a pernicious data error, which highlights the pivotal role of data preparation in producing credible forecasts of credit risk.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17008&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arno Botha, Tanja Verster, Roelinde Bester</name></author><category term="stat.AP" /><summary type="html">A novel procedure is presented for finding the true but latent endpoints within the repayment histories of individual loans. The monthly observations beyond these true endpoints are false, largely due to operational failures that delay account closure, thereby corrupting some loans in the dataset with `false’ observations. Detecting these false observations is difficult at scale since each affected loan history might have a different sequence of zero (or very small) month-end balances that persist towards the end. Identifying these trails of diminutive balances would require an exact definition of a “small balance”, which can be found using our so-called TruEnd-procedure. We demonstrate this procedure and isolate the ideal small-balance definition using residential mortgages from a large South African bank. Evidently, corrupted loans are remarkably prevalent and have excess histories that are surprisingly long, which ruin the timing of certain risk events and compromise any subsequent time-to-event model such as survival analysis. Excess histories can be discarded using the ideal small-balance definition, which demonstrably improves the accuracy of both the predicted timing and severity of risk events, without materially impacting the monetary value of the portfolio. The resulting estimates of credit losses are lower and less biased, which augurs well for raising accurate credit impairments under the IFRS 9 accounting standard. Our work therefore addresses a pernicious data error, which highlights the pivotal role of data preparation in producing credible forecasts of credit risk.</summary></entry><entry><title type="html">The effect of estimating prevalences on the population-wise error rate</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Theeffectofestimatingprevalencesonthepopulationwiseerrorrate.html" rel="alternate" type="text/html" title="The effect of estimating prevalences on the population-wise error rate" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Theeffectofestimatingprevalencesonthepopulationwiseerrorrate</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Theeffectofestimatingprevalencesonthepopulationwiseerrorrate.html">&lt;p&gt;The population-wise error rate (PWER) is a type I error rate for clinical trials with multiple target populations. In such trials, one treatment is tested for its efficacy in each population. The PWER is defined as the probability that a randomly selected, future patient will be exposed to an inefficient treatment based on the study results. The PWER can be understood and computed as an average of strata-specific family-wise error rates and involves the prevalences of these strata. A major issue of this concept is that the prevalences are usually not known in practice, so that the PWER cannot be directly controlled. Instead, one could use an estimator based on the given sample, like their maximum-likelihood estimator under a multinomial distribution. In this paper we show in simulations that this does not substantially inflate the true PWER. We differentiate between the expected PWER, which is almost perfectly controlled, and study-specific values of the PWER which are conditioned on all subgroup sample sizes and vary within a narrow range. Thereby, we consider up to eight different overlapping patient populations and moderate to large sample sizes. In these settings, we also consider the maximum strata-wise family-wise error rate, which is found to be, on average, at least bounded by twice the significance level used for PWER control. Finally, we introduce an adjustment of the PWER that could be made when, by chance, no patients are recruited from a stratum, so that this stratum is not counted in PWER control. We would then reduce the PWER in order to control for multiplicity in this stratum as well.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.09988&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Remi Luschei, Werner Brannath</name></author><category term="stat.ME" /><summary type="html">The population-wise error rate (PWER) is a type I error rate for clinical trials with multiple target populations. In such trials, one treatment is tested for its efficacy in each population. The PWER is defined as the probability that a randomly selected, future patient will be exposed to an inefficient treatment based on the study results. The PWER can be understood and computed as an average of strata-specific family-wise error rates and involves the prevalences of these strata. A major issue of this concept is that the prevalences are usually not known in practice, so that the PWER cannot be directly controlled. Instead, one could use an estimator based on the given sample, like their maximum-likelihood estimator under a multinomial distribution. In this paper we show in simulations that this does not substantially inflate the true PWER. We differentiate between the expected PWER, which is almost perfectly controlled, and study-specific values of the PWER which are conditioned on all subgroup sample sizes and vary within a narrow range. Thereby, we consider up to eight different overlapping patient populations and moderate to large sample sizes. In these settings, we also consider the maximum strata-wise family-wise error rate, which is found to be, on average, at least bounded by twice the significance level used for PWER control. Finally, we introduce an adjustment of the PWER that could be made when, by chance, no patients are recruited from a stratum, so that this stratum is not counted in PWER control. We would then reduce the PWER in order to control for multiplicity in this stratum as well.</summary></entry><entry><title type="html">Uncertainty Quantification and Confidence Intervals for Naive Rare-Event Estimators</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/UncertaintyQuantificationandConfidenceIntervalsforNaiveRareEventEstimators.html" rel="alternate" type="text/html" title="Uncertainty Quantification and Confidence Intervals for Naive Rare-Event Estimators" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/UncertaintyQuantificationandConfidenceIntervalsforNaiveRareEventEstimators</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/UncertaintyQuantificationandConfidenceIntervalsforNaiveRareEventEstimators.html">&lt;p&gt;We consider the estimation of rare-event probabilities using sample proportions output by naive Monte Carlo or collected data. Unlike using variance reduction techniques, this naive estimator does not have a priori relative efficiency guarantee. On the other hand, due to the recent surge of sophisticated rare-event problems arising in safety evaluations of intelligent systems, efficiency-guaranteed variance reduction may face implementation challenges which, coupled with the availability of computation or data collection power, motivate the use of such a naive estimator. In this paper we study the uncertainty quantification, namely the construction, coverage validity and tightness of confidence intervals, for rare-event probabilities using only sample proportions. In addition to the known normality, Wilson’s and exact intervals, we investigate and compare them with two new intervals derived from Chernoff’s inequality and the Berry-Esseen theorem. Moreover, we generalize our results to the natural situation where sampling stops by reaching a target number of rare-event hits. Our findings show that the normality and Wilson’s intervals are not always valid, but they are close to the newly developed valid intervals in terms of half-width. In contrast, the exact interval is conservative, but safely guarantees the attainment of the nominal confidence level. Our new intervals, while being more conservative than the exact interval, provide useful insights in understanding the tightness of the considered intervals.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.02434&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuanlu Bai, Henry Lam</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We consider the estimation of rare-event probabilities using sample proportions output by naive Monte Carlo or collected data. Unlike using variance reduction techniques, this naive estimator does not have a priori relative efficiency guarantee. On the other hand, due to the recent surge of sophisticated rare-event problems arising in safety evaluations of intelligent systems, efficiency-guaranteed variance reduction may face implementation challenges which, coupled with the availability of computation or data collection power, motivate the use of such a naive estimator. In this paper we study the uncertainty quantification, namely the construction, coverage validity and tightness of confidence intervals, for rare-event probabilities using only sample proportions. In addition to the known normality, Wilson’s and exact intervals, we investigate and compare them with two new intervals derived from Chernoff’s inequality and the Berry-Esseen theorem. Moreover, we generalize our results to the natural situation where sampling stops by reaching a target number of rare-event hits. Our findings show that the normality and Wilson’s intervals are not always valid, but they are close to the newly developed valid intervals in terms of half-width. In contrast, the exact interval is conservative, but safely guarantees the attainment of the nominal confidence level. Our new intervals, while being more conservative than the exact interval, provide useful insights in understanding the tightness of the considered intervals.</summary></entry><entry><title type="html">What To Do (and Not to Do) with Causal Panel Analysis under Parallel Trends: Lessons from A Large Reanalysis Study</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/WhatToDoandNottoDowithCausalPanelAnalysisunderParallelTrendsLessonsfromALargeReanalysisStudy.html" rel="alternate" type="text/html" title="What To Do (and Not to Do) with Causal Panel Analysis under Parallel Trends: Lessons from A Large Reanalysis Study" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/WhatToDoandNottoDowithCausalPanelAnalysisunderParallelTrendsLessonsfromALargeReanalysisStudy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/WhatToDoandNottoDowithCausalPanelAnalysisunderParallelTrendsLessonsfromALargeReanalysisStudy.html">&lt;p&gt;Two-way fixed effects (TWFE) models are ubiquitous in causal panel analysis in political science. However, recent methodological discussions challenge their validity in the presence of heterogeneous treatment effects (HTE) and violations of the parallel trends assumption (PTA). This burgeoning literature has introduced multiple estimators and diagnostics, leading to confusion among empirical researchers on two fronts: the reliability of existing results based on TWFE models and the current best practices. To address these concerns, we examined, replicated, and reanalyzed 37 articles from three leading political science journals that employed observational panel data with binary treatments. Using six newly introduced HTE-robust estimators, we find that although precision may be affected, the core conclusions derived from TWFE estimates largely remain unchanged. PTA violations and insufficient statistical power, however, continue to be significant obstacles to credible inferences. Based on these findings, we offer recommendations for improving practice in empirical research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.15983&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Albert Chiu, Xingchen Lan, Ziyi Liu, Yiqing Xu</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Two-way fixed effects (TWFE) models are ubiquitous in causal panel analysis in political science. However, recent methodological discussions challenge their validity in the presence of heterogeneous treatment effects (HTE) and violations of the parallel trends assumption (PTA). This burgeoning literature has introduced multiple estimators and diagnostics, leading to confusion among empirical researchers on two fronts: the reliability of existing results based on TWFE models and the current best practices. To address these concerns, we examined, replicated, and reanalyzed 37 articles from three leading political science journals that employed observational panel data with binary treatments. Using six newly introduced HTE-robust estimators, we find that although precision may be affected, the core conclusions derived from TWFE estimates largely remain unchanged. PTA violations and insufficient statistical power, however, continue to be significant obstacles to credible inferences. Based on these findings, we offer recommendations for improving practice in empirical research.</summary></entry></feed>