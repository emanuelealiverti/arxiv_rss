<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-08T07:14:48+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Primer on the Analysis of Randomized Experiments and a Survey of some Recent Advances</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/APrimerontheAnalysisofRandomizedExperimentsandaSurveyofsomeRecentAdvances.html" rel="alternate" type="text/html" title="A Primer on the Analysis of Randomized Experiments and a Survey of some Recent Advances" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/APrimerontheAnalysisofRandomizedExperimentsandaSurveyofsomeRecentAdvances</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/APrimerontheAnalysisofRandomizedExperimentsandaSurveyofsomeRecentAdvances.html">&lt;p&gt;The past two decades have witnessed a surge of new research in the analysis of randomized experiments. The emergence of this literature may seem surprising given the widespread use and long history of experiments as the “gold standard” in program evaluation, but this body of work has revealed many subtle aspects of randomized experiments that may have been previously unappreciated. This article provides an overview of some of these topics, primarily focused on stratification, regression adjustment, and cluster randomization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03910&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuehao Bai, Azeem M. Shaikh, Max Tabord-Meehan</name></author><category term="stat.ME" /><summary type="html">The past two decades have witnessed a surge of new research in the analysis of randomized experiments. The emergence of this literature may seem surprising given the widespread use and long history of experiments as the “gold standard” in program evaluation, but this body of work has revealed many subtle aspects of randomized experiments that may have been previously unappreciated. This article provides an overview of some of these topics, primarily focused on stratification, regression adjustment, and cluster randomization.</summary></entry><entry><title type="html">A Virtual Solar Wind Monitor at Mars with Uncertainty Quantification using Gaussian Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/AVirtualSolarWindMonitoratMarswithUncertaintyQuantificationusingGaussianProcesses.html" rel="alternate" type="text/html" title="A Virtual Solar Wind Monitor at Mars with Uncertainty Quantification using Gaussian Processes" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/AVirtualSolarWindMonitoratMarswithUncertaintyQuantificationusingGaussianProcesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/AVirtualSolarWindMonitoratMarswithUncertaintyQuantificationusingGaussianProcesses.html">&lt;p&gt;Single spacecraft missions do not measure the pristine solar wind continuously because of the spacecrafts’ orbital trajectory. The infrequent spatiotemporal cadence of measurement fundamentally limits conclusions about solar wind-magnetosphere coupling throughout the solar system. At Mars, such single spacecraft missions result in limitations for assessing the solar wind’s role in causing lower altitude observations such as auroral dynamics or atmospheric loss. In this work, we detail the development of a virtual solar wind monitor from the Mars Atmosphere and Volatile Evolution (MAVEN) mission; a single spacecraft. This virtual solar wind monitor provides a continuous estimate of the solar wind upstream from Mars with uncertainties. We specifically employ Gaussian process regression to estimate the upstream solar wind and uncertainty estimations that scale with the data sparsity of our real observations. This proxy enables continuous solar wind estimation at Mars with representative uncertainties for the majority of the time since since late 2014. We conclude by discussing suggested uses of this virtual solar wind monitor for statistical studies of the Mars space environment and heliosphere.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.01932&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>A. R. Azari, E. Abrahams, F. Sapienza, J. Halekas, J. Biersteker, D. L. Mitchell, F. Pérez, M. Marquette, M. J. Rutala, C. F. Bowers, C. M. Jackman, S. M. Curry</name></author><category term="stat.AP" /><summary type="html">Single spacecraft missions do not measure the pristine solar wind continuously because of the spacecrafts’ orbital trajectory. The infrequent spatiotemporal cadence of measurement fundamentally limits conclusions about solar wind-magnetosphere coupling throughout the solar system. At Mars, such single spacecraft missions result in limitations for assessing the solar wind’s role in causing lower altitude observations such as auroral dynamics or atmospheric loss. In this work, we detail the development of a virtual solar wind monitor from the Mars Atmosphere and Volatile Evolution (MAVEN) mission; a single spacecraft. This virtual solar wind monitor provides a continuous estimate of the solar wind upstream from Mars with uncertainties. We specifically employ Gaussian process regression to estimate the upstream solar wind and uncertainty estimations that scale with the data sparsity of our real observations. This proxy enables continuous solar wind estimation at Mars with representative uncertainties for the majority of the time since since late 2014. We conclude by discussing suggested uses of this virtual solar wind monitor for statistical studies of the Mars space environment and heliosphere.</summary></entry><entry><title type="html">A generalized ordinal quasi-symmetry model and its separability for analyzing multi-way tables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Ageneralizedordinalquasisymmetrymodelanditsseparabilityforanalyzingmultiwaytables.html" rel="alternate" type="text/html" title="A generalized ordinal quasi-symmetry model and its separability for analyzing multi-way tables" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Ageneralizedordinalquasisymmetrymodelanditsseparabilityforanalyzingmultiwaytables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Ageneralizedordinalquasisymmetrymodelanditsseparabilityforanalyzingmultiwaytables.html">&lt;p&gt;This paper addresses the challenge of modeling multi-way contingency tables for matched set data with ordinal categories. Although the complete symmetry and marginal homogeneity models are well established, they may not always provide a satisfactory fit to the data. To address this issue, we propose a generalized ordinal quasi-symmetry model that offers increased flexibility when the complete symmetry model fails to capture the underlying structure. We investigate the properties of this new model and provide an information-theoretic interpretation, elucidating its relationship to the ordinal quasi-symmetry model. Moreover, we revisit Agresti’s findings and present a new necessary and sufficient condition for the complete symmetry model, proving that the proposed model and the marginal moment equality model are separable hypotheses. The separability of the proposed model and marginal moment equality model is a significant development in the analysis of multi-way contingency tables. It enables researchers to examine the symmetry structure in the data with greater precision, providing a more thorough understanding of the underlying patterns. This powerful framework equips researchers with the necessary tools to explore the complexities of ordinal variable relationships in matched set data, paving the way for new discoveries and insights.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04193&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hisaya Okahara, Kouji Tahata</name></author><category term="stat.ME" /><summary type="html">This paper addresses the challenge of modeling multi-way contingency tables for matched set data with ordinal categories. Although the complete symmetry and marginal homogeneity models are well established, they may not always provide a satisfactory fit to the data. To address this issue, we propose a generalized ordinal quasi-symmetry model that offers increased flexibility when the complete symmetry model fails to capture the underlying structure. We investigate the properties of this new model and provide an information-theoretic interpretation, elucidating its relationship to the ordinal quasi-symmetry model. Moreover, we revisit Agresti’s findings and present a new necessary and sufficient condition for the complete symmetry model, proving that the proposed model and the marginal moment equality model are separable hypotheses. The separability of the proposed model and marginal moment equality model is a significant development in the analysis of multi-way contingency tables. It enables researchers to examine the symmetry structure in the data with greater precision, providing a more thorough understanding of the underlying patterns. This powerful framework equips researchers with the necessary tools to explore the complexities of ordinal variable relationships in matched set data, paving the way for new discoveries and insights.</summary></entry><entry><title type="html">A group testing based exploration of age-varying factors in chlamydia infections among Iowa residents</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/AgrouptestingbasedexplorationofagevaryingfactorsinchlamydiainfectionsamongIowaresidents.html" rel="alternate" type="text/html" title="A group testing based exploration of age-varying factors in chlamydia infections among Iowa residents" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/AgrouptestingbasedexplorationofagevaryingfactorsinchlamydiainfectionsamongIowaresidents</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/AgrouptestingbasedexplorationofagevaryingfactorsinchlamydiainfectionsamongIowaresidents.html">&lt;p&gt;Group testing, a method that screens subjects in pooled samples rather than individually, has been employed as a cost-effective strategy for chlamydia screening among Iowa residents. In efforts to deepen our understanding of chlamydia epidemiology in Iowa, several group testing regression models have been proposed. Different than previous approaches, we expand upon the varying coefficient model to capture potential age-varying associations with chlamydia infection risk. In general, our model operates within a Bayesian framework, allowing regression associations to vary with a covariate of key interest. We employ a stochastic search variable selection process for regularization in estimation. Additionally, our model can integrate random effects to consider potential geographical factors and estimate unknown assay accuracy probabilities. The performance of our model is assessed through comprehensive simulation studies. Upon application to the Iowa group testing dataset, we reveal a significant age-varying racial disparity in chlamydia infections. We believe this discovery has the potential to inform the enhancement of interventions and prevention strategies, leading to more effective chlamydia control and management, thereby promoting health equity across all populations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.01469&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yizeng Li, Dewei Wang, Joshua M. Tebbs</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Group testing, a method that screens subjects in pooled samples rather than individually, has been employed as a cost-effective strategy for chlamydia screening among Iowa residents. In efforts to deepen our understanding of chlamydia epidemiology in Iowa, several group testing regression models have been proposed. Different than previous approaches, we expand upon the varying coefficient model to capture potential age-varying associations with chlamydia infection risk. In general, our model operates within a Bayesian framework, allowing regression associations to vary with a covariate of key interest. We employ a stochastic search variable selection process for regularization in estimation. Additionally, our model can integrate random effects to consider potential geographical factors and estimate unknown assay accuracy probabilities. The performance of our model is assessed through comprehensive simulation studies. Upon application to the Iowa group testing dataset, we reveal a significant age-varying racial disparity in chlamydia infections. We believe this discovery has the potential to inform the enhancement of interventions and prevention strategies, leading to more effective chlamydia control and management, thereby promoting health equity across all populations.</summary></entry><entry><title type="html">An Analysis of Sea Level Spatial Variability by Topological Indicators and $k$-means Clustering Algorithm</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/AnAnalysisofSeaLevelSpatialVariabilitybyTopologicalIndicatorsandkmeansClusteringAlgorithm.html" rel="alternate" type="text/html" title="An Analysis of Sea Level Spatial Variability by Topological Indicators and $k$-means Clustering Algorithm" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/AnAnalysisofSeaLevelSpatialVariabilitybyTopologicalIndicatorsandkmeansClusteringAlgorithm</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/AnAnalysisofSeaLevelSpatialVariabilitybyTopologicalIndicatorsandkmeansClusteringAlgorithm.html">&lt;p&gt;The time-series data of sea level rise and fall contains crucial information on the variability of sea level patterns. Traditional $k$-means clustering is commonly used for categorizing regional variability of sea level, however, its results are not robust against a number of factors. This study analyzed fourteen datasets of monthly sea level in fourteen shoreline regions of Peninsular Malaysia. We applied a hybridization of clustering technique to analyze data categorization and topological data analysis method to enhance the performance of our clustering analysis. Specifically, our approach utilized the persistent homology and $k$-means/$k$-means++ clustering. The fourteen data sets from fourteen tide gauge stations were categorized in classes based on a prior categorization that was determined by topological information, and the probability of data points that belong to certain groups that is yielded by $k$-means/$k$-means++ clustering. Our results demonstrated that our method significantly improves the performance of traditional clustering techniques.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04269&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zixin Lin, Nur Fariha Syaqina Zulkepli, Mohd Shareduwan Mohd Kasihmuddin, R. U. Gobithaasan</name></author><category term="stat.AP" /><summary type="html">The time-series data of sea level rise and fall contains crucial information on the variability of sea level patterns. Traditional $k$-means clustering is commonly used for categorizing regional variability of sea level, however, its results are not robust against a number of factors. This study analyzed fourteen datasets of monthly sea level in fourteen shoreline regions of Peninsular Malaysia. We applied a hybridization of clustering technique to analyze data categorization and topological data analysis method to enhance the performance of our clustering analysis. Specifically, our approach utilized the persistent homology and $k$-means/$k$-means++ clustering. The fourteen data sets from fourteen tide gauge stations were categorized in classes based on a prior categorization that was determined by topological information, and the probability of data points that belong to certain groups that is yielded by $k$-means/$k$-means++ clustering. Our results demonstrated that our method significantly improves the performance of traditional clustering techniques.</summary></entry><entry><title type="html">An Autoregressive Model for Time Series of Random Objects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/AnAutoregressiveModelforTimeSeriesofRandomObjects.html" rel="alternate" type="text/html" title="An Autoregressive Model for Time Series of Random Objects" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/AnAutoregressiveModelforTimeSeriesofRandomObjects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/AnAutoregressiveModelforTimeSeriesofRandomObjects.html">&lt;p&gt;Random variables in metric spaces indexed by time and observed at equally spaced time points are receiving increased attention due to their broad applicability. However, the absence of inherent structure in metric spaces has resulted in a literature that is predominantly non-parametric and model-free. To address this gap in models for time series of random objects, we introduce an adaptation of the classical linear autoregressive model tailored for data lying in a Hadamard space. The parameters of interest in this model are the Fr&apos;echet mean and a concentration parameter, both of which we prove can be consistently estimated from data. Additionally, we propose a test statistic and establish its asymptotic normality, thereby enabling hypothesis testing for the absence of serial dependence. Finally, we introduce a bootstrap procedure to obtain critical values for the test statistic under the null hypothesis. Theoretical results of our method, including the convergence of the estimators as well as the size and power of the test, are illustrated through simulations, and the utility of the model is demonstrated by an analysis of a time series of consumer inflation expectations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03778&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matthieu Bulté, Helle S{\o}rensen</name></author><category term="stat.ME" /><summary type="html">Random variables in metric spaces indexed by time and observed at equally spaced time points are receiving increased attention due to their broad applicability. However, the absence of inherent structure in metric spaces has resulted in a literature that is predominantly non-parametric and model-free. To address this gap in models for time series of random objects, we introduce an adaptation of the classical linear autoregressive model tailored for data lying in a Hadamard space. The parameters of interest in this model are the Fr&apos;echet mean and a concentration parameter, both of which we prove can be consistently estimated from data. Additionally, we propose a test statistic and establish its asymptotic normality, thereby enabling hypothesis testing for the absence of serial dependence. Finally, we introduce a bootstrap procedure to obtain critical values for the test statistic under the null hypothesis. Theoretical results of our method, including the convergence of the estimators as well as the size and power of the test, are illustrated through simulations, and the utility of the model is demonstrated by an analysis of a time series of consumer inflation expectations.</summary></entry><entry><title type="html">Anisotropic local constant smoothing for change-point regression function estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Anisotropiclocalconstantsmoothingforchangepointregressionfunctionestimation.html" rel="alternate" type="text/html" title="Anisotropic local constant smoothing for change-point regression function estimation" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Anisotropiclocalconstantsmoothingforchangepointregressionfunctionestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Anisotropiclocalconstantsmoothingforchangepointregressionfunctionestimation.html">&lt;p&gt;Understanding forest fire spread in any region of Canada is critical to promoting forest health, and protecting human life and infrastructure. Quantifying fire spread from noisy images, where regions of a fire are separated by change-point boundaries, is critical to faithfully estimating fire spread rates. In this research, we develop a statistically consistent smooth estimator that allows us to denoise fire spread imagery from micro-fire experiments. We develop an anisotropic smoothing method for change-point data that uses estimates of the underlying data generating process to inform smoothing. We show that the anisotropic local constant regression estimator is consistent with convergence rate $O\left(n^{-1/{(q+2)}}\right)$. We demonstrate its effectiveness on simulated one- and two-dimensional change-point data and fire spread imagery from micro-fire experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2012.00180&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>John R. J. Thompson, W. John Braun</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.TH" /><summary type="html">Understanding forest fire spread in any region of Canada is critical to promoting forest health, and protecting human life and infrastructure. Quantifying fire spread from noisy images, where regions of a fire are separated by change-point boundaries, is critical to faithfully estimating fire spread rates. In this research, we develop a statistically consistent smooth estimator that allows us to denoise fire spread imagery from micro-fire experiments. We develop an anisotropic smoothing method for change-point data that uses estimates of the underlying data generating process to inform smoothing. We show that the anisotropic local constant regression estimator is consistent with convergence rate $O\left(n^{-1/{(q+2)}}\right)$. We demonstrate its effectiveness on simulated one- and two-dimensional change-point data and fire spread imagery from micro-fire experiments.</summary></entry><entry><title type="html">Bayesian Copula Density Estimation Using Bernstein Yett-Uniform Priors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/BayesianCopulaDensityEstimationUsingBernsteinYettUniformPriors.html" rel="alternate" type="text/html" title="Bayesian Copula Density Estimation Using Bernstein Yett-Uniform Priors" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/BayesianCopulaDensityEstimationUsingBernsteinYettUniformPriors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/BayesianCopulaDensityEstimationUsingBernsteinYettUniformPriors.html">&lt;p&gt;Probability density estimation is a central task in statistics. Copula-based models provide a great deal of flexibility in modelling multivariate distributions, allowing for the specifications of models for the marginal distributions separately from the dependence structure (copula) that links them to form a joint distribution. Choosing a class of copula models is not a trivial task and its misspecification can lead to wrong conclusions. We introduce a novel class of random Bernstein copula functions, and studied its support and the behavior of its posterior distribution. The proposal is based on a particular class of random grid-uniform copulas, referred to as yett-uniform copulas. Alternative Markov chain Monte Carlo algorithms for exploring the posterior distribution under the proposed model are also studied. The methodology is illustrated by means of simulated and real data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04475&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nicolás Kuschinski, Richard Warr, Alejandro Jara</name></author><category term="stat.ME" /><summary type="html">Probability density estimation is a central task in statistics. Copula-based models provide a great deal of flexibility in modelling multivariate distributions, allowing for the specifications of models for the marginal distributions separately from the dependence structure (copula) that links them to form a joint distribution. Choosing a class of copula models is not a trivial task and its misspecification can lead to wrong conclusions. We introduce a novel class of random Bernstein copula functions, and studied its support and the behavior of its posterior distribution. The proposal is based on a particular class of random grid-uniform copulas, referred to as yett-uniform copulas. Alternative Markov chain Monte Carlo algorithms for exploring the posterior distribution under the proposed model are also studied. The methodology is illustrated by means of simulated and real data.</summary></entry><entry><title type="html">Bayesian Multilevel Compositional Data Analysis: Introduction, Evaluation, and Application</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/BayesianMultilevelCompositionalDataAnalysisIntroductionEvaluationandApplication.html" rel="alternate" type="text/html" title="Bayesian Multilevel Compositional Data Analysis: Introduction, Evaluation, and Application" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/BayesianMultilevelCompositionalDataAnalysisIntroductionEvaluationandApplication</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/BayesianMultilevelCompositionalDataAnalysisIntroductionEvaluationandApplication.html">&lt;p&gt;Multilevel compositional data commonly occur in various fields, particularly in intensive, longitudinal studies using ecological momentary assessments. Examples include data repeatedly measured over time that are non-negative and sum to a constant value, such as sleep-wake movement behaviours in a 24-hour day. This article presents a novel methodology for analysing multilevel compositional data using a Bayesian inference approach. This method can be used to investigate how reallocation of time between sleep-wake movement behaviours may be associated with other phenomena (e.g., emotions, cognitions) at a daily level. We explain the theoretical details of the data and the models, and outline the steps necessary to implement this method. We introduce the R package multilevelcoda to facilitate the application of this method and illustrate using a real data example. An extensive parameter recovery simulation study verified the robust performance of the method. Across all simulation conditions investigated in the simulation study, the model had minimal convergence issues (convergence rate &amp;gt; 99%) and achieved excellent quality of parameter estimates and inference, with an average bias of 0.00 (range -0.09, 0.05) and coverage of 0.95 (range 0.93, 0.97). We conclude the article with recommendations on the use of the Bayesian compositional multilevel modelling approach, and hope to promote wider application of this method to answer robust questions using the increasingly available data from intensive, longitudinal studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03985&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Flora Le, Tyman E. Stanford, Dorothea Dumuid, Joshua F. Wiley</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Multilevel compositional data commonly occur in various fields, particularly in intensive, longitudinal studies using ecological momentary assessments. Examples include data repeatedly measured over time that are non-negative and sum to a constant value, such as sleep-wake movement behaviours in a 24-hour day. This article presents a novel methodology for analysing multilevel compositional data using a Bayesian inference approach. This method can be used to investigate how reallocation of time between sleep-wake movement behaviours may be associated with other phenomena (e.g., emotions, cognitions) at a daily level. We explain the theoretical details of the data and the models, and outline the steps necessary to implement this method. We introduce the R package multilevelcoda to facilitate the application of this method and illustrate using a real data example. An extensive parameter recovery simulation study verified the robust performance of the method. Across all simulation conditions investigated in the simulation study, the model had minimal convergence issues (convergence rate &amp;gt; 99%) and achieved excellent quality of parameter estimates and inference, with an average bias of 0.00 (range -0.09, 0.05) and coverage of 0.95 (range 0.93, 0.97). We conclude the article with recommendations on the use of the Bayesian compositional multilevel modelling approach, and hope to promote wider application of this method to answer robust questions using the increasingly available data from intensive, longitudinal studies.</summary></entry><entry><title type="html">Bayesian Quantile Regression with Subset Selection: A Posterior Summarization Perspective</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/BayesianQuantileRegressionwithSubsetSelectionAPosteriorSummarizationPerspective.html" rel="alternate" type="text/html" title="Bayesian Quantile Regression with Subset Selection: A Posterior Summarization Perspective" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/BayesianQuantileRegressionwithSubsetSelectionAPosteriorSummarizationPerspective</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/BayesianQuantileRegressionwithSubsetSelectionAPosteriorSummarizationPerspective.html">&lt;p&gt;Quantile regression is a powerful tool for inferring how covariates affect specific percentiles of the response distribution. Existing methods either estimate conditional quantiles separately for each quantile of interest or estimate the entire conditional distribution using semi- or non-parametric models. The former often produce inadequate models for real data and do not share information across quantiles, while the latter are characterized by complex and constrained models that can be difficult to interpret and computationally inefficient. Further, neither approach is well-suited for quantile-specific subset selection. Instead, we pose the fundamental problems of linear quantile estimation, uncertainty quantification, and subset selection from a Bayesian decision analysis perspective. For any Bayesian regression model, we derive optimal and interpretable linear estimates and uncertainty quantification for each model-based conditional quantile. Our approach introduces a quantile-focused squared error loss, which enables efficient, closed-form computing and maintains a close relationship with Wasserstein-based density estimation. In an extensive simulation study, our methods demonstrate substantial gains in quantile estimation accuracy, variable selection, and inference over frequentist and Bayesian competitors. We apply these tools to identify the quantile-specific impacts of social and environmental stressors on educational outcomes for a large cohort of children in North Carolina.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.02043&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joseph Feldman, Daniel Kowal</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Quantile regression is a powerful tool for inferring how covariates affect specific percentiles of the response distribution. Existing methods either estimate conditional quantiles separately for each quantile of interest or estimate the entire conditional distribution using semi- or non-parametric models. The former often produce inadequate models for real data and do not share information across quantiles, while the latter are characterized by complex and constrained models that can be difficult to interpret and computationally inefficient. Further, neither approach is well-suited for quantile-specific subset selection. Instead, we pose the fundamental problems of linear quantile estimation, uncertainty quantification, and subset selection from a Bayesian decision analysis perspective. For any Bayesian regression model, we derive optimal and interpretable linear estimates and uncertainty quantification for each model-based conditional quantile. Our approach introduces a quantile-focused squared error loss, which enables efficient, closed-form computing and maintains a close relationship with Wasserstein-based density estimation. In an extensive simulation study, our methods demonstrate substantial gains in quantile estimation accuracy, variable selection, and inference over frequentist and Bayesian competitors. We apply these tools to identify the quantile-specific impacts of social and environmental stressors on educational outcomes for a large cohort of children in North Carolina.</summary></entry><entry><title type="html">Causal Inference in the Multiverse of Hazard</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/CausalInferenceintheMultiverseofHazard.html" rel="alternate" type="text/html" title="Causal Inference in the Multiverse of Hazard" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/CausalInferenceintheMultiverseofHazard</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/CausalInferenceintheMultiverseofHazard.html">&lt;p&gt;Hazard serves as a pivotal estimand in both practical applications and methodological frameworks. However, its causal interpretation poses notable challenges, including inherent selection biases and ill-defined populations to be compared between different treatment groups. In response, we propose a novel definition of counterfactual hazard within the framework of possible worlds. Instead of conditioning on prior survival status as a conditional probability, our new definition involves intervening in the prior status, treating it as a marginal probability. Using single-world intervention graphs, we demonstrate that the proposed counterfactual hazard is a type of controlled direct effect. Conceptually, intervening in survival status at each time point generates a new possible world, where the proposed hazards across time points represent risks in these hypothetical scenarios, forming a “multiverse of hazard.” The cumulative and average counterfactual hazards correspond to the sum and average of risks across this multiverse, respectively, with the actual world’s risk lying between the two. This conceptual shift reframes hazards in the actual world as a collection of risks across possible worlds, marking a significant advancement in the causal interpretation of hazards.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04446&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>En-Yu Lai, Yen-Tsung Huang</name></author><category term="stat.ME" /><summary type="html">Hazard serves as a pivotal estimand in both practical applications and methodological frameworks. However, its causal interpretation poses notable challenges, including inherent selection biases and ill-defined populations to be compared between different treatment groups. In response, we propose a novel definition of counterfactual hazard within the framework of possible worlds. Instead of conditioning on prior survival status as a conditional probability, our new definition involves intervening in the prior status, treating it as a marginal probability. Using single-world intervention graphs, we demonstrate that the proposed counterfactual hazard is a type of controlled direct effect. Conceptually, intervening in survival status at each time point generates a new possible world, where the proposed hazards across time points represent risks in these hypothetical scenarios, forming a “multiverse of hazard.” The cumulative and average counterfactual hazards correspond to the sum and average of risks across this multiverse, respectively, with the actual world’s risk lying between the two. This conceptual shift reframes hazards in the actual world as a collection of risks across possible worlds, marking a significant advancement in the causal interpretation of hazards.</summary></entry><entry><title type="html">Covariance-free Multifidelity Control Variates Importance Sampling for Reliability Analysis of Rare Events</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/CovariancefreeMultifidelityControlVariatesImportanceSamplingforReliabilityAnalysisofRareEvents.html" rel="alternate" type="text/html" title="Covariance-free Multifidelity Control Variates Importance Sampling for Reliability Analysis of Rare Events" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/CovariancefreeMultifidelityControlVariatesImportanceSamplingforReliabilityAnalysisofRareEvents</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/CovariancefreeMultifidelityControlVariatesImportanceSamplingforReliabilityAnalysisofRareEvents.html">&lt;p&gt;Multifidelity modeling has been steadily gaining attention as a tool to address the problem of exorbitant model evaluation costs that makes the estimation of failure probabilities a significant computational challenge for complex real-world problems, particularly when failure is a rare event. To implement multifidelity modeling, estimators that efficiently combine information from multiple models/sources are necessary. In past works, the variance reduction techniques of Control Variates (CV) and Importance Sampling (IS) have been leveraged for this task. In this paper, we present the CVIS framework; a creative take on a coupled Control Variates and Importance Sampling estimator for bifidelity reliability analysis. The framework addresses some of the practical challenges of the CV method by using an estimator for the control variate mean and side-stepping the need to estimate the covariance between the original estimator and the control variate through a clever choice for the tuning constant. The task of selecting an efficient IS distribution is also considered, with a view towards maximally leveraging the bifidelity structure and maintaining expressivity. Additionally, a diagnostic is provided that indicates both the efficiency of the algorithm as well as the relative predictive quality of the models utilized. Finally, the behavior and performance of the framework is explored through analytical and numerical examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03834&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Promit Chakroborty , Somayajulu L. N. Dhulipala , Michael D. Shields</name></author><category term="stat.ME" /><summary type="html">Multifidelity modeling has been steadily gaining attention as a tool to address the problem of exorbitant model evaluation costs that makes the estimation of failure probabilities a significant computational challenge for complex real-world problems, particularly when failure is a rare event. To implement multifidelity modeling, estimators that efficiently combine information from multiple models/sources are necessary. In past works, the variance reduction techniques of Control Variates (CV) and Importance Sampling (IS) have been leveraged for this task. In this paper, we present the CVIS framework; a creative take on a coupled Control Variates and Importance Sampling estimator for bifidelity reliability analysis. The framework addresses some of the practical challenges of the CV method by using an estimator for the control variate mean and side-stepping the need to estimate the covariance between the original estimator and the control variate through a clever choice for the tuning constant. The task of selecting an efficient IS distribution is also considered, with a view towards maximally leveraging the bifidelity structure and maintaining expressivity. Additionally, a diagnostic is provided that indicates both the efficiency of the algorithm as well as the relative predictive quality of the models utilized. Finally, the behavior and performance of the framework is explored through analytical and numerical examples.</summary></entry><entry><title type="html">Distributed variable screening for generalized linear models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Distributedvariablescreeningforgeneralizedlinearmodels.html" rel="alternate" type="text/html" title="Distributed variable screening for generalized linear models" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Distributedvariablescreeningforgeneralizedlinearmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Distributedvariablescreeningforgeneralizedlinearmodels.html">&lt;p&gt;In this article, we develop a distributed variable screening method for generalized linear models. This method is designed to handle situations where both the sample size and the number of covariates are large. Specifically, the proposed method selects relevant covariates by using a sparsity-restricted surrogate likelihood estimator. It takes into account the joint effects of the covariates rather than just the marginal effect, and this characteristic enhances the reliability of the screening results. We establish the sure screening property of the proposed method, which ensures that with a high probability, the true model is included in the selected model. Simulation studies are conducted to evaluate the finite sample performance of the proposed method, and an application to a real dataset showcases its practical utility.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04254&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tianbo Diao, Lianqiang Qu, Bo Li, Liuquan Sun</name></author><category term="stat.ME" /><summary type="html">In this article, we develop a distributed variable screening method for generalized linear models. This method is designed to handle situations where both the sample size and the number of covariates are large. Specifically, the proposed method selects relevant covariates by using a sparsity-restricted surrogate likelihood estimator. It takes into account the joint effects of the covariates rather than just the marginal effect, and this characteristic enhances the reliability of the screening results. We establish the sure screening property of the proposed method, which ensures that with a high probability, the true model is included in the selected model. Simulation studies are conducted to evaluate the finite sample performance of the proposed method, and an application to a real dataset showcases its practical utility.</summary></entry><entry><title type="html">FOKE: A Personalized and Explainable Education Framework Integrating Foundation Models, Knowledge Graphs, and Prompt Engineering</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/FOKEAPersonalizedandExplainableEducationFrameworkIntegratingFoundationModelsKnowledgeGraphsandPromptEngineering.html" rel="alternate" type="text/html" title="FOKE: A Personalized and Explainable Education Framework Integrating Foundation Models, Knowledge Graphs, and Prompt Engineering" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/FOKEAPersonalizedandExplainableEducationFrameworkIntegratingFoundationModelsKnowledgeGraphsandPromptEngineering</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/FOKEAPersonalizedandExplainableEducationFrameworkIntegratingFoundationModelsKnowledgeGraphsandPromptEngineering.html">&lt;p&gt;Integrating large language models (LLMs) and knowledge graphs (KGs) holds great promise for revolutionizing intelligent education, but challenges remain in achieving personalization, interactivity, and explainability. We propose FOKE, a Forest Of Knowledge and Education framework that synergizes foundation models, knowledge graphs, and prompt engineering to address these challenges. FOKE introduces three key innovations: (1) a hierarchical knowledge forest for structured domain knowledge representation; (2) a multi-dimensional user profiling mechanism for comprehensive learner modeling; and (3) an interactive prompt engineering scheme for generating precise and tailored learning guidance.
  We showcase FOKE’s application in programming education, homework assessment, and learning path planning, demonstrating its effectiveness and practicality. Additionally, we implement Scholar Hero, a real-world instantiation of FOKE. Our research highlights the potential of integrating foundation models, knowledge graphs, and prompt engineering to revolutionize intelligent education practices, ultimately benefiting learners worldwide. FOKE provides a principled and unified approach to harnessing cutting-edge AI technologies for personalized, interactive, and explainable educational services, paving the way for further research and development in this critical direction.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03734&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Silan Hu, Xiaoning Wang</name></author><category term="stat.AP" /><summary type="html">Integrating large language models (LLMs) and knowledge graphs (KGs) holds great promise for revolutionizing intelligent education, but challenges remain in achieving personalization, interactivity, and explainability. We propose FOKE, a Forest Of Knowledge and Education framework that synergizes foundation models, knowledge graphs, and prompt engineering to address these challenges. FOKE introduces three key innovations: (1) a hierarchical knowledge forest for structured domain knowledge representation; (2) a multi-dimensional user profiling mechanism for comprehensive learner modeling; and (3) an interactive prompt engineering scheme for generating precise and tailored learning guidance. We showcase FOKE’s application in programming education, homework assessment, and learning path planning, demonstrating its effectiveness and practicality. Additionally, we implement Scholar Hero, a real-world instantiation of FOKE. Our research highlights the potential of integrating foundation models, knowledge graphs, and prompt engineering to revolutionize intelligent education practices, ultimately benefiting learners worldwide. FOKE provides a principled and unified approach to harnessing cutting-edge AI technologies for personalized, interactive, and explainable educational services, paving the way for further research and development in this critical direction.</summary></entry><entry><title type="html">Generative adversarial learning with optimal input dimension and its adaptive generator architecture</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Generativeadversariallearningwithoptimalinputdimensionanditsadaptivegeneratorarchitecture.html" rel="alternate" type="text/html" title="Generative adversarial learning with optimal input dimension and its adaptive generator architecture" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Generativeadversariallearningwithoptimalinputdimensionanditsadaptivegeneratorarchitecture</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Generativeadversariallearningwithoptimalinputdimensionanditsadaptivegeneratorarchitecture.html">&lt;p&gt;We investigate the impact of the input dimension on the generalization error in generative adversarial networks (GANs). In particular, we first provide both theoretical and practical evidence to validate the existence of an optimal input dimension (OID) that minimizes the generalization error. Then, to identify the OID, we introduce a novel framework called generalized GANs (G-GANs), which includes existing GANs as a special case. By incorporating the group penalty and the architecture penalty developed in the paper, G-GANs have several intriguing features. First, our framework offers adaptive dimensionality reduction from the initial dimension to a dimension necessary for generating the target distribution. Second, this reduction in dimensionality also shrinks the required size of the generator network architecture, which is automatically identified by the proposed architecture penalty. Both reductions in dimensionality and the generator network significantly improve the stability and the accuracy of the estimation and prediction. Theoretical support for the consistent selection of the input dimension and the generator network is provided. Third, the proposed algorithm involves an end-to-end training process, and the algorithm allows for dynamic adjustments between the input dimension and the generator network during training, further enhancing the overall performance of G-GANs. Extensive experiments conducted with simulated and benchmark data demonstrate the superior performance of G-GANs. In particular, compared to that of off-the-shelf methods, G-GANs achieves an average improvement of 45.68% in the CT slice dataset, 43.22% in the MNIST dataset and 46.94% in the FashionMNIST dataset in terms of the maximum mean discrepancy or Frechet inception distance. Moreover, the features generated based on the input dimensions identified by G-GANs align with visually significant features.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03723&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhiyao Tan, Ling Zhou, Huazhen Lin</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We investigate the impact of the input dimension on the generalization error in generative adversarial networks (GANs). In particular, we first provide both theoretical and practical evidence to validate the existence of an optimal input dimension (OID) that minimizes the generalization error. Then, to identify the OID, we introduce a novel framework called generalized GANs (G-GANs), which includes existing GANs as a special case. By incorporating the group penalty and the architecture penalty developed in the paper, G-GANs have several intriguing features. First, our framework offers adaptive dimensionality reduction from the initial dimension to a dimension necessary for generating the target distribution. Second, this reduction in dimensionality also shrinks the required size of the generator network architecture, which is automatically identified by the proposed architecture penalty. Both reductions in dimensionality and the generator network significantly improve the stability and the accuracy of the estimation and prediction. Theoretical support for the consistent selection of the input dimension and the generator network is provided. Third, the proposed algorithm involves an end-to-end training process, and the algorithm allows for dynamic adjustments between the input dimension and the generator network during training, further enhancing the overall performance of G-GANs. Extensive experiments conducted with simulated and benchmark data demonstrate the superior performance of G-GANs. In particular, compared to that of off-the-shelf methods, G-GANs achieves an average improvement of 45.68% in the CT slice dataset, 43.22% in the MNIST dataset and 46.94% in the FashionMNIST dataset in terms of the maximum mean discrepancy or Frechet inception distance. Moreover, the features generated based on the input dimensions identified by G-GANs align with visually significant features.</summary></entry><entry><title type="html">Homogeneity of multinomial populations when data are classified into a large number of groups</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Homogeneityofmultinomialpopulationswhendataareclassifiedintoalargenumberofgroups.html" rel="alternate" type="text/html" title="Homogeneity of multinomial populations when data are classified into a large number of groups" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Homogeneityofmultinomialpopulationswhendataareclassifiedintoalargenumberofgroups</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Homogeneityofmultinomialpopulationswhendataareclassifiedintoalargenumberofgroups.html">&lt;p&gt;Suppose that we are interested in the comparison of two independent categorical variables. Suppose also that the population is divided into subpopulations or groups. Notice that the distribution of the target variable may vary across subpopulations, moreover, it may happen that the two independent variables have the same distribution in the whole population, but their distributions could differ in some groups. So, instead of testing the homogeneity of the two categorical variables, one may be interested in simultaneously testing the homogeneity in all groups. A novel procedure is proposed for carrying out such a testing problem. The test statistic is shown to be asymptotically normal, avoiding the use of complicated resampling methods to get $p$-values. Here by asymptotic we mean when the number of groups increases; the sample sizes of the data from each group can either stay bounded or grow with the number of groups. The finite sample performance of the proposal is empirically evaluated through an extensive simulation study. The usefulness of the proposal is illustrated by three data sets coming from diverse experimental fields such as education, the COVID-19 pandemic and digital elevation models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04238&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>M. V. Alba-Fernández, M. D. Jiménez--Gamero, F. J. Ariza-López</name></author><category term="stat.ME" /><summary type="html">Suppose that we are interested in the comparison of two independent categorical variables. Suppose also that the population is divided into subpopulations or groups. Notice that the distribution of the target variable may vary across subpopulations, moreover, it may happen that the two independent variables have the same distribution in the whole population, but their distributions could differ in some groups. So, instead of testing the homogeneity of the two categorical variables, one may be interested in simultaneously testing the homogeneity in all groups. A novel procedure is proposed for carrying out such a testing problem. The test statistic is shown to be asymptotically normal, avoiding the use of complicated resampling methods to get $p$-values. Here by asymptotic we mean when the number of groups increases; the sample sizes of the data from each group can either stay bounded or grow with the number of groups. The finite sample performance of the proposal is empirically evaluated through an extensive simulation study. The usefulness of the proposal is illustrated by three data sets coming from diverse experimental fields such as education, the COVID-19 pandemic and digital elevation models.</summary></entry><entry><title type="html">Limit Order Book Dynamics and Order Size Modelling Using Compound Hawkes Process</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/LimitOrderBookDynamicsandOrderSizeModellingUsingCompoundHawkesProcess.html" rel="alternate" type="text/html" title="Limit Order Book Dynamics and Order Size Modelling Using Compound Hawkes Process" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/LimitOrderBookDynamicsandOrderSizeModellingUsingCompoundHawkesProcess</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/LimitOrderBookDynamicsandOrderSizeModellingUsingCompoundHawkesProcess.html">&lt;p&gt;Hawkes Process has been used to model Limit Order Book (LOB) dynamics in several ways in the literature however the focus has been limited to capturing the inter-event times while the order size is usually assumed to be constant. We propose a novel methodology of using Compound Hawkes Process for the LOB where each event has an order size sampled from a calibrated distribution. The process is formulated in a novel way such that the spread of the process always remains positive. Further, we condition the model parameters on time of day to support empirical observations. We make use of an enhanced non-parametric method to calibrate the Hawkes kernels and allow for inhibitory cross-excitation kernels. We showcase the results and quality of fits for an equity stock’s LOB in the NASDAQ exchange and compare them against several baselines. Finally, we conduct a market impact study of the simulator and show the empirical observation of a concave market impact function is indeed replicated.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.08927&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Konark Jain, Nick Firoozye, Jonathan Kochems, Philip Treleaven</name></author><category term="stat.AP" /><summary type="html">Hawkes Process has been used to model Limit Order Book (LOB) dynamics in several ways in the literature however the focus has been limited to capturing the inter-event times while the order size is usually assumed to be constant. We propose a novel methodology of using Compound Hawkes Process for the LOB where each event has an order size sampled from a calibrated distribution. The process is formulated in a novel way such that the spread of the process always remains positive. Further, we condition the model parameters on time of day to support empirical observations. We make use of an enhanced non-parametric method to calibrate the Hawkes kernels and allow for inhibitory cross-excitation kernels. We showcase the results and quality of fits for an equity stock’s LOB in the NASDAQ exchange and compare them against several baselines. Finally, we conduct a market impact study of the simulator and show the empirical observation of a concave market impact function is indeed replicated.</summary></entry><entry><title type="html">Mitigating Nonlinear Algorithmic Bias in Binary Classification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/MitigatingNonlinearAlgorithmicBiasinBinaryClassification.html" rel="alternate" type="text/html" title="Mitigating Nonlinear Algorithmic Bias in Binary Classification" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/MitigatingNonlinearAlgorithmicBiasinBinaryClassification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/MitigatingNonlinearAlgorithmicBiasinBinaryClassification.html">&lt;p&gt;This paper proposes the use of causal modeling to detect and mitigate algorithmic bias that is nonlinear in the protected attribute. We provide a general overview of our approach. We use the German Credit data set, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on age bias and the problem of binary classification. We show that the probability of getting correctly classified as “low risk” is lowest among young people. The probability increases with age nonlinearly. To incorporate the nonlinearity into the causal model, we introduce a higher order polynomial term. Based on the fitted causal model, the de-biased probability estimates are computed, showing improved fairness with little impact on overall classification accuracy. Causal modeling is intuitive and, hence, its use can enhance explicability and promotes trust among different stakeholders of AI.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.05429&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wendy Hui, Wai Kwong Lau</name></author><category term="stat.AP" /><summary type="html">This paper proposes the use of causal modeling to detect and mitigate algorithmic bias that is nonlinear in the protected attribute. We provide a general overview of our approach. We use the German Credit data set, which is available for download from the UC Irvine Machine Learning Repository, to develop (1) a prediction model, which is treated as a black box, and (2) a causal model for bias mitigation. In this paper, we focus on age bias and the problem of binary classification. We show that the probability of getting correctly classified as “low risk” is lowest among young people. The probability increases with age nonlinearly. To incorporate the nonlinearity into the causal model, we introduce a higher order polynomial term. Based on the fitted causal model, the de-biased probability estimates are computed, showing improved fairness with little impact on overall classification accuracy. Causal modeling is intuitive and, hence, its use can enhance explicability and promotes trust among different stakeholders of AI.</summary></entry><entry><title type="html">NEST: Neural Estimation by Sequential Testing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/NESTNeuralEstimationbySequentialTesting.html" rel="alternate" type="text/html" title="NEST: Neural Estimation by Sequential Testing" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/NESTNeuralEstimationbySequentialTesting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/NESTNeuralEstimationbySequentialTesting.html">&lt;p&gt;Adaptive psychophysical procedures aim to increase the efficiency and reliability of measurements. With increasing stimulus and experiment complexity in the last decade, estimating multi-dimensional psychometric functions has become a challenging task for adaptive procedures. If the experimenter has limited information about the underlying psychometric function, it is not possible to use parametric techniques developed for the multi-dimensional stimulus space. Although there are non-parametric approaches that use Gaussian process methods and specific hand-crafted acquisition functions, their performance is sensitive to proper selection of the kernel function, which is not always straightforward. In this work, we use a neural network as the psychometric function estimator and introduce a novel acquisition function for stimulus selection. We thoroughly benchmark our technique both using simulations and by conducting psychovisual experiments under realistic conditions. We show that our method outperforms the state of the art without the need to select a kernel function and significantly reduces the experiment duration.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04226&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sjoerd Bruin, Jiří Kosinka, Cara Tursun</name></author><category term="stat.ME" /><summary type="html">Adaptive psychophysical procedures aim to increase the efficiency and reliability of measurements. With increasing stimulus and experiment complexity in the last decade, estimating multi-dimensional psychometric functions has become a challenging task for adaptive procedures. If the experimenter has limited information about the underlying psychometric function, it is not possible to use parametric techniques developed for the multi-dimensional stimulus space. Although there are non-parametric approaches that use Gaussian process methods and specific hand-crafted acquisition functions, their performance is sensitive to proper selection of the kernel function, which is not always straightforward. In this work, we use a neural network as the psychometric function estimator and introduce a novel acquisition function for stimulus selection. We thoroughly benchmark our technique both using simulations and by conducting psychovisual experiments under realistic conditions. We show that our method outperforms the state of the art without the need to select a kernel function and significantly reduces the experiment duration.</summary></entry><entry><title type="html">New allometric models for the USA create a step-change in forest carbon estimation, modeling, and mapping</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/NewallometricmodelsfortheUSAcreateastepchangeinforestcarbonestimationmodelingandmapping.html" rel="alternate" type="text/html" title="New allometric models for the USA create a step-change in forest carbon estimation, modeling, and mapping" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/NewallometricmodelsfortheUSAcreateastepchangeinforestcarbonestimationmodelingandmapping</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/NewallometricmodelsfortheUSAcreateastepchangeinforestcarbonestimationmodelingandmapping.html">&lt;p&gt;The United States national forest inventory (NFI) serves as the foundation for forest aboveground biomass (AGB) and carbon accounting across the nation. These data enable design-based estimates of forest carbon stocks and stock-changes at state and regional levels, but also serve as inputs to model-based approaches for characterizing forest carbon stocks and stock-changes at finer resolutions. Although NFI tree and plot-level data are often treated as truth in these models, they are in fact estimates based on regional species-group models known collectively as the Component Ratio Method (CRM). In late 2023 the Forest Inventory and Analysis (FIA) program introduced a new National Scale Volume and Biomass Estimators (NSVB) system to replace CRM nationwide and offer more precise and accurate representations of forest AGB and carbon. Given the prevalence of model-based AGB studies relying on FIA, there is concern about the transferability of methods from CRM to NSVB models, as well as the comparability of existing CRM AGB products (e.g. maps) to new and forthcoming NSVB AGB products. To begin addressing these concerns we compared previously published CRM AGB maps to new maps produced using identical methods with NSVB AGB reference data. Our results suggest that models relying on passive satellite imagery (e.g. Landsat) provide acceptable estimates of point-in-time NSVB AGB and carbon stocks, but fail to accurately quantify growth in mature closed-canopy forests. We highlight that existing estimates, models, and maps based on FIA reference data are no longer compatible with NSVB, and recommend new methods as well as updated models and maps for accommodating this step-change. Our collective ability to adopt NSVB in our modeling and mapping workflows will help us provide the most accurate spatial forest carbon data possible in order to better inform local management and decision making.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04507&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lucas K. Johnson , Michael J. Mahoney , Grant Domke , Colin M. Beier</name></author><category term="stat.AP" /><summary type="html">The United States national forest inventory (NFI) serves as the foundation for forest aboveground biomass (AGB) and carbon accounting across the nation. These data enable design-based estimates of forest carbon stocks and stock-changes at state and regional levels, but also serve as inputs to model-based approaches for characterizing forest carbon stocks and stock-changes at finer resolutions. Although NFI tree and plot-level data are often treated as truth in these models, they are in fact estimates based on regional species-group models known collectively as the Component Ratio Method (CRM). In late 2023 the Forest Inventory and Analysis (FIA) program introduced a new National Scale Volume and Biomass Estimators (NSVB) system to replace CRM nationwide and offer more precise and accurate representations of forest AGB and carbon. Given the prevalence of model-based AGB studies relying on FIA, there is concern about the transferability of methods from CRM to NSVB models, as well as the comparability of existing CRM AGB products (e.g. maps) to new and forthcoming NSVB AGB products. To begin addressing these concerns we compared previously published CRM AGB maps to new maps produced using identical methods with NSVB AGB reference data. Our results suggest that models relying on passive satellite imagery (e.g. Landsat) provide acceptable estimates of point-in-time NSVB AGB and carbon stocks, but fail to accurately quantify growth in mature closed-canopy forests. We highlight that existing estimates, models, and maps based on FIA reference data are no longer compatible with NSVB, and recommend new methods as well as updated models and maps for accommodating this step-change. Our collective ability to adopt NSVB in our modeling and mapping workflows will help us provide the most accurate spatial forest carbon data possible in order to better inform local management and decision making.</summary></entry><entry><title type="html">Non-locality and Spillover Effects of Residential Flood Damage on Community Recovery: Insights from High-resolution Flood Claim and Mobility Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/NonlocalityandSpilloverEffectsofResidentialFloodDamageonCommunityRecoveryInsightsfromHighresolutionFloodClaimandMobilityData.html" rel="alternate" type="text/html" title="Non-locality and Spillover Effects of Residential Flood Damage on Community Recovery: Insights from High-resolution Flood Claim and Mobility Data" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/NonlocalityandSpilloverEffectsofResidentialFloodDamageonCommunityRecoveryInsightsfromHighresolutionFloodClaimandMobilityData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/NonlocalityandSpilloverEffectsofResidentialFloodDamageonCommunityRecoveryInsightsfromHighresolutionFloodClaimandMobilityData.html">&lt;p&gt;Examining the relationship between vulnerability of the built environment and community recovery is crucial for understanding disaster resilience. Yet, this relationship is rather neglected in the existing literature due to previous limitations in the availability of empirical datasets needed for such analysis. In this study, we combine fine-resolution flood damage claims data (composed of both insured and uninsured losses) and human mobility data (composed of millions of movement trajectories) during the 2017 Hurricane Harvey in Harris County, Texas, to specify the extent to which vulnerability of the built environment (i.e., flood property damage) affects community recovery (based on the speed of human mobility recovery) locally and regionally. We examine this relationship using a spatial lag, spatial reach, and spatial decay models to measure the extent of spillover effects of residential damage on community recovery. The findings show that: first, the severity of residential damage significantly affects the speed of community recovery. A greater extent of residential damage suppresses community recovery not only locally but also in the surrounding areas. Second, the spatial spillover effect of residential damage on community recovery speed decays with distance from the highly damaged areas. Third, spatial areas display heterogeneous spatial decay coefficients, which are associated with urban structure features such as the density of points-of-interest facilities and roads. These findings provide a novel data-driven characterization of the spatial diffusion of residential flood damage effects on community recovery and move us closer to a better understanding of complex spatial processes that shape community resilience to hazards. This study also provides valuable insights for emergency managers and public officials seeking to mitigate the non-local effects of residential damage.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03874&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Junwei Ma, Russell Blessing, Samuel Brody, Ali Mostafavi</name></author><category term="stat.AP" /><summary type="html">Examining the relationship between vulnerability of the built environment and community recovery is crucial for understanding disaster resilience. Yet, this relationship is rather neglected in the existing literature due to previous limitations in the availability of empirical datasets needed for such analysis. In this study, we combine fine-resolution flood damage claims data (composed of both insured and uninsured losses) and human mobility data (composed of millions of movement trajectories) during the 2017 Hurricane Harvey in Harris County, Texas, to specify the extent to which vulnerability of the built environment (i.e., flood property damage) affects community recovery (based on the speed of human mobility recovery) locally and regionally. We examine this relationship using a spatial lag, spatial reach, and spatial decay models to measure the extent of spillover effects of residential damage on community recovery. The findings show that: first, the severity of residential damage significantly affects the speed of community recovery. A greater extent of residential damage suppresses community recovery not only locally but also in the surrounding areas. Second, the spatial spillover effect of residential damage on community recovery speed decays with distance from the highly damaged areas. Third, spatial areas display heterogeneous spatial decay coefficients, which are associated with urban structure features such as the density of points-of-interest facilities and roads. These findings provide a novel data-driven characterization of the spatial diffusion of residential flood damage effects on community recovery and move us closer to a better understanding of complex spatial processes that shape community resilience to hazards. This study also provides valuable insights for emergency managers and public officials seeking to mitigate the non-local effects of residential damage.</summary></entry><entry><title type="html">Optimal empirical Bayes estimation for the Poisson model via minimum-distance methods</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/OptimalempiricalBayesestimationforthePoissonmodelviaminimumdistancemethods.html" rel="alternate" type="text/html" title="Optimal empirical Bayes estimation for the Poisson model via minimum-distance methods" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/OptimalempiricalBayesestimationforthePoissonmodelviaminimumdistancemethods</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/OptimalempiricalBayesestimationforthePoissonmodelviaminimumdistancemethods.html">&lt;p&gt;The Robbins estimator is the most iconic and widely used procedure in the empirical Bayes literature for the Poisson model. On one hand, this method has been recently shown to be minimax optimal in terms of the regret (excess risk over the Bayesian oracle that knows the true prior) for various nonparametric classes of priors. On the other hand, it has been long recognized in practice that Robbins estimator lacks the desired smoothness and monotonicity of Bayes estimators and can be easily derailed by those data points that were rarely observed before. Based on the minimum-distance distance method, we propose a suite of empirical Bayes estimators, including the classical nonparametric maximum likelihood, that outperform the Robbins method in a variety of synthetic and real data sets and retain its optimality in terms of minimax regret.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2209.01328&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Soham Jana, Yury Polyanskiy, Yihong Wu</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">The Robbins estimator is the most iconic and widely used procedure in the empirical Bayes literature for the Poisson model. On one hand, this method has been recently shown to be minimax optimal in terms of the regret (excess risk over the Bayesian oracle that knows the true prior) for various nonparametric classes of priors. On the other hand, it has been long recognized in practice that Robbins estimator lacks the desired smoothness and monotonicity of Bayes estimators and can be easily derailed by those data points that were rarely observed before. Based on the minimum-distance distance method, we propose a suite of empirical Bayes estimators, including the classical nonparametric maximum likelihood, that outperform the Robbins method in a variety of synthetic and real data sets and retain its optimality in terms of minimax regret.</summary></entry><entry><title type="html">Perspectives on locally weighted ensemble Kalman methods</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/PerspectivesonlocallyweightedensembleKalmanmethods.html" rel="alternate" type="text/html" title="Perspectives on locally weighted ensemble Kalman methods" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/PerspectivesonlocallyweightedensembleKalmanmethods</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/PerspectivesonlocallyweightedensembleKalmanmethods.html">&lt;p&gt;This manuscript derives locally weighted ensemble Kalman methods from the point of view of ensemble-based function approximation. This is done by using pointwise evaluations to build up a local linear or quadratic approximation of a function, tapering off the effect of distant particles via local weighting. This introduces a candidate method (the locally weighted Ensemble Kalman method for inversion) with the motivation of combining some of the strengths of the particle filter (ability to cope with nonlinear maps and non-Gaussian distributions) and the Ensemble Kalman filter (no filter degeneracy).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.00027&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Philipp Wacker</name></author><category term="stat.CO" /><summary type="html">This manuscript derives locally weighted ensemble Kalman methods from the point of view of ensemble-based function approximation. This is done by using pointwise evaluations to build up a local linear or quadratic approximation of a function, tapering off the effect of distant particles via local weighting. This introduces a candidate method (the locally weighted Ensemble Kalman method for inversion) with the motivation of combining some of the strengths of the particle filter (ability to cope with nonlinear maps and non-Gaussian distributions) and the Ensemble Kalman filter (no filter degeneracy).</summary></entry><entry><title type="html">Random Effect Restricted Mean Survival Time Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/RandomEffectRestrictedMeanSurvivalTimeModel.html" rel="alternate" type="text/html" title="Random Effect Restricted Mean Survival Time Model" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/RandomEffectRestrictedMeanSurvivalTimeModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/RandomEffectRestrictedMeanSurvivalTimeModel.html">&lt;p&gt;The restricted mean survival time (RMST) model has been garnering attention as a way to provide a clinically intuitive measure: the mean survival time. RMST models, which use methods based on pseudo time-to-event values and inverse probability censoring weighting, can adjust covariates. However, no approach has yet been introduced that considers random effects for clusters. In this paper, we propose a new random-effect RMST. We present two methods of analysis that consider variable effects by i) using a generalized mixed model with pseudo-values and ii) integrating the estimated results from the inverse probability censoring weighting estimating equations for each cluster. We evaluate our proposed methods through computer simulations. In addition, we analyze the effect of a mother’s age at birth on under-five deaths in India using states as clusters.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.02048&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Keisuke Hanada, Masahiro Kojima</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The restricted mean survival time (RMST) model has been garnering attention as a way to provide a clinically intuitive measure: the mean survival time. RMST models, which use methods based on pseudo time-to-event values and inverse probability censoring weighting, can adjust covariates. However, no approach has yet been introduced that considers random effects for clusters. In this paper, we propose a new random-effect RMST. We present two methods of analysis that consider variable effects by i) using a generalized mixed model with pseudo-values and ii) integrating the estimated results from the inverse probability censoring weighting estimating equations for each cluster. We evaluate our proposed methods through computer simulations. In addition, we analyze the effect of a mother’s age at birth on under-five deaths in India using states as clusters.</summary></entry><entry><title type="html">ReBoot: Distributed statistical learning via refitting bootstrap samples</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ReBootDistributedstatisticallearningviarefittingbootstrapsamples.html" rel="alternate" type="text/html" title="ReBoot: Distributed statistical learning via refitting bootstrap samples" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ReBootDistributedstatisticallearningviarefittingbootstrapsamples</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ReBootDistributedstatisticallearningviarefittingbootstrapsamples.html">&lt;p&gt;In this paper, we propose a one-shot distributed learning algorithm via refitting bootstrap samples, which we refer to as ReBoot. ReBoot refits a new model to mini-batches of bootstrap samples that are continuously drawn from each of the locally fitted models. It requires only one round of communication of model parameters without much memory. Theoretically, we analyze the statistical error rate of ReBoot for generalized linear models (GLM) and noisy phase retrieval, which represent convex and non-convex problems, respectively. In both cases, ReBoot provably achieves the full-sample statistical rate. In particular, we show that the systematic bias of ReBoot, the error that is independent of the number of subsamples (i.e., the number of sites), is $O(n ^ {-2})$ in GLM, where $n$ is the subsample size (the sample size of each local site). This rate is sharper than that of model parameter averaging and its variants, implying the higher tolerance of ReBoot with respect to data splits to maintain the full-sample rate. Our simulation study demonstrates the statistical advantage of ReBoot over competing methods. Finally, we propose FedReBoot, an iterative version of ReBoot, to aggregate convolutional neural networks for image classification. FedReBoot exhibits substantial superiority over Federated Averaging (FedAvg) within early rounds of communication.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2207.09098&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yumeng Wang, Ziwei Zhu, Xuming He</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In this paper, we propose a one-shot distributed learning algorithm via refitting bootstrap samples, which we refer to as ReBoot. ReBoot refits a new model to mini-batches of bootstrap samples that are continuously drawn from each of the locally fitted models. It requires only one round of communication of model parameters without much memory. Theoretically, we analyze the statistical error rate of ReBoot for generalized linear models (GLM) and noisy phase retrieval, which represent convex and non-convex problems, respectively. In both cases, ReBoot provably achieves the full-sample statistical rate. In particular, we show that the systematic bias of ReBoot, the error that is independent of the number of subsamples (i.e., the number of sites), is $O(n ^ {-2})$ in GLM, where $n$ is the subsample size (the sample size of each local site). This rate is sharper than that of model parameter averaging and its variants, implying the higher tolerance of ReBoot with respect to data splits to maintain the full-sample rate. Our simulation study demonstrates the statistical advantage of ReBoot over competing methods. Finally, we propose FedReBoot, an iterative version of ReBoot, to aggregate convolutional neural networks for image classification. FedReBoot exhibits substantial superiority over Federated Averaging (FedAvg) within early rounds of communication.</summary></entry><entry><title type="html">Reasoning with fuzzy and uncertain evidence using epistemic random fuzzy sets: general framework and practical models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Reasoningwithfuzzyanduncertainevidenceusingepistemicrandomfuzzysetsgeneralframeworkandpracticalmodels.html" rel="alternate" type="text/html" title="Reasoning with fuzzy and uncertain evidence using epistemic random fuzzy sets: general framework and practical models" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Reasoningwithfuzzyanduncertainevidenceusingepistemicrandomfuzzysetsgeneralframeworkandpracticalmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Reasoningwithfuzzyanduncertainevidenceusingepistemicrandomfuzzysetsgeneralframeworkandpracticalmodels.html">&lt;p&gt;We introduce a general theory of epistemic random fuzzy sets for reasoning with fuzzy or crisp evidence. This framework generalizes both the Dempster-Shafer theory of belief functions, and possibility theory. Independent epistemic random fuzzy sets are combined by the generalized product-intersection rule, which extends both Dempster’s rule for combining belief functions, and the product conjunctive combination of possibility distributions. We introduce Gaussian random fuzzy numbers and their multi-dimensional extensions, Gaussian random fuzzy vectors, as practical models for quantifying uncertainty about scalar or vector quantities. Closed-form expressions for the combination, projection and vacuous extension of Gaussian random fuzzy numbers and vectors are derived.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2202.08081&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thierry Denoeux</name></author><category term="stat.ME" /><summary type="html">We introduce a general theory of epistemic random fuzzy sets for reasoning with fuzzy or crisp evidence. This framework generalizes both the Dempster-Shafer theory of belief functions, and possibility theory. Independent epistemic random fuzzy sets are combined by the generalized product-intersection rule, which extends both Dempster’s rule for combining belief functions, and the product conjunctive combination of possibility distributions. We introduce Gaussian random fuzzy numbers and their multi-dimensional extensions, Gaussian random fuzzy vectors, as practical models for quantifying uncertainty about scalar or vector quantities. Closed-form expressions for the combination, projection and vacuous extension of Gaussian random fuzzy numbers and vectors are derived.</summary></entry><entry><title type="html">Recursive identification with regularization and on-line hyperparameters estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Recursiveidentificationwithregularizationandonlinehyperparametersestimation.html" rel="alternate" type="text/html" title="Recursive identification with regularization and on-line hyperparameters estimation" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Recursiveidentificationwithregularizationandonlinehyperparametersestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Recursiveidentificationwithregularizationandonlinehyperparametersestimation.html">&lt;p&gt;This paper presents a regularized recursive identification algorithm with simultaneous on-line estimation of both the model parameters and the algorithms hyperparameters. A new kernel is proposed to facilitate the algorithm development. The performance of this novel scheme is compared with that of the recursive least squares algorithm in simulation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.00097&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bernard Vau, Tudor-Bogdan Airimitoaie</name></author><category term="stat.ME" /><summary type="html">This paper presents a regularized recursive identification algorithm with simultaneous on-line estimation of both the model parameters and the algorithms hyperparameters. A new kernel is proposed to facilitate the algorithm development. The performance of this novel scheme is compared with that of the recursive least squares algorithm in simulation.</summary></entry><entry><title type="html">Return to Office and the Tenure Distribution</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ReturntoOfficeandtheTenureDistribution.html" rel="alternate" type="text/html" title="Return to Office and the Tenure Distribution" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ReturntoOfficeandtheTenureDistribution</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ReturntoOfficeandtheTenureDistribution.html">&lt;p&gt;With the official end of the COVID-19 pandemic, debates about the return to office have taken center stage among companies and employees. Despite their ubiquity, the economic implications of return to office policies are not fully understood. Using 260 million resumes matched to company data, we analyze the causal effects of such policies on employees’ tenure and seniority levels at three of the largest US tech companies: Microsoft, SpaceX, and Apple. Our estimation procedure is nonparametric and captures the full heterogeneity of tenure and seniority of employees in a distributional synthetic controls framework. We estimate a reduction in counterfactual tenure that increases for employees with longer tenure. Similarly, we document a leftward shift in the seniority distribution towards positions below the senior level. These shifts appear to be driven by employees leaving to larger firms that are direct competitors. Our results suggest that return to office policies can lead to an outflow of senior employees, posing a potential threat to the productivity, innovation, and competitiveness of the wider firm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04352&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David Van Dijcke, Florian Gunsilius, Austin Wright</name></author><category term="stat.AP" /><summary type="html">With the official end of the COVID-19 pandemic, debates about the return to office have taken center stage among companies and employees. Despite their ubiquity, the economic implications of return to office policies are not fully understood. Using 260 million resumes matched to company data, we analyze the causal effects of such policies on employees’ tenure and seniority levels at three of the largest US tech companies: Microsoft, SpaceX, and Apple. Our estimation procedure is nonparametric and captures the full heterogeneity of tenure and seniority of employees in a distributional synthetic controls framework. We estimate a reduction in counterfactual tenure that increases for employees with longer tenure. Similarly, we document a leftward shift in the seniority distribution towards positions below the senior level. These shifts appear to be driven by employees leaving to larger firms that are direct competitors. Our results suggest that return to office policies can lead to an outflow of senior employees, posing a potential threat to the productivity, innovation, and competitiveness of the wider firm.</summary></entry><entry><title type="html">Scalable Amortized GPLVMs for Single Cell Transcriptomics Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ScalableAmortizedGPLVMsforSingleCellTranscriptomicsData.html" rel="alternate" type="text/html" title="Scalable Amortized GPLVMs for Single Cell Transcriptomics Data" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ScalableAmortizedGPLVMsforSingleCellTranscriptomicsData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ScalableAmortizedGPLVMsforSingleCellTranscriptomicsData.html">&lt;p&gt;Dimensionality reduction is crucial for analyzing large-scale single-cell RNA-seq data. Gaussian Process Latent Variable Models (GPLVMs) offer an interpretable dimensionality reduction method, but current scalable models lack effectiveness in clustering cell types. We introduce an improved model, the amortized stochastic variational Bayesian GPLVM (BGPLVM), tailored for single-cell RNA-seq with specialized encoder, kernel, and likelihood designs. This model matches the performance of the leading single-cell variational inference (scVI) approach on synthetic and real-world COVID datasets and effectively incorporates cell-cycle and batch information to reveal more interpretable latent structures as we demonstrate on an innate immunity dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03879&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sarah Zhao, Aditya Ravuri, Vidhi Lalchand, Neil D. Lawrence</name></author><category term="stat.ML," /><category term="stat.AP" /><summary type="html">Dimensionality reduction is crucial for analyzing large-scale single-cell RNA-seq data. Gaussian Process Latent Variable Models (GPLVMs) offer an interpretable dimensionality reduction method, but current scalable models lack effectiveness in clustering cell types. We introduce an improved model, the amortized stochastic variational Bayesian GPLVM (BGPLVM), tailored for single-cell RNA-seq with specialized encoder, kernel, and likelihood designs. This model matches the performance of the leading single-cell variational inference (scVI) approach on synthetic and real-world COVID datasets and effectively incorporates cell-cycle and batch information to reveal more interpretable latent structures as we demonstrate on an innate immunity dataset.</summary></entry><entry><title type="html">Scalable Vertical Federated Learning via Data Augmentation and Amortized Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ScalableVerticalFederatedLearningviaDataAugmentationandAmortizedInference.html" rel="alternate" type="text/html" title="Scalable Vertical Federated Learning via Data Augmentation and Amortized Inference" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ScalableVerticalFederatedLearningviaDataAugmentationandAmortizedInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ScalableVerticalFederatedLearningviaDataAugmentationandAmortizedInference.html">&lt;p&gt;Vertical federated learning (VFL) has emerged as a paradigm for collaborative model estimation across multiple clients, each holding a distinct set of covariates. This paper introduces the first comprehensive framework for fitting Bayesian models in the VFL setting. We propose a novel approach that leverages data augmentation techniques to transform VFL problems into a form compatible with existing Bayesian federated learning algorithms. We present an innovative model formulation for specific VFL scenarios where the joint likelihood factorizes into a product of client-specific likelihoods. To mitigate the dimensionality challenge posed by data augmentation, which scales with the number of observations and clients, we develop a factorized amortized variational approximation that achieves scalability independent of the number of observations. We showcase the efficacy of our framework through extensive numerical experiments on logistic regression, multilevel regression, and a novel hierarchical Bayesian split neural net model. Our work paves the way for privacy-preserving, decentralized Bayesian inference in vertically partitioned data scenarios, opening up new avenues for research and applications in various domains.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04043&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Conor Hassan, Matthew Sutton, Antonietta Mira, Kerrie Mengersen</name></author><category term="stat.CO," /><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Vertical federated learning (VFL) has emerged as a paradigm for collaborative model estimation across multiple clients, each holding a distinct set of covariates. This paper introduces the first comprehensive framework for fitting Bayesian models in the VFL setting. We propose a novel approach that leverages data augmentation techniques to transform VFL problems into a form compatible with existing Bayesian federated learning algorithms. We present an innovative model formulation for specific VFL scenarios where the joint likelihood factorizes into a product of client-specific likelihoods. To mitigate the dimensionality challenge posed by data augmentation, which scales with the number of observations and clients, we develop a factorized amortized variational approximation that achieves scalability independent of the number of observations. We showcase the efficacy of our framework through extensive numerical experiments on logistic regression, multilevel regression, and a novel hierarchical Bayesian split neural net model. Our work paves the way for privacy-preserving, decentralized Bayesian inference in vertically partitioned data scenarios, opening up new avenues for research and applications in various domains.</summary></entry><entry><title type="html">Scalable network reconstruction in subquadratic time</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Scalablenetworkreconstructioninsubquadratictime.html" rel="alternate" type="text/html" title="Scalable network reconstruction in subquadratic time" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Scalablenetworkreconstructioninsubquadratictime</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Scalablenetworkreconstructioninsubquadratictime.html">&lt;p&gt;Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings – typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $\Omega(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that significantly outperforms this quadratic baseline. Our algorithm relies on a stochastic second neighbor search (Dong et al., 2011) that produces the best edge candidates with high probability, thus bypassing an exhaustive quadratic search. If we rely on the conjecture that the second-neighbor search finishes in log-linear time (Baron &amp;amp; Darling, 2020; 2022), we demonstrate theoretically that our algorithm finishes in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. In practice, we show that our algorithm achieves a performance that is many orders of magnitude faster than the quadratic baseline – in a manner consistent with our theoretical analysis – allows for easy parallelization, and thus enables the reconstruction of networks with hundreds of thousands and even millions of nodes and edges.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.01404&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tiago P. Peixoto</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings – typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $\Omega(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that significantly outperforms this quadratic baseline. Our algorithm relies on a stochastic second neighbor search (Dong et al., 2011) that produces the best edge candidates with high probability, thus bypassing an exhaustive quadratic search. If we rely on the conjecture that the second-neighbor search finishes in log-linear time (Baron &amp;amp; Darling, 2020; 2022), we demonstrate theoretically that our algorithm finishes in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. In practice, we show that our algorithm achieves a performance that is many orders of magnitude faster than the quadratic baseline – in a manner consistent with our theoretical analysis – allows for easy parallelization, and thus enables the reconstruction of networks with hundreds of thousands and even millions of nodes and edges.</summary></entry><entry><title type="html">Spatial Transfer Learning with Simple MLP</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/SpatialTransferLearningwithSimpleMLP.html" rel="alternate" type="text/html" title="Spatial Transfer Learning with Simple MLP" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/SpatialTransferLearningwithSimpleMLP</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/SpatialTransferLearningwithSimpleMLP.html">&lt;p&gt;First step to investigate the potential of transfer learning applied to the field of spatial statistics&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03720&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hongjian Yang</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">First step to investigate the potential of transfer learning applied to the field of spatial statistics</summary></entry><entry><title type="html">Statistical inference for a stochastic generalized logistic differential equation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Statisticalinferenceforastochasticgeneralizedlogisticdifferentialequation.html" rel="alternate" type="text/html" title="Statistical inference for a stochastic generalized logistic differential equation" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Statisticalinferenceforastochasticgeneralizedlogisticdifferentialequation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Statisticalinferenceforastochasticgeneralizedlogisticdifferentialequation.html">&lt;p&gt;This research aims to estimate three parameters in a stochastic generalized logistic differential equation. We assume the intrinsic growth rate and shape parameters are constant but unknown. To estimate these two parameters, we use the maximum likelihood method and establish that the estimators for these two parameters are strongly consistent. We estimate the diffusion parameter by using the quadratic variation processes. To test our results, we evaluate two data scenarios, complete and incomplete, with fixed values assigned to the three parameters. In the incomplete data scenario, we apply an Expectation Maximization algorithm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03815&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fernando Baltazar-Larios, Francisco Delgado-Vences, Saul Diaz-Infante, Eduardo Lince Gomez</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This research aims to estimate three parameters in a stochastic generalized logistic differential equation. We assume the intrinsic growth rate and shape parameters are constant but unknown. To estimate these two parameters, we use the maximum likelihood method and establish that the estimators for these two parameters are strongly consistent. We estimate the diffusion parameter by using the quadratic variation processes. To test our results, we evaluate two data scenarios, complete and incomplete, with fixed values assigned to the three parameters. In the incomplete data scenario, we apply an Expectation Maximization algorithm.</summary></entry><entry><title type="html">Stochastic Gradient MCMC for Massive Geostatistical Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/StochasticGradientMCMCforMassiveGeostatisticalData.html" rel="alternate" type="text/html" title="Stochastic Gradient MCMC for Massive Geostatistical Data" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/StochasticGradientMCMCforMassiveGeostatisticalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/StochasticGradientMCMCforMassiveGeostatisticalData.html">&lt;p&gt;Gaussian processes (GPs) are commonly used for prediction and inference for spatial data analyses. However, since estimation and prediction tasks have cubic time and quadratic memory complexity in number of locations, GPs are difficult to scale to large spatial datasets. The Vecchia approximation induces sparsity in the dependence structure and is one of several methods proposed to scale GP inference. Our work adds to the substantial research in this area by developing a stochastic gradient Markov chain Monte Carlo (SGMCMC) framework for efficient computation in GPs. At each step, the algorithm subsamples a minibatch of locations and subsequently updates process parameters through a Vecchia-approximated GP likelihood. Since the Vecchia-approximated GP has a time complexity that is linear in the number of locations, this results in scalable estimation in GPs. Through simulation studies, we demonstrate that SGMCMC is competitive with state-of-the-art scalable GP algorithms in terms of computational time and parameter estimation. An application of our method is also provided using the Argo dataset of ocean temperature measurements.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04531&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mohamed A. Abba, Brian J. Reich, Reetam Majumder, Brandon Feng</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Gaussian processes (GPs) are commonly used for prediction and inference for spatial data analyses. However, since estimation and prediction tasks have cubic time and quadratic memory complexity in number of locations, GPs are difficult to scale to large spatial datasets. The Vecchia approximation induces sparsity in the dependence structure and is one of several methods proposed to scale GP inference. Our work adds to the substantial research in this area by developing a stochastic gradient Markov chain Monte Carlo (SGMCMC) framework for efficient computation in GPs. At each step, the algorithm subsamples a minibatch of locations and subsequently updates process parameters through a Vecchia-approximated GP likelihood. Since the Vecchia-approximated GP has a time complexity that is linear in the number of locations, this results in scalable estimation in GPs. Through simulation studies, we demonstrate that SGMCMC is competitive with state-of-the-art scalable GP algorithms in terms of computational time and parameter estimation. An application of our method is also provided using the Argo dataset of ocean temperature measurements.</summary></entry><entry><title type="html">Stochastic behavior of an n-node blockchain under cyber attacks from multiple hackers with random re-setting times</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Stochasticbehaviorofannnodeblockchainundercyberattacksfrommultiplehackerswithrandomresettingtimes.html" rel="alternate" type="text/html" title="Stochastic behavior of an n-node blockchain under cyber attacks from multiple hackers with random re-setting times" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Stochasticbehaviorofannnodeblockchainundercyberattacksfrommultiplehackerswithrandomresettingtimes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/Stochasticbehaviorofannnodeblockchainundercyberattacksfrommultiplehackerswithrandomresettingtimes.html">&lt;p&gt;This paper investigates the stochastic behavior of an n-node blockchain which is continuously monitored and faces non-stop cyber attacks from multiple hackers. The blockchain will start being re-set once hacking is detected, forfeiting previous efforts of all hackers. It is assumed the re-setting process takes a random amount of time. Multiple independent hackers will keep attempting to hack into the blockchain until one of them succeeds. For arbitrary distributions of the hacking times, detecting times, and re-setting times, we derive the instantaneous functional probability, the limiting functional probability, and the mean functional time of the blockchain. Moreover, we establish that these quantities are increasing functions of the number of nodes, formalizing the intuition that the more nodes a blockchain has the more secure it is.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03814&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xiufeng Xu, Liang Hong</name></author><category term="stat.AP" /><summary type="html">This paper investigates the stochastic behavior of an n-node blockchain which is continuously monitored and faces non-stop cyber attacks from multiple hackers. The blockchain will start being re-set once hacking is detected, forfeiting previous efforts of all hackers. It is assumed the re-setting process takes a random amount of time. Multiple independent hackers will keep attempting to hack into the blockchain until one of them succeeds. For arbitrary distributions of the hacking times, detecting times, and re-setting times, we derive the instantaneous functional probability, the limiting functional probability, and the mean functional time of the blockchain. Moreover, we establish that these quantities are increasing functions of the number of nodes, formalizing the intuition that the more nodes a blockchain has the more secure it is.</summary></entry><entry><title type="html">The Projected Covariance Measure for assumption-lean variable significance testing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/TheProjectedCovarianceMeasureforassumptionleanvariablesignificancetesting.html" rel="alternate" type="text/html" title="The Projected Covariance Measure for assumption-lean variable significance testing" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/TheProjectedCovarianceMeasureforassumptionleanvariablesignificancetesting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/TheProjectedCovarianceMeasureforassumptionleanvariablesignificancetesting.html">&lt;p&gt;Testing the significance of a variable or group of variables $X$ for predicting a response $Y$, given additional covariates $Z$, is a ubiquitous task in statistics. A simple but common approach is to specify a linear model, and then test whether the regression coefficient for $X$ is non-zero. However, when the model is misspecified, the test may have poor power, for example when $X$ is involved in complex interactions, or lead to many false rejections. In this work we study the problem of testing the model-free null of conditional mean independence, i.e. that the conditional mean of $Y$ given $X$ and $Z$ does not depend on $X$. We propose a simple and general framework that can leverage flexible nonparametric or machine learning methods, such as additive models or random forests, to yield both robust error control and high power. The procedure involves using these methods to perform regressions, first to estimate a form of projection of $Y$ on $X$ and $Z$ using one half of the data, and then to estimate the expected conditional covariance between this projection and $Y$ on the remaining half of the data. While the approach is general, we show that a version of our procedure using spline regression achieves what we show is the minimax optimal rate in this nonparametric testing problem. Numerical experiments demonstrate the effectiveness of our approach both in terms of maintaining Type I error control, and power, compared to several existing approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.02039&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anton Rask Lundborg, Ilmun Kim, Rajen D. Shah, Richard J. Samworth</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Testing the significance of a variable or group of variables $X$ for predicting a response $Y$, given additional covariates $Z$, is a ubiquitous task in statistics. A simple but common approach is to specify a linear model, and then test whether the regression coefficient for $X$ is non-zero. However, when the model is misspecified, the test may have poor power, for example when $X$ is involved in complex interactions, or lead to many false rejections. In this work we study the problem of testing the model-free null of conditional mean independence, i.e. that the conditional mean of $Y$ given $X$ and $Z$ does not depend on $X$. We propose a simple and general framework that can leverage flexible nonparametric or machine learning methods, such as additive models or random forests, to yield both robust error control and high power. The procedure involves using these methods to perform regressions, first to estimate a form of projection of $Y$ on $X$ and $Z$ using one half of the data, and then to estimate the expected conditional covariance between this projection and $Y$ on the remaining half of the data. While the approach is general, we show that a version of our procedure using spline regression achieves what we show is the minimax optimal rate in this nonparametric testing problem. Numerical experiments demonstrate the effectiveness of our approach both in terms of maintaining Type I error control, and power, compared to several existing approaches.</summary></entry><entry><title type="html">The transcoding sampler for stick-breaking inferences on Dirichlet process mixtures</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ThetranscodingsamplerforstickbreakinginferencesonDirichletprocessmixtures.html" rel="alternate" type="text/html" title="The transcoding sampler for stick-breaking inferences on Dirichlet process mixtures" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ThetranscodingsamplerforstickbreakinginferencesonDirichletprocessmixtures</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/ThetranscodingsamplerforstickbreakinginferencesonDirichletprocessmixtures.html">&lt;p&gt;Dirichlet process mixture models suffer from slow mixing of the MCMC posterior chain produced by stick-breaking Gibbs samplers, as opposed to collapsed Gibbs samplers based on the Polya urn representation which have shorter integrated autocorrelation time (IAT).
  We study how cluster membership information is encoded under the two aforementioned samplers, and we introduce the transcoding algorithm to switch between encodings. We also develop the transcoding sampler, which consists of undertaking posterior partition inference with any high-efficiency sampler, such as collapsed Gibbs, and to subsequently transcode it to the stick-breaking representation via the transcoding algorithm, thereby allowing inference on all stick-breaking parameters of interest while retaining the shorter IAT of the high-efficiency sampler.
  The transcoding sampler is substantially simpler to implement than the slice sampler, it can inherit the shorter IAT of collapsed Gibbs samplers and it can also achieve zero IAT when paired with a posterior partition sampler that is i.i.d., such as the sequential importance sampler.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.02563&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carlo Vicentini</name></author><category term="stat.ME" /><summary type="html">Dirichlet process mixture models suffer from slow mixing of the MCMC posterior chain produced by stick-breaking Gibbs samplers, as opposed to collapsed Gibbs samplers based on the Polya urn representation which have shorter integrated autocorrelation time (IAT). We study how cluster membership information is encoded under the two aforementioned samplers, and we introduce the transcoding algorithm to switch between encodings. We also develop the transcoding sampler, which consists of undertaking posterior partition inference with any high-efficiency sampler, such as collapsed Gibbs, and to subsequently transcode it to the stick-breaking representation via the transcoding algorithm, thereby allowing inference on all stick-breaking parameters of interest while retaining the shorter IAT of the high-efficiency sampler. The transcoding sampler is substantially simpler to implement than the slice sampler, it can inherit the shorter IAT of collapsed Gibbs samplers and it can also achieve zero IAT when paired with a posterior partition sampler that is i.i.d., such as the sequential importance sampler.</summary></entry><entry><title type="html">Transportability of Principal Causal Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/TransportabilityofPrincipalCausalEffects.html" rel="alternate" type="text/html" title="Transportability of Principal Causal Effects" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/TransportabilityofPrincipalCausalEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/TransportabilityofPrincipalCausalEffects.html">&lt;p&gt;Recent research in causal inference has made important progress in addressing challenges to the external validity of trial findings. Such methods weight trial participant data to more closely resemble the distribution of effect-modifying covariates in a well-defined target population. In the presence of participant non-adherence to study medication, these methods effectively transport an intention-to-treat effect that averages over heterogeneous compliance behaviors. In this paper, we develop a principal stratification framework to identify causal effects conditioning on both on compliance behavior and membership in the target population. We also develop non-parametric efficiency theory for and construct efficient estimators of such “transported” principal causal effects and characterize their finite-sample performance in simulation experiments. While this work focuses on treatment non-adherence, the framework is applicable to a broad class of estimands that target effects in clinically-relevant, possibly latent subsets of a target population.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04419&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Justin M. Clark, Kollin W. Rott, James S. Hodges, Jared D. Huling</name></author><category term="stat.ME" /><summary type="html">Recent research in causal inference has made important progress in addressing challenges to the external validity of trial findings. Such methods weight trial participant data to more closely resemble the distribution of effect-modifying covariates in a well-defined target population. In the presence of participant non-adherence to study medication, these methods effectively transport an intention-to-treat effect that averages over heterogeneous compliance behaviors. In this paper, we develop a principal stratification framework to identify causal effects conditioning on both on compliance behavior and membership in the target population. We also develop non-parametric efficiency theory for and construct efficient estimators of such “transported” principal causal effects and characterize their finite-sample performance in simulation experiments. While this work focuses on treatment non-adherence, the framework is applicable to a broad class of estimands that target effects in clinically-relevant, possibly latent subsets of a target population.</summary></entry><entry><title type="html">UQ state-dependent framework for seismic fragility assessment of industrial components</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/UQstatedependentframeworkforseismicfragilityassessmentofindustrialcomponents.html" rel="alternate" type="text/html" title="UQ state-dependent framework for seismic fragility assessment of industrial components" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/UQstatedependentframeworkforseismicfragilityassessmentofindustrialcomponents</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/UQstatedependentframeworkforseismicfragilityassessmentofindustrialcomponents.html">&lt;p&gt;In this study, we propose a novel surrogate modelling approach to efficiently and accurately approximate the response of complex dynamical systems driven by time-varying Recently, there has been increased interest in assessing the seismic fragility of industrial plants and process equipment. This is reflected in the growing number of studies, community-funded research projects and experimental campaigns on the matter.Nonetheless, the complexity of the problem and its inherent modelling, coupled with a general scarcity of available data on process equipment, has limited the development of risk assessment methods. In fact, these limitations have led to the creation of simplified and quick-to-run models. In this context, we propose an innovative framework for developing state-dependent fragility functions. This new methodology combines limited data with the power of metamodelling and statistical techniques, namely polynomial chaos expansions (PCE) and bootstrapping. Therefore, we validated the framework on a simplified and inexpensive-to-run MDoF system endowed with Bouc-Wen hysteresis.Then, we tested it on a real nonstructural industrial process component. Specifically, we applied the state-dependent fragility framework to a critical vertical tank of a multicomponent full-scale 3D steel braced frame (BF). The seismic performance of the BF endowed with process components was captured by means of shake table campaign within the European SPIF project. Finally, we derived state-dependent fragility functions based on the combination of PCE and bootstrap at a greatly reduced computational cost.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04487&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>C. Nardin, S. Marelli, O. S. Bursi, B. Sudret, M. Broccardo</name></author><category term="stat.CO," /><category term="stat.AP" /><summary type="html">In this study, we propose a novel surrogate modelling approach to efficiently and accurately approximate the response of complex dynamical systems driven by time-varying Recently, there has been increased interest in assessing the seismic fragility of industrial plants and process equipment. This is reflected in the growing number of studies, community-funded research projects and experimental campaigns on the matter.Nonetheless, the complexity of the problem and its inherent modelling, coupled with a general scarcity of available data on process equipment, has limited the development of risk assessment methods. In fact, these limitations have led to the creation of simplified and quick-to-run models. In this context, we propose an innovative framework for developing state-dependent fragility functions. This new methodology combines limited data with the power of metamodelling and statistical techniques, namely polynomial chaos expansions (PCE) and bootstrapping. Therefore, we validated the framework on a simplified and inexpensive-to-run MDoF system endowed with Bouc-Wen hysteresis.Then, we tested it on a real nonstructural industrial process component. Specifically, we applied the state-dependent fragility framework to a critical vertical tank of a multicomponent full-scale 3D steel braced frame (BF). The seismic performance of the BF endowed with process components was captured by means of shake table campaign within the European SPIF project. Finally, we derived state-dependent fragility functions based on the combination of PCE and bootstrap at a greatly reduced computational cost.</summary></entry><entry><title type="html">Using Pre-training and Interaction Modeling for ancestry-specific disease prediction in UK Biobank</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/UsingPretrainingandInteractionModelingforancestryspecificdiseasepredictioninUKBiobank.html" rel="alternate" type="text/html" title="Using Pre-training and Interaction Modeling for ancestry-specific disease prediction in UK Biobank" /><published>2024-05-08T00:00:00+00:00</published><updated>2024-05-08T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/UsingPretrainingandInteractionModelingforancestryspecificdiseasepredictioninUKBiobank</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/08/UsingPretrainingandInteractionModelingforancestryspecificdiseasepredictioninUKBiobank.html">&lt;p&gt;Recent genome-wide association studies (GWAS) have uncovered the genetic basis of complex traits, but show an under-representation of non-European descent individuals, underscoring a critical gap in genetic research. Here, we assess whether we can improve disease prediction across diverse ancestries using multiomic data. We evaluate the performance of Group-LASSO INTERaction-NET (glinternet) and pretrained lasso in disease prediction focusing on diverse ancestries in the UK Biobank. Models were trained on data from White British and other ancestries and validated across a cohort of over 96,000 individuals for 8 diseases. Out of 96 models trained, we report 16 with statistically significant incremental predictive performance in terms of ROC-AUC scores (p-value &amp;lt; 0.05), found for diabetes, arthritis, gall stones, cystitis, asthma and osteoarthritis. For the interaction and pretrained models that outperformed the baseline, the PRS score was the primary driver behind prediction. Our findings indicate that both interaction terms and pre-training can enhance prediction accuracy but for a limited set of diseases and moderate improvements in accuracy&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17626&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thomas Le Menestrel, Erin Craig, Robert Tibshirani, Trevor Hastie, Manuel Rivas</name></author><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Recent genome-wide association studies (GWAS) have uncovered the genetic basis of complex traits, but show an under-representation of non-European descent individuals, underscoring a critical gap in genetic research. Here, we assess whether we can improve disease prediction across diverse ancestries using multiomic data. We evaluate the performance of Group-LASSO INTERaction-NET (glinternet) and pretrained lasso in disease prediction focusing on diverse ancestries in the UK Biobank. Models were trained on data from White British and other ancestries and validated across a cohort of over 96,000 individuals for 8 diseases. Out of 96 models trained, we report 16 with statistically significant incremental predictive performance in terms of ROC-AUC scores (p-value &amp;lt; 0.05), found for diabetes, arthritis, gall stones, cystitis, asthma and osteoarthritis. For the interaction and pretrained models that outperformed the baseline, the PRS score was the primary driver behind prediction. Our findings indicate that both interaction terms and pre-training can enhance prediction accuracy but for a limited set of diseases and moderate improvements in accuracy</summary></entry></feed>