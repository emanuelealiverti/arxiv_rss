<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-16T07:11:34+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html"></title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/2024-05-16-Propertiesofstationarycyclicalprocesses.html" rel="alternate" type="text/html" title="" /><published>2024-05-16T07:11:34+00:00</published><updated>2024-05-16T07:11:34+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/2024-05-16-Propertiesofstationarycyclicalprocesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/2024-05-16-Propertiesofstationarycyclicalprocesses.html">&lt;p&gt;The paper investigates the theoretical properties of zero-mean stationary time series with cyclical components, admitting the representation $y_t=\alpha_t \cos \lambda t + \beta_t \sin \lambda t$, with $\lambda \in (0,\pi]$ and $[\alpha_t\,\, \beta_t]$ following some bivariate process. We diagnose that in the extant literature on cyclic time series, a prevalent assumption of Gaussianity for $[\alpha_t\,\, \beta_t]$ imposes inadvertently a severe restriction on the amplitude of the process. Moreover, it is shown that other common distributions may suffer from either similar defects or fail to guarantee the stationarity of $y_t$. To address both of the issues, we propose to introduce a direct stochastic modulation of the amplitude and phase shift in an almost periodic function. We prove that this novel approach may lead, in general, to a stationary (up to any order) time series, and specifically, to a zero-mean stationary time series featuring cyclicity, with a pseudo-cyclical autocovariance function that may even decay at a very slow rate. The proposed process fills an important gap in this type of models and allows for flexible modeling of amplitude and phase shift.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08907&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Aliverti</name></author></entry><entry><title type="html">A new adaptive local polynomial density estimation procedure on complicated domains</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Anewadaptivelocalpolynomialdensityestimationprocedureoncomplicateddomains.html" rel="alternate" type="text/html" title="A new adaptive local polynomial density estimation procedure on complicated domains" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Anewadaptivelocalpolynomialdensityestimationprocedureoncomplicateddomains</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Anewadaptivelocalpolynomialdensityestimationprocedureoncomplicateddomains.html">&lt;p&gt;This paper presents a novel approach for pointwise estimation of multivariate density functions on known domains of arbitrary dimensions using nonparametric local polynomial estimators. Our method is highly flexible, as it applies to both simple domains, such as open connected sets, and more complicated domains that are not star-shaped around the point of estimation. This enables us to handle domains with sharp concavities, holes, and local pinches, such as polynomial sectors. Additionally, we introduce a data-driven selection rule based on the general ideas of Goldenshluger and Lepski. Our results demonstrate that the local polynomial estimators are minimax under a $L^2$ risk across a wide range of H&quot;older-type functional classes. In the adaptive case, we provide oracle inequalities and explicitly determine the convergence rate of our statistical procedure. Simulations on polynomial sectors show that our oracle estimates outperform those of the most popular alternative method, found in the sparr package for the R software. Our statistical procedure is implemented in an online R package which is readily accessible.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.01156&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Karine Bertin, Nicolas Klutchnikoff, Frédéric Ouimet</name></author><category term="stat.AP," /><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This paper presents a novel approach for pointwise estimation of multivariate density functions on known domains of arbitrary dimensions using nonparametric local polynomial estimators. Our method is highly flexible, as it applies to both simple domains, such as open connected sets, and more complicated domains that are not star-shaped around the point of estimation. This enables us to handle domains with sharp concavities, holes, and local pinches, such as polynomial sectors. Additionally, we introduce a data-driven selection rule based on the general ideas of Goldenshluger and Lepski. Our results demonstrate that the local polynomial estimators are minimax under a $L^2$ risk across a wide range of H&quot;older-type functional classes. In the adaptive case, we provide oracle inequalities and explicitly determine the convergence rate of our statistical procedure. Simulations on polynomial sectors show that our oracle estimates outperform those of the most popular alternative method, found in the sparr package for the R software. Our statistical procedure is implemented in an online R package which is readily accessible.</summary></entry><entry><title type="html">A new set of tools for goodness-of-fit validation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Anewsetoftoolsforgoodnessoffitvalidation.html" rel="alternate" type="text/html" title="A new set of tools for goodness-of-fit validation" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Anewsetoftoolsforgoodnessoffitvalidation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Anewsetoftoolsforgoodnessoffitvalidation.html">&lt;p&gt;We introduce two new tools to assess the validity of statistical distributions. These tools are based on components derived from a new statistical quantity, the $comparison$ $curve$. The first tool is a graphical representation of these components on a $bar$ $plot$ (B plot), which can provide a detailed appraisal of the validity of the statistical model, in particular when supplemented by acceptance regions related to the model. The knowledge gained from this representation can sometimes suggest an existing $goodness$-$of$-$fit$ test to supplement this visual assessment with a control of the type I error. Otherwise, an adaptive test may be preferable and the second tool is the combination of these components to produce a powerful $\chi^2$-type goodness-of-fit test. Because the number of these components can be large, we introduce a new selection rule to decide, in a data driven fashion, on their proper number to take into consideration. In a simulation, our goodness-of-fit tests are seen to be powerwise competitive with the best solutions that have been recommended in the context of a fully specified model as well as when some parameters must be estimated. Practical examples show how to use these tools to derive principled information about where the model departs from the data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2209.07295&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Gilles R. Ducharme, Teresa Ledwina</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We introduce two new tools to assess the validity of statistical distributions. These tools are based on components derived from a new statistical quantity, the $comparison$ $curve$. The first tool is a graphical representation of these components on a $bar$ $plot$ (B plot), which can provide a detailed appraisal of the validity of the statistical model, in particular when supplemented by acceptance regions related to the model. The knowledge gained from this representation can sometimes suggest an existing $goodness$-$of$-$fit$ test to supplement this visual assessment with a control of the type I error. Otherwise, an adaptive test may be preferable and the second tool is the combination of these components to produce a powerful $\chi^2$-type goodness-of-fit test. Because the number of these components can be large, we introduce a new selection rule to decide, in a data driven fashion, on their proper number to take into consideration. In a simulation, our goodness-of-fit tests are seen to be powerwise competitive with the best solutions that have been recommended in the context of a fully specified model as well as when some parameters must be estimated. Practical examples show how to use these tools to derive principled information about where the model departs from the data.</summary></entry><entry><title type="html">A review of regularised estimation methods and cross-validation in spatiotemporal statistics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Areviewofregularisedestimationmethodsandcrossvalidationinspatiotemporalstatistics.html" rel="alternate" type="text/html" title="A review of regularised estimation methods and cross-validation in spatiotemporal statistics" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Areviewofregularisedestimationmethodsandcrossvalidationinspatiotemporalstatistics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Areviewofregularisedestimationmethodsandcrossvalidationinspatiotemporalstatistics.html">&lt;p&gt;This review article focuses on regularised estimation procedures applicable to geostatistical and spatial econometric models. These methods are particularly relevant in the case of big geospatial data for dimensionality reduction or model selection. To structure the review, we initially consider the most general case of multivariate spatiotemporal processes (i.e., $g &amp;gt; 1$ dimensions of the spatial domain, a one-dimensional temporal domain, and $q \geq 1$ random variables). Then, the idea of regularised/penalised estimation procedures and different choices of shrinkage targets are discussed. Finally, guided by the elements of a mixed-effects model setup, which allows for a variety of spatiotemporal models, we show different regularisation procedures and how they can be used for the analysis of geo-referenced data, e.g. for selection of relevant regressors, dimensionality reduction of the covariance matrices, detection of conditionally independent locations, or the estimation of a full spatial interaction matrix.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.00183&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Philipp Otto, Alessandro Fassò, Paolo Maranzano</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.OT" /><summary type="html">This review article focuses on regularised estimation procedures applicable to geostatistical and spatial econometric models. These methods are particularly relevant in the case of big geospatial data for dimensionality reduction or model selection. To structure the review, we initially consider the most general case of multivariate spatiotemporal processes (i.e., $g &amp;gt; 1$ dimensions of the spatial domain, a one-dimensional temporal domain, and $q \geq 1$ random variables). Then, the idea of regularised/penalised estimation procedures and different choices of shrinkage targets are discussed. Finally, guided by the elements of a mixed-effects model setup, which allows for a variety of spatiotemporal models, we show different regularisation procedures and how they can be used for the analysis of geo-referenced data, e.g. for selection of relevant regressors, dimensionality reduction of the covariance matrices, detection of conditionally independent locations, or the estimation of a full spatial interaction matrix.</summary></entry><entry><title type="html">Asymptotically Unbiased Synthetic Control Methods by Distribution Matching</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/AsymptoticallyUnbiasedSyntheticControlMethodsbyDistributionMatching.html" rel="alternate" type="text/html" title="Asymptotically Unbiased Synthetic Control Methods by Distribution Matching" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/AsymptoticallyUnbiasedSyntheticControlMethodsbyDistributionMatching</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/AsymptoticallyUnbiasedSyntheticControlMethodsbyDistributionMatching.html">&lt;p&gt;Synthetic Control Methods (SCMs) have become an essential tool for comparative case studies. The fundamental idea of SCMs is to estimate the counterfactual outcomes of a treated unit using a weighted sum of the observed outcomes of untreated units. The accuracy of the synthetic control (SC) is critical for evaluating the treatment effect of a policy intervention; therefore, the estimation of SC weights has been the focus of extensive research. In this study, we first point out that existing SCMs suffer from an endogeneity problem, the correlation between the outcomes of untreated units and the error term of the synthetic control, which yields a bias in the treatment effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the joint density of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matching the moments of treated outcomes with the weighted sum of moments of untreated outcomes. Our proposed method has three advantages over existing methods: first, our estimator is asymptotically unbiased under the assumption of the mixture model; second, due to the asymptotic unbiasedness, we can reduce the mean squared error in counterfactual predictions; third, our method generates full densities of the treatment effect, not merely expected values, which broadens the applicability of SCMs. We provide experimental results to demonstrate the effectiveness of our proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.11127&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Masahiro Kato, Akari Ohda, Masaaki Imaizumi</name></author><category term="stat.ME" /><summary type="html">Synthetic Control Methods (SCMs) have become an essential tool for comparative case studies. The fundamental idea of SCMs is to estimate the counterfactual outcomes of a treated unit using a weighted sum of the observed outcomes of untreated units. The accuracy of the synthetic control (SC) is critical for evaluating the treatment effect of a policy intervention; therefore, the estimation of SC weights has been the focus of extensive research. In this study, we first point out that existing SCMs suffer from an endogeneity problem, the correlation between the outcomes of untreated units and the error term of the synthetic control, which yields a bias in the treatment effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the joint density of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matching the moments of treated outcomes with the weighted sum of moments of untreated outcomes. Our proposed method has three advantages over existing methods: first, our estimator is asymptotically unbiased under the assumption of the mixture model; second, due to the asymptotic unbiasedness, we can reduce the mean squared error in counterfactual predictions; third, our method generates full densities of the treatment effect, not merely expected values, which broadens the applicability of SCMs. We provide experimental results to demonstrate the effectiveness of our proposed method.</summary></entry><entry><title type="html">Best practices for estimating and reporting epidemiological delay distributions of infectious diseases using public health surveillance and healthcare data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Bestpracticesforestimatingandreportingepidemiologicaldelaydistributionsofinfectiousdiseasesusingpublichealthsurveillanceandhealthcaredata.html" rel="alternate" type="text/html" title="Best practices for estimating and reporting epidemiological delay distributions of infectious diseases using public health surveillance and healthcare data" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Bestpracticesforestimatingandreportingepidemiologicaldelaydistributionsofinfectiousdiseasesusingpublichealthsurveillanceandhealthcaredata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Bestpracticesforestimatingandreportingepidemiologicaldelaydistributionsofinfectiousdiseasesusingpublichealthsurveillanceandhealthcaredata.html">&lt;p&gt;Epidemiological delays, such as incubation periods, serial intervals, and hospital lengths of stay, are among key quantities in infectious disease epidemiology that inform public health policy and clinical practice. This information is used to inform mathematical and statistical models, which in turn can inform control strategies. There are three main challenges that make delay distributions difficult to estimate. First, the data are commonly censored (e.g., symptom onset may only be reported by date instead of the exact time of day). Second, delays are often right truncated when being estimated in real time (not all events that have occurred have been observed yet). Third, during a rapidly growing or declining outbreak, overrepresentation or underrepresentation, respectively, of recently infected cases in the data can lead to bias in estimates. Studies that estimate delays rarely address all these factors and sometimes report several estimates using different combinations of adjustments, which can lead to conflicting answers and confusion about which estimates are most accurate. In this work, we formulate a checklist of best practices for estimating and reporting epidemiological delays with a focus on the incubation period and serial interval. We also propose strategies for handling common biases and identify areas where more work is needed. Our recommendations can help improve the robustness and utility of reported estimates and provide guidance for the evaluation of estimates for downstream use in transmission models or other analyses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08841&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kelly Charniga , Sang Woo Park , Andrei R Akhmetzhanov , Anne Cori , Jonathan Dushoff , Sebastian Funk , Katelyn M Gostic , Natalie M Linton , Adrian Lison , Christopher E Overton , Juliet R C Pulliam , Thomas Ward , Simon Cauchemez , Sam Abbott</name></author><category term="stat.ME" /><summary type="html">Epidemiological delays, such as incubation periods, serial intervals, and hospital lengths of stay, are among key quantities in infectious disease epidemiology that inform public health policy and clinical practice. This information is used to inform mathematical and statistical models, which in turn can inform control strategies. There are three main challenges that make delay distributions difficult to estimate. First, the data are commonly censored (e.g., symptom onset may only be reported by date instead of the exact time of day). Second, delays are often right truncated when being estimated in real time (not all events that have occurred have been observed yet). Third, during a rapidly growing or declining outbreak, overrepresentation or underrepresentation, respectively, of recently infected cases in the data can lead to bias in estimates. Studies that estimate delays rarely address all these factors and sometimes report several estimates using different combinations of adjustments, which can lead to conflicting answers and confusion about which estimates are most accurate. In this work, we formulate a checklist of best practices for estimating and reporting epidemiological delays with a focus on the incubation period and serial interval. We also propose strategies for handling common biases and identify areas where more work is needed. Our recommendations can help improve the robustness and utility of reported estimates and provide guidance for the evaluation of estimates for downstream use in transmission models or other analyses.</summary></entry><entry><title type="html">Causal Inference for a Hidden Treatment</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/CausalInferenceforaHiddenTreatment.html" rel="alternate" type="text/html" title="Causal Inference for a Hidden Treatment" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/CausalInferenceforaHiddenTreatment</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/CausalInferenceforaHiddenTreatment.html">&lt;p&gt;In many empirical settings, directly observing a treatment variable may be infeasible although an error-prone surrogate measurement of the latter will often be available. Causal inference based solely on the observed surrogate measurement of the hidden treatment may be particularly challenging without an additional assumption or auxiliary data. To address this issue, we propose a method that carefully incorporates the surrogate measurement together with a proxy of the hidden treatment to identify its causal effect on any scale for which identification would in principle be feasible had contrary to fact the treatment been observed error-free. Beyond identification, we provide general semiparametric theory for causal effects identified using our approach, and we derive a large class of semiparametric estimators with an appealing multiple robustness property. A significant obstacle to our approach is the estimation of nuisance functions involving the hidden treatment, which prevents the direct application of standard machine learning algorithms. To resolve this, we introduce a novel semiparametric EM algorithm, thus adding a practical dimension to our theoretical contributions. This methodology can be adapted to analyze a large class of causal parameters in the proposed hidden treatment model, including the population average treatment effect, the effect of treatment on the treated, quantile treatment effects, and causal effects under marginal structural models. We examine the finite-sample performance of our method using simulations and an application which aims to estimate the causal effect of Alzheimer’s disease on hippocampal volume using data from the Alzheimer’s Disease Neuroimaging Initiative.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09080&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ying Zhou, Eric Tchetgen Tchetgen</name></author><category term="stat.ME" /><summary type="html">In many empirical settings, directly observing a treatment variable may be infeasible although an error-prone surrogate measurement of the latter will often be available. Causal inference based solely on the observed surrogate measurement of the hidden treatment may be particularly challenging without an additional assumption or auxiliary data. To address this issue, we propose a method that carefully incorporates the surrogate measurement together with a proxy of the hidden treatment to identify its causal effect on any scale for which identification would in principle be feasible had contrary to fact the treatment been observed error-free. Beyond identification, we provide general semiparametric theory for causal effects identified using our approach, and we derive a large class of semiparametric estimators with an appealing multiple robustness property. A significant obstacle to our approach is the estimation of nuisance functions involving the hidden treatment, which prevents the direct application of standard machine learning algorithms. To resolve this, we introduce a novel semiparametric EM algorithm, thus adding a practical dimension to our theoretical contributions. This methodology can be adapted to analyze a large class of causal parameters in the proposed hidden treatment model, including the population average treatment effect, the effect of treatment on the treated, quantile treatment effects, and causal effects under marginal structural models. We examine the finite-sample performance of our method using simulations and an application which aims to estimate the causal effect of Alzheimer’s disease on hippocampal volume using data from the Alzheimer’s Disease Neuroimaging Initiative.</summary></entry><entry><title type="html">Classification by sparse generalized additive models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Classificationbysparsegeneralizedadditivemodels.html" rel="alternate" type="text/html" title="Classification by sparse generalized additive models" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Classificationbysparsegeneralizedadditivemodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Classificationbysparsegeneralizedadditivemodels.html">&lt;p&gt;We consider (nonparametric) sparse (generalized) additive models (SpAM) for classification. The design of a SpAM classifier is based on minimizing the logistic loss with a sparse group Lasso/Slope-type penalties on the coefficients of univariate additive components’ expansions in orthonormal series (e.g., Fourier or wavelets). The resulting classifier is inherently adaptive to the unknown sparsity and smoothness. We show that under certain sparse group restricted eigenvalue condition it is nearly-minimax (up to log-factors) simultaneously across the entire range of analytic, Sobolev and Besov classes. The performance of the proposed classifier is illustrated on a simulated and a real-data examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.01792&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Felix Abramovich</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We consider (nonparametric) sparse (generalized) additive models (SpAM) for classification. The design of a SpAM classifier is based on minimizing the logistic loss with a sparse group Lasso/Slope-type penalties on the coefficients of univariate additive components’ expansions in orthonormal series (e.g., Fourier or wavelets). The resulting classifier is inherently adaptive to the unknown sparsity and smoothness. We show that under certain sparse group restricted eigenvalue condition it is nearly-minimax (up to log-factors) simultaneously across the entire range of analytic, Sobolev and Besov classes. The performance of the proposed classifier is illustrated on a simulated and a real-data examples.</summary></entry><entry><title type="html">Efficient pooling designs and screening performance in group testing for two type defectives</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Efficientpoolingdesignsandscreeningperformanceingrouptestingfortwotypedefectives.html" rel="alternate" type="text/html" title="Efficient pooling designs and screening performance in group testing for two type defectives" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Efficientpoolingdesignsandscreeningperformanceingrouptestingfortwotypedefectives</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Efficientpoolingdesignsandscreeningperformanceingrouptestingfortwotypedefectives.html">&lt;p&gt;Group testing is utilized in the case when we want to find a few defectives among large amount of items. Testing n items one by one requires n tests, but if the ratio of defectives is small, group testing is an efficient way to reduce the number of tests. Many research have been developed for group testing for a single type of defectives. In this paper, we consider the case where two types of defective A and B exist. For two types of defectives, we develop a belief propagation algorithm to compute marginal posterior probability of defectives. Furthermore, we construct several kinds of collections of pools in order to test for A and B. And by utilizing our belief propagation algorithm, we evaluate the performance of group testing by conducting simulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09455&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hiroyasu Matsushima, Yusuke Tajima, Xiao-Nan Lu, Masakazu Jimbo</name></author><category term="stat.CO" /><summary type="html">Group testing is utilized in the case when we want to find a few defectives among large amount of items. Testing n items one by one requires n tests, but if the ratio of defectives is small, group testing is an efficient way to reduce the number of tests. Many research have been developed for group testing for a single type of defectives. In this paper, we consider the case where two types of defective A and B exist. For two types of defectives, we develop a belief propagation algorithm to compute marginal posterior probability of defectives. Furthermore, we construct several kinds of collections of pools in order to test for A and B. And by utilizing our belief propagation algorithm, we evaluate the performance of group testing by conducting simulations.</summary></entry><entry><title type="html">Emperical Study on the Effect of Multi-Sampling in the Prediction Step of the Particle Filter</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/EmpericalStudyontheEffectofMultiSamplinginthePredictionStepoftheParticleFilter.html" rel="alternate" type="text/html" title="Emperical Study on the Effect of Multi-Sampling in the Prediction Step of the Particle Filter" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/EmpericalStudyontheEffectofMultiSamplinginthePredictionStepoftheParticleFilter</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/EmpericalStudyontheEffectofMultiSamplinginthePredictionStepoftheParticleFilter.html">&lt;p&gt;Particle filters are applicable to a wide range of nonlinear, non-Gaussian state-space models and have already been applied to a variety of problems. However, there is a problem in the calculation of smoothed distributions, where particles gradually degenerate and accuracy is reduced. The purpose of this paper is to consider the possibility of generating multiple particles in the prediction step of the particle filter and to empirically verify the effect using real data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09167&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>G. Kitagawa</name></author><category term="stat.CO" /><summary type="html">Particle filters are applicable to a wide range of nonlinear, non-Gaussian state-space models and have already been applied to a variety of problems. However, there is a problem in the calculation of smoothed distributions, where particles gradually degenerate and accuracy is reduced. The purpose of this paper is to consider the possibility of generating multiple particles in the prediction step of the particle filter and to empirically verify the effect using real data.</summary></entry><entry><title type="html">Enhancing Airline Customer Satisfaction: A Machine Learning and Causal Analysis Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/EnhancingAirlineCustomerSatisfactionAMachineLearningandCausalAnalysisApproach.html" rel="alternate" type="text/html" title="Enhancing Airline Customer Satisfaction: A Machine Learning and Causal Analysis Approach" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/EnhancingAirlineCustomerSatisfactionAMachineLearningandCausalAnalysisApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/EnhancingAirlineCustomerSatisfactionAMachineLearningandCausalAnalysisApproach.html">&lt;p&gt;This study explores the enhancement of customer satisfaction in the airline industry, a critical factor for retaining customers and building brand reputation, which are vital for revenue growth. Utilizing a combination of machine learning and causal inference methods, we examine the specific impact of service improvements on customer satisfaction, with a focus on the online boarding pass experience. Through detailed data analysis involving several predictive and causal models, we demonstrate that improvements in the digital aspects of customer service significantly elevate overall customer satisfaction. This paper highlights how airlines can strategically leverage these insights to make data-driven decisions that enhance customer experiences and, consequently, their market competitiveness.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09076&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tejas Mirthipati</name></author><category term="stat.ME" /><summary type="html">This study explores the enhancement of customer satisfaction in the airline industry, a critical factor for retaining customers and building brand reputation, which are vital for revenue growth. Utilizing a combination of machine learning and causal inference methods, we examine the specific impact of service improvements on customer satisfaction, with a focus on the online boarding pass experience. Through detailed data analysis involving several predictive and causal models, we demonstrate that improvements in the digital aspects of customer service significantly elevate overall customer satisfaction. This paper highlights how airlines can strategically leverage these insights to make data-driven decisions that enhance customer experiences and, consequently, their market competitiveness.</summary></entry><entry><title type="html">Evaluating the Uncertainty in Mean Residual Times: Estimators Based on Residence Times from Discrete Time Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/EvaluatingtheUncertaintyinMeanResidualTimesEstimatorsBasedonResidenceTimesfromDiscreteTimeProcesses.html" rel="alternate" type="text/html" title="Evaluating the Uncertainty in Mean Residual Times: Estimators Based on Residence Times from Discrete Time Processes" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/EvaluatingtheUncertaintyinMeanResidualTimesEstimatorsBasedonResidenceTimesfromDiscreteTimeProcesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/EvaluatingtheUncertaintyinMeanResidualTimesEstimatorsBasedonResidenceTimesfromDiscreteTimeProcesses.html">&lt;p&gt;In this work, we propose estimators for the uncertainty in mean residual times that require, for their evaluation, statistically independent individual residence times obtained from a discrete time process. We examine their performance through numerical experiments involving well-known probability distributions, and an application example using molecular dynamics simulation results, from an aqueous NaCl solution, is provided. These computationally inexpensive estimators, capable of achieving very accurate outcomes, serve as useful tools for assessing and reporting uncertainties in mean residual times across a wide range of simulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08853&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hernán R. Sánchez, Javier Garcia</name></author><category term="stat.ME" /><summary type="html">In this work, we propose estimators for the uncertainty in mean residual times that require, for their evaluation, statistically independent individual residence times obtained from a discrete time process. We examine their performance through numerical experiments involving well-known probability distributions, and an application example using molecular dynamics simulation results, from an aqueous NaCl solution, is provided. These computationally inexpensive estimators, capable of achieving very accurate outcomes, serve as useful tools for assessing and reporting uncertainties in mean residual times across a wide range of simulations.</summary></entry><entry><title type="html">Exploring uniformity and maximum entropy distribution on torus through intrinsic geometry: Application to protein-chemistry</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/ExploringuniformityandmaximumentropydistributionontorusthroughintrinsicgeometryApplicationtoproteinchemistry.html" rel="alternate" type="text/html" title="Exploring uniformity and maximum entropy distribution on torus through intrinsic geometry: Application to protein-chemistry" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/ExploringuniformityandmaximumentropydistributionontorusthroughintrinsicgeometryApplicationtoproteinchemistry</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/ExploringuniformityandmaximumentropydistributionontorusthroughintrinsicgeometryApplicationtoproteinchemistry.html">&lt;p&gt;A generic family of distributions, defined on the surface of a curved torus is introduced using the area element of it. The area uniformity and the maximum entropy distribution are identified using the trigonometric moments of the proposed family. A marginal distribution is obtained as a three-parameter modification of the von Mises distribution that encompasses the von Mises, Cardioid, and Uniform distributions as special cases. The proposed family of the marginal distribution exhibits both symmetric and asymmetric, unimodal or bimodal shapes, contingent upon parameters. Furthermore, we scrutinize a two-parameter symmetric submodel, examining its moments, measure of variation, Kullback-Leibler divergence, and maximum likelihood estimation, among other properties. In addition, we introduce a modified acceptance-rejection sampling with a thin envelope obtained from the upper-Riemann-sum of a circular density, achieving a high rate of acceptance. This proposed sampling scheme will accelerate the empirical studies for a large-scale simulation reducing the processing time. Furthermore, we extend the Uniform, Wrapped Cauchy, and Kato-Jones distributions to the surface of the curved torus and implemented the proposed bivariate toroidal distribution for different groups of protein data, namely, $\alpha$-helix, $\beta$-sheet, and their mixture. A marginal of this proposed distribution is fitted to the wind direction data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09149&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Surojit Biswas, Buddhananda Banerjee</name></author><category term="stat.ME" /><summary type="html">A generic family of distributions, defined on the surface of a curved torus is introduced using the area element of it. The area uniformity and the maximum entropy distribution are identified using the trigonometric moments of the proposed family. A marginal distribution is obtained as a three-parameter modification of the von Mises distribution that encompasses the von Mises, Cardioid, and Uniform distributions as special cases. The proposed family of the marginal distribution exhibits both symmetric and asymmetric, unimodal or bimodal shapes, contingent upon parameters. Furthermore, we scrutinize a two-parameter symmetric submodel, examining its moments, measure of variation, Kullback-Leibler divergence, and maximum likelihood estimation, among other properties. In addition, we introduce a modified acceptance-rejection sampling with a thin envelope obtained from the upper-Riemann-sum of a circular density, achieving a high rate of acceptance. This proposed sampling scheme will accelerate the empirical studies for a large-scale simulation reducing the processing time. Furthermore, we extend the Uniform, Wrapped Cauchy, and Kato-Jones distributions to the surface of the curved torus and implemented the proposed bivariate toroidal distribution for different groups of protein data, namely, $\alpha$-helix, $\beta$-sheet, and their mixture. A marginal of this proposed distribution is fitted to the wind direction data.</summary></entry><entry><title type="html">Generative Adversarial Networks Applied to Synthetic Financial Scenarios Generation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/GenerativeAdversarialNetworksAppliedtoSyntheticFinancialScenariosGeneration.html" rel="alternate" type="text/html" title="Generative Adversarial Networks Applied to Synthetic Financial Scenarios Generation" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/GenerativeAdversarialNetworksAppliedtoSyntheticFinancialScenariosGeneration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/GenerativeAdversarialNetworksAppliedtoSyntheticFinancialScenariosGeneration.html">&lt;p&gt;The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of commodities, balance sheet data and more recently extra-financial data such as ESG scores. This broadening of the factors retained as influential constitutes a serious challenge for statistical modeling. Indeed, the instability of the correlations between these factors makes it practically impossible to identify the joint laws needed to construct scenarios. Fortunately, spectacular advances in Deep Learning field in recent years have given rise to GANs. GANs are a type of generative machine learning models that produce new data samples with the same characteristics as a training data distribution in an unsupervised way, avoiding data assumptions and human induced biases. In this work, we are exploring the use of GANs for synthetic financial scenarios generation. This pilot study is the result of a collaboration between Fujitsu and Advestis and it will be followed by a thorough exploration of the use cases that can benefit from the proposed solution. We propose a GANs-based algorithm that allows the replication of multivariate data representing several properties (including, but not limited to, price, market capitalization, ESG score, controversy score,. . .) of a set of stocks. This approach differs from examples in the financial literature, which are mainly focused on the reproduction of temporal asset price scenarios. We also propose several metrics to evaluate the quality of the data generated by the GANs. This approach is well fit for the generation of scenarios, the time direction simply arising as a subsequent (eventually conditioned) generation of data points drawn from the learned distribution. Our method will allow to simulate high dimensional scenarios (compared to $\lesssim10$ features currently employed in most recent use cases) where network complexity is reduced thanks to a wisely performed feature engineering and selection. Complete results will be presented in a forthcoming study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2209.03935&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matteo Rizzato, Julien Wallart, Christophe Geissler, Nicolas Morizet, Noureddine Boumlaik</name></author><category term="stat.AP" /><summary type="html">The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of commodities, balance sheet data and more recently extra-financial data such as ESG scores. This broadening of the factors retained as influential constitutes a serious challenge for statistical modeling. Indeed, the instability of the correlations between these factors makes it practically impossible to identify the joint laws needed to construct scenarios. Fortunately, spectacular advances in Deep Learning field in recent years have given rise to GANs. GANs are a type of generative machine learning models that produce new data samples with the same characteristics as a training data distribution in an unsupervised way, avoiding data assumptions and human induced biases. In this work, we are exploring the use of GANs for synthetic financial scenarios generation. This pilot study is the result of a collaboration between Fujitsu and Advestis and it will be followed by a thorough exploration of the use cases that can benefit from the proposed solution. We propose a GANs-based algorithm that allows the replication of multivariate data representing several properties (including, but not limited to, price, market capitalization, ESG score, controversy score,. . .) of a set of stocks. This approach differs from examples in the financial literature, which are mainly focused on the reproduction of temporal asset price scenarios. We also propose several metrics to evaluate the quality of the data generated by the GANs. This approach is well fit for the generation of scenarios, the time direction simply arising as a subsequent (eventually conditioned) generation of data points drawn from the learned distribution. Our method will allow to simulate high dimensional scenarios (compared to $\lesssim10$ features currently employed in most recent use cases) where network complexity is reduced thanks to a wisely performed feature engineering and selection. Complete results will be presented in a forthcoming study.</summary></entry><entry><title type="html">High dimensional test for functional covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Highdimensionaltestforfunctionalcovariates.html" rel="alternate" type="text/html" title="High dimensional test for functional covariates" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Highdimensionaltestforfunctionalcovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Highdimensionaltestforfunctionalcovariates.html">&lt;p&gt;As medical devices become more complex, they routinely collect extensive and complicated data. While classical regressions typically examine the relationship between an outcome and a vector of predictors, it becomes imperative to identify the relationship with predictors possessing functional structures. In this article, we introduce a novel inference procedure for examining the relationship between outcomes and large-scale functional predictors. We target testing the linear hypothesis on the functional parameters under the generalized functional linear regression framework, where the number of the functional parameters grows with the sample size. We develop the estimation procedure for the high dimensional generalized functional linear model incorporating B-spline functional approximation and amenable regularization. Furthermore, we construct a procedure that is able to test the local alternative hypothesis on the linear combinations of the functional parameters. We establish the statistical guarantees in terms of non-asymptotic convergence of the parameter estimation and the oracle property and asymptotic normality of the estimators. Moreover, we derive the asymptotic distribution of the test statistic. We carry out intensive simulations and illustrate with a new dataset from an Alzheimer’s disease magnetoencephalography study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08912&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Huaqing Jin, Fei Jiang</name></author><category term="stat.ME" /><summary type="html">As medical devices become more complex, they routinely collect extensive and complicated data. While classical regressions typically examine the relationship between an outcome and a vector of predictors, it becomes imperative to identify the relationship with predictors possessing functional structures. In this article, we introduce a novel inference procedure for examining the relationship between outcomes and large-scale functional predictors. We target testing the linear hypothesis on the functional parameters under the generalized functional linear regression framework, where the number of the functional parameters grows with the sample size. We develop the estimation procedure for the high dimensional generalized functional linear model incorporating B-spline functional approximation and amenable regularization. Furthermore, we construct a procedure that is able to test the local alternative hypothesis on the linear combinations of the functional parameters. We establish the statistical guarantees in terms of non-asymptotic convergence of the parameter estimation and the oracle property and asymptotic normality of the estimators. Moreover, we derive the asymptotic distribution of the test statistic. We carry out intensive simulations and illustrate with a new dataset from an Alzheimer’s disease magnetoencephalography study.</summary></entry><entry><title type="html">Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/IntegratingLargeLanguageModelsinCausalDiscoveryAStatisticalCausalApproach.html" rel="alternate" type="text/html" title="Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/IntegratingLargeLanguageModelsinCausalDiscoveryAStatisticalCausalApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/IntegratingLargeLanguageModelsinCausalDiscoveryAStatisticalCausalApproach.html">&lt;p&gt;In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)’’ for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, by using an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve SCD on this dataset, even if this dataset has never been included in the training data of the LLM. The proposed approach can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.01454&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Masayuki Takayama, Tadahisa Okuda, Thong Pham, Tatsuyoshi Ikenoue, Shingo Fukuma, Shohei Shimizu, Akiyoshi Sannai</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)’’ for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, by using an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve SCD on this dataset, even if this dataset has never been included in the training data of the LLM. The proposed approach can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains.</summary></entry><entry><title type="html">Leveraging graphical model techniques to study evolution on phylogenetic networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Leveraginggraphicalmodeltechniquestostudyevolutiononphylogeneticnetworks.html" rel="alternate" type="text/html" title="Leveraging graphical model techniques to study evolution on phylogenetic networks" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Leveraginggraphicalmodeltechniquestostudyevolutiononphylogeneticnetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/Leveraginggraphicalmodeltechniquestostudyevolutiononphylogeneticnetworks.html">&lt;p&gt;The evolution of molecular and phenotypic traits is commonly modelled using Markov processes along a rooted phylogeny. This phylogeny can be a tree, or a network if it includes reticulations, representing events such as hybridization or admixture. Computing the likelihood of data observed at the leaves is costly as the size and complexity of the phylogeny grows. Efficient algorithms exist for trees, but cannot be applied to networks. We show that a vast array of models for trait evolution along phylogenetic networks can be reformulated as graphical models, for which efficient belief propagation algorithms exist. We provide a brief review of belief propagation on general graphical models, then focus on linear Gaussian models for continuous traits. We show how belief propagation techniques can be applied for exact or approximate (but more scalable) likelihood and gradient calculations, and prove novel results for efficient parameter inference of some models. We highlight the possible fruitful interactions between graphical models and phylogenetic methods. For example, approximate likelihood approaches have the potential to greatly reduce computational costs for phylogenies with reticulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09327&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Benjamin Teo, Paul Bastide, Cécile Ané</name></author><category term="stat.CO" /><summary type="html">The evolution of molecular and phenotypic traits is commonly modelled using Markov processes along a rooted phylogeny. This phylogeny can be a tree, or a network if it includes reticulations, representing events such as hybridization or admixture. Computing the likelihood of data observed at the leaves is costly as the size and complexity of the phylogeny grows. Efficient algorithms exist for trees, but cannot be applied to networks. We show that a vast array of models for trait evolution along phylogenetic networks can be reformulated as graphical models, for which efficient belief propagation algorithms exist. We provide a brief review of belief propagation on general graphical models, then focus on linear Gaussian models for continuous traits. We show how belief propagation techniques can be applied for exact or approximate (but more scalable) likelihood and gradient calculations, and prove novel results for efficient parameter inference of some models. We highlight the possible fruitful interactions between graphical models and phylogenetic methods. For example, approximate likelihood approaches have the potential to greatly reduce computational costs for phylogenies with reticulations.</summary></entry><entry><title type="html">Multi-Source Conformal Inference Under Distribution Shift</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/MultiSourceConformalInferenceUnderDistributionShift.html" rel="alternate" type="text/html" title="Multi-Source Conformal Inference Under Distribution Shift" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/MultiSourceConformalInferenceUnderDistributionShift</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/MultiSourceConformalInferenceUnderDistributionShift.html">&lt;p&gt;Recent years have experienced increasing utilization of complex machine learning models across multiple sources of data to inform more generalizable decision-making. However, distribution shifts across data sources and privacy concerns related to sharing individual-level data, coupled with a lack of uncertainty quantification from machine learning predictions, make it challenging to achieve valid inferences in multi-source environments. In this paper, we consider the problem of obtaining distribution-free prediction intervals for a target population, leveraging multiple potentially biased data sources. We derive the efficient influence functions for the quantiles of unobserved outcomes in the target and source populations, and show that one can incorporate machine learning prediction algorithms in the estimation of nuisance functions while still achieving parametric rates of convergence to nominal coverage probabilities. Moreover, when conditional outcome invariance is violated, we propose a data-adaptive strategy to upweight informative data sources for efficiency gain and downweight non-informative data sources for bias reduction. We highlight the robustness and efficiency of our proposals for a variety of conformal scores and data-generating mechanisms via extensive synthetic experiments. Hospital length of stay prediction intervals for pediatric patients undergoing a high-risk cardiac surgical procedure between 2016-2022 in the U.S. illustrate the utility of our methodology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09331&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yi Liu, Alexander W. Levis, Sharon-Lise Normand, Larry Han</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Recent years have experienced increasing utilization of complex machine learning models across multiple sources of data to inform more generalizable decision-making. However, distribution shifts across data sources and privacy concerns related to sharing individual-level data, coupled with a lack of uncertainty quantification from machine learning predictions, make it challenging to achieve valid inferences in multi-source environments. In this paper, we consider the problem of obtaining distribution-free prediction intervals for a target population, leveraging multiple potentially biased data sources. We derive the efficient influence functions for the quantiles of unobserved outcomes in the target and source populations, and show that one can incorporate machine learning prediction algorithms in the estimation of nuisance functions while still achieving parametric rates of convergence to nominal coverage probabilities. Moreover, when conditional outcome invariance is violated, we propose a data-adaptive strategy to upweight informative data sources for efficiency gain and downweight non-informative data sources for bias reduction. We highlight the robustness and efficiency of our proposals for a variety of conformal scores and data-generating mechanisms via extensive synthetic experiments. Hospital length of stay prediction intervals for pediatric patients undergoing a high-risk cardiac surgical procedure between 2016-2022 in the U.S. illustrate the utility of our methodology.</summary></entry><entry><title type="html">Nonparametric Inference on Dose-Response Curves Without the Positivity Condition</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/NonparametricInferenceonDoseResponseCurvesWithoutthePositivityCondition.html" rel="alternate" type="text/html" title="Nonparametric Inference on Dose-Response Curves Without the Positivity Condition" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/NonparametricInferenceonDoseResponseCurvesWithoutthePositivityCondition</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/NonparametricInferenceonDoseResponseCurvesWithoutthePositivityCondition.html">&lt;p&gt;Existing statistical methods in causal inference often rely on the assumption that every individual has some chance of receiving any treatment level regardless of its associated covariates, which is known as the positivity condition. This assumption could be violated in observational studies with continuous treatments. In this paper, we present a novel integral estimator of the causal effects with continuous treatments (i.e., dose-response curves) without requiring the positivity condition. Our approach involves estimating the derivative function of the treatment effect on each observed data sample and integrating it to the treatment level of interest so as to address the bias resulting from the lack of positivity condition. The validity of our approach relies on an alternative weaker assumption that can be satisfied by additive confounding models. We provide a fast and reliable numerical recipe for computing our estimator in practice and derive its related asymptotic theory. To conduct valid inference on the dose-response curve and its derivative, we propose using the nonparametric bootstrap and establish its consistency. The practical performances of our proposed estimators are validated through simulation studies and an analysis of the effect of air pollution exposure (PM$_{2.5}$) on cardiovascular mortality rates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09003&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yikun Zhang, Yen-Chi Chen, Alexander Giessing</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.TH" /><summary type="html">Existing statistical methods in causal inference often rely on the assumption that every individual has some chance of receiving any treatment level regardless of its associated covariates, which is known as the positivity condition. This assumption could be violated in observational studies with continuous treatments. In this paper, we present a novel integral estimator of the causal effects with continuous treatments (i.e., dose-response curves) without requiring the positivity condition. Our approach involves estimating the derivative function of the treatment effect on each observed data sample and integrating it to the treatment level of interest so as to address the bias resulting from the lack of positivity condition. The validity of our approach relies on an alternative weaker assumption that can be satisfied by additive confounding models. We provide a fast and reliable numerical recipe for computing our estimator in practice and derive its related asymptotic theory. To conduct valid inference on the dose-response curve and its derivative, we propose using the nonparametric bootstrap and establish its consistency. The practical performances of our proposed estimators are validated through simulation studies and an analysis of the effect of air pollution exposure (PM$_{2.5}$) on cardiovascular mortality rates.</summary></entry><entry><title type="html">On foundation of generative statistics with F-entropy: a gradient-based approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach.html" rel="alternate" type="text/html" title="On foundation of generative statistics with F-entropy: a gradient-based approach" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach.html">&lt;p&gt;This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can {\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g. missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisher’s divergence. (The F-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for generative model selection. Possible future projects include discrete data and Bayesian variational inference.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05389&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bing Cheng, Howell Tong</name></author><category term="stat.ME" /><summary type="html">This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can {\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g. missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisher’s divergence. (The F-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for generative model selection. Possible future projects include discrete data and Bayesian variational inference.</summary></entry><entry><title type="html">Powerful Partial Conjunction Hypothesis Testing via Conditioning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/PowerfulPartialConjunctionHypothesisTestingviaConditioning.html" rel="alternate" type="text/html" title="Powerful Partial Conjunction Hypothesis Testing via Conditioning" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/PowerfulPartialConjunctionHypothesisTestingviaConditioning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/PowerfulPartialConjunctionHypothesisTestingviaConditioning.html">&lt;p&gt;A Partial Conjunction Hypothesis (PCH) test combines information across a set of base hypotheses to determine whether some subset is non-null. PCH tests arise in a diverse array of fields, but standard PCH testing methods can be highly conservative, leading to low power especially in low signal settings commonly encountered in applications. In this paper, we introduce the conditional PCH (cPCH) test, a new method for testing a single PCH that directly corrects the conservativeness of standard approaches by conditioning on certain order statistics of the base p-values. Under distributional assumptions commonly encountered in PCH testing, the cPCH test is valid and produces nearly uniformly distributed p-values under the null (i.e., cPCH p-values are only very slightly conservative). We demonstrate that the cPCH test matches or outperforms existing single PCH tests with particular power gains in low signal settings, maintains Type I error control even under model misspecification, and can be used to outperform state-of-the-art multiple PCH testing procedures in certain settings, particularly when side information is present. Finally, we illustrate an application of the cPCH test through a replicability analysis across DNA microarray studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.11304&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Biyonka Liang, Lu Zhang, Lucas Janson</name></author><category term="stat.ME" /><summary type="html">A Partial Conjunction Hypothesis (PCH) test combines information across a set of base hypotheses to determine whether some subset is non-null. PCH tests arise in a diverse array of fields, but standard PCH testing methods can be highly conservative, leading to low power especially in low signal settings commonly encountered in applications. In this paper, we introduce the conditional PCH (cPCH) test, a new method for testing a single PCH that directly corrects the conservativeness of standard approaches by conditioning on certain order statistics of the base p-values. Under distributional assumptions commonly encountered in PCH testing, the cPCH test is valid and produces nearly uniformly distributed p-values under the null (i.e., cPCH p-values are only very slightly conservative). We demonstrate that the cPCH test matches or outperforms existing single PCH tests with particular power gains in low signal settings, maintains Type I error control even under model misspecification, and can be used to outperform state-of-the-art multiple PCH testing procedures in certain settings, particularly when side information is present. Finally, we illustrate an application of the cPCH test through a replicability analysis across DNA microarray studies.</summary></entry><entry><title type="html">Predicting Future Change-points in Time Series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/PredictingFutureChangepointsinTimeSeries.html" rel="alternate" type="text/html" title="Predicting Future Change-points in Time Series" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/PredictingFutureChangepointsinTimeSeries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/PredictingFutureChangepointsinTimeSeries.html">&lt;p&gt;Change-point detection and estimation procedures have been widely developed in the literature. However, commonly used approaches in change-point analysis have mainly been focusing on detecting change-points within an entire time series (off-line methods), or quickest detection of change-points in sequentially observed data (on-line methods). Both classes of methods are concerned with change-points that have already occurred. The arguably more important question of when future change-points may occur, remains largely unexplored. In this paper, we develop a novel statistical model that describes the mechanism of change-point occurrence. Specifically, the model assumes a latent process in the form of a random walk driven by non-negative innovations, and an observed process which behaves differently when the latent process belongs to different regimes. By construction, an occurrence of a change-point is equivalent to hitting a regime threshold by the latent process. Therefore, by predicting when the latent process will hit the next regime threshold, future change-points can be forecasted. The probabilistic properties of the model such as stationarity and ergodicity are established. A composite likelihood-based approach is developed for parameter estimation and model selection. Moreover, we construct the predictor and prediction interval for future change points based on the estimated model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09485&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chak Fung Choi, Chunxue Li, Chun Yip Yau, Zifeng Zhao</name></author><category term="stat.ME" /><summary type="html">Change-point detection and estimation procedures have been widely developed in the literature. However, commonly used approaches in change-point analysis have mainly been focusing on detecting change-points within an entire time series (off-line methods), or quickest detection of change-points in sequentially observed data (on-line methods). Both classes of methods are concerned with change-points that have already occurred. The arguably more important question of when future change-points may occur, remains largely unexplored. In this paper, we develop a novel statistical model that describes the mechanism of change-point occurrence. Specifically, the model assumes a latent process in the form of a random walk driven by non-negative innovations, and an observed process which behaves differently when the latent process belongs to different regimes. By construction, an occurrence of a change-point is equivalent to hitting a regime threshold by the latent process. Therefore, by predicting when the latent process will hit the next regime threshold, future change-points can be forecasted. The probabilistic properties of the model such as stationarity and ergodicity are established. A composite likelihood-based approach is developed for parameter estimation and model selection. Moreover, we construct the predictor and prediction interval for future change points based on the estimated model.</summary></entry><entry><title type="html">Rumor Forwarding Prediction Model Based on Uncertain Time Series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/RumorForwardingPredictionModelBasedonUncertainTimeSeries.html" rel="alternate" type="text/html" title="Rumor Forwarding Prediction Model Based on Uncertain Time Series" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/RumorForwardingPredictionModelBasedonUncertainTimeSeries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/RumorForwardingPredictionModelBasedonUncertainTimeSeries.html">&lt;p&gt;The rapid spread of rumors in social media is mainly caused by individual retweets. This paper applies uncertainty time series analysis (UTSA) to analyze a rumor retweeting behavior on Weibo. First, the rumor forwarding is modeled using uncertain time series, including order selection, parameter estimation, residual analysis, uncertainty hypothesis testing and forecast, and the validity of using uncertain time series analysis is further supported by analyzing the characteristics of the residual plot. The experimental results show that the uncertain time series can better predict the next stage of rumor forwarding. The results of the study have important practical significance for rumor management and the management of social media information dissemination.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.08493&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ruihong Wang, Fengming Liu</name></author><category term="stat.AP" /><summary type="html">The rapid spread of rumors in social media is mainly caused by individual retweets. This paper applies uncertainty time series analysis (UTSA) to analyze a rumor retweeting behavior on Weibo. First, the rumor forwarding is modeled using uncertain time series, including order selection, parameter estimation, residual analysis, uncertainty hypothesis testing and forecast, and the validity of using uncertain time series analysis is further supported by analyzing the characteristics of the residual plot. The experimental results show that the uncertain time series can better predict the next stage of rumor forwarding. The results of the study have important practical significance for rumor management and the management of social media information dissemination.</summary></entry><entry><title type="html">Semiparametrically Efficient Score for the Survival Odds Ratio</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/SemiparametricallyEfficientScorefortheSurvivalOddsRatio.html" rel="alternate" type="text/html" title="Semiparametrically Efficient Score for the Survival Odds Ratio" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/SemiparametricallyEfficientScorefortheSurvivalOddsRatio</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/SemiparametricallyEfficientScorefortheSurvivalOddsRatio.html">&lt;p&gt;We consider a general proportional odds model for survival data under binary treatment, where the functional form of the covariates is left unspecified. We derive the efficient score for the conditional survival odds ratio given the covariates using modern semiparametric theory. The efficient score may be useful in the development of doubly robust estimators, although computational challenges remain.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.14448&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Denise Rava, Jelena Bradic, Ronghui Xu</name></author><category term="stat.ME" /><summary type="html">We consider a general proportional odds model for survival data under binary treatment, where the functional form of the covariates is left unspecified. We derive the efficient score for the conditional survival odds ratio given the covariates using modern semiparametric theory. The efficient score may be useful in the development of doubly robust estimators, although computational challenges remain.</summary></entry><entry><title type="html">Spatio-temporal quasi-experimental methods for rare disease outcomes: The impact of reformulated gasoline on childhood hematologic cancer</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/SpatiotemporalquasiexperimentalmethodsforrarediseaseoutcomesTheimpactofreformulatedgasolineonchildhoodhematologiccancer.html" rel="alternate" type="text/html" title="Spatio-temporal quasi-experimental methods for rare disease outcomes: The impact of reformulated gasoline on childhood hematologic cancer" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/SpatiotemporalquasiexperimentalmethodsforrarediseaseoutcomesTheimpactofreformulatedgasolineonchildhoodhematologiccancer</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/SpatiotemporalquasiexperimentalmethodsforrarediseaseoutcomesTheimpactofreformulatedgasolineonchildhoodhematologiccancer.html">&lt;p&gt;Although some pollutants emitted in vehicle exhaust, such as benzene, are known to cause leukemia in adults with high exposure levels, less is known about the relationship between traffic-related air pollution (TRAP) and childhood hematologic cancer. In the 1990s, the US EPA enacted the reformulated gasoline program in select areas of the US, which drastically reduced ambient TRAP in affected areas. This created an ideal quasi-experiment to study the effects of TRAP on childhood hematologic cancers. However, existing methods for quasi-experimental analyses can perform poorly when outcomes are rare and unstable, as with childhood cancer incidence. We develop Bayesian spatio-temporal matrix completion methods to conduct causal inference in quasi-experimental settings with rare outcomes. Selective information sharing across space and time enables stable estimation, and the Bayesian approach facilitates uncertainty quantification. We evaluate the methods through simulations and apply them to estimate the causal effects of TRAP on childhood leukemia and lymphoma.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.09546&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sofia L. Vega, Rachel C. Nethery</name></author><category term="stat.AP" /><summary type="html">Although some pollutants emitted in vehicle exhaust, such as benzene, are known to cause leukemia in adults with high exposure levels, less is known about the relationship between traffic-related air pollution (TRAP) and childhood hematologic cancer. In the 1990s, the US EPA enacted the reformulated gasoline program in select areas of the US, which drastically reduced ambient TRAP in affected areas. This created an ideal quasi-experiment to study the effects of TRAP on childhood hematologic cancers. However, existing methods for quasi-experimental analyses can perform poorly when outcomes are rare and unstable, as with childhood cancer incidence. We develop Bayesian spatio-temporal matrix completion methods to conduct causal inference in quasi-experimental settings with rare outcomes. Selective information sharing across space and time enables stable estimation, and the Bayesian approach facilitates uncertainty quantification. We evaluate the methods through simulations and apply them to estimate the causal effects of TRAP on childhood leukemia and lymphoma.</summary></entry><entry><title type="html">Wasserstein Gradient Boosting: A General Framework with Applications to Posterior Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/WassersteinGradientBoostingAGeneralFrameworkwithApplicationstoPosteriorRegression.html" rel="alternate" type="text/html" title="Wasserstein Gradient Boosting: A General Framework with Applications to Posterior Regression" /><published>2024-05-16T00:00:00+00:00</published><updated>2024-05-16T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/WassersteinGradientBoostingAGeneralFrameworkwithApplicationstoPosteriorRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/16/WassersteinGradientBoostingAGeneralFrameworkwithApplicationstoPosteriorRegression.html">&lt;p&gt;Gradient boosting is a sequential ensemble method that fits a new base learner to the gradient of the remaining loss at each step. We propose a novel family of gradient boosting, Wasserstein gradient boosting, which fits a new base learner to an exactly or approximately available Wasserstein gradient of a loss functional on the space of probability distributions. Wasserstein gradient boosting returns a set of particles that approximates a target probability distribution assigned at each input. In probabilistic prediction, a parametric probability distribution is often specified on the space of output variables, and a point estimate of the output-distribution parameter is produced for each input by a model. Our main application of Wasserstein gradient boosting is a novel distributional estimate of the output-distribution parameter, which approximates the posterior distribution over the output-distribution parameter determined pointwise at each data point. We empirically demonstrate the superior performance of the probabilistic prediction by Wasserstein gradient boosting in comparison with various existing methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09536&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Takuo Matsubara</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Gradient boosting is a sequential ensemble method that fits a new base learner to the gradient of the remaining loss at each step. We propose a novel family of gradient boosting, Wasserstein gradient boosting, which fits a new base learner to an exactly or approximately available Wasserstein gradient of a loss functional on the space of probability distributions. Wasserstein gradient boosting returns a set of particles that approximates a target probability distribution assigned at each input. In probabilistic prediction, a parametric probability distribution is often specified on the space of output variables, and a point estimate of the output-distribution parameter is produced for each input by a model. Our main application of Wasserstein gradient boosting is a novel distributional estimate of the output-distribution parameter, which approximates the posterior distribution over the output-distribution parameter determined pointwise at each data point. We empirically demonstrate the superior performance of the probabilistic prediction by Wasserstein gradient boosting in comparison with various existing methods.</summary></entry></feed>