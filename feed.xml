<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-07T07:13:54+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Multiscale Perspective on Maximum Marginal Likelihood Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/AMultiscalePerspectiveonMaximumMarginalLikelihoodEstimation.html" rel="alternate" type="text/html" title="A Multiscale Perspective on Maximum Marginal Likelihood Estimation" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/AMultiscalePerspectiveonMaximumMarginalLikelihoodEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/AMultiscalePerspectiveonMaximumMarginalLikelihoodEstimation.html">&lt;p&gt;In this paper, we provide a multiscale perspective on the problem of maximum marginal likelihood estimation. We consider and analyse a diffusion-based maximum marginal likelihood estimation scheme using ideas from multiscale dynamics. Our perspective is based on stochastic averaging; we make an explicit connection between ideas in applied probability and parameter inference in computational statistics. In particular, we consider a general class of coupled Langevin diffusions for joint inference of latent variables and parameters in statistical models, where the latent variables are sampled from a fast Langevin process (which acts as a sampler), and the parameters are updated using a slow Langevin process (which acts as an optimiser). We show that the resulting system of stochastic differential equations (SDEs) can be viewed as a two-time scale system. To demonstrate the utility of such a perspective, we show that the averaged parameter dynamics obtained in the limit of scale separation can be used to estimate the optimal parameter, within the strongly convex setting. We do this by using recent uniform-in-time non-asymptotic averaging bounds. Finally, we conclude by showing that the slow-fast algorithm we consider here, termed Slow-Fast Langevin Algorithm, performs on par with state-of-the-art methods on a variety of examples. We believe that the stochastic averaging approach we provide in this paper enables us to look at these algorithms from a fresh angle, as well as unlocking the path to develop and analyse new methods using well-established averaging principles.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04187&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>O. Deniz Akyildiz, Iain Souttar, Michela Ottobre</name></author><category term="stat.CO" /><summary type="html">In this paper, we provide a multiscale perspective on the problem of maximum marginal likelihood estimation. We consider and analyse a diffusion-based maximum marginal likelihood estimation scheme using ideas from multiscale dynamics. Our perspective is based on stochastic averaging; we make an explicit connection between ideas in applied probability and parameter inference in computational statistics. In particular, we consider a general class of coupled Langevin diffusions for joint inference of latent variables and parameters in statistical models, where the latent variables are sampled from a fast Langevin process (which acts as a sampler), and the parameters are updated using a slow Langevin process (which acts as an optimiser). We show that the resulting system of stochastic differential equations (SDEs) can be viewed as a two-time scale system. To demonstrate the utility of such a perspective, we show that the averaged parameter dynamics obtained in the limit of scale separation can be used to estimate the optimal parameter, within the strongly convex setting. We do this by using recent uniform-in-time non-asymptotic averaging bounds. Finally, we conclude by showing that the slow-fast algorithm we consider here, termed Slow-Fast Langevin Algorithm, performs on par with state-of-the-art methods on a variety of examples. We believe that the stochastic averaging approach we provide in this paper enables us to look at these algorithms from a fresh angle, as well as unlocking the path to develop and analyse new methods using well-established averaging principles.</summary></entry><entry><title type="html">A Multivariate Equivalence Test Based on Mahalanobis Distance with a Data-Driven Margin</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/AMultivariateEquivalenceTestBasedonMahalanobisDistancewithaDataDrivenMargin.html" rel="alternate" type="text/html" title="A Multivariate Equivalence Test Based on Mahalanobis Distance with a Data-Driven Margin" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/AMultivariateEquivalenceTestBasedonMahalanobisDistancewithaDataDrivenMargin</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/AMultivariateEquivalenceTestBasedonMahalanobisDistancewithaDataDrivenMargin.html">&lt;p&gt;Multivariate equivalence testing is needed in a variety of scenarios for drug development. For example, drug products obtained from natural sources may contain many components for which the individual effects and/or their interactions on clinical efficacy and safety cannot be completely characterized. Such lack of sufficient characterization poses a challenge for both generic drug developers to demonstrate and regulatory authorities to determine the sameness of a proposed generic product to its reference product. Another case is to ensure batch-to-batch consistency of naturally derived products containing a vast number of components, such as botanical products. The equivalence or sameness between products containing many components that cannot be individually evaluated needs to be studied in a holistic manner. Multivariate equivalence test based on Mahalanobis distance may be suitable to evaluate many variables holistically. Existing studies based on such method assumed either a predetermined constant margin, for which a consensus is difficult to achieve, or a margin derived from the data, where, however, the randomness is ignored during the testing. In this study, we propose a multivariate equivalence test based on Mahalanobis distance with a data-drive margin with the randomness in the margin considered. Several possible implementations are compared with existing approaches via extensive simulation studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03596&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chao Wang, Yu-Ting Weng, Shaobo Liu, Tengfei Li, Meiyu Shen, Yi Tsong</name></author><category term="stat.ME" /><summary type="html">Multivariate equivalence testing is needed in a variety of scenarios for drug development. For example, drug products obtained from natural sources may contain many components for which the individual effects and/or their interactions on clinical efficacy and safety cannot be completely characterized. Such lack of sufficient characterization poses a challenge for both generic drug developers to demonstrate and regulatory authorities to determine the sameness of a proposed generic product to its reference product. Another case is to ensure batch-to-batch consistency of naturally derived products containing a vast number of components, such as botanical products. The equivalence or sameness between products containing many components that cannot be individually evaluated needs to be studied in a holistic manner. Multivariate equivalence test based on Mahalanobis distance may be suitable to evaluate many variables holistically. Existing studies based on such method assumed either a predetermined constant margin, for which a consensus is difficult to achieve, or a margin derived from the data, where, however, the randomness is ignored during the testing. In this study, we propose a multivariate equivalence test based on Mahalanobis distance with a data-drive margin with the randomness in the margin considered. Several possible implementations are compared with existing approaches via extensive simulation studies.</summary></entry><entry><title type="html">A Noise-robust Multi-head Attention Mechanism for Formation Resistivity Prediction: Frequency Aware LSTM</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/ANoiserobustMultiheadAttentionMechanismforFormationResistivityPredictionFrequencyAwareLSTM.html" rel="alternate" type="text/html" title="A Noise-robust Multi-head Attention Mechanism for Formation Resistivity Prediction: Frequency Aware LSTM" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/ANoiserobustMultiheadAttentionMechanismforFormationResistivityPredictionFrequencyAwareLSTM</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/ANoiserobustMultiheadAttentionMechanismforFormationResistivityPredictionFrequencyAwareLSTM.html">&lt;p&gt;The prediction of formation resistivity plays a crucial role in the evaluation of oil and gas reservoirs, identification and assessment of geothermal energy resources, groundwater detection and monitoring, and carbon capture and storage. However, traditional well logging techniques fail to measure accurate resistivity in cased boreholes, and the transient electromagnetic method for cased borehole resistivity logging encounters challenges of high-frequency disaster (the problem of inadequate learning by neural networks in high-frequency features) and noise interference, badly affecting accuracy. To address these challenges, frequency-aware framework and temporal anti-noise block are proposed to build frequency aware LSTM (FAL). The frequency-aware framework implements a dual-stream structure through wavelet transformation, allowing the neural network to simultaneously handle high-frequency and low-frequency flows of time-series data, thus avoiding high-frequency disaster. The temporal anti-noise block integrates multiple attention mechanisms and soft-threshold attention mechanisms, enabling the model to better distinguish noise from redundant features. Ablation experiments demonstrate that the frequency-aware framework and temporal anti-noise block contribute significantly to performance improvement. FAL achieves a 24.3% improvement in R2 over LSTM, reaching the highest value of 0.91 among all models. In robustness experiments, the impact of noise on FAL is approximately 1/8 of the baseline, confirming the noise resistance of FAL. The proposed FAL effectively reduces noise interference in predicting formation resistivity from cased transient electromagnetic well logging curves, better learns high-frequency features, and thereby enhances the prediction accuracy and noise resistance of the neural network model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03849&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yongan Zhang, Junfeng Zhao, Jian Li, Xuanran Wang, Youzhuang Sun, Yuntian Chen, Dongxiao Zhang</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">The prediction of formation resistivity plays a crucial role in the evaluation of oil and gas reservoirs, identification and assessment of geothermal energy resources, groundwater detection and monitoring, and carbon capture and storage. However, traditional well logging techniques fail to measure accurate resistivity in cased boreholes, and the transient electromagnetic method for cased borehole resistivity logging encounters challenges of high-frequency disaster (the problem of inadequate learning by neural networks in high-frequency features) and noise interference, badly affecting accuracy. To address these challenges, frequency-aware framework and temporal anti-noise block are proposed to build frequency aware LSTM (FAL). The frequency-aware framework implements a dual-stream structure through wavelet transformation, allowing the neural network to simultaneously handle high-frequency and low-frequency flows of time-series data, thus avoiding high-frequency disaster. The temporal anti-noise block integrates multiple attention mechanisms and soft-threshold attention mechanisms, enabling the model to better distinguish noise from redundant features. Ablation experiments demonstrate that the frequency-aware framework and temporal anti-noise block contribute significantly to performance improvement. FAL achieves a 24.3% improvement in R2 over LSTM, reaching the highest value of 0.91 among all models. In robustness experiments, the impact of noise on FAL is approximately 1/8 of the baseline, confirming the noise resistance of FAL. The proposed FAL effectively reduces noise interference in predicting formation resistivity from cased transient electromagnetic well logging curves, better learns high-frequency features, and thereby enhances the prediction accuracy and noise resistance of the neural network model.</summary></entry><entry><title type="html">A Practical Analysis Procedure on Generalizing Comparative Effectiveness in the Randomized Clinical Trial to the Real-world Trialeligible Population</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/APracticalAnalysisProcedureonGeneralizingComparativeEffectivenessintheRandomizedClinicalTrialtotheRealworldTrialeligiblePopulation.html" rel="alternate" type="text/html" title="A Practical Analysis Procedure on Generalizing Comparative Effectiveness in the Randomized Clinical Trial to the Real-world Trialeligible Population" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/APracticalAnalysisProcedureonGeneralizingComparativeEffectivenessintheRandomizedClinicalTrialtotheRealworldTrialeligiblePopulation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/APracticalAnalysisProcedureonGeneralizingComparativeEffectivenessintheRandomizedClinicalTrialtotheRealworldTrialeligiblePopulation.html">&lt;p&gt;When evaluating the effectiveness of a drug, a Randomized Controlled Trial (RCT) is often considered the gold standard due to its perfect randomization. While RCT assures strong internal validity, its restricted external validity poses challenges in extending treatment effects to the broader real-world population due to possible heterogeneity in covariates. In this paper, we introduce a procedure to generalize the RCT findings to the real-world trial-eligible population based on the adaption of existing statistical methods. We utilized the augmented inversed probability of sampling weighting (AIPSW) estimator for the estimation and omitted variable bias framework to assess the robustness of the estimate against the assumption violation caused by potentially unmeasured confounders. We analyzed an RCT comparing the effectiveness of lowering hypertension between Songling Xuemaikang Capsule (SXC), a traditional Chinese medicine (TCM), and Losartan as an illustration. The generalization results indicated that although SXC is less effective in lowering blood pressure than Losartan on week 2, week 4, and week 6, there is no statistically significant difference among the trial-eligible population at week 8, and the generalization is robust against potential unmeasured confounders.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04107&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kuan Jiang, Xin-xing Lai, Shu Yang, Ying Gao, Xiao-Hua Zhou</name></author><category term="stat.AP" /><summary type="html">When evaluating the effectiveness of a drug, a Randomized Controlled Trial (RCT) is often considered the gold standard due to its perfect randomization. While RCT assures strong internal validity, its restricted external validity poses challenges in extending treatment effects to the broader real-world population due to possible heterogeneity in covariates. In this paper, we introduce a procedure to generalize the RCT findings to the real-world trial-eligible population based on the adaption of existing statistical methods. We utilized the augmented inversed probability of sampling weighting (AIPSW) estimator for the estimation and omitted variable bias framework to assess the robustness of the estimate against the assumption violation caused by potentially unmeasured confounders. We analyzed an RCT comparing the effectiveness of lowering hypertension between Songling Xuemaikang Capsule (SXC), a traditional Chinese medicine (TCM), and Losartan as an illustration. The generalization results indicated that although SXC is less effective in lowering blood pressure than Losartan on week 2, week 4, and week 6, there is no statistically significant difference among the trial-eligible population at week 8, and the generalization is robust against potential unmeasured confounders.</summary></entry><entry><title type="html">A likelihood-based sensitivity analysis for addressing publication bias in meta-analysis of diagnostic studies using exact likelihood</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Alikelihoodbasedsensitivityanalysisforaddressingpublicationbiasinmetaanalysisofdiagnosticstudiesusingexactlikelihood.html" rel="alternate" type="text/html" title="A likelihood-based sensitivity analysis for addressing publication bias in meta-analysis of diagnostic studies using exact likelihood" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Alikelihoodbasedsensitivityanalysisforaddressingpublicationbiasinmetaanalysisofdiagnosticstudiesusingexactlikelihood</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Alikelihoodbasedsensitivityanalysisforaddressingpublicationbiasinmetaanalysisofdiagnosticstudiesusingexactlikelihood.html">&lt;p&gt;Publication bias (PB) poses a significant threat to meta-analysis, as studies yielding notable results are more likely to be published in scientific journals. Sensitivity analysis provides a flexible method to address PB and to examine the impact of unpublished studies. A selection model based on t-statistics to sensitivity analysis is proposed by Copas. This t-statistics selection model is interpretable and enables the modeling of biased publication sampling across studies, as indicated by the asymmetry in the funnel-plot. In meta-analysis of diagnostic studies, the summary receiver operating characteristic curve is an essential tool for synthesizing the bivariate outcomes of sensitivity and specificity reported by individual studies. Previous studies address PB upon the bivariate normal model but these methods rely on the normal approximation for the empirical logit-transformed sensitivity and specificity, which is not suitable for sparse data scenarios. Compared to the bivariate normal model, the bivariate binomial model which replaces the normal approximation in the within-study model with the exact within-study model has better finite sample properties. In this study, we applied the Copas t-statistics selection model to the meta-analysis of diagnostic studies using the bivariate binomial model. To our knowledge, this is the first study to apply the Copas t-statistics selection model to the bivariate binomial model. We have evaluated our proposed method through several real-world meta-analyses of diagnostic studies and simulation studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04095&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Taojun Hu, Yi Zhou, Xiao-Hua Zhou, Satoshi Hattori</name></author><category term="stat.AP" /><summary type="html">Publication bias (PB) poses a significant threat to meta-analysis, as studies yielding notable results are more likely to be published in scientific journals. Sensitivity analysis provides a flexible method to address PB and to examine the impact of unpublished studies. A selection model based on t-statistics to sensitivity analysis is proposed by Copas. This t-statistics selection model is interpretable and enables the modeling of biased publication sampling across studies, as indicated by the asymmetry in the funnel-plot. In meta-analysis of diagnostic studies, the summary receiver operating characteristic curve is an essential tool for synthesizing the bivariate outcomes of sensitivity and specificity reported by individual studies. Previous studies address PB upon the bivariate normal model but these methods rely on the normal approximation for the empirical logit-transformed sensitivity and specificity, which is not suitable for sparse data scenarios. Compared to the bivariate normal model, the bivariate binomial model which replaces the normal approximation in the within-study model with the exact within-study model has better finite sample properties. In this study, we applied the Copas t-statistics selection model to the meta-analysis of diagnostic studies using the bivariate binomial model. To our knowledge, this is the first study to apply the Copas t-statistics selection model to the bivariate binomial model. We have evaluated our proposed method through several real-world meta-analyses of diagnostic studies and simulation studies.</summary></entry><entry><title type="html">A novel robust meta-analysis model using the $t$ distribution for outlier accommodation and detection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Anovelrobustmetaanalysismodelusingthetdistributionforoutlieraccommodationanddetection.html" rel="alternate" type="text/html" title="A novel robust meta-analysis model using the $t$ distribution for outlier accommodation and detection" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Anovelrobustmetaanalysismodelusingthetdistributionforoutlieraccommodationanddetection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Anovelrobustmetaanalysismodelusingthetdistributionforoutlieraccommodationanddetection.html">&lt;p&gt;Random effects meta-analysis model is an important tool for integrating results from multiple independent studies. However, the standard model is based on the assumption of normal distributions for both random effects and within-study errors, making it susceptible to outlying studies. Although robust modeling using the $t$ distribution is an appealing idea, the existing work, that explores the use of the $t$ distribution only for random effects, involves complicated numerical integration and numerical optimization. In this paper, a novel robust meta-analysis model using the $t$ distribution is proposed ($t$Meta). The novelty is that the marginal distribution of the effect size in $t$Meta follows the $t$ distribution, enabling that $t$Meta can simultaneously accommodate and detect outlying studies in a simple and adaptive manner. A simple and fast EM-type algorithm is developed for maximum likelihood estimation. Due to the mathematical tractability of the $t$ distribution, $t$Meta frees from numerical integration and allows for efficient optimization. Experiments on real data demonstrate that $t$Meta is compared favorably with related competitors in situations involving mild outliers. Moreover, in the presence of gross outliers, while related competitors may fail, $t$Meta continues to perform consistently and robustly.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04150&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yue Wang, Jianhua Zhao, Fen Jiang, Lei Shi, Jianxin Pan</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Random effects meta-analysis model is an important tool for integrating results from multiple independent studies. However, the standard model is based on the assumption of normal distributions for both random effects and within-study errors, making it susceptible to outlying studies. Although robust modeling using the $t$ distribution is an appealing idea, the existing work, that explores the use of the $t$ distribution only for random effects, involves complicated numerical integration and numerical optimization. In this paper, a novel robust meta-analysis model using the $t$ distribution is proposed ($t$Meta). The novelty is that the marginal distribution of the effect size in $t$Meta follows the $t$ distribution, enabling that $t$Meta can simultaneously accommodate and detect outlying studies in a simple and adaptive manner. A simple and fast EM-type algorithm is developed for maximum likelihood estimation. Due to the mathematical tractability of the $t$ distribution, $t$Meta frees from numerical integration and allows for efficient optimization. Experiments on real data demonstrate that $t$Meta is compared favorably with related competitors in situations involving mild outliers. Moreover, in the presence of gross outliers, while related competitors may fail, $t$Meta continues to perform consistently and robustly.</summary></entry><entry><title type="html">Augmented balancing weights as linear regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Augmentedbalancingweightsaslinearregression.html" rel="alternate" type="text/html" title="Augmented balancing weights as linear regression" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Augmentedbalancingweightsaslinearregression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Augmentedbalancingweightsaslinearregression.html">&lt;p&gt;We provide a novel characterization of augmented balancing weights, also known as automatic debiased machine learning (AutoDML). These popular doubly robust or de-biased machine learning estimators combine outcome modeling with balancing weights - weights that achieve covariate balance directly in lieu of estimating and inverting the propensity score. When the outcome and weighting models are both linear in some (possibly infinite) basis, we show that the augmented estimator is equivalent to a single linear model with coefficients that combine the coefficients from the original outcome model and coefficients from an unpenalized ordinary least squares (OLS) fit on the same data. We see that, under certain choices of regularization parameters, the augmented estimator often collapses to the OLS estimator alone; this occurs for example in a re-analysis of the Lalonde 1986 dataset. We then extend these results to specific choices of outcome and weighting models. We first show that the augmented estimator that uses (kernel) ridge regression for both outcome and weighting models is equivalent to a single, undersmoothed (kernel) ridge regression. This holds numerically in finite samples and lays the groundwork for a novel analysis of undersmoothing and asymptotic rates of convergence. When the weighting model is instead lasso-penalized regression, we give closed-form expressions for special cases and demonstrate a ``double selection’’ property. Our framework opens the black box on this increasingly popular class of estimators, bridges the gap between existing results on the semiparametric efficiency of undersmoothed and doubly robust estimators, and provides new insights into the performance of augmented balancing weights.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.14545&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David Bruns-Smith, Oliver Dukes, Avi Feller, Elizabeth L. Ogburn</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We provide a novel characterization of augmented balancing weights, also known as automatic debiased machine learning (AutoDML). These popular doubly robust or de-biased machine learning estimators combine outcome modeling with balancing weights - weights that achieve covariate balance directly in lieu of estimating and inverting the propensity score. When the outcome and weighting models are both linear in some (possibly infinite) basis, we show that the augmented estimator is equivalent to a single linear model with coefficients that combine the coefficients from the original outcome model and coefficients from an unpenalized ordinary least squares (OLS) fit on the same data. We see that, under certain choices of regularization parameters, the augmented estimator often collapses to the OLS estimator alone; this occurs for example in a re-analysis of the Lalonde 1986 dataset. We then extend these results to specific choices of outcome and weighting models. We first show that the augmented estimator that uses (kernel) ridge regression for both outcome and weighting models is equivalent to a single, undersmoothed (kernel) ridge regression. This holds numerically in finite samples and lays the groundwork for a novel analysis of undersmoothing and asymptotic rates of convergence. When the weighting model is instead lasso-penalized regression, we give closed-form expressions for special cases and demonstrate a ``double selection’’ property. Our framework opens the black box on this increasingly popular class of estimators, bridges the gap between existing results on the semiparametric efficiency of undersmoothed and doubly robust estimators, and provides new insights into the performance of augmented balancing weights.</summary></entry><entry><title type="html">Bayesian generalized method of moments applied to pseudo-observations in survival analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Bayesiangeneralizedmethodofmomentsappliedtopseudoobservationsinsurvivalanalysis.html" rel="alternate" type="text/html" title="Bayesian generalized method of moments applied to pseudo-observations in survival analysis" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Bayesiangeneralizedmethodofmomentsappliedtopseudoobservationsinsurvivalanalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Bayesiangeneralizedmethodofmomentsappliedtopseudoobservationsinsurvivalanalysis.html">&lt;p&gt;Bayesian inference for survival regression modeling offers numerous advantages, especially for decision-making and external data borrowing, but demands the specification of the baseline hazard function, which may be a challenging task. We propose an alternative approach that does not need the specification of this function. Our approach combines pseudo-observations to convert censored data into longitudinal data with the Generalized Methods of Moments (GMM) to estimate the parameters of interest from the survival function directly. GMM may be viewed as an extension of the Generalized Estimating Equation (GEE) currently used for frequentist pseudo-observations analysis and can be extended to the Bayesian framework using a pseudo-likelihood function. We assessed the behavior of the frequentist and Bayesian GMM in the new context of analyzing pseudo-observations. We compared their performances to the Cox, GEE, and Bayesian piecewise exponential models through a simulation study of two-arm randomized clinical trials. Frequentist and Bayesian GMM gave valid inferences with similar performances compared to the three benchmark methods, except for small sample sizes and high censoring rates. For illustration, three post-hoc efficacy analyses were performed on randomized clinical trials involving patients with Ewing Sarcoma, producing results similar to those of the benchmark methods. Through a simple application of estimating hazard ratios, these findings confirm the effectiveness of this new Bayesian approach based on pseudo-observations and the generalized method of moments. This offers new insights on using pseudo-observations for Bayesian survival analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03821&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Léa Orsini , Caroline Brard , Emmanuel Lesaffre , Guosheng Yin , David Dejardin , Gwénaël Le Teuff</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Bayesian inference for survival regression modeling offers numerous advantages, especially for decision-making and external data borrowing, but demands the specification of the baseline hazard function, which may be a challenging task. We propose an alternative approach that does not need the specification of this function. Our approach combines pseudo-observations to convert censored data into longitudinal data with the Generalized Methods of Moments (GMM) to estimate the parameters of interest from the survival function directly. GMM may be viewed as an extension of the Generalized Estimating Equation (GEE) currently used for frequentist pseudo-observations analysis and can be extended to the Bayesian framework using a pseudo-likelihood function. We assessed the behavior of the frequentist and Bayesian GMM in the new context of analyzing pseudo-observations. We compared their performances to the Cox, GEE, and Bayesian piecewise exponential models through a simulation study of two-arm randomized clinical trials. Frequentist and Bayesian GMM gave valid inferences with similar performances compared to the three benchmark methods, except for small sample sizes and high censoring rates. For illustration, three post-hoc efficacy analyses were performed on randomized clinical trials involving patients with Ewing Sarcoma, producing results similar to those of the benchmark methods. Through a simple application of estimating hazard ratios, these findings confirm the effectiveness of this new Bayesian approach based on pseudo-observations and the generalized method of moments. This offers new insights on using pseudo-observations for Bayesian survival analysis.</summary></entry><entry><title type="html">Bayesian sequential design of computer experiments for quantile set inversion</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Bayesiansequentialdesignofcomputerexperimentsforquantilesetinversion.html" rel="alternate" type="text/html" title="Bayesian sequential design of computer experiments for quantile set inversion" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Bayesiansequentialdesignofcomputerexperimentsforquantilesetinversion</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Bayesiansequentialdesignofcomputerexperimentsforquantilesetinversion.html">&lt;p&gt;We consider an unknown multivariate function representing a system-such as a complex numerical simulator-taking both deterministic and uncertain inputs. Our objective is to estimate the set of deterministic inputs leading to outputs whose probability (with respect to the distribution of the uncertain inputs) of belonging to a given set is less than a given threshold. This problem, which we call Quantile Set Inversion (QSI), occurs for instance in the context of robust (reliability-based) optimization problems, when looking for the set of solutions that satisfy the constraints with sufficiently large probability.   To solve the QSI problem we propose a Bayesian strategy, based on Gaussian process modeling and the Stepwise Uncertainty Reduction (SUR) principle, to sequentially choose the points at which the function should be evaluated to efficiently approximate the set of interest. We illustrate the performance and interest of the proposed SUR strategy through several numerical experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.01008&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Romain Ait Abdelmalek-Lomenech , Julien Bect , Vincent Chabridon , Emmanuel Vazquez</name></author><category term="stat.ML," /><category term="stat.AP" /><summary type="html">We consider an unknown multivariate function representing a system-such as a complex numerical simulator-taking both deterministic and uncertain inputs. Our objective is to estimate the set of deterministic inputs leading to outputs whose probability (with respect to the distribution of the uncertain inputs) of belonging to a given set is less than a given threshold. This problem, which we call Quantile Set Inversion (QSI), occurs for instance in the context of robust (reliability-based) optimization problems, when looking for the set of solutions that satisfy the constraints with sufficiently large probability. To solve the QSI problem we propose a Bayesian strategy, based on Gaussian process modeling and the Stepwise Uncertainty Reduction (SUR) principle, to sequentially choose the points at which the function should be evaluated to efficiently approximate the set of interest. We illustrate the performance and interest of the proposed SUR strategy through several numerical experiments.</summary></entry><entry><title type="html">Comparing estimators of discriminative performance of time-to-event models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Comparingestimatorsofdiscriminativeperformanceoftimetoeventmodels.html" rel="alternate" type="text/html" title="Comparing estimators of discriminative performance of time-to-event models" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Comparingestimatorsofdiscriminativeperformanceoftimetoeventmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Comparingestimatorsofdiscriminativeperformanceoftimetoeventmodels.html">&lt;p&gt;Predicting the timing and occurrence of events is a major focus of data science applications, especially in the context of biomedical research. Performance for models estimating these outcomes, often referred to as time-to-event or survival outcomes, is frequently summarized using measures of discrimination, in particular time-dependent AUC and concordance. Many estimators for these quantities have been proposed which can be broadly categorized as either semi-parametric estimators or non-parametric estimators. In this paper, we review various estimators’ mathematical construction and compare the behavior of the two classes of estimators. Importantly, we identify a previously unknown feature of the class of semi-parametric estimators that can result in vastly over-optimistic out-of-sample estimation of discriminative performance in common applied tasks. Although these semi-parametric estimators are popular in practice, the phenomenon we identify here suggests this class of estimators may be inappropriate for use in model assessment and selection based on out-of-sample evaluation criteria. This is due to the semi-parametric estimators’ bias in favor of models that are overfit when using out-of-sample prediction criteria (e.g., cross validation). Non-parametric estimators, which do not exhibit this behavior, are highly variable for local discrimination. We propose to address the high variability problem through penalized regression splines smoothing. The behavior of various estimators of time-dependent AUC and concordance are illustrated via a simulation study using two different mechanisms that produce over-optimistic out-of-sample estimates using semi-parametric estimators. Estimators are further compared using a case study using data from the National Health and Nutrition Examination Survey (NHANES) 2011-2014.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04167&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ying Jin, Andrew Leroux</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Predicting the timing and occurrence of events is a major focus of data science applications, especially in the context of biomedical research. Performance for models estimating these outcomes, often referred to as time-to-event or survival outcomes, is frequently summarized using measures of discrimination, in particular time-dependent AUC and concordance. Many estimators for these quantities have been proposed which can be broadly categorized as either semi-parametric estimators or non-parametric estimators. In this paper, we review various estimators’ mathematical construction and compare the behavior of the two classes of estimators. Importantly, we identify a previously unknown feature of the class of semi-parametric estimators that can result in vastly over-optimistic out-of-sample estimation of discriminative performance in common applied tasks. Although these semi-parametric estimators are popular in practice, the phenomenon we identify here suggests this class of estimators may be inappropriate for use in model assessment and selection based on out-of-sample evaluation criteria. This is due to the semi-parametric estimators’ bias in favor of models that are overfit when using out-of-sample prediction criteria (e.g., cross validation). Non-parametric estimators, which do not exhibit this behavior, are highly variable for local discrimination. We propose to address the high variability problem through penalized regression splines smoothing. The behavior of various estimators of time-dependent AUC and concordance are illustrated via a simulation study using two different mechanisms that produce over-optimistic out-of-sample estimates using semi-parametric estimators. Estimators are further compared using a case study using data from the National Health and Nutrition Examination Survey (NHANES) 2011-2014.</summary></entry><entry><title type="html">Copula-based models for correlated circular data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Copulabasedmodelsforcorrelatedcirculardata.html" rel="alternate" type="text/html" title="Copula-based models for correlated circular data" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Copulabasedmodelsforcorrelatedcirculardata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Copulabasedmodelsforcorrelatedcirculardata.html">&lt;p&gt;We exploit Gaussian copulas to specify a class of multivariate circular distributions and obtain parametric models for the analysis of correlated circular data. This approach provides a straightforward extension of traditional multivariate normal models to the circular setting, without imposing restrictions on the marginal data distribution nor requiring overwhelming routines for parameter estimation. The proposal is illustrated on two case studies of animal orientation and sea currents, where we propose an autoregressive model for circular time series and a geostatistical model for circular spatial series.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04085&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Francesco Lagona, Marco Mingione</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We exploit Gaussian copulas to specify a class of multivariate circular distributions and obtain parametric models for the analysis of correlated circular data. This approach provides a straightforward extension of traditional multivariate normal models to the circular setting, without imposing restrictions on the marginal data distribution nor requiring overwhelming routines for parameter estimation. The proposal is illustrated on two case studies of animal orientation and sea currents, where we propose an autoregressive model for circular time series and a geostatistical model for circular spatial series.</summary></entry><entry><title type="html">Cross-variable Linear Integrated ENhanced Transformer for Photovoltaic power forecasting</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/CrossvariableLinearIntegratedENhancedTransformerforPhotovoltaicpowerforecasting.html" rel="alternate" type="text/html" title="Cross-variable Linear Integrated ENhanced Transformer for Photovoltaic power forecasting" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/CrossvariableLinearIntegratedENhancedTransformerforPhotovoltaicpowerforecasting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/CrossvariableLinearIntegratedENhancedTransformerforPhotovoltaicpowerforecasting.html">&lt;p&gt;Photovoltaic (PV) power forecasting plays a crucial role in optimizing the operation and planning of PV systems, thereby enabling efficient energy management and grid integration. However, un certainties caused by fluctuating weather conditions and complex interactions between different variables pose significant challenges to accurate PV power forecasting. In this study, we propose PV-Client (Cross-variable Linear Integrated ENhanced Transformer for Photovoltaic power forecasting) to address these challenges and enhance PV power forecasting accuracy. PV-Client employs an ENhanced Transformer module to capture complex interactions of various features in PV systems, and utilizes a linear module to learn trend information in PV power. Diverging from conventional time series-based Transformer models that use cross-time Attention to learn dependencies between different time steps, the Enhanced Transformer module integrates cross-variable Attention to capture dependencies between PV power and weather factors. Furthermore, PV-Client streamlines the embedding and position encoding layers by replacing the Decoder module with a projection layer. Experimental results on three real-world PV power datasets affirm PV-Client’s state-of-the-art (SOTA) performance in PV power forecasting. Specifically, PV-Client surpasses the second-best model GRU by 5.3% in MSE metrics and 0.9% in accuracy metrics at the Jingang Station. Similarly, PV-Client outperforms the second-best model SVR by 10.1% in MSE metrics and 0.2% in accuracy metrics at the Xinqingnian Station, and PV-Client exhibits superior performance compared to the second-best model SVR with enhancements of 3.4% in MSE metrics and 0.9% in accuracy metrics at the Hongxing Station.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03808&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiaxin Gao, Qinglong Cao, Yuntian Chen, Dongxiao Zhang</name></author><category term="stat.AP" /><summary type="html">Photovoltaic (PV) power forecasting plays a crucial role in optimizing the operation and planning of PV systems, thereby enabling efficient energy management and grid integration. However, un certainties caused by fluctuating weather conditions and complex interactions between different variables pose significant challenges to accurate PV power forecasting. In this study, we propose PV-Client (Cross-variable Linear Integrated ENhanced Transformer for Photovoltaic power forecasting) to address these challenges and enhance PV power forecasting accuracy. PV-Client employs an ENhanced Transformer module to capture complex interactions of various features in PV systems, and utilizes a linear module to learn trend information in PV power. Diverging from conventional time series-based Transformer models that use cross-time Attention to learn dependencies between different time steps, the Enhanced Transformer module integrates cross-variable Attention to capture dependencies between PV power and weather factors. Furthermore, PV-Client streamlines the embedding and position encoding layers by replacing the Decoder module with a projection layer. Experimental results on three real-world PV power datasets affirm PV-Client’s state-of-the-art (SOTA) performance in PV power forecasting. Specifically, PV-Client surpasses the second-best model GRU by 5.3% in MSE metrics and 0.9% in accuracy metrics at the Jingang Station. Similarly, PV-Client outperforms the second-best model SVR by 10.1% in MSE metrics and 0.2% in accuracy metrics at the Xinqingnian Station, and PV-Client exhibits superior performance compared to the second-best model SVR with enhancements of 3.4% in MSE metrics and 0.9% in accuracy metrics at the Hongxing Station.</summary></entry><entry><title type="html">Design-Based Causal Inference with Missing Outcomes: Missingness Mechanisms, Imputation-Assisted Randomization Tests, and Covariate Adjustment</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/DesignBasedCausalInferencewithMissingOutcomesMissingnessMechanismsImputationAssistedRandomizationTestsandCovariateAdjustment.html" rel="alternate" type="text/html" title="Design-Based Causal Inference with Missing Outcomes: Missingness Mechanisms, Imputation-Assisted Randomization Tests, and Covariate Adjustment" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/DesignBasedCausalInferencewithMissingOutcomesMissingnessMechanismsImputationAssistedRandomizationTestsandCovariateAdjustment</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/DesignBasedCausalInferencewithMissingOutcomesMissingnessMechanismsImputationAssistedRandomizationTestsandCovariateAdjustment.html">&lt;p&gt;Design-based causal inference, also known as randomization-based or finite-population causal inference, is one of the most widely used causal inference frameworks, largely due to the merit that its validity can be guaranteed by study design (e.g., randomized experiments) and does not require assuming specific outcome-generating distributions or super-population models. Despite its advantages, design-based causal inference can still suffer from other data-related issues, among which outcome missingness is a prevalent and significant challenge. This work systematically studies the outcome missingness problem in design-based causal inference. First, we propose a general and flexible outcome missingness mechanism that can facilitate finite-population-exact randomization tests for the null effect. Second, under this general missingness mechanism, we propose a general framework called ``imputation and re-imputation” for conducting finite-population-exact randomization tests in design-based causal inference with missing outcomes. This framework can incorporate any imputation algorithms (from linear models to advanced machine learning-based imputation algorithms) while ensuring finite-population-exact type-I error rate control. Third, we extend our framework to conduct covariate adjustment in randomization tests and construct finite-population-valid confidence regions with missing outcomes. Our framework is evaluated via extensive simulation studies and applied to a large-scale randomized experiment. Corresponding Python and R packages are also developed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.18556&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Siyu Heng, Jiawei Zhang, Yang Feng</name></author><category term="stat.ME" /><summary type="html">Design-based causal inference, also known as randomization-based or finite-population causal inference, is one of the most widely used causal inference frameworks, largely due to the merit that its validity can be guaranteed by study design (e.g., randomized experiments) and does not require assuming specific outcome-generating distributions or super-population models. Despite its advantages, design-based causal inference can still suffer from other data-related issues, among which outcome missingness is a prevalent and significant challenge. This work systematically studies the outcome missingness problem in design-based causal inference. First, we propose a general and flexible outcome missingness mechanism that can facilitate finite-population-exact randomization tests for the null effect. Second, under this general missingness mechanism, we propose a general framework called ``imputation and re-imputation” for conducting finite-population-exact randomization tests in design-based causal inference with missing outcomes. This framework can incorporate any imputation algorithms (from linear models to advanced machine learning-based imputation algorithms) while ensuring finite-population-exact type-I error rate control. Third, we extend our framework to conduct covariate adjustment in randomization tests and construct finite-population-valid confidence regions with missing outcomes. Our framework is evaluated via extensive simulation studies and applied to a large-scale randomized experiment. Corresponding Python and R packages are also developed.</summary></entry><entry><title type="html">DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/DoWhyGCMAnextensionofDoWhyforcausalinferenceingraphicalcausalmodels.html" rel="alternate" type="text/html" title="DoWhy-GCM: An extension of DoWhy for causal inference in graphical causal models" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/DoWhyGCMAnextensionofDoWhyforcausalinferenceingraphicalcausalmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/DoWhyGCMAnextensionofDoWhyforcausalinferenceingraphicalcausalmodels.html">&lt;p&gt;We present DoWhy-GCM, an extension of the DoWhy Python library, which leverages graphical causal models. Unlike existing causality libraries, which mainly focus on effect estimation, DoWhy-GCM addresses diverse causal queries, such as identifying the root causes of outliers and distributional changes, attributing causal influences to the data generating process of each node, or diagnosis of causal structures. With DoWhy-GCM, users typically specify cause-effect relations via a causal graph, fit causal mechanisms, and pose causal queries – all with just a few lines of code. The general documentation is available at https://www.pywhy.org/dowhy and the DoWhy-GCM specific code at https://github.com/py-why/dowhy/tree/main/dowhy/gcm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2206.06821&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Patrick Blöbaum, Peter Götz, Kailash Budhathoki, Atalanti A. Mastakouri, Dominik Janzing</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We present DoWhy-GCM, an extension of the DoWhy Python library, which leverages graphical causal models. Unlike existing causality libraries, which mainly focus on effect estimation, DoWhy-GCM addresses diverse causal queries, such as identifying the root causes of outliers and distributional changes, attributing causal influences to the data generating process of each node, or diagnosis of causal structures. With DoWhy-GCM, users typically specify cause-effect relations via a causal graph, fit causal mechanisms, and pose causal queries – all with just a few lines of code. The general documentation is available at https://www.pywhy.org/dowhy and the DoWhy-GCM specific code at https://github.com/py-why/dowhy/tree/main/dowhy/gcm.</summary></entry><entry><title type="html">Enhanced variable selection for boosting sparser and less complex models in distributional copula regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Enhancedvariableselectionforboostingsparserandlesscomplexmodelsindistributionalcopularegression.html" rel="alternate" type="text/html" title="Enhanced variable selection for boosting sparser and less complex models in distributional copula regression" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Enhancedvariableselectionforboostingsparserandlesscomplexmodelsindistributionalcopularegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Enhancedvariableselectionforboostingsparserandlesscomplexmodelsindistributionalcopularegression.html">&lt;p&gt;Structured additive distributional copula regression allows to model the joint distribution of multivariate outcomes by relating all distribution parameters to covariates. Estimation via statistical boosting enables accounting for high-dimensional data and incorporating data-driven variable selection, both of which are useful given the complexity of the model class. However, as known from univariate (distributional) regression, the standard boosting algorithm tends to select too many variables with minor importance, particularly in settings with large sample sizes, leading to complex models with difficult interpretation. To counteract this behavior and to avoid selecting base-learners with only a negligible impact, we combined the ideas of probing, stability selection and a new deselection approach with statistical boosting for distributional copula regression. In a simulation study and an application to the joint modelling of weight and length of newborns, we found that all proposed methods enhance variable selection by reducing the number of false positives. However, only stability selection and the deselection approach yielded similar predictive performance to classical boosting. Finally, the deselection approach is better scalable to larger datasets and led to a competitive predictive performance, which we further illustrated in a genomic cohort study from the UK Biobank by modelling the joint genetic predisposition for two phenotypes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03900&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Annika Strömer, Nadja Klein, Christian Staerk, Florian Faschingbauer, Hannah Klinkhammer, Andreas Mayr</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Structured additive distributional copula regression allows to model the joint distribution of multivariate outcomes by relating all distribution parameters to covariates. Estimation via statistical boosting enables accounting for high-dimensional data and incorporating data-driven variable selection, both of which are useful given the complexity of the model class. However, as known from univariate (distributional) regression, the standard boosting algorithm tends to select too many variables with minor importance, particularly in settings with large sample sizes, leading to complex models with difficult interpretation. To counteract this behavior and to avoid selecting base-learners with only a negligible impact, we combined the ideas of probing, stability selection and a new deselection approach with statistical boosting for distributional copula regression. In a simulation study and an application to the joint modelling of weight and length of newborns, we found that all proposed methods enhance variable selection by reducing the number of false positives. However, only stability selection and the deselection approach yielded similar predictive performance to classical boosting. Finally, the deselection approach is better scalable to larger datasets and led to a competitive predictive performance, which we further illustrated in a genomic cohort study from the UK Biobank by modelling the joint genetic predisposition for two phenotypes.</summary></entry><entry><title type="html">Flexible Clustering with a Sparse Mixture of Generalized Hyperbolic Distributions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/FlexibleClusteringwithaSparseMixtureofGeneralizedHyperbolicDistributions.html" rel="alternate" type="text/html" title="Flexible Clustering with a Sparse Mixture of Generalized Hyperbolic Distributions" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/FlexibleClusteringwithaSparseMixtureofGeneralizedHyperbolicDistributions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/FlexibleClusteringwithaSparseMixtureofGeneralizedHyperbolicDistributions.html">&lt;p&gt;Robust clustering of high-dimensional data is an important topic because clusters in real datasets are often heavy-tailed and/or asymmetric. Traditional approaches to model-based clustering often fail for high dimensional data, e.g., due to the number of free covariance parameters. A parametrization of the component scale matrices for the mixture of generalized hyperbolic distributions is proposed. This parameterization includes a penalty term in the likelihood. An analytically feasible expectation-maximization algorithm is developed by placing a gamma-lasso penalty constraining the concentration matrix. The proposed methodology is investigated through simulation studies and illustrated using two real datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1903.05054&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexa A. Sochaniwsky, Michael P. B. Gallaugher, Yang Tang, Paul D. McNicholas</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Robust clustering of high-dimensional data is an important topic because clusters in real datasets are often heavy-tailed and/or asymmetric. Traditional approaches to model-based clustering often fail for high dimensional data, e.g., due to the number of free covariance parameters. A parametrization of the component scale matrices for the mixture of generalized hyperbolic distributions is proposed. This parameterization includes a penalty term in the likelihood. An analytically feasible expectation-maximization algorithm is developed by placing a gamma-lasso penalty constraining the concentration matrix. The proposed methodology is investigated through simulation studies and illustrated using two real datasets.</summary></entry><entry><title type="html">Gradient Boosting for Hierarchical Data in Small Area Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/GradientBoostingforHierarchicalDatainSmallAreaEstimation.html" rel="alternate" type="text/html" title="Gradient Boosting for Hierarchical Data in Small Area Estimation" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/GradientBoostingforHierarchicalDatainSmallAreaEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/GradientBoostingforHierarchicalDatainSmallAreaEstimation.html">&lt;p&gt;This paper introduces Mixed Effect Gradient Boosting (MEGB), which combines the strengths of Gradient Boosting with Mixed Effects models to address complex, hierarchical data structures often encountered in statistical analysis. The methodological foundations, including a review of the Mixed Effects model and the Extreme Gradient Boosting method, leading to the introduction of MEGB are shown in detail. It highlights how MEGB can derive area-level mean estimations from unit-level data and calculate Mean Squared Error (MSE) estimates using a nonparametric bootstrap approach. The paper evaluates MEGB’s performance through model-based and design-based simulation studies, comparing it against established estimators. The findings indicate that MEGB provides promising area mean estimations and may outperform existing small area estimators in various scenarios. The paper concludes with a discussion on future research directions, highlighting the possibility of extending MEGB’s framework to accommodate different types of outcome variables or non-linear area level indicators.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04256&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Paul Messer, Timo Schmid</name></author><category term="stat.ME" /><summary type="html">This paper introduces Mixed Effect Gradient Boosting (MEGB), which combines the strengths of Gradient Boosting with Mixed Effects models to address complex, hierarchical data structures often encountered in statistical analysis. The methodological foundations, including a review of the Mixed Effects model and the Extreme Gradient Boosting method, leading to the introduction of MEGB are shown in detail. It highlights how MEGB can derive area-level mean estimations from unit-level data and calculate Mean Squared Error (MSE) estimates using a nonparametric bootstrap approach. The paper evaluates MEGB’s performance through model-based and design-based simulation studies, comparing it against established estimators. The findings indicate that MEGB provides promising area mean estimations and may outperform existing small area estimators in various scenarios. The paper concludes with a discussion on future research directions, highlighting the possibility of extending MEGB’s framework to accommodate different types of outcome variables or non-linear area level indicators.</summary></entry><entry><title type="html">Impact of aleatoric, stochastic and epistemic uncertainties on project cost contingency reserves</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Impactofaleatoricstochasticandepistemicuncertaintiesonprojectcostcontingencyreserves.html" rel="alternate" type="text/html" title="Impact of aleatoric, stochastic and epistemic uncertainties on project cost contingency reserves" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Impactofaleatoricstochasticandepistemicuncertaintiesonprojectcostcontingencyreserves</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Impactofaleatoricstochasticandepistemicuncertaintiesonprojectcostcontingencyreserves.html">&lt;p&gt;In construction projects, contingency reserves have traditionally been estimated based on a percentage of the total project cost, which is arbitrary and, thus, unreliable in practical cases. Monte Carlo simulation provides a more reliable estimation. However, works on this topic have focused exclusively on the effects of aleatoric uncertainty, but ignored the impacts of other uncertainty types. In this paper, we present a method to quantitatively determine project cost contingency reserves based on Monte Carlo Simulation that considers the impact of not only aleatoric uncertainty, but also of the effects of other uncertainty kinds (stochastic, epistemic) on the total project cost. The proposed method has been validated with a real-case construction project in Spain. The obtained results demonstrate that the approach will be helpful for construction Project Managers because the obtained cost contingency reserves are consistent with the actual uncertainty type that affects the risks identified in their projects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03500&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David Curto, Fernando Acebes, Jose M Gonzalez-Varona, David Poza</name></author><category term="stat.AP" /><summary type="html">In construction projects, contingency reserves have traditionally been estimated based on a percentage of the total project cost, which is arbitrary and, thus, unreliable in practical cases. Monte Carlo simulation provides a more reliable estimation. However, works on this topic have focused exclusively on the effects of aleatoric uncertainty, but ignored the impacts of other uncertainty types. In this paper, we present a method to quantitatively determine project cost contingency reserves based on Monte Carlo Simulation that considers the impact of not only aleatoric uncertainty, but also of the effects of other uncertainty kinds (stochastic, epistemic) on the total project cost. The proposed method has been validated with a real-case construction project in Spain. The obtained results demonstrate that the approach will be helpful for construction Project Managers because the obtained cost contingency reserves are consistent with the actual uncertainty type that affects the risks identified in their projects.</summary></entry><entry><title type="html">Improving efficiency in transporting average treatment effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Improvingefficiencyintransportingaveragetreatmenteffects.html" rel="alternate" type="text/html" title="Improving efficiency in transporting average treatment effects" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Improvingefficiencyintransportingaveragetreatmenteffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Improvingefficiencyintransportingaveragetreatmenteffects.html">&lt;p&gt;We develop flexible, semiparametric estimators of the average treatment effect (ATE) transported to a new population (“target population”) that offer potential efficiency gains. Transport may be of value when the ATE may differ across populations. We consider the setting where differences in the ATE are due to differences in the distribution of baseline covariates that modify the treatment effect (“effect modifiers”). First, we propose a collaborative one-step semiparametric estimator that can improve efficiency. This approach does not require researchers to have knowledge about which covariates are effect modifiers and which differ in distribution between the populations, but does require all covariates to be measured in the target population. Second, we propose two one-step semiparametric estimators that assume knowledge of which covariates are effect modifiers and which are both effect modifiers and differentially distributed between the populations. These estimators can be used even when not all covariates are observed in the target population; one requires that only effect modifiers are observed, and the other requires that only those modifiers that are also differentially distributed are observed. We use simulation to compare finite sample performance across our proposed estimators and an existing semiparametric estimator of the transported ATE, including in the presence of practical violations of the positivity assumption. Lastly, we apply our proposed estimators to a large-scale housing trial.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.00117&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kara E. Rudolph, Nicholas T. Williams, Elizabeth A. Stuart, Ivan Diaz</name></author><category term="stat.ME" /><summary type="html">We develop flexible, semiparametric estimators of the average treatment effect (ATE) transported to a new population (“target population”) that offer potential efficiency gains. Transport may be of value when the ATE may differ across populations. We consider the setting where differences in the ATE are due to differences in the distribution of baseline covariates that modify the treatment effect (“effect modifiers”). First, we propose a collaborative one-step semiparametric estimator that can improve efficiency. This approach does not require researchers to have knowledge about which covariates are effect modifiers and which differ in distribution between the populations, but does require all covariates to be measured in the target population. Second, we propose two one-step semiparametric estimators that assume knowledge of which covariates are effect modifiers and which are both effect modifiers and differentially distributed between the populations. These estimators can be used even when not all covariates are observed in the target population; one requires that only effect modifiers are observed, and the other requires that only those modifiers that are also differentially distributed are observed. We use simulation to compare finite sample performance across our proposed estimators and an existing semiparametric estimator of the transported ATE, including in the presence of practical violations of the positivity assumption. Lastly, we apply our proposed estimators to a large-scale housing trial.</summary></entry><entry><title type="html">Multiscale Tests for Point Processes and Longitudinal Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/MultiscaleTestsforPointProcessesandLongitudinalNetworks.html" rel="alternate" type="text/html" title="Multiscale Tests for Point Processes and Longitudinal Networks" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/MultiscaleTestsforPointProcessesandLongitudinalNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/MultiscaleTestsforPointProcessesandLongitudinalNetworks.html">&lt;p&gt;We propose a new testing framework applicable to both the two-sample problem on point processes and the community detection problem on rectangular arrays of point processes, which we refer to as longitudinal networks; the latter problem is useful in situations where we observe interactions among a group of individuals over time. Our framework is based on a multiscale discretization scheme that consider not just the global null but also a collection of nulls local to small regions in the domain; in the two-sample problem, the local rejections tell us where the intensity functions differ and in the longitudinal network problem, the local rejections tell us when the community structure is most salient. We provide theoretical analysis for the two-sample problem and show that our method has minimax optimal power under a Holder continuity condition. We provide extensive simulation and real data analysis demonstrating the practicality of our proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03681&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Youmeng Jiang, Min Xu</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We propose a new testing framework applicable to both the two-sample problem on point processes and the community detection problem on rectangular arrays of point processes, which we refer to as longitudinal networks; the latter problem is useful in situations where we observe interactions among a group of individuals over time. Our framework is based on a multiscale discretization scheme that consider not just the global null but also a collection of nulls local to small regions in the domain; in the two-sample problem, the local rejections tell us where the intensity functions differ and in the longitudinal network problem, the local rejections tell us when the community structure is most salient. We provide theoretical analysis for the two-sample problem and show that our method has minimax optimal power under a Holder continuity condition. We provide extensive simulation and real data analysis demonstrating the practicality of our proposed method.</summary></entry><entry><title type="html">NFT Wash Trading: Direct vs. Indirect Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/NFTWashTradingDirectvsIndirectEstimation.html" rel="alternate" type="text/html" title="NFT Wash Trading: Direct vs. Indirect Estimation" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/NFTWashTradingDirectvsIndirectEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/NFTWashTradingDirectvsIndirectEstimation.html">&lt;p&gt;Recent studies estimate around 70% of traded value on off-chain crypto exchanges like Binance is wash trading. This paper turns to NFT markets, where the on-chain nature of transactions-a key tenet of Web3 innovation-enables more direct estimation methods to be applied. Focusing on three of the largest NFT marketplaces, we find 30-40% of NFT volume and 25-95% of traded value involve wash trading. We leverage this direct approach to critically evaluate recent indirect estimation methods suggested in the literature, revealing major differences in effectiveness, with some failing altogether. Trade-roundedness filters, as suggested in Cong et al. (2023), emerge as the most accurate indirect estimation method. In fact, we show how direct and indirect approaches can be closely aligned via hyper-parameter fine-tuning. Our findings underscore the crucial role of technological innovation in detecting and regulating financial misconduct in digital finance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.18717&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Brett Hemenway Falk, Gerry Tsoukalas, Niuniu Zhang</name></author><category term="stat.AP" /><summary type="html">Recent studies estimate around 70% of traded value on off-chain crypto exchanges like Binance is wash trading. This paper turns to NFT markets, where the on-chain nature of transactions-a key tenet of Web3 innovation-enables more direct estimation methods to be applied. Focusing on three of the largest NFT marketplaces, we find 30-40% of NFT volume and 25-95% of traded value involve wash trading. We leverage this direct approach to critically evaluate recent indirect estimation methods suggested in the literature, revealing major differences in effectiveness, with some failing altogether. Trade-roundedness filters, as suggested in Cong et al. (2023), emerge as the most accurate indirect estimation method. In fact, we show how direct and indirect approaches can be closely aligned via hyper-parameter fine-tuning. Our findings underscore the crucial role of technological innovation in detecting and regulating financial misconduct in digital finance.</summary></entry><entry><title type="html">Optimizing Language Models for Human Preferences is a Causal Inference Problem</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/OptimizingLanguageModelsforHumanPreferencesisaCausalInferenceProblem.html" rel="alternate" type="text/html" title="Optimizing Language Models for Human Preferences is a Causal Inference Problem" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/OptimizingLanguageModelsforHumanPreferencesisaCausalInferenceProblem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/OptimizingLanguageModelsforHumanPreferencesisaCausalInferenceProblem.html">&lt;p&gt;As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader’s response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method–causal preference optimization (CPO)–that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarantees on bias. Finally, we empirically demonstrate the effectiveness of (DR-)CPO in optimizing state-of-the-art LLMs for human preferences on direct outcome data, and we validate the robustness of DR-CPO under difficult confounding conditions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.14979&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Victoria Lin, Eli Ben-Michael, Louis-Philippe Morency</name></author><category term="stat.ME" /><summary type="html">As large language models (LLMs) see greater use in academic and commercial settings, there is increasing interest in methods that allow language models to generate texts aligned with human preferences. In this paper, we present an initial exploration of language model optimization for human preferences from direct outcome datasets, where each sample consists of a text and an associated numerical outcome measuring the reader’s response. We first propose that language model optimization should be viewed as a causal problem to ensure that the model correctly learns the relationship between the text and the outcome. We formalize this causal language optimization problem, and we develop a method–causal preference optimization (CPO)–that solves an unbiased surrogate objective for the problem. We further extend CPO with doubly robust CPO (DR-CPO), which reduces the variance of the surrogate objective while retaining provably strong guarantees on bias. Finally, we empirically demonstrate the effectiveness of (DR-)CPO in optimizing state-of-the-art LLMs for human preferences on direct outcome data, and we validate the robustness of DR-CPO under difficult confounding conditions.</summary></entry><entry><title type="html">Small area estimation with generalized random forests: Estimating poverty rates in Mexico</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/SmallareaestimationwithgeneralizedrandomforestsEstimatingpovertyratesinMexico.html" rel="alternate" type="text/html" title="Small area estimation with generalized random forests: Estimating poverty rates in Mexico" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/SmallareaestimationwithgeneralizedrandomforestsEstimatingpovertyratesinMexico</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/SmallareaestimationwithgeneralizedrandomforestsEstimatingpovertyratesinMexico.html">&lt;p&gt;Identifying and addressing poverty is challenging in administrative units with limited information on income distribution and well-being. To overcome this obstacle, small area estimation methods have been developed to provide reliable and efficient estimators at disaggregated levels, enabling informed decision-making by policymakers despite the data scarcity. From a theoretical perspective, we propose a robust and flexible approach for estimating poverty indicators based on binary response variables within the small area estimation context: the generalized mixed effects random forest. Our method employs machine learning techniques to identify predictive, non-linear relationships from data, while also modeling hierarchical structures. Mean squared error estimation is explored using a parametric bootstrap. From an applied perspective, we examine the impact of information loss due to converting continuous variables into binary variables on the performance of small area estimation methods. We evaluate the proposed point and uncertainty estimates in both model- and design-based simulations. Finally, we apply our method to a case study revealing spatial patterns of poverty in the Mexican state of Tlaxcala.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03861&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nicolas Frink, Timo Schmid</name></author><category term="stat.ME" /><summary type="html">Identifying and addressing poverty is challenging in administrative units with limited information on income distribution and well-being. To overcome this obstacle, small area estimation methods have been developed to provide reliable and efficient estimators at disaggregated levels, enabling informed decision-making by policymakers despite the data scarcity. From a theoretical perspective, we propose a robust and flexible approach for estimating poverty indicators based on binary response variables within the small area estimation context: the generalized mixed effects random forest. Our method employs machine learning techniques to identify predictive, non-linear relationships from data, while also modeling hierarchical structures. Mean squared error estimation is explored using a parametric bootstrap. From an applied perspective, we examine the impact of information loss due to converting continuous variables into binary variables on the performance of small area estimation methods. We evaluate the proposed point and uncertainty estimates in both model- and design-based simulations. Finally, we apply our method to a case study revealing spatial patterns of poverty in the Mexican state of Tlaxcala.</summary></entry><entry><title type="html">Socio-demographic Determinants of Child Malnutrition Age 0-5 years in Bangladesh: NB and ZINB Approaches</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/SociodemographicDeterminantsofChildMalnutritionAge05yearsinBangladeshNBandZINBApproaches.html" rel="alternate" type="text/html" title="Socio-demographic Determinants of Child Malnutrition Age 0-5 years in Bangladesh: NB and ZINB Approaches" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/SociodemographicDeterminantsofChildMalnutritionAge05yearsinBangladeshNBandZINBApproaches</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/SociodemographicDeterminantsofChildMalnutritionAge05yearsinBangladeshNBandZINBApproaches.html">&lt;p&gt;Although child malnutrition is improving over the world in the last couple of decades, still now it is concerning issue among the developing countries including Bangladesh. In general, malnutrition is a dichotomous response variable fitted with logistic regression model. But in this study, counting number of malnourished children in each household is defined as response variable. UNICEF with co-operating Bangladesh Bureau of Statistics (BBS) conducted Multiple Indicator Cluster Survey (MICS) covering 64000 households in Bangladesh by using two stage stratified sampling technique, where 21000 households have children age 0-5 years. We use bivariate analysis figuring out significant association between target and socio-demographic predictor variables. Then Negative binomial regression model is used over poisson regression model due to arising over-dispersion problem ($variance &amp;gt; mean$). Zero inflated negative binomial model also is applied for the excess of zeros in the target variable. Considering standard error and significant level of individual factors NB model provides better result as compare to ZINB.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03515&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Md Mehedi Hasan Bhuiyan</name></author><category term="stat.AP" /><summary type="html">Although child malnutrition is improving over the world in the last couple of decades, still now it is concerning issue among the developing countries including Bangladesh. In general, malnutrition is a dichotomous response variable fitted with logistic regression model. But in this study, counting number of malnourished children in each household is defined as response variable. UNICEF with co-operating Bangladesh Bureau of Statistics (BBS) conducted Multiple Indicator Cluster Survey (MICS) covering 64000 households in Bangladesh by using two stage stratified sampling technique, where 21000 households have children age 0-5 years. We use bivariate analysis figuring out significant association between target and socio-demographic predictor variables. Then Negative binomial regression model is used over poisson regression model due to arising over-dispersion problem ($variance &amp;gt; mean$). Zero inflated negative binomial model also is applied for the excess of zeros in the target variable. Considering standard error and significant level of individual factors NB model provides better result as compare to ZINB.</summary></entry><entry><title type="html">Statistical Multicriteria Benchmarking via the GSD-Front</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/StatisticalMulticriteriaBenchmarkingviatheGSDFront.html" rel="alternate" type="text/html" title="Statistical Multicriteria Benchmarking via the GSD-Front" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/StatisticalMulticriteriaBenchmarkingviatheGSDFront</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/StatisticalMulticriteriaBenchmarkingviatheGSDFront.html">&lt;p&gt;Given the vast number of classifiers that have been (and continue to be) proposed, reliable methods for comparing them are becoming increasingly important. The desire for reliability is broken down into three main aspects: (1) Comparisons should allow for different quality metrics simultaneously. (2) Comparisons should take into account the statistical uncertainty induced by the choice of benchmark suite. (3) The robustness of the comparisons under small deviations in the underlying assumptions should be verifiable. To address (1), we propose to compare classifiers using a generalized stochastic dominance ordering (GSD) and present the GSD-front as an information-efficient alternative to the classical Pareto-front. For (2), we propose a consistent statistical estimator for the GSD-front and construct a statistical test for whether a (potentially new) classifier lies in the GSD-front of a set of state-of-the-art classifiers. For (3), we relax our proposed test using techniques from robust statistics and imprecise probabilities. We illustrate our concepts on the benchmark suite PMLB and on the platform OpenML.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03924&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Christoph Jansen , Georg Schollmeyer , Julian Rodemann , Hannah Blocher , Thomas Augustin</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Given the vast number of classifiers that have been (and continue to be) proposed, reliable methods for comparing them are becoming increasingly important. The desire for reliability is broken down into three main aspects: (1) Comparisons should allow for different quality metrics simultaneously. (2) Comparisons should take into account the statistical uncertainty induced by the choice of benchmark suite. (3) The robustness of the comparisons under small deviations in the underlying assumptions should be verifiable. To address (1), we propose to compare classifiers using a generalized stochastic dominance ordering (GSD) and present the GSD-front as an information-efficient alternative to the classical Pareto-front. For (2), we propose a consistent statistical estimator for the GSD-front and construct a statistical test for whether a (potentially new) classifier lies in the GSD-front of a set of state-of-the-art classifiers. For (3), we relax our proposed test using techniques from robust statistics and imprecise probabilities. We illustrate our concepts on the benchmark suite PMLB and on the platform OpenML.</summary></entry><entry><title type="html">Strong Approximations for Empirical Processes Indexed by Lipschitz Functions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/StrongApproximationsforEmpiricalProcessesIndexedbyLipschitzFunctions.html" rel="alternate" type="text/html" title="Strong Approximations for Empirical Processes Indexed by Lipschitz Functions" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/StrongApproximationsforEmpiricalProcessesIndexedbyLipschitzFunctions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/StrongApproximationsforEmpiricalProcessesIndexedbyLipschitzFunctions.html">&lt;p&gt;This paper presents new uniform Gaussian strong approximations for empirical processes indexed by classes of functions based on $d$-variate random vectors ($d\geq1$). First, a uniform Gaussian strong approximation is established for general empirical processes indexed by Lipschitz functions, encompassing and improving on all previous results in the literature. When specialized to the setting considered by Rio (1994), and certain constraints on the function class hold, our result improves the approximation rate $n^{-1/(2d)}$ to $n^{-1/\max{d,2}}$, up to the same $\operatorname{polylog} n$ term, where $n$ denotes the sample size. Remarkably, we establish a valid uniform Gaussian strong approximation at the optimal rate $n^{-1/2}\log n$ for $d=2$, which was previously known to be valid only for univariate ($d=1$) empirical processes via the celebrated Hungarian construction (Koml&apos;os et al., 1975). Second, a uniform Gaussian strong approximation is established for a class of multiplicative separable empirical processes indexed by Lipschitz functions, which address some outstanding problems in the literature (Chernozhukov et al., 2014, Section 3). In addition, two other uniform Gaussian strong approximation results are presented for settings where the function class takes the form of a sequence of Haar basis based on generalized quasi-uniform partitions. We demonstrate the improvements and usefulness of our new strong approximation results with several statistical applications to nonparametric density and regression estimation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04191&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matias D. Cattaneo ,  Ruiqi ,  Yu</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This paper presents new uniform Gaussian strong approximations for empirical processes indexed by classes of functions based on $d$-variate random vectors ($d\geq1$). First, a uniform Gaussian strong approximation is established for general empirical processes indexed by Lipschitz functions, encompassing and improving on all previous results in the literature. When specialized to the setting considered by Rio (1994), and certain constraints on the function class hold, our result improves the approximation rate $n^{-1/(2d)}$ to $n^{-1/\max{d,2}}$, up to the same $\operatorname{polylog} n$ term, where $n$ denotes the sample size. Remarkably, we establish a valid uniform Gaussian strong approximation at the optimal rate $n^{-1/2}\log n$ for $d=2$, which was previously known to be valid only for univariate ($d=1$) empirical processes via the celebrated Hungarian construction (Koml&apos;os et al., 1975). Second, a uniform Gaussian strong approximation is established for a class of multiplicative separable empirical processes indexed by Lipschitz functions, which address some outstanding problems in the literature (Chernozhukov et al., 2014, Section 3). In addition, two other uniform Gaussian strong approximation results are presented for settings where the function class takes the form of a sequence of Haar basis based on generalized quasi-uniform partitions. We demonstrate the improvements and usefulness of our new strong approximation results with several statistical applications to nonparametric density and regression estimation.</summary></entry><entry><title type="html">Thinking inside the bounds: Improved error distributions for indifference point data analysis and simulation via beta regression using common discounting functions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/ThinkinginsidetheboundsImprovederrordistributionsforindifferencepointdataanalysisandsimulationviabetaregressionusingcommondiscountingfunctions.html" rel="alternate" type="text/html" title="Thinking inside the bounds: Improved error distributions for indifference point data analysis and simulation via beta regression using common discounting functions" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/ThinkinginsidetheboundsImprovederrordistributionsforindifferencepointdataanalysisandsimulationviabetaregressionusingcommondiscountingfunctions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/ThinkinginsidetheboundsImprovederrordistributionsforindifferencepointdataanalysisandsimulationviabetaregressionusingcommondiscountingfunctions.html">&lt;p&gt;Standard nonlinear regression is commonly used when modeling indifference points due to its ability to closely follow observed data, resulting in a good model fit. However, standard nonlinear regression currently lacks a reasonable distribution-based framework for indifference points, which limits its ability to adequately describe the inherent variability in the data. Software commonly assumes data follow a normal distribution with constant variance. However, typical indifference points do not follow a normal distribution or exhibit constant variance. To address these limitations, this paper introduces a class of nonlinear beta regression models that offers excellent fit to discounting data and enhances simulation-based approaches. This beta regression model can accommodate popular discounting functions. This work proposes three specific advances. First, our model automatically captures non-constant variance as a function of delay. Second, our model improves simulation-based approaches since it obeys the natural boundaries of observable data, unlike the ordinary assumption of normal residuals and constant variance. Finally, we introduce a scale-location-truncation trick that allows beta regression to accommodate observed values of zero and one. A comparison between beta regression and standard nonlinear regression reveals close agreement in the estimated discounting rate k obtained from both methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18000&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mingang Kim, Mikhail N. Koffarnus, Christopher T Franck</name></author><category term="stat.ME" /><summary type="html">Standard nonlinear regression is commonly used when modeling indifference points due to its ability to closely follow observed data, resulting in a good model fit. However, standard nonlinear regression currently lacks a reasonable distribution-based framework for indifference points, which limits its ability to adequately describe the inherent variability in the data. Software commonly assumes data follow a normal distribution with constant variance. However, typical indifference points do not follow a normal distribution or exhibit constant variance. To address these limitations, this paper introduces a class of nonlinear beta regression models that offers excellent fit to discounting data and enhances simulation-based approaches. This beta regression model can accommodate popular discounting functions. This work proposes three specific advances. First, our model automatically captures non-constant variance as a function of delay. Second, our model improves simulation-based approaches since it obeys the natural boundaries of observable data, unlike the ordinary assumption of normal residuals and constant variance. Finally, we introduce a scale-location-truncation trick that allows beta regression to accommodate observed values of zero and one. A comparison between beta regression and standard nonlinear regression reveals close agreement in the estimated discounting rate k obtained from both methods.</summary></entry><entry><title type="html">Variational Prior Replacement in Bayesian Inference and Inversion</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/VariationalPriorReplacementinBayesianInferenceandInversion.html" rel="alternate" type="text/html" title="Variational Prior Replacement in Bayesian Inference and Inversion" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/VariationalPriorReplacementinBayesianInferenceandInversion</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/VariationalPriorReplacementinBayesianInferenceandInversion.html">&lt;p&gt;Many scientific investigations require that the values of a set of model parameters are estimated using recorded data. In Bayesian inference, information from both observed data and prior knowledge is combined to update model parameters probabilistically. Prior information represents our belief about the range of values that the variables can take, and their relative probabilities when considered independently of recorded data. Situations arise in which we wish to change prior information: (i) the subjective nature of prior information, (ii) cases in which we wish to test different states of prior information as hypothesis tests, and (iii) information from new studies may emerge so prior information may evolve over time. Estimating the solution to any single inference problem is usually computationally costly, as it typically requires thousands of model samples and their forward simulations. Therefore, recalculating the Bayesian solution every time prior information changes can be extremely expensive. We develop a mathematical formulation that allows prior information to be changed in a solution using variational methods, without performing Bayesian inference on each occasion. In this method, existing prior information is removed from a previously obtained posterior distribution and is replaced by new prior information. We therefore call the methodology variational prior replacement (VPR). We demonstrate VPR using a 2D seismic full waveform inversion example, where VPR provides almost identical posterior solutions compared to those obtained by solving independent inference problems using different priors. The former can be completed within minutes even on a laptop whereas the latter requires days of computations using high-performance computing resources. We demonstrate the value of the method by comparing the posterior solutions obtained using three different types of prior information.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04072&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xuebin Zhao, Andrew Curtis</name></author><category term="stat.ME" /><summary type="html">Many scientific investigations require that the values of a set of model parameters are estimated using recorded data. In Bayesian inference, information from both observed data and prior knowledge is combined to update model parameters probabilistically. Prior information represents our belief about the range of values that the variables can take, and their relative probabilities when considered independently of recorded data. Situations arise in which we wish to change prior information: (i) the subjective nature of prior information, (ii) cases in which we wish to test different states of prior information as hypothesis tests, and (iii) information from new studies may emerge so prior information may evolve over time. Estimating the solution to any single inference problem is usually computationally costly, as it typically requires thousands of model samples and their forward simulations. Therefore, recalculating the Bayesian solution every time prior information changes can be extremely expensive. We develop a mathematical formulation that allows prior information to be changed in a solution using variational methods, without performing Bayesian inference on each occasion. In this method, existing prior information is removed from a previously obtained posterior distribution and is replaced by new prior information. We therefore call the methodology variational prior replacement (VPR). We demonstrate VPR using a 2D seismic full waveform inversion example, where VPR provides almost identical posterior solutions compared to those obtained by solving independent inference problems using different priors. The former can be completed within minutes even on a laptop whereas the latter requires days of computations using high-performance computing resources. We demonstrate the value of the method by comparing the posterior solutions obtained using three different types of prior information.</summary></entry><entry><title type="html">Why recommended visit intervals should be extracted when conducting longitudinal analyses using electronic health record data: examining visit mechanism and sensitivity to assessment not at random</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Whyrecommendedvisitintervalsshouldbeextractedwhenconductinglongitudinalanalysesusingelectronichealthrecorddataexaminingvisitmechanismandsensitivitytoassessmentnotatrandom.html" rel="alternate" type="text/html" title="Why recommended visit intervals should be extracted when conducting longitudinal analyses using electronic health record data: examining visit mechanism and sensitivity to assessment not at random" /><published>2024-06-07T00:00:00+00:00</published><updated>2024-06-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Whyrecommendedvisitintervalsshouldbeextractedwhenconductinglongitudinalanalysesusingelectronichealthrecorddataexaminingvisitmechanismandsensitivitytoassessmentnotatrandom</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/07/Whyrecommendedvisitintervalsshouldbeextractedwhenconductinglongitudinalanalysesusingelectronichealthrecorddataexaminingvisitmechanismandsensitivitytoassessmentnotatrandom.html">&lt;p&gt;Electronic health records (EHRs) provide an efficient approach to generating rich longitudinal datasets. However, since patients visit as needed, the assessment times are typically irregular and may be related to the patient’s health. Failing to account for this informative assessment process could result in biased estimates of the disease course. In this paper, we show how estimation of the disease trajectory can be enhanced by leveraging an underutilized piece of information that is often in the patient’s EHR: physician-recommended intervals between visits. Specifically, we demonstrate how recommended intervals can be used in characterizing the assessment process, and in investigating the sensitivity of the results to assessment not at random (ANAR). We illustrate our proposed approach in a clinic-based cohort study of juvenile dermatomyositis (JDM). In this study, we found that the recommended intervals explained 78% of the variability in the assessment times. Under a specific case of ANAR where we assumed that a worsening in disease led to patients visiting earlier than recommended, the estimated population average disease activity trajectory was shifted downward relative to the trajectory assuming assessment at random. These results demonstrate the crucial role recommended intervals play in improving the rigour of the analysis by allowing us to assess both the plausibility of the AAR assumption and the sensitivity of the results to departures from this assumption. Thus, we advise that studies using irregular longitudinal data should extract recommended visit intervals and follow our procedure for incorporating them into analyses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04077&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rose Garrett, Masum Patel, Brian Feldman, Eleanor Pullenayegum</name></author><category term="stat.ME" /><summary type="html">Electronic health records (EHRs) provide an efficient approach to generating rich longitudinal datasets. However, since patients visit as needed, the assessment times are typically irregular and may be related to the patient’s health. Failing to account for this informative assessment process could result in biased estimates of the disease course. In this paper, we show how estimation of the disease trajectory can be enhanced by leveraging an underutilized piece of information that is often in the patient’s EHR: physician-recommended intervals between visits. Specifically, we demonstrate how recommended intervals can be used in characterizing the assessment process, and in investigating the sensitivity of the results to assessment not at random (ANAR). We illustrate our proposed approach in a clinic-based cohort study of juvenile dermatomyositis (JDM). In this study, we found that the recommended intervals explained 78% of the variability in the assessment times. Under a specific case of ANAR where we assumed that a worsening in disease led to patients visiting earlier than recommended, the estimated population average disease activity trajectory was shifted downward relative to the trajectory assuming assessment at random. These results demonstrate the crucial role recommended intervals play in improving the rigour of the analysis by allowing us to assess both the plausibility of the AAR assumption and the sensitivity of the results to departures from this assumption. Thus, we advise that studies using irregular longitudinal data should extract recommended visit intervals and follow our procedure for incorporating them into analyses.</summary></entry></feed>