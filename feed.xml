<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-21T07:14:53+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Unified Statistical And Computational Framework For Ex-Post Harmonisation Of Aggregate Statistics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AUnifiedStatisticalAndComputationalFrameworkForExPostHarmonisationOfAggregateStatistics.html" rel="alternate" type="text/html" title="A Unified Statistical And Computational Framework For Ex-Post Harmonisation Of Aggregate Statistics" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AUnifiedStatisticalAndComputationalFrameworkForExPostHarmonisationOfAggregateStatistics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AUnifiedStatisticalAndComputationalFrameworkForExPostHarmonisationOfAggregateStatistics.html">&lt;p&gt;Ex-post harmonisation is one of many data preprocessing processes used to combine the increasingly vast and diverse sources of data available for research and analysis. Documenting provenance and ensuring the quality of multi-source datasets is vital for ensuring trustworthy scientific research and encouraging reuse of existing harmonisation efforts. However, capturing and communicating statistically relevant properties of harmonised datasets is difficult without a universal standard for describing harmonisation operations. Our paper combines mathematical and computer science perspectives to address this need. The Crossmaps Framework defines a new approach for transforming existing variables collected under a specific measurement or classification standard to an imputed counterfactual variable indexed by some target standard. It uses computational graphs to separate intended transformation logic from actual data transformations, and avoid the risk of syntactically valid data manipulation scripts resulting in statistically questionable data. In this paper, we introduce the Crossmaps Framework through the example of ex-post harmonisation of aggregated statistics in the social sciences. We define a new provenance task abstraction, the crossmap transform, and formalise two associated objects, the shared mass array and the crossmap. We further define graph, matrix and list encodings of crossmaps and discuss resulting implications for understanding statistical properties of ex-post harmonisation and designing error minimising workflows.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14163&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Cynthia A. Huang</name></author><category term="stat.ME" /><summary type="html">Ex-post harmonisation is one of many data preprocessing processes used to combine the increasingly vast and diverse sources of data available for research and analysis. Documenting provenance and ensuring the quality of multi-source datasets is vital for ensuring trustworthy scientific research and encouraging reuse of existing harmonisation efforts. However, capturing and communicating statistically relevant properties of harmonised datasets is difficult without a universal standard for describing harmonisation operations. Our paper combines mathematical and computer science perspectives to address this need. The Crossmaps Framework defines a new approach for transforming existing variables collected under a specific measurement or classification standard to an imputed counterfactual variable indexed by some target standard. It uses computational graphs to separate intended transformation logic from actual data transformations, and avoid the risk of syntactically valid data manipulation scripts resulting in statistically questionable data. In this paper, we introduce the Crossmaps Framework through the example of ex-post harmonisation of aggregated statistics in the social sciences. We define a new provenance task abstraction, the crossmap transform, and formalise two associated objects, the shared mass array and the crossmap. We further define graph, matrix and list encodings of crossmaps and discuss resulting implications for understanding statistical properties of ex-post harmonisation and designing error minimising workflows.</summary></entry><entry><title type="html">Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choices</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ActiveAdaptiveExperimentalDesignforTreatmentEffectEstimationwithCovariateChoices.html" rel="alternate" type="text/html" title="Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choices" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ActiveAdaptiveExperimentalDesignforTreatmentEffectEstimationwithCovariateChoices</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ActiveAdaptiveExperimentalDesignforTreatmentEffectEstimationwithCovariateChoices.html">&lt;p&gt;This study designs an adaptive experiment for efficiently estimating average treatment effects (ATEs). In each round of our adaptive experiment, an experimenter sequentially samples an experimental unit, assigns a treatment, and observes the corresponding outcome immediately. At the end of the experiment, the experimenter estimates an ATE using the gathered samples. The objective is to estimate the ATE with a smaller asymptotic variance. Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability). As a generalization of such an approach, we propose optimizing the covariate density as well as the propensity score. First, we derive the efficient covariate density and propensity score that minimize the semiparametric efficiency bound and find that optimizing both covariate density and propensity score minimizes the semiparametric efficiency bound more effectively than optimizing only the propensity score. Next, we design an adaptive experiment using the efficient covariate density and propensity score sequentially estimated during the experiment. Lastly, we propose an ATE estimator whose asymptotic variance aligns with the minimized semiparametric efficiency bound.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.03589&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Masahiro Kato, Akihiro Oga, Wataru Komatsubara, Ryo Inokuchi</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">This study designs an adaptive experiment for efficiently estimating average treatment effects (ATEs). In each round of our adaptive experiment, an experimenter sequentially samples an experimental unit, assigns a treatment, and observes the corresponding outcome immediately. At the end of the experiment, the experimenter estimates an ATE using the gathered samples. The objective is to estimate the ATE with a smaller asymptotic variance. Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability). As a generalization of such an approach, we propose optimizing the covariate density as well as the propensity score. First, we derive the efficient covariate density and propensity score that minimize the semiparametric efficiency bound and find that optimizing both covariate density and propensity score minimizes the semiparametric efficiency bound more effectively than optimizing only the propensity score. Next, we design an adaptive experiment using the efficient covariate density and propensity score sequentially estimated during the experiment. Lastly, we propose an ATE estimator whose asymptotic variance aligns with the minimized semiparametric efficiency bound.</summary></entry><entry><title type="html">A finite-infinite shared atoms nested model for the Bayesian analysis of large grouped data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AfiniteinfinitesharedatomsnestedmodelfortheBayesiananalysisoflargegroupeddata.html" rel="alternate" type="text/html" title="A finite-infinite shared atoms nested model for the Bayesian analysis of large grouped data" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AfiniteinfinitesharedatomsnestedmodelfortheBayesiananalysisoflargegroupeddata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AfiniteinfinitesharedatomsnestedmodelfortheBayesiananalysisoflargegroupeddata.html">&lt;p&gt;The use of hierarchical mixture priors with shared atoms has recently flourished in the Bayesian literature for partially exchangeable data. Leveraging on nested levels of mixtures, these models allow the estimation of a two-layered data partition: across groups and across observations. This paper discusses and compares the properties of such modeling strategies when the mixing weights are assigned either a finite-dimensional Dirichlet distribution or a Dirichlet process prior. Based on these considerations, we introduce a novel hierarchical nonparametric prior based on a finite set of shared atoms, a specification that enhances the flexibility of the induced random measures and the availability of fast posterior inference. To support these findings, we analytically derive the induced prior correlation structure and partially exchangeable partition probability function. Additionally, we develop a novel mean-field variational algorithm for posterior inference to boost the applicability of our nested model to large multivariate data. We then assess and compare the performance of the different shared-atom specifications via simulation. We also show that our variational proposal is highly scalable and that the accuracy of the posterior density estimate and the estimated partition is comparable with state-of-the-art Gibbs sampler algorithms. Finally, we apply our model to a real dataset of Spotify’s song features, simultaneously segmenting artists and songs with similar characteristics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13310&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Laura D&apos;Angelo, Francesco Denti</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The use of hierarchical mixture priors with shared atoms has recently flourished in the Bayesian literature for partially exchangeable data. Leveraging on nested levels of mixtures, these models allow the estimation of a two-layered data partition: across groups and across observations. This paper discusses and compares the properties of such modeling strategies when the mixing weights are assigned either a finite-dimensional Dirichlet distribution or a Dirichlet process prior. Based on these considerations, we introduce a novel hierarchical nonparametric prior based on a finite set of shared atoms, a specification that enhances the flexibility of the induced random measures and the availability of fast posterior inference. To support these findings, we analytically derive the induced prior correlation structure and partially exchangeable partition probability function. Additionally, we develop a novel mean-field variational algorithm for posterior inference to boost the applicability of our nested model to large multivariate data. We then assess and compare the performance of the different shared-atom specifications via simulation. We also show that our variational proposal is highly scalable and that the accuracy of the posterior density estimate and the estimated partition is comparable with state-of-the-art Gibbs sampler algorithms. Finally, we apply our model to a real dataset of Spotify’s song features, simultaneously segmenting artists and songs with similar characteristics.</summary></entry><entry><title type="html">An Empirical Bayes Jackknife Regression Framework for Covariance Matrix Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AnEmpiricalBayesJackknifeRegressionFrameworkforCovarianceMatrixEstimation.html" rel="alternate" type="text/html" title="An Empirical Bayes Jackknife Regression Framework for Covariance Matrix Estimation" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AnEmpiricalBayesJackknifeRegressionFrameworkforCovarianceMatrixEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AnEmpiricalBayesJackknifeRegressionFrameworkforCovarianceMatrixEstimation.html">&lt;p&gt;Covariance matrix estimation, a classical statistical topic, poses significant challenges when the sample size is comparable to or smaller than the number of features. In this paper, we frame covariance matrix estimation as a compound decision problem and apply an optimal decision rule to estimate covariance parameters. To approximate this rule, we introduce an algorithm that integrates jackknife techniques with machine learning regression methods. This algorithm exhibits adaptability across diverse scenarios without relying on assumptions about data distribution. Simulation results and gene network inference from an RNA-seq experiment in mice demonstrate that our approach either matches or surpasses several state-of-the-art methods&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13876&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Huqin Xin, Sihai Dave Zhao</name></author><category term="stat.ME" /><summary type="html">Covariance matrix estimation, a classical statistical topic, poses significant challenges when the sample size is comparable to or smaller than the number of features. In this paper, we frame covariance matrix estimation as a compound decision problem and apply an optimal decision rule to estimate covariance parameters. To approximate this rule, we introduce an algorithm that integrates jackknife techniques with machine learning regression methods. This algorithm exhibits adaptability across diverse scenarios without relying on assumptions about data distribution. Simulation results and gene network inference from an RNA-seq experiment in mice demonstrate that our approach either matches or surpasses several state-of-the-art methods</summary></entry><entry><title type="html">An agent-based model of behaviour change calibrated to reversal learning data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Anagentbasedmodelofbehaviourchangecalibratedtoreversallearningdata.html" rel="alternate" type="text/html" title="An agent-based model of behaviour change calibrated to reversal learning data" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Anagentbasedmodelofbehaviourchangecalibratedtoreversallearningdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Anagentbasedmodelofbehaviourchangecalibratedtoreversallearningdata.html">&lt;p&gt;Behaviour change lies at the heart of many observable collective phenomena such as the transmission and control of infectious diseases, adoption of public health policies, and migration of animals to new habitats. Representing the process of individual behaviour change in computer simulations of these phenomena remains an open challenge. Often, computational models use phenomenological implementations with limited support from behavioural data. Without a strong connection to observable quantities, such models have limited utility for simulating observed and counterfactual scenarios of emergent phenomena because they cannot be validated or calibrated. Here, we present a simple stochastic individual-based model of reversal learning that captures fundamental properties of individual behaviour change, namely, the capacity to learn based on accumulated reward signals, and the transient persistence of learned behaviour after rewards are removed or altered. The model has only two parameters, and we use approximate Bayesian computation to demonstrate that they are fully identifiable from empirical reversal learning time series data. Finally, we demonstrate how the model can be extended to account for the increased complexity of behavioural dynamics over longer time scales involving fluctuating stimuli. This work is a step towards the development and evaluation of fully identifiable individual-level behaviour change models that can function as validated submodels for complex simulations of collective behaviour change.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14062&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roben Delos Reyes, Hugo Lyons Keenan, Cameron Zachreson</name></author><category term="stat.CO" /><summary type="html">Behaviour change lies at the heart of many observable collective phenomena such as the transmission and control of infectious diseases, adoption of public health policies, and migration of animals to new habitats. Representing the process of individual behaviour change in computer simulations of these phenomena remains an open challenge. Often, computational models use phenomenological implementations with limited support from behavioural data. Without a strong connection to observable quantities, such models have limited utility for simulating observed and counterfactual scenarios of emergent phenomena because they cannot be validated or calibrated. Here, we present a simple stochastic individual-based model of reversal learning that captures fundamental properties of individual behaviour change, namely, the capacity to learn based on accumulated reward signals, and the transient persistence of learned behaviour after rewards are removed or altered. The model has only two parameters, and we use approximate Bayesian computation to demonstrate that they are fully identifiable from empirical reversal learning time series data. Finally, we demonstrate how the model can be extended to account for the increased complexity of behavioural dynamics over longer time scales involving fluctuating stimuli. This work is a step towards the development and evaluation of fully identifiable individual-level behaviour change models that can function as validated submodels for complex simulations of collective behaviour change.</summary></entry><entry><title type="html">Association of neighborhood disadvantage with cognitive function and cortical disorganization in an unimpaired cohort</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Associationofneighborhooddisadvantagewithcognitivefunctionandcorticaldisorganizationinanunimpairedcohort.html" rel="alternate" type="text/html" title="Association of neighborhood disadvantage with cognitive function and cortical disorganization in an unimpaired cohort" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Associationofneighborhooddisadvantagewithcognitivefunctionandcorticaldisorganizationinanunimpairedcohort</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Associationofneighborhooddisadvantagewithcognitivefunctionandcorticaldisorganizationinanunimpairedcohort.html">&lt;p&gt;Neighborhood disadvantage is associated with worse health and cognitive outcomes. Morphological similarity network (MSN) is a promising approach to elucidate cortical network patterns underlying complex cognitive functions. We hypothesized that MSNs could capture changes in cortical patterns related to neighborhood disadvantage and cognitive function. This cross-sectional study included cognitively unimpaired participants from two large Alzheimers studies at University of Wisconsin-Madison. Neighborhood disadvantage status was obtained using the Area Deprivation Index (ADI). Cognitive performance was assessed on memory, processing speed and executive function. Morphological Similarity Networks (MSN) were constructed for each participant based on the similarity in distribution of cortical thickness of brain regions, followed by computation of local and global network features. Association of ADI with cognitive scores and MSN features were examined using linear regression and mediation analysis. ADI showed negative association with category fluency,implicit learning speed, story recall and modified pre-clinical Alzheimers cognitive composite scores, indicating worse cognitive function among those living in more disadvantaged neighborhoods. Local network features of frontal and temporal regions differed based on ADI status. Centrality of left lateral orbitofrontal region showed a partial mediating effect between association of neighborhood disadvantage and story recall performance. Our preliminary findings suggest differences in local cortical organization by neighborhood disadvantage, which partially mediated the relationship between ADI and cognitive performance, providing a possible network-based mechanism to, in-part, explain the risk for poor cognitive functioning associated with disadvantaged neighborhoods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13822&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Apoorva Safai, Erin Jonaitis, Rebecca E Langhough, William R Buckingham, Sterling C. Johnson, W. Ryan Powell, Amy J. H. Kind, Barbara B. Bendlin, Pallavi Tiwari</name></author><category term="stat.AP" /><summary type="html">Neighborhood disadvantage is associated with worse health and cognitive outcomes. Morphological similarity network (MSN) is a promising approach to elucidate cortical network patterns underlying complex cognitive functions. We hypothesized that MSNs could capture changes in cortical patterns related to neighborhood disadvantage and cognitive function. This cross-sectional study included cognitively unimpaired participants from two large Alzheimers studies at University of Wisconsin-Madison. Neighborhood disadvantage status was obtained using the Area Deprivation Index (ADI). Cognitive performance was assessed on memory, processing speed and executive function. Morphological Similarity Networks (MSN) were constructed for each participant based on the similarity in distribution of cortical thickness of brain regions, followed by computation of local and global network features. Association of ADI with cognitive scores and MSN features were examined using linear regression and mediation analysis. ADI showed negative association with category fluency,implicit learning speed, story recall and modified pre-clinical Alzheimers cognitive composite scores, indicating worse cognitive function among those living in more disadvantaged neighborhoods. Local network features of frontal and temporal regions differed based on ADI status. Centrality of left lateral orbitofrontal region showed a partial mediating effect between association of neighborhood disadvantage and story recall performance. Our preliminary findings suggest differences in local cortical organization by neighborhood disadvantage, which partially mediated the relationship between ADI and cognitive performance, providing a possible network-based mechanism to, in-part, explain the risk for poor cognitive functioning associated with disadvantaged neighborhoods.</summary></entry><entry><title type="html">Augmented Degree Correction for Bipartite Networks with Applications to Recommender Systems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AugmentedDegreeCorrectionforBipartiteNetworkswithApplicationstoRecommenderSystems.html" rel="alternate" type="text/html" title="Augmented Degree Correction for Bipartite Networks with Applications to Recommender Systems" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AugmentedDegreeCorrectionforBipartiteNetworkswithApplicationstoRecommenderSystems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AugmentedDegreeCorrectionforBipartiteNetworkswithApplicationstoRecommenderSystems.html">&lt;p&gt;In recommender systems, users rate items, and are subsequently served other product recommendations based on these ratings. Even though users usually rate a tiny percentage of the available items, the system tries to estimate unobserved preferences by finding similarities across users and across items. In this work, we treat the observed ratings data as partially observed, dense, weighted, bipartite networks. For a class of systems without outside information, we adapt an approach developed for dense, weighted networks to account for unobserved edges and the bipartite nature of the problem. This approach allows for community structure, and for local estimation of flexible patterns of ratings across different pairs of communities. We compare the performance of our proposed approach to existing methods on a simulated data set, as well as on a data set of joke ratings, examining model performance in both cases at differing levels of sparsity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.06436&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Benjamin Leinwand, Vladas Pipiras</name></author><category term="stat.AP" /><summary type="html">In recommender systems, users rate items, and are subsequently served other product recommendations based on these ratings. Even though users usually rate a tiny percentage of the available items, the system tries to estimate unobserved preferences by finding similarities across users and across items. In this work, we treat the observed ratings data as partially observed, dense, weighted, bipartite networks. For a class of systems without outside information, we adapt an approach developed for dense, weighted networks to account for unobserved edges and the bipartite nature of the problem. This approach allows for community structure, and for local estimation of flexible patterns of ratings across different pairs of communities. We compare the performance of our proposed approach to existing methods on a simulated data set, as well as on a data set of joke ratings, examining model performance in both cases at differing levels of sparsity.</summary></entry><entry><title type="html">Averaging polyhazard models using Piecewise deterministic Monte Carlo with applications to data with long-term survivors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AveragingpolyhazardmodelsusingPiecewisedeterministicMonteCarlowithapplicationstodatawithlongtermsurvivors.html" rel="alternate" type="text/html" title="Averaging polyhazard models using Piecewise deterministic Monte Carlo with applications to data with long-term survivors" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AveragingpolyhazardmodelsusingPiecewisedeterministicMonteCarlowithapplicationstodatawithlongtermsurvivors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/AveragingpolyhazardmodelsusingPiecewisedeterministicMonteCarlowithapplicationstodatawithlongtermsurvivors.html">&lt;p&gt;Polyhazard models are a class of flexible parametric models for modelling survival over extended time horizons. Their additive hazard structure allows for flexible, non-proportional hazards whose characteristics can change over time while retaining a parametric form, which allows for survival to be extrapolated beyond the observation period of a study. Significant user input is required, however, in selecting the number of latent hazards to model, their distributions and the choice of which variables to associate with each hazard. The resulting set of models is too large to explore manually, limiting their practical usefulness. Motivated by applications to stroke survivor and kidney transplant patient survival times we extend the standard polyhazard model through a prior structure allowing for joint inference of parameters and structural quantities, and develop a sampling scheme that utilises state-of-the-art Piecewise Deterministic Markov Processes to sample from the resulting transdimensional posterior with minimal user tuning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14182&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luke Hardcastle, Samuel Livingstone, Gianluca Baio</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Polyhazard models are a class of flexible parametric models for modelling survival over extended time horizons. Their additive hazard structure allows for flexible, non-proportional hazards whose characteristics can change over time while retaining a parametric form, which allows for survival to be extrapolated beyond the observation period of a study. Significant user input is required, however, in selecting the number of latent hazards to model, their distributions and the choice of which variables to associate with each hazard. The resulting set of models is too large to explore manually, limiting their practical usefulness. Motivated by applications to stroke survivor and kidney transplant patient survival times we extend the standard polyhazard model through a prior structure allowing for joint inference of parameters and structural quantities, and develop a sampling scheme that utilises state-of-the-art Piecewise Deterministic Markov Processes to sample from the resulting transdimensional posterior with minimal user tuning.</summary></entry><entry><title type="html">Bayesian Structural Model Updating with Multimodal Variational Autoencoder</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/BayesianStructuralModelUpdatingwithMultimodalVariationalAutoencoder.html" rel="alternate" type="text/html" title="Bayesian Structural Model Updating with Multimodal Variational Autoencoder" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/BayesianStructuralModelUpdatingwithMultimodalVariationalAutoencoder</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/BayesianStructuralModelUpdatingwithMultimodalVariationalAutoencoder.html">&lt;p&gt;A novel framework for Bayesian structural model updating is presented in this study. The proposed method utilizes the surrogate unimodal encoders of a multimodal variational autoencoder (VAE). The method facilitates an approximation of the likelihood when dealing with a small number of observations. It is particularly suitable for high-dimensional correlated simultaneous observations applicable to various dynamic analysis models. The proposed approach was benchmarked using a numerical model of a single-story frame building with acceleration and dynamic strain measurements. Additionally, an example involving a Bayesian update of nonlinear model parameters for a three-degree-of-freedom lumped mass model demonstrates computational efficiency when compared to using the original VAE, while maintaining adequate accuracy for practical applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09051&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tatsuya Itoi, Kazuho Amishiki, Sangwon Lee, Taro Yaoyama</name></author><category term="stat.ML," /><category term="stat.AP" /><summary type="html">A novel framework for Bayesian structural model updating is presented in this study. The proposed method utilizes the surrogate unimodal encoders of a multimodal variational autoencoder (VAE). The method facilitates an approximation of the likelihood when dealing with a small number of observations. It is particularly suitable for high-dimensional correlated simultaneous observations applicable to various dynamic analysis models. The proposed approach was benchmarked using a numerical model of a single-story frame building with acceleration and dynamic strain measurements. Additionally, an example involving a Bayesian update of nonlinear model parameters for a three-degree-of-freedom lumped mass model demonstrates computational efficiency when compared to using the original VAE, while maintaining adequate accuracy for practical applications.</summary></entry><entry><title type="html">Causal Inference with Latent Variables: Recent Advances and Future Prospectives</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/CausalInferencewithLatentVariablesRecentAdvancesandFutureProspectives.html" rel="alternate" type="text/html" title="Causal Inference with Latent Variables: Recent Advances and Future Prospectives" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/CausalInferencewithLatentVariablesRecentAdvancesandFutureProspectives</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/CausalInferencewithLatentVariablesRecentAdvancesandFutureProspectives.html">&lt;p&gt;Causality lays the foundation for the trajectory of our world. Causal inference (CI), which aims to infer intrinsic causal relations among variables of interest, has emerged as a crucial research topic. Nevertheless, the lack of observation of important variables (e.g., confounders, mediators, exogenous variables, etc.) severely compromises the reliability of CI methods. The issue may arise from the inherent difficulty in measuring the variables. Additionally, in observational studies where variables are passively recorded, certain covariates might be inadvertently omitted by the experimenter. Depending on the type of unobserved variables and the specific CI task, various consequences can be incurred if these latent variables are carelessly handled, such as biased estimation of causal effects, incomplete understanding of causal mechanisms, lack of individual-level causal consideration, etc. In this survey, we provide a comprehensive review of recent developments in CI with latent variables. We start by discussing traditional CI techniques when variables of interest are assumed to be fully observed. Afterward, under the taxonomy of circumvention and inference-based methods, we provide an in-depth discussion of various CI strategies to handle latent variables, covering the tasks of causal effect estimation, mediation analysis, counterfactual reasoning, and causal discovery. Furthermore, we generalize the discussion to graph data where interference among units may exist. Finally, we offer fresh aspects for further advancement of CI with latent variables, especially new opportunities in the era of large language models (LLMs).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13966&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yaochen Zhu, Yinhan He, Jing Ma, Mengxuan Hu, Sheng Li, Jundong Li</name></author><category term="stat.ME" /><summary type="html">Causality lays the foundation for the trajectory of our world. Causal inference (CI), which aims to infer intrinsic causal relations among variables of interest, has emerged as a crucial research topic. Nevertheless, the lack of observation of important variables (e.g., confounders, mediators, exogenous variables, etc.) severely compromises the reliability of CI methods. The issue may arise from the inherent difficulty in measuring the variables. Additionally, in observational studies where variables are passively recorded, certain covariates might be inadvertently omitted by the experimenter. Depending on the type of unobserved variables and the specific CI task, various consequences can be incurred if these latent variables are carelessly handled, such as biased estimation of causal effects, incomplete understanding of causal mechanisms, lack of individual-level causal consideration, etc. In this survey, we provide a comprehensive review of recent developments in CI with latent variables. We start by discussing traditional CI techniques when variables of interest are assumed to be fully observed. Afterward, under the taxonomy of circumvention and inference-based methods, we provide an in-depth discussion of various CI strategies to handle latent variables, covering the tasks of causal effect estimation, mediation analysis, counterfactual reasoning, and causal discovery. Furthermore, we generalize the discussion to graph data where interference among units may exist. Finally, we offer fresh aspects for further advancement of CI with latent variables, especially new opportunities in the era of large language models (LLMs).</summary></entry><entry><title type="html">Cluster Quilting: Spectral Clustering for Patchwork Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ClusterQuiltingSpectralClusteringforPatchworkLearning.html" rel="alternate" type="text/html" title="Cluster Quilting: Spectral Clustering for Patchwork Learning" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ClusterQuiltingSpectralClusteringforPatchworkLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ClusterQuiltingSpectralClusteringforPatchworkLearning.html">&lt;p&gt;Patchwork learning arises as a new and challenging data collection paradigm where both samples and features are observed in fragmented subsets. Due to technological limits, measurement expense, or multimodal data integration, such patchwork data structures are frequently seen in neuroscience, healthcare, and genomics, among others. Instead of analyzing each data patch separately, it is highly desirable to extract comprehensive knowledge from the whole data set. In this work, we focus on the clustering problem in patchwork learning, aiming at discovering clusters amongst all samples even when some are never jointly observed for any feature. We propose a novel spectral clustering method called Cluster Quilting, consisting of (i) patch ordering that exploits the overlapping structure amongst all patches, (ii) patchwise SVD, (iii) sequential linear mapping of top singular vectors for patch overlaps, followed by (iv) k-means on the combined and weighted singular vectors. Under a sub-Gaussian mixture model, we establish theoretical guarantees via a non-asymptotic misclustering rate bound that reflects both properties of the patch-wise observation regime as well as the clustering signal and noise dependencies. We also validate our Cluster Quilting algorithm through extensive empirical studies on both simulated and real data sets in neuroscience and genomics, where it discovers more accurate and scientifically more plausible clusters than other approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13833&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lili Zheng, Andersen Chang, Genevera I. Allen</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Patchwork learning arises as a new and challenging data collection paradigm where both samples and features are observed in fragmented subsets. Due to technological limits, measurement expense, or multimodal data integration, such patchwork data structures are frequently seen in neuroscience, healthcare, and genomics, among others. Instead of analyzing each data patch separately, it is highly desirable to extract comprehensive knowledge from the whole data set. In this work, we focus on the clustering problem in patchwork learning, aiming at discovering clusters amongst all samples even when some are never jointly observed for any feature. We propose a novel spectral clustering method called Cluster Quilting, consisting of (i) patch ordering that exploits the overlapping structure amongst all patches, (ii) patchwise SVD, (iii) sequential linear mapping of top singular vectors for patch overlaps, followed by (iv) k-means on the combined and weighted singular vectors. Under a sub-Gaussian mixture model, we establish theoretical guarantees via a non-asymptotic misclustering rate bound that reflects both properties of the patch-wise observation regime as well as the clustering signal and noise dependencies. We also validate our Cluster Quilting algorithm through extensive empirical studies on both simulated and real data sets in neuroscience and genomics, where it discovers more accurate and scientifically more plausible clusters than other approaches.</summary></entry><entry><title type="html">Computationally efficient multi-level Gaussian process regression for functional data observed under completely or partially regular sampling designs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ComputationallyefficientmultilevelGaussianprocessregressionforfunctionaldataobservedundercompletelyorpartiallyregularsamplingdesigns.html" rel="alternate" type="text/html" title="Computationally efficient multi-level Gaussian process regression for functional data observed under completely or partially regular sampling designs" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ComputationallyefficientmultilevelGaussianprocessregressionforfunctionaldataobservedundercompletelyorpartiallyregularsamplingdesigns</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ComputationallyefficientmultilevelGaussianprocessregressionforfunctionaldataobservedundercompletelyorpartiallyregularsamplingdesigns.html">&lt;p&gt;Gaussian process regression is a frequently used statistical method for flexible yet fully probabilistic non-linear regression modeling. A common obstacle is its computational complexity which scales poorly with the number of observations. This is especially an issue when applying Gaussian process models to multiple functions simultaneously in various applications of functional data analysis.
  We consider a multi-level Gaussian process regression model where a common mean function and individual subject-specific deviations are modeled simultaneously as latent Gaussian processes. We derive exact analytic and computationally efficient expressions for the log-likelihood function and the posterior distributions in the case where the observations are sampled on either a completely or partially regular grid. This enables us to fit the model to large data sets that are currently computationally inaccessible using a standard implementation. We show through a simulation study that our analytic expressions are several orders of magnitude faster compared to a standard implementation, and we provide an implementation in the probabilistic programming language Stan.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13691&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Adam Gorm Hoffmann, Claus Thorn Ekstr{\o}m, Andreas Kryger Jensen</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Gaussian process regression is a frequently used statistical method for flexible yet fully probabilistic non-linear regression modeling. A common obstacle is its computational complexity which scales poorly with the number of observations. This is especially an issue when applying Gaussian process models to multiple functions simultaneously in various applications of functional data analysis. We consider a multi-level Gaussian process regression model where a common mean function and individual subject-specific deviations are modeled simultaneously as latent Gaussian processes. We derive exact analytic and computationally efficient expressions for the log-likelihood function and the posterior distributions in the case where the observations are sampled on either a completely or partially regular grid. This enables us to fit the model to large data sets that are currently computationally inaccessible using a standard implementation. We show through a simulation study that our analytic expressions are several orders of magnitude faster compared to a standard implementation, and we provide an implementation in the probabilistic programming language Stan.</summary></entry><entry><title type="html">Coverage of Credible Sets for Regression under Variable Selection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/CoverageofCredibleSetsforRegressionunderVariableSelection.html" rel="alternate" type="text/html" title="Coverage of Credible Sets for Regression under Variable Selection" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/CoverageofCredibleSetsforRegressionunderVariableSelection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/CoverageofCredibleSetsforRegressionunderVariableSelection.html">&lt;p&gt;We study the asymptotic frequentist coverage of credible sets based on a novel Bayesian approach for a multiple linear regression model under variable selection. We initially ignore the issue of variable selection, which allows us to put a conjugate normal prior on the coefficient vector. The variable selection step is incorporated directly in the posterior through a sparsity-inducing map and uses the induced prior for making an inference instead of the natural conjugate posterior. The sparsity-inducing map minimizes the sum of the squared l2-distance weighted by the data matrix and a suitably scaled l1-penalty term. We obtain the limiting coverage of various credible regions and demonstrate that a modified credible interval for a component has the exact asymptotic frequentist coverage if the corresponding predictor is asymptotically uncorrelated with other predictors. Through extensive simulation, we provide a guideline for choosing the penalty parameter as a function of the credibility level appropriate for the corresponding coverage. We also show finite-sample numerical results that support the conclusions from the asymptotic theory. We also provide the credInt package that implements the method in R to obtain the credible intervals along with the posterior samples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13938&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Samhita Pal, Subhashis Ghosal</name></author><category term="stat.ME" /><summary type="html">We study the asymptotic frequentist coverage of credible sets based on a novel Bayesian approach for a multiple linear regression model under variable selection. We initially ignore the issue of variable selection, which allows us to put a conjugate normal prior on the coefficient vector. The variable selection step is incorporated directly in the posterior through a sparsity-inducing map and uses the induced prior for making an inference instead of the natural conjugate posterior. The sparsity-inducing map minimizes the sum of the squared l2-distance weighted by the data matrix and a suitably scaled l1-penalty term. We obtain the limiting coverage of various credible regions and demonstrate that a modified credible interval for a component has the exact asymptotic frequentist coverage if the corresponding predictor is asymptotically uncorrelated with other predictors. Through extensive simulation, we provide a guideline for choosing the penalty parameter as a function of the credibility level appropriate for the corresponding coverage. We also show finite-sample numerical results that support the conclusions from the asymptotic theory. We also provide the credInt package that implements the method in R to obtain the credible intervals along with the posterior samples.</summary></entry><entry><title type="html">Deep Optimal Experimental Design for Parameter Estimation Problems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/DeepOptimalExperimentalDesignforParameterEstimationProblems.html" rel="alternate" type="text/html" title="Deep Optimal Experimental Design for Parameter Estimation Problems" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/DeepOptimalExperimentalDesignforParameterEstimationProblems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/DeepOptimalExperimentalDesignforParameterEstimationProblems.html">&lt;p&gt;Optimal experimental design is a well studied field in applied science and engineering. Techniques for estimating such a design are commonly used within the framework of parameter estimation. Nonetheless, in recent years parameter estimation techniques are changing rapidly with the introduction of deep learning techniques to replace traditional estimation methods. This in turn requires the adaptation of optimal experimental design that is associated with these new techniques. In this paper we investigate a new experimental design methodology that uses deep learning. We show that the training of a network as a Likelihood Free Estimator can be used to significantly simplify the design process and circumvent the need for the computationally expensive bi-level optimization problem that is inherent in optimal experimental design for non-linear systems. Furthermore, deep design improves the quality of the recovery process for parameter estimation problems. As proof of concept we apply our methodology to two different systems of Ordinary Differential Equations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14003&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Md Shahriar Rahim Siddiqui, Arman Rahmim, Eldad Haber</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Optimal experimental design is a well studied field in applied science and engineering. Techniques for estimating such a design are commonly used within the framework of parameter estimation. Nonetheless, in recent years parameter estimation techniques are changing rapidly with the introduction of deep learning techniques to replace traditional estimation methods. This in turn requires the adaptation of optimal experimental design that is associated with these new techniques. In this paper we investigate a new experimental design methodology that uses deep learning. We show that the training of a network as a Likelihood Free Estimator can be used to significantly simplify the design process and circumvent the need for the computationally expensive bi-level optimization problem that is inherent in optimal experimental design for non-linear systems. Furthermore, deep design improves the quality of the recovery process for parameter estimation problems. As proof of concept we apply our methodology to two different systems of Ordinary Differential Equations.</summary></entry><entry><title type="html">Design-based variance estimation of the Hájek effect estimator in stratified and clustered experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/DesignbasedvarianceestimationoftheH%C3%A1jekeffectestimatorinstratifiedandclusteredexperiments.html" rel="alternate" type="text/html" title="Design-based variance estimation of the Hájek effect estimator in stratified and clustered experiments" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/DesignbasedvarianceestimationoftheH%C3%A1jekeffectestimatorinstratifiedandclusteredexperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/DesignbasedvarianceestimationoftheH%C3%A1jekeffectestimatorinstratifiedandclusteredexperiments.html">&lt;p&gt;Randomized controlled trials (RCTs) are used to evaluate treatment effects. When individuals are grouped together, clustered RCTs are conducted. Stratification is recommended to reduce imbalance of baseline covariates between treatment and control. In practice, this can lead to comparisons between clusters of very different sizes. As a result, direct adjustment estimators that average differences of means within the strata may be inconsistent. We study differences of inverse probability weighted means of a treatment and a control group – H&apos;ajek effect estimators – under two common forms of stratification: small strata that increase in number; or larger strata with growing numbers of clusters in each. Under either scenario, mild conditions give consistency and asymptotic Normality. We propose a variance estimator applicable to designs with any number of strata and strata of any size. We describe a special use of the variance estimator that improves small sample performance of Wald-type confidence intervals. The H&apos;ajek effect estimator lends itself to covariance adjustment, and our variance estimator remains applicable. Simulations and real-world applications in children’s nutrition and education confirm favorable operating characteristics, demonstrating advantages of the H&apos;ajek effect estimator beyond its simplicity and ease of use.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10473&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinhe Wang, Ben B. Hansen</name></author><category term="stat.ME" /><summary type="html">Randomized controlled trials (RCTs) are used to evaluate treatment effects. When individuals are grouped together, clustered RCTs are conducted. Stratification is recommended to reduce imbalance of baseline covariates between treatment and control. In practice, this can lead to comparisons between clusters of very different sizes. As a result, direct adjustment estimators that average differences of means within the strata may be inconsistent. We study differences of inverse probability weighted means of a treatment and a control group – H&apos;ajek effect estimators – under two common forms of stratification: small strata that increase in number; or larger strata with growing numbers of clusters in each. Under either scenario, mild conditions give consistency and asymptotic Normality. We propose a variance estimator applicable to designs with any number of strata and strata of any size. We describe a special use of the variance estimator that improves small sample performance of Wald-type confidence intervals. The H&apos;ajek effect estimator lends itself to covariance adjustment, and our variance estimator remains applicable. Simulations and real-world applications in children’s nutrition and education confirm favorable operating characteristics, demonstrating advantages of the H&apos;ajek effect estimator beyond its simplicity and ease of use.</summary></entry><entry><title type="html">Distance Covariance, Independence, and Pairwise Differences</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/DistanceCovarianceIndependenceandPairwiseDifferences.html" rel="alternate" type="text/html" title="Distance Covariance, Independence, and Pairwise Differences" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/DistanceCovarianceIndependenceandPairwiseDifferences</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/DistanceCovarianceIndependenceandPairwiseDifferences.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;(To appear in The American Statistician.) Distance covariance (Sz&apos;ekely, Rizzo, and Bakirov, 2007) is a fascinating recent notion, which is popular as a test for dependence of any type between random variables $X$ and $Y$. This approach deserves to be touched upon in modern courses on mathematical statistics. It makes use of distances of the type $&lt;/td&gt;
      &lt;td&gt;X-X’&lt;/td&gt;
      &lt;td&gt;$ and $&lt;/td&gt;
      &lt;td&gt;Y-Y’&lt;/td&gt;
      &lt;td&gt;$, where $(X’,Y’)$ is an independent copy of $(X,Y)$. This raises natural questions about independence of variables like $X-X’$ and $Y-Y’$, about the connection between Cov$(&lt;/td&gt;
      &lt;td&gt;X-X’&lt;/td&gt;
      &lt;td&gt;,&lt;/td&gt;
      &lt;td&gt;Y-Y’&lt;/td&gt;
      &lt;td&gt;)$ and the covariance between doubly centered distances, and about necessary and sufficient conditions for independence. We show some basic results and present a new and nontechnical counterexample to a common fallacy, which provides more insight. We also show some motivating examples involving bivariate distributions and contingency tables, which can be used as didactic material for introducing distance correlation.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13052&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jakob Raymaekers, Peter J. Rousseeuw</name></author><category term="stat.ME" /><summary type="html">(To appear in The American Statistician.) Distance covariance (Sz&apos;ekely, Rizzo, and Bakirov, 2007) is a fascinating recent notion, which is popular as a test for dependence of any type between random variables $X$ and $Y$. This approach deserves to be touched upon in modern courses on mathematical statistics. It makes use of distances of the type $ X-X’ $ and $ Y-Y’ $, where $(X’,Y’)$ is an independent copy of $(X,Y)$. This raises natural questions about independence of variables like $X-X’$ and $Y-Y’$, about the connection between Cov$( X-X’ , Y-Y’ )$ and the covariance between doubly centered distances, and about necessary and sufficient conditions for independence. We show some basic results and present a new and nontechnical counterexample to a common fallacy, which provides more insight. We also show some motivating examples involving bivariate distributions and contingency tables, which can be used as didactic material for introducing distance correlation.</summary></entry><entry><title type="html">Effect of Systematic Uncertainties on Density and Temperature Estimates in Coronae of Capella</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/EffectofSystematicUncertaintiesonDensityandTemperatureEstimatesinCoronaeofCapella.html" rel="alternate" type="text/html" title="Effect of Systematic Uncertainties on Density and Temperature Estimates in Coronae of Capella" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/EffectofSystematicUncertaintiesonDensityandTemperatureEstimatesinCoronaeofCapella</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/EffectofSystematicUncertaintiesonDensityandTemperatureEstimatesinCoronaeofCapella.html">&lt;p&gt;We estimate the coronal density of Capella using the O VII and Fe XVII line systems in the soft X-ray regime that have been observed over the course of the Chandra mission. Our analysis combines measures of error due to uncertainty in the underlying atomic data with statistical errors in the Chandra data to derive meaningful overall uncertainties on the plasma density of the coronae of Capella. We consider two Bayesian frameworks. First, the so-called pragmatic-Bayesian approach considers the atomic data and their uncertainties as fully specified and uncorrectable. The fully-Bayesian approach, on the other hand, allows the observed spectral data to update the atomic data and their uncertainties, thereby reducing the overall errors on the inferred parameters. To incorporate atomic data uncertainties, we obtain a set of atomic data replicates, the distribution of which captures their uncertainty. A principal component analysis of these replicates allows us to represent the atomic uncertainty with a lower-dimensional multivariate Gaussian distribution. A $t$-distribution approximation of the uncertainties of a subset of plasma parameters including a priori temperature information, obtained from the temperature-sensitive-only Fe XVII spectral line analysis, is carried forward into the density- and temperature-sensitive O VII spectral line analysis. Markov Chain Monte Carlo based model fitting is implemented including Multi-step Monte Carlo Gibbs Sampler and Hamiltonian Monte Carlo. Our analysis recovers an isothermally approximated coronal plasma temperature of $\approx$5 MK and a coronal plasma density of $\approx$10$^{10}$ cm$^{-3}$, with uncertainties of 0.1 and 0.2 dex respectively.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.10427&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xixi Yu, Vinay L. Kashyap, Giulio Del Zanna, David A. van Dyk, David C. Stenning, Connor P. Ballance, Harry P. Warren</name></author><category term="stat.ME" /><summary type="html">We estimate the coronal density of Capella using the O VII and Fe XVII line systems in the soft X-ray regime that have been observed over the course of the Chandra mission. Our analysis combines measures of error due to uncertainty in the underlying atomic data with statistical errors in the Chandra data to derive meaningful overall uncertainties on the plasma density of the coronae of Capella. We consider two Bayesian frameworks. First, the so-called pragmatic-Bayesian approach considers the atomic data and their uncertainties as fully specified and uncorrectable. The fully-Bayesian approach, on the other hand, allows the observed spectral data to update the atomic data and their uncertainties, thereby reducing the overall errors on the inferred parameters. To incorporate atomic data uncertainties, we obtain a set of atomic data replicates, the distribution of which captures their uncertainty. A principal component analysis of these replicates allows us to represent the atomic uncertainty with a lower-dimensional multivariate Gaussian distribution. A $t$-distribution approximation of the uncertainties of a subset of plasma parameters including a priori temperature information, obtained from the temperature-sensitive-only Fe XVII spectral line analysis, is carried forward into the density- and temperature-sensitive O VII spectral line analysis. Markov Chain Monte Carlo based model fitting is implemented including Multi-step Monte Carlo Gibbs Sampler and Hamiltonian Monte Carlo. Our analysis recovers an isothermally approximated coronal plasma temperature of $\approx$5 MK and a coronal plasma density of $\approx$10$^{10}$ cm$^{-3}$, with uncertainties of 0.1 and 0.2 dex respectively.</summary></entry><entry><title type="html">Enhancing multivariate post-processed visibility predictions utilizing CAMS forecasts</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/EnhancingmultivariatepostprocessedvisibilitypredictionsutilizingCAMSforecasts.html" rel="alternate" type="text/html" title="Enhancing multivariate post-processed visibility predictions utilizing CAMS forecasts" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/EnhancingmultivariatepostprocessedvisibilitypredictionsutilizingCAMSforecasts</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/EnhancingmultivariatepostprocessedvisibilitypredictionsutilizingCAMSforecasts.html">&lt;p&gt;In our contemporary era, meteorological weather forecasts increasingly incorporate ensemble predictions of visibility - a parameter of great importance in aviation, maritime navigation, and air quality assessment, with direct implications for public health. However, this weather variable falls short of the predictive accuracy achieved for other quantities issued by meteorological centers. Therefore, statistical post-processing is recommended to enhance the reliability and accuracy of predictions. By estimating the predictive distributions of the variables with the aid of historical observations and forecasts, one can achieve statistical consistency between true observations and ensemble predictions. Visibility observations, following the recommendation of the World Meteorological Organization, are typically reported in discrete values; hence, the predictive distribution of the weather quantity takes the form of a discrete parametric law. Recent studies demonstrated that the application of classification algorithms can successfully improve the skill of such discrete forecasts; however, a frequently emerging issue is that certain spatial and/or temporal dependencies could be lost between marginals. Based on visibility ensemble forecasts of the European Centre for Medium-Range Weather Forecasts for 30 locations in Central Europe, we investigate whether the inclusion of Copernicus Atmosphere Monitoring Service (CAMS) predictions of the same weather quantity as an additional covariate could enhance the skill of the post-processing methods and whether it contributes to the successful integration of spatial dependence between marginals. Our study confirms that post-processed forecasts are substantially superior to raw and climatological predictions, and the utilization of CAMS forecasts provides a further significant enhancement both in the univariate and multivariate setup.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14159&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mária Lakatos, Sándor Baran</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">In our contemporary era, meteorological weather forecasts increasingly incorporate ensemble predictions of visibility - a parameter of great importance in aviation, maritime navigation, and air quality assessment, with direct implications for public health. However, this weather variable falls short of the predictive accuracy achieved for other quantities issued by meteorological centers. Therefore, statistical post-processing is recommended to enhance the reliability and accuracy of predictions. By estimating the predictive distributions of the variables with the aid of historical observations and forecasts, one can achieve statistical consistency between true observations and ensemble predictions. Visibility observations, following the recommendation of the World Meteorological Organization, are typically reported in discrete values; hence, the predictive distribution of the weather quantity takes the form of a discrete parametric law. Recent studies demonstrated that the application of classification algorithms can successfully improve the skill of such discrete forecasts; however, a frequently emerging issue is that certain spatial and/or temporal dependencies could be lost between marginals. Based on visibility ensemble forecasts of the European Centre for Medium-Range Weather Forecasts for 30 locations in Central Europe, we investigate whether the inclusion of Copernicus Atmosphere Monitoring Service (CAMS) predictions of the same weather quantity as an additional covariate could enhance the skill of the post-processing methods and whether it contributes to the successful integration of spatial dependence between marginals. Our study confirms that post-processed forecasts are substantially superior to raw and climatological predictions, and the utilization of CAMS forecasts provides a further significant enhancement both in the univariate and multivariate setup.</summary></entry><entry><title type="html">Estimating Treatment Effects under Recommender Interference: A Structured Neural Networks Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/EstimatingTreatmentEffectsunderRecommenderInterferenceAStructuredNeuralNetworksApproach.html" rel="alternate" type="text/html" title="Estimating Treatment Effects under Recommender Interference: A Structured Neural Networks Approach" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/EstimatingTreatmentEffectsunderRecommenderInterferenceAStructuredNeuralNetworksApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/EstimatingTreatmentEffectsunderRecommenderInterferenceAStructuredNeuralNetworksApproach.html">&lt;p&gt;Recommender systems are essential for content-sharing platforms by curating personalized content. To evaluate updates of recommender systems targeting content creators, platforms frequently engage in creator-side randomized experiments to estimate treatment effect, defined as the difference in outcomes when a new (vs. the status quo) algorithm is deployed on the platform. We show that the standard difference-in-means estimator can lead to a biased treatment effect estimate. This bias arises because of recommender interference, which occurs when treated and control creators compete for exposure through the recommender system. We propose a “recommender choice model” that captures how an item is chosen among a pool comprised of both treated and control content items. By combining a structural choice model with neural networks, the framework directly models the interference pathway in a microfounded way while accounting for rich viewer-content heterogeneity. Using the model, we construct a double/debiased estimator of the treatment effect that is consistent and asymptotically normal. We demonstrate its empirical performance with a field experiment on Weixin short-video platform: besides the standard creator-side experiment, we carry out a costly blocked double-sided randomization design to obtain a benchmark estimate without interference bias. We show that the proposed estimator significantly reduces the bias in treatment effect estimates compared to the standard difference-in-means estimator.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14380&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ruohan Zhan, Shichao Han, Yuchen Hu, Zhenling Jiang</name></author><category term="stat.ME" /><summary type="html">Recommender systems are essential for content-sharing platforms by curating personalized content. To evaluate updates of recommender systems targeting content creators, platforms frequently engage in creator-side randomized experiments to estimate treatment effect, defined as the difference in outcomes when a new (vs. the status quo) algorithm is deployed on the platform. We show that the standard difference-in-means estimator can lead to a biased treatment effect estimate. This bias arises because of recommender interference, which occurs when treated and control creators compete for exposure through the recommender system. We propose a “recommender choice model” that captures how an item is chosen among a pool comprised of both treated and control content items. By combining a structural choice model with neural networks, the framework directly models the interference pathway in a microfounded way while accounting for rich viewer-content heterogeneity. Using the model, we construct a double/debiased estimator of the treatment effect that is consistent and asymptotically normal. We demonstrate its empirical performance with a field experiment on Weixin short-video platform: besides the standard creator-side experiment, we carry out a costly blocked double-sided randomization design to obtain a benchmark estimate without interference bias. We show that the proposed estimator significantly reduces the bias in treatment effect estimates compared to the standard difference-in-means estimator.</summary></entry><entry><title type="html">Evaluation of Missing Data Analytical Techniques in Longitudinal Research: Traditional and Machine Learning Approaches</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/EvaluationofMissingDataAnalyticalTechniquesinLongitudinalResearchTraditionalandMachineLearningApproaches.html" rel="alternate" type="text/html" title="Evaluation of Missing Data Analytical Techniques in Longitudinal Research: Traditional and Machine Learning Approaches" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/EvaluationofMissingDataAnalyticalTechniquesinLongitudinalResearchTraditionalandMachineLearningApproaches</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/EvaluationofMissingDataAnalyticalTechniquesinLongitudinalResearchTraditionalandMachineLearningApproaches.html">&lt;p&gt;Missing Not at Random (MNAR) and nonnormal data are challenging to handle. Traditional missing data analytical techniques such as full information maximum likelihood estimation (FIML) may fail with nonnormal data as they are built on normal distribution assumptions. Two-Stage Robust Estimation (TSRE) does manage nonnormal data, but both FIML and TSRE are less explored in longitudinal studies under MNAR conditions with nonnormal distributions. Unlike traditional statistical approaches, machine learning approaches do not require distributional assumptions about the data. More importantly, they have shown promise for MNAR data; however, their application in longitudinal studies, addressing both Missing at Random (MAR) and MNAR scenarios, is also underexplored. This study utilizes Monte Carlo simulations to assess and compare the effectiveness of six analytical techniques for missing data within the growth curve modeling framework. These techniques include traditional approaches like FIML and TSRE, machine learning approaches by single imputation (K-Nearest Neighbors and missForest), and machine learning approaches by multiple imputation (micecart and miceForest). We investigate the influence of sample size, missing data rate, missing data mechanism, and data distribution on the accuracy and efficiency of model estimation. Our findings indicate that FIML is most effective for MNAR data among the tested approaches. TSRE excels in handling MAR data, while missForest is only advantageous in limited conditions with a combination of very skewed distributions, very large sample sizes (e.g., n larger than 1000), and low missing data rates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13814&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Dandan Tang, Xin Tong</name></author><category term="stat.AP," /><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Missing Not at Random (MNAR) and nonnormal data are challenging to handle. Traditional missing data analytical techniques such as full information maximum likelihood estimation (FIML) may fail with nonnormal data as they are built on normal distribution assumptions. Two-Stage Robust Estimation (TSRE) does manage nonnormal data, but both FIML and TSRE are less explored in longitudinal studies under MNAR conditions with nonnormal distributions. Unlike traditional statistical approaches, machine learning approaches do not require distributional assumptions about the data. More importantly, they have shown promise for MNAR data; however, their application in longitudinal studies, addressing both Missing at Random (MAR) and MNAR scenarios, is also underexplored. This study utilizes Monte Carlo simulations to assess and compare the effectiveness of six analytical techniques for missing data within the growth curve modeling framework. These techniques include traditional approaches like FIML and TSRE, machine learning approaches by single imputation (K-Nearest Neighbors and missForest), and machine learning approaches by multiple imputation (micecart and miceForest). We investigate the influence of sample size, missing data rate, missing data mechanism, and data distribution on the accuracy and efficiency of model estimation. Our findings indicate that FIML is most effective for MNAR data among the tested approaches. TSRE excels in handling MAR data, while missForest is only advantageous in limited conditions with a combination of very skewed distributions, very large sample sizes (e.g., n larger than 1000), and low missing data rates.</summary></entry><entry><title type="html">Existence and approximation of densities of chord length- and cross section area distributions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Existenceandapproximationofdensitiesofchordlengthandcrosssectionareadistributions.html" rel="alternate" type="text/html" title="Existence and approximation of densities of chord length- and cross section area distributions" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Existenceandapproximationofdensitiesofchordlengthandcrosssectionareadistributions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Existenceandapproximationofdensitiesofchordlengthandcrosssectionareadistributions.html">&lt;p&gt;In various stereological problems an $n$-dimensional convex body is intersected with an $(n-1)$-dimensional Isotropic Uniformly Random (IUR) hyperplane. In this paper the cumulative distribution function associated with the $(n-1)$-dimensional volume of such a random section is studied. This distribution is also known as chord length distribution and cross section area distribution in the planar and spatial case respectively. For various classes of convex bodies it is shown that these distribution functions are absolutely continuous with respect to Lebesgue measure. A Monte Carlo simulation scheme is proposed for approximating the corresponding probability density functions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.02864&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thomas van der Jagt, Geurt Jongbloed, Martina Vittorietti</name></author><category term="stat.AP" /><summary type="html">In various stereological problems an $n$-dimensional convex body is intersected with an $(n-1)$-dimensional Isotropic Uniformly Random (IUR) hyperplane. In this paper the cumulative distribution function associated with the $(n-1)$-dimensional volume of such a random section is studied. This distribution is also known as chord length distribution and cross section area distribution in the planar and spatial case respectively. For various classes of convex bodies it is shown that these distribution functions are absolutely continuous with respect to Lebesgue measure. A Monte Carlo simulation scheme is proposed for approximating the corresponding probability density functions.</summary></entry><entry><title type="html">From isotonic to Lipschitz regression: a new interpolative perspective on shape-restricted estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/FromisotonictoLipschitzregressionanewinterpolativeperspectiveonshaperestrictedestimation.html" rel="alternate" type="text/html" title="From isotonic to Lipschitz regression: a new interpolative perspective on shape-restricted estimation" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/FromisotonictoLipschitzregressionanewinterpolativeperspectiveonshaperestrictedestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/FromisotonictoLipschitzregressionanewinterpolativeperspectiveonshaperestrictedestimation.html">&lt;p&gt;This manuscript seeks to bridge two seemingly disjoint paradigms of nonparametric regression estimation based on smoothness assumptions and shape constraints. The proposed approach is motivated by a conceptually simple observation: Every Lipschitz function is a sum of monotonic and linear functions. This principle is further generalized to the higher-order monotonicity and multivariate covariates. A family of estimators is proposed based on a sample-splitting procedure, which inherits desirable methodological, theoretical, and computational properties of shape-restricted estimators. Our theoretical analysis provides convergence guarantees of the estimator under heteroscedastic and heavy-tailed errors, as well as adaptive properties to the complexity of the true regression function. The generality of the proposed decomposition framework is demonstrated through new approximation results, and extensive numerical studies validate the theoretical properties and empirical evidence for the practicalities of the proposed estimation framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.05732&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kenta Takatsu, Tianyu Zhang, Arun Kumar Kuchibhotla</name></author><category term="stat.ME" /><summary type="html">This manuscript seeks to bridge two seemingly disjoint paradigms of nonparametric regression estimation based on smoothness assumptions and shape constraints. The proposed approach is motivated by a conceptually simple observation: Every Lipschitz function is a sum of monotonic and linear functions. This principle is further generalized to the higher-order monotonicity and multivariate covariates. A family of estimators is proposed based on a sample-splitting procedure, which inherits desirable methodological, theoretical, and computational properties of shape-restricted estimators. Our theoretical analysis provides convergence guarantees of the estimator under heteroscedastic and heavy-tailed errors, as well as adaptive properties to the complexity of the true regression function. The generality of the proposed decomposition framework is demonstrated through new approximation results, and extensive numerical studies validate the theoretical properties and empirical evidence for the practicalities of the proposed estimation framework.</summary></entry><entry><title type="html">Functional Principal Component Analysis for Distribution-Valued Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/FunctionalPrincipalComponentAnalysisforDistributionValuedProcesses.html" rel="alternate" type="text/html" title="Functional Principal Component Analysis for Distribution-Valued Processes" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/FunctionalPrincipalComponentAnalysisforDistributionValuedProcesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/FunctionalPrincipalComponentAnalysisforDistributionValuedProcesses.html">&lt;p&gt;We develop statistical models for samples of distribution-valued stochastic processes featuring time-indexed univariate distributions, with emphasis on functional principal component analysis. The proposed model presents an intrinsic rather than transformation-based approach. The starting point is a transport process representation for distribution-valued processes under the Wasserstein metric. Substituting transports for distributions addresses the challenge of centering distribution-valued processes and leads to a useful and interpretable decomposition of each realized process into a process-specific single transport and a real-valued trajectory. This representation makes it possible to utilize a scalar multiplication operation for transports and facilitates not only functional principal component analysis but also to introduce a latent Gaussian process. This Gaussian process proves especially useful for the case where the distribution-valued processes are only observed on a sparse grid of time points, establishing an approach for longitudinal distribution-valued data. We study the convergence of the key components of this novel representation to their population targets and demonstrate the practical utility of the proposed approach through simulations and several data illustrations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.20088&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hang Zhou, Hans-Georg Müller</name></author><category term="stat.ME" /><summary type="html">We develop statistical models for samples of distribution-valued stochastic processes featuring time-indexed univariate distributions, with emphasis on functional principal component analysis. The proposed model presents an intrinsic rather than transformation-based approach. The starting point is a transport process representation for distribution-valued processes under the Wasserstein metric. Substituting transports for distributions addresses the challenge of centering distribution-valued processes and leads to a useful and interpretable decomposition of each realized process into a process-specific single transport and a real-valued trajectory. This representation makes it possible to utilize a scalar multiplication operation for transports and facilitates not only functional principal component analysis but also to introduce a latent Gaussian process. This Gaussian process proves especially useful for the case where the distribution-valued processes are only observed on a sparse grid of time points, establishing an approach for longitudinal distribution-valued data. We study the convergence of the key components of this novel representation to their population targets and demonstrate the practical utility of the proposed approach through simulations and several data illustrations.</summary></entry><entry><title type="html">Generalization error of min-norm interpolators in transfer learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Generalizationerrorofminnorminterpolatorsintransferlearning.html" rel="alternate" type="text/html" title="Generalization error of min-norm interpolators in transfer learning" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Generalizationerrorofminnorminterpolatorsintransferlearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Generalizationerrorofminnorminterpolatorsintransferlearning.html">&lt;p&gt;This paper establishes the generalization error of pooled min-$\ell_2$-norm interpolation in transfer learning where data from diverse distributions are available. Min-norm interpolators emerge naturally as implicit regularized limits of modern machine learning algorithms. Previous work characterized their out-of-distribution risk when samples from the test distribution are unavailable during training. However, in many applications, a limited amount of test data may be available during training, yet properties of min-norm interpolation in this setting are not well-understood. We address this gap by characterizing the bias and variance of pooled min-$\ell_2$-norm interpolation under covariate and model shifts. The pooled interpolator captures both early fusion and a form of intermediate fusion. Our results have several implications: under model shift, for low signal-to-noise ratio (SNR), adding data always hurts. For higher SNR, transfer learning helps as long as the shift-to-signal (SSR) ratio lies below a threshold that we characterize explicitly. By consistently estimating these ratios, we provide a data-driven method to determine: (i) when the pooled interpolator outperforms the target-based interpolator, and (ii) the optimal number of target samples that minimizes the generalization error. Under covariate shift, if the source sample size is small relative to the dimension, heterogeneity between between domains improves the risk, and vice versa. We establish a novel anisotropic local law to achieve these characterizations, which may be of independent interest in random matrix theory. We supplement our theoretical characterizations with comprehensive simulations that demonstrate the finite-sample efficacy of our results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13944&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yanke Song, Sohom Bhattacharya, Pragya Sur</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">This paper establishes the generalization error of pooled min-$\ell_2$-norm interpolation in transfer learning where data from diverse distributions are available. Min-norm interpolators emerge naturally as implicit regularized limits of modern machine learning algorithms. Previous work characterized their out-of-distribution risk when samples from the test distribution are unavailable during training. However, in many applications, a limited amount of test data may be available during training, yet properties of min-norm interpolation in this setting are not well-understood. We address this gap by characterizing the bias and variance of pooled min-$\ell_2$-norm interpolation under covariate and model shifts. The pooled interpolator captures both early fusion and a form of intermediate fusion. Our results have several implications: under model shift, for low signal-to-noise ratio (SNR), adding data always hurts. For higher SNR, transfer learning helps as long as the shift-to-signal (SSR) ratio lies below a threshold that we characterize explicitly. By consistently estimating these ratios, we provide a data-driven method to determine: (i) when the pooled interpolator outperforms the target-based interpolator, and (ii) the optimal number of target samples that minimizes the generalization error. Under covariate shift, if the source sample size is small relative to the dimension, heterogeneity between between domains improves the risk, and vice versa. We establish a novel anisotropic local law to achieve these characterizations, which may be of independent interest in random matrix theory. We supplement our theoretical characterizations with comprehensive simulations that demonstrate the finite-sample efficacy of our results.</summary></entry><entry><title type="html">Gradient-Boosted Generalized Linear Models for Conditional Vine Copulas</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/GradientBoostedGeneralizedLinearModelsforConditionalVineCopulas.html" rel="alternate" type="text/html" title="Gradient-Boosted Generalized Linear Models for Conditional Vine Copulas" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/GradientBoostedGeneralizedLinearModelsforConditionalVineCopulas</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/GradientBoostedGeneralizedLinearModelsforConditionalVineCopulas.html">&lt;p&gt;Vine copulas are flexible dependence models using bivariate copulas as building blocks. If the parameters of the bivariate copulas in the vine copula depend on covariates, one obtains a conditional vine copula. We propose an extension for the estimation of continuous conditional vine copulas, where the parameters of continuous conditional bivariate copulas are estimated sequentially and separately via gradient-boosting. For this purpose, we link covariates via generalized linear models (GLMs) to Kendall’s $\tau$ correlation coefficient from which the corresponding copula parameter can be obtained. Consequently, the gradient-boosting algorithm estimates the copula parameters providing a natural covariate selection. In a second step, an additional covariate deselection procedure is applied. The performance of the gradient-boosted conditional vine copulas is illustrated in a simulation study. Linear covariate effects in low- and high-dimensional settings are investigated for the conditional bivariate copulas separately and for conditional vine copulas. Moreover, the gradient-boosted conditional vine copulas are applied to the temporal postprocessing of ensemble weather forecasts in a low-dimensional setting. The results show, that our suggested method is able to outperform the benchmark methods and identifies temporal correlations better. Eventually, we provide an R-package called boostCopula for this method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13500&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David Jobst, Annette Möller, Jürgen Gro{\ss}</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Vine copulas are flexible dependence models using bivariate copulas as building blocks. If the parameters of the bivariate copulas in the vine copula depend on covariates, one obtains a conditional vine copula. We propose an extension for the estimation of continuous conditional vine copulas, where the parameters of continuous conditional bivariate copulas are estimated sequentially and separately via gradient-boosting. For this purpose, we link covariates via generalized linear models (GLMs) to Kendall’s $\tau$ correlation coefficient from which the corresponding copula parameter can be obtained. Consequently, the gradient-boosting algorithm estimates the copula parameters providing a natural covariate selection. In a second step, an additional covariate deselection procedure is applied. The performance of the gradient-boosted conditional vine copulas is illustrated in a simulation study. Linear covariate effects in low- and high-dimensional settings are investigated for the conditional bivariate copulas separately and for conditional vine copulas. Moreover, the gradient-boosted conditional vine copulas are applied to the temporal postprocessing of ensemble weather forecasts in a low-dimensional setting. The results show, that our suggested method is able to outperform the benchmark methods and identifies temporal correlations better. Eventually, we provide an R-package called boostCopula for this method.</summary></entry><entry><title type="html">Gradient Estimation via Differentiable Metropolis-Hastings</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/GradientEstimationviaDifferentiableMetropolisHastings.html" rel="alternate" type="text/html" title="Gradient Estimation via Differentiable Metropolis-Hastings" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/GradientEstimationviaDifferentiableMetropolisHastings</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/GradientEstimationviaDifferentiableMetropolisHastings.html">&lt;p&gt;Metropolis-Hastings estimates intractable expectations - can differentiating the algorithm estimate their gradients? The challenge is that Metropolis-Hastings trajectories are not conventionally differentiable due to the discrete accept/reject steps. Using a technique based on recoupling chains, our method differentiates through the Metropolis-Hastings sampler itself, allowing us to estimate gradients with respect to a parameter of otherwise intractable expectations. Our main contribution is a proof of strong consistency and a central limit theorem for our estimator under assumptions that hold in common Bayesian inference problems. The proofs augment the sampler chain with latent information, and formulate the estimator as a stopping tail functional of this augmented chain. We demonstrate our method on examples of Bayesian sensitivity analysis and optimizing a random walk Metropolis proposal.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14451&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Gaurav Arya, Moritz Schauer, Ruben Seyer</name></author><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Metropolis-Hastings estimates intractable expectations - can differentiating the algorithm estimate their gradients? The challenge is that Metropolis-Hastings trajectories are not conventionally differentiable due to the discrete accept/reject steps. Using a technique based on recoupling chains, our method differentiates through the Metropolis-Hastings sampler itself, allowing us to estimate gradients with respect to a parameter of otherwise intractable expectations. Our main contribution is a proof of strong consistency and a central limit theorem for our estimator under assumptions that hold in common Bayesian inference problems. The proofs augment the sampler chain with latent information, and formulate the estimator as a stopping tail functional of this augmented chain. We demonstrate our method on examples of Bayesian sensitivity analysis and optimizing a random walk Metropolis proposal.</summary></entry><entry><title type="html">Hierarchical Regression Discontinuity Design: Pursuing Subgroup Treatment Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/HierarchicalRegressionDiscontinuityDesignPursuingSubgroupTreatmentEffects.html" rel="alternate" type="text/html" title="Hierarchical Regression Discontinuity Design: Pursuing Subgroup Treatment Effects" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/HierarchicalRegressionDiscontinuityDesignPursuingSubgroupTreatmentEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/HierarchicalRegressionDiscontinuityDesignPursuingSubgroupTreatmentEffects.html">&lt;p&gt;Regression discontinuity design (RDD) is widely adopted for causal inference under intervention determined by a continuous variable. While one is interested in treatment effect heterogeneity by subgroups in many applications, RDD typically suffers from small subgroup-wise sample sizes, which makes the estimation results highly instable. To solve this issue, we introduce hierarchical RDD (HRDD), a hierarchical Bayes approach for pursuing treatment effect heterogeneity in RDD. A key feature of HRDD is to employ a pseudo-model based on a loss function to estimate subgroup-level parameters of treatment effects under RDD, and assign a hierarchical prior distribution to ‘‘borrow strength’’ from other subgroups. The posterior computation can be easily done by a simple Gibbs sampling, and the optimal bandwidth can be automatically selected by the Hyv&quot;{a}rinen scores for unnormalized models. We demonstrate the proposed HRDD through simulation and real data analysis, and show that HRDD provides much more stable point and interval estimation than separately applying the standard RDD method to each subgroup.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.01404&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shonosuke Sugasawa, Takuya Ishihara, Daisuke Kurisu</name></author><category term="stat.ME" /><summary type="html">Regression discontinuity design (RDD) is widely adopted for causal inference under intervention determined by a continuous variable. While one is interested in treatment effect heterogeneity by subgroups in many applications, RDD typically suffers from small subgroup-wise sample sizes, which makes the estimation results highly instable. To solve this issue, we introduce hierarchical RDD (HRDD), a hierarchical Bayes approach for pursuing treatment effect heterogeneity in RDD. A key feature of HRDD is to employ a pseudo-model based on a loss function to estimate subgroup-level parameters of treatment effects under RDD, and assign a hierarchical prior distribution to ‘‘borrow strength’’ from other subgroups. The posterior computation can be easily done by a simple Gibbs sampling, and the optimal bandwidth can be automatically selected by the Hyv&quot;{a}rinen scores for unnormalized models. We demonstrate the proposed HRDD through simulation and real data analysis, and show that HRDD provides much more stable point and interval estimation than separately applying the standard RDD method to each subgroup.</summary></entry><entry><title type="html">Imputation of missing values in multi-view data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Imputationofmissingvaluesinmultiviewdata.html" rel="alternate" type="text/html" title="Imputation of missing values in multi-view data" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Imputationofmissingvaluesinmultiviewdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Imputationofmissingvaluesinmultiviewdata.html">&lt;p&gt;Data for which a set of objects is described by multiple distinct feature sets (called views) is known as multi-view data. When missing values occur in multi-view data, all features in a view are likely to be missing simultaneously. This may lead to very large quantities of missing data which, especially when combined with high-dimensionality, can make the application of conditional imputation methods computationally infeasible. However, the multi-view structure could be leveraged to reduce the complexity and computational load of imputation. We introduce a new imputation method based on the existing stacked penalized logistic regression (StaPLR) algorithm for multi-view learning. It performs imputation in a dimension-reduced space to address computational challenges inherent to the multi-view context. We compare the performance of the new imputation method with several existing imputation algorithms in simulated data sets and a real data application. The results show that the new imputation method leads to competitive results at a much lower computational cost, and makes the use of advanced imputation algorithms such as missForest and predictive mean matching possible in settings where they would otherwise be computationally infeasible.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.14484&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wouter van Loon, Marjolein Fokkema, Frank de Vos, Marisa Koini, Reinhold Schmidt, Mark de Rooij</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Data for which a set of objects is described by multiple distinct feature sets (called views) is known as multi-view data. When missing values occur in multi-view data, all features in a view are likely to be missing simultaneously. This may lead to very large quantities of missing data which, especially when combined with high-dimensionality, can make the application of conditional imputation methods computationally infeasible. However, the multi-view structure could be leveraged to reduce the complexity and computational load of imputation. We introduce a new imputation method based on the existing stacked penalized logistic regression (StaPLR) algorithm for multi-view learning. It performs imputation in a dimension-reduced space to address computational challenges inherent to the multi-view context. We compare the performance of the new imputation method with several existing imputation algorithms in simulated data sets and a real data application. The results show that the new imputation method leads to competitive results at a much lower computational cost, and makes the use of advanced imputation algorithms such as missForest and predictive mean matching possible in settings where they would otherwise be computationally infeasible.</summary></entry><entry><title type="html">Integrating time-resolved $nrf2$ gene-expression data into a full GUTS model as a proxy for toxicodynamic damage in zebrafish embryo</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Integratingtimeresolvednrf2geneexpressiondataintoafullGUTSmodelasaproxyfortoxicodynamicdamageinzebrafishembryo.html" rel="alternate" type="text/html" title="Integrating time-resolved $nrf2$ gene-expression data into a full GUTS model as a proxy for toxicodynamic damage in zebrafish embryo" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Integratingtimeresolvednrf2geneexpressiondataintoafullGUTSmodelasaproxyfortoxicodynamicdamageinzebrafishembryo</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Integratingtimeresolvednrf2geneexpressiondataintoafullGUTSmodelasaproxyfortoxicodynamicdamageinzebrafishembryo.html">&lt;p&gt;The immense production of the chemical industry requires an improved predictive risk assessment that can handle constantly evolving challenges while reducing the dependency of risk assessment on animal testing. Integrating ‘omics data into mechanistic models offers a promising solution by linking cellular processes triggered after chemical exposure with observed effects in the organism. With the emerging availability of time-resolved RNA data, the goal of integrating gene expression data into mechanistic models can be approached. We propose a biologically anchored TKTD model, which describes key processes that link the gene expression level of the stress regulator $nrf2$ to detoxification and lethality by associating toxicodynamic damage with $nrf2$ expression. Fitting such a model to complex datasets consisting of multiple endpoints required the combination of methods from molecular biology, mechanistic dynamic systems modeling and Bayesian inference. In this study we successfully integrate time-resolved gene expression data into TKTD models, and thus provide a method for assessing the influence of molecular markers on survival. This novel method was used to test whether, $nrf2$, can be applied to predict lethality in zebrafish embryos. With the presented approach we outline a method to successively approach the goal of a predictive risk assessment based on molecular data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12949&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Florian Schunck, Bernhard Kodritsch, Wibke Busch, Martin Krauss, Andreas Focks</name></author><category term="stat.AP" /><summary type="html">The immense production of the chemical industry requires an improved predictive risk assessment that can handle constantly evolving challenges while reducing the dependency of risk assessment on animal testing. Integrating ‘omics data into mechanistic models offers a promising solution by linking cellular processes triggered after chemical exposure with observed effects in the organism. With the emerging availability of time-resolved RNA data, the goal of integrating gene expression data into mechanistic models can be approached. We propose a biologically anchored TKTD model, which describes key processes that link the gene expression level of the stress regulator $nrf2$ to detoxification and lethality by associating toxicodynamic damage with $nrf2$ expression. Fitting such a model to complex datasets consisting of multiple endpoints required the combination of methods from molecular biology, mechanistic dynamic systems modeling and Bayesian inference. In this study we successfully integrate time-resolved gene expression data into TKTD models, and thus provide a method for assessing the influence of molecular markers on survival. This novel method was used to test whether, $nrf2$, can be applied to predict lethality in zebrafish embryos. With the presented approach we outline a method to successively approach the goal of a predictive risk assessment based on molecular data.</summary></entry><entry><title type="html">Lazy Data Practices Harm Fairness Research</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/LazyDataPracticesHarmFairnessResearch.html" rel="alternate" type="text/html" title="Lazy Data Practices Harm Fairness Research" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/LazyDataPracticesHarmFairnessResearch</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/LazyDataPracticesHarmFairnessResearch.html">&lt;p&gt;Data practices shape research and practice on fairness in machine learning (fair ML). Critical data studies offer important reflections and critiques for the responsible advancement of the field by highlighting shortcomings and proposing recommendations for improvement. In this work, we present a comprehensive analysis of fair ML datasets, demonstrating how unreflective yet common practices hinder the reach and reliability of algorithmic fairness findings. We systematically study protected information encoded in tabular datasets and their usage in 280 experiments across 142 publications.
  Our analyses identify three main areas of concern: (1) a \textbf{lack of representation for certain protected attributes} in both data and evaluations; (2) the widespread \textbf{exclusion of minorities} during data preprocessing; and (3) \textbf{opaque data processing} threatening the generalization of fairness research. By conducting exemplary analyses on the utilization of prominent datasets, we demonstrate how unreflective data decisions disproportionately affect minority groups, fairness metrics, and resultant model comparisons. Additionally, we identify supplementary factors such as limitations in publicly available data, privacy considerations, and a general lack of awareness, which exacerbate these challenges. To address these issues, we propose a set of recommendations for data usage in fairness research centered on transparency and responsible inclusion. This study underscores the need for a critical reevaluation of data practices in fair ML and offers directions to improve both the sourcing and usage of datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17293&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jan Simson, Alessandro Fabris, Christoph Kern</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Data practices shape research and practice on fairness in machine learning (fair ML). Critical data studies offer important reflections and critiques for the responsible advancement of the field by highlighting shortcomings and proposing recommendations for improvement. In this work, we present a comprehensive analysis of fair ML datasets, demonstrating how unreflective yet common practices hinder the reach and reliability of algorithmic fairness findings. We systematically study protected information encoded in tabular datasets and their usage in 280 experiments across 142 publications. Our analyses identify three main areas of concern: (1) a \textbf{lack of representation for certain protected attributes} in both data and evaluations; (2) the widespread \textbf{exclusion of minorities} during data preprocessing; and (3) \textbf{opaque data processing} threatening the generalization of fairness research. By conducting exemplary analyses on the utilization of prominent datasets, we demonstrate how unreflective data decisions disproportionately affect minority groups, fairness metrics, and resultant model comparisons. Additionally, we identify supplementary factors such as limitations in publicly available data, privacy considerations, and a general lack of awareness, which exacerbate these challenges. To address these issues, we propose a set of recommendations for data usage in fairness research centered on transparency and responsible inclusion. This study underscores the need for a critical reevaluation of data practices in fair ML and offers directions to improve both the sourcing and usage of datasets.</summary></entry><entry><title type="html">Markov Chain Monte Carlo Significance Tests</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/MarkovChainMonteCarloSignificanceTests.html" rel="alternate" type="text/html" title="Markov Chain Monte Carlo Significance Tests" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/MarkovChainMonteCarloSignificanceTests</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/MarkovChainMonteCarloSignificanceTests.html">&lt;p&gt;Monte Carlo significance tests are a general tool that produce p-values by generating samples from the null distribution. However, Monte Carlo tests are limited to null hypothesis which we can exactly sample from. Markov chain Monte Carlo (MCMC) significance tests are a way to produce statistical valid p-values for null hypothesis we can only approximately sample from. These methods were first introduced by Besag and Clifford in 1989 and make no assumptions on the mixing time of the MCMC procedure. Here we review the two methods of Besag and Clifford and introduce a new method that unifies the existing procedures. We use simple examples to highlight the difference between MCMC significance tests and standard Monte Carlo tests based on exact sampling. We also survey a range of contemporary applications in the literature including goodness-of-fit testing for the Rasch model, tests for detecting gerrymandering [8] and a permutation based test of conditional independence [3].&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.04924&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael Howes</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Monte Carlo significance tests are a general tool that produce p-values by generating samples from the null distribution. However, Monte Carlo tests are limited to null hypothesis which we can exactly sample from. Markov chain Monte Carlo (MCMC) significance tests are a way to produce statistical valid p-values for null hypothesis we can only approximately sample from. These methods were first introduced by Besag and Clifford in 1989 and make no assumptions on the mixing time of the MCMC procedure. Here we review the two methods of Besag and Clifford and introduce a new method that unifies the existing procedures. We use simple examples to highlight the difference between MCMC significance tests and standard Monte Carlo tests based on exact sampling. We also survey a range of contemporary applications in the literature including goodness-of-fit testing for the Rasch model, tests for detecting gerrymandering [8] and a permutation based test of conditional independence [3].</summary></entry><entry><title type="html">Mastering Rare Event Analysis: Optimal Subsample Size in Logistic and Cox Regressions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/MasteringRareEventAnalysisOptimalSubsampleSizeinLogisticandCoxRegressions.html" rel="alternate" type="text/html" title="Mastering Rare Event Analysis: Optimal Subsample Size in Logistic and Cox Regressions" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/MasteringRareEventAnalysisOptimalSubsampleSizeinLogisticandCoxRegressions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/MasteringRareEventAnalysisOptimalSubsampleSizeinLogisticandCoxRegressions.html">&lt;p&gt;In the realm of contemporary data analysis, the use of massive datasets has taken on heightened significance, albeit often entailing considerable demands on computational time and memory. While a multitude of existing works offer optimal subsampling methods for conducting analyses on subsamples with minimized efficiency loss, they notably lack tools for judiciously selecting the optimal subsample size. To bridge this gap, our work introduces tools designed for choosing the optimal subsample size. We focus on three settings: the Cox regression model for survival data with rare events and logistic regression for both balanced and imbalanced datasets. Additionally, we present a novel optimal subsampling procedure tailored for logistic regression with imbalanced data. The efficacy of these tools and procedures is demonstrated through an extensive simulation study and meticulous analyses of two sizable datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13836&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tal Agassi, Nir Keret, Malka Gorfine</name></author><category term="stat.ME" /><summary type="html">In the realm of contemporary data analysis, the use of massive datasets has taken on heightened significance, albeit often entailing considerable demands on computational time and memory. While a multitude of existing works offer optimal subsampling methods for conducting analyses on subsamples with minimized efficiency loss, they notably lack tools for judiciously selecting the optimal subsample size. To bridge this gap, our work introduces tools designed for choosing the optimal subsample size. We focus on three settings: the Cox regression model for survival data with rare events and logistic regression for both balanced and imbalanced datasets. Additionally, we present a novel optimal subsampling procedure tailored for logistic regression with imbalanced data. The efficacy of these tools and procedures is demonstrated through an extensive simulation study and meticulous analyses of two sizable datasets.</summary></entry><entry><title type="html">Multinomial Link Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/MultinomialLinkModels.html" rel="alternate" type="text/html" title="Multinomial Link Models" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/MultinomialLinkModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/MultinomialLinkModels.html">&lt;p&gt;We propose a unified multinomial link model for analyzing categorical responses. It not only covers the existing multinomial logistic models and their extensions as special cases, but also includes new models that can incorporate the observations with NA or Unknown responses in the data analysis. We provide explicit formulae and detailed algorithms for finding the maximum likelihood estimates of the model parameters and computing the Fisher information matrix. Our algorithms solve the infeasibility issue of existing statistical software on estimating parameters of cumulative link models. The applications to real datasets show that the new models can fit the data significantly better, and the corresponding data analysis may correct the misleading conclusions due to missing responses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.16260&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tianmeng Wang, Liping Tong, Jie Yang</name></author><category term="stat.ME" /><summary type="html">We propose a unified multinomial link model for analyzing categorical responses. It not only covers the existing multinomial logistic models and their extensions as special cases, but also includes new models that can incorporate the observations with NA or Unknown responses in the data analysis. We provide explicit formulae and detailed algorithms for finding the maximum likelihood estimates of the model parameters and computing the Fisher information matrix. Our algorithms solve the infeasibility issue of existing statistical software on estimating parameters of cumulative link models. The applications to real datasets show that the new models can fit the data significantly better, and the corresponding data analysis may correct the misleading conclusions due to missing responses.</summary></entry><entry><title type="html">Multivariate Bayesian dynamic modeling for causal prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/MultivariateBayesiandynamicmodelingforcausalprediction.html" rel="alternate" type="text/html" title="Multivariate Bayesian dynamic modeling for causal prediction" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/MultivariateBayesiandynamicmodelingforcausalprediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/MultivariateBayesiandynamicmodelingforcausalprediction.html">&lt;p&gt;Bayesian forecasting is developed in multivariate time series analysis for causal inference. Causal evaluation of sequentially observed time series data from control and treated units focuses on the impacts of interventions using contemporaneous outcomes in control units. Methodological developments here concern multivariate dynamic models for time-varying effects across multiple treated units with explicit foci on sequential learning and aggregation of intervention effects. Analysis explores dimension reduction across multiple synthetic counterfactual predictors. Computational advances leverage fully conjugate models for efficient sequential learning and inference, including cross-unit correlations and their time variation. This allows full uncertainty quantification on model hyper-parameters via Bayesian model averaging. A detailed case study evaluates interventions in a supermarket promotions experiment, with coupled predictive analyses in selected regions of a large-scale commercial system. Comparisons with existing methods highlight the issues of appropriate uncertainty quantification in casual inference in aggregation across treated units, among other practical concerns.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.03200&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Graham Tierney, Christoph Hellmayr, Greg Barkimer, Kevin Li, Mike West</name></author><category term="stat.ME" /><summary type="html">Bayesian forecasting is developed in multivariate time series analysis for causal inference. Causal evaluation of sequentially observed time series data from control and treated units focuses on the impacts of interventions using contemporaneous outcomes in control units. Methodological developments here concern multivariate dynamic models for time-varying effects across multiple treated units with explicit foci on sequential learning and aggregation of intervention effects. Analysis explores dimension reduction across multiple synthetic counterfactual predictors. Computational advances leverage fully conjugate models for efficient sequential learning and inference, including cross-unit correlations and their time variation. This allows full uncertainty quantification on model hyper-parameters via Bayesian model averaging. A detailed case study evaluates interventions in a supermarket promotions experiment, with coupled predictive analyses in selected regions of a large-scale commercial system. Comparisons with existing methods highlight the issues of appropriate uncertainty quantification in casual inference in aggregation across treated units, among other practical concerns.</summary></entry><entry><title type="html">Nonparametric Motion Control in Functional Connectivity Studies in Children with Autism Spectrum Disorder</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/NonparametricMotionControlinFunctionalConnectivityStudiesinChildrenwithAutismSpectrumDisorder.html" rel="alternate" type="text/html" title="Nonparametric Motion Control in Functional Connectivity Studies in Children with Autism Spectrum Disorder" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/NonparametricMotionControlinFunctionalConnectivityStudiesinChildrenwithAutismSpectrumDisorder</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/NonparametricMotionControlinFunctionalConnectivityStudiesinChildrenwithAutismSpectrumDisorder.html">&lt;p&gt;Autism Spectrum Disorder (ASD) is a neurodevelopmental condition associated with difficulties with social interactions, communication, and restricted or repetitive behaviors. To characterize ASD, investigators often use functional connectivity derived from resting-state functional magnetic resonance imaging of the brain. However, participants’ head motion during the scanning session can induce motion artifacts. Many studies remove scans with excessive motion, which can lead to drastic reductions in sample size and introduce selection bias. To avoid such exclusions, we propose an estimand inspired by causal inference methods that quantifies the difference in average functional connectivity in autistic and non-ASD children while standardizing motion relative to the low motion distribution in scans that pass motion quality control. We introduce a nonparametric estimator for motion control, called MoCo, that uses all participants and flexibly models the impacts of motion and other relevant features using an ensemble of machine learning methods. We establish large-sample efficiency and multiple robustness of our proposed estimator. The framework is applied to estimate the difference in functional connectivity between 132 autistic and 245 non-ASD children, of which 34 and 126 pass motion quality control. MoCo appears to dramatically reduce motion artifacts relative to no participant removal, while more efficiently utilizing participant data and accounting for possible selection biases relative to the na&quot;ive approach with participant removal.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13111&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jialu Ran, Sarah Shultz, Benjamin B. Risk, David Benkeser</name></author><category term="stat.ME" /><summary type="html">Autism Spectrum Disorder (ASD) is a neurodevelopmental condition associated with difficulties with social interactions, communication, and restricted or repetitive behaviors. To characterize ASD, investigators often use functional connectivity derived from resting-state functional magnetic resonance imaging of the brain. However, participants’ head motion during the scanning session can induce motion artifacts. Many studies remove scans with excessive motion, which can lead to drastic reductions in sample size and introduce selection bias. To avoid such exclusions, we propose an estimand inspired by causal inference methods that quantifies the difference in average functional connectivity in autistic and non-ASD children while standardizing motion relative to the low motion distribution in scans that pass motion quality control. We introduce a nonparametric estimator for motion control, called MoCo, that uses all participants and flexibly models the impacts of motion and other relevant features using an ensemble of machine learning methods. We establish large-sample efficiency and multiple robustness of our proposed estimator. The framework is applied to estimate the difference in functional connectivity between 132 autistic and 245 non-ASD children, of which 34 and 126 pass motion quality control. MoCo appears to dramatically reduce motion artifacts relative to no participant removal, while more efficiently utilizing participant data and accounting for possible selection biases relative to the na&quot;ive approach with participant removal.</summary></entry><entry><title type="html">On estimation and order selection for multivariate extremes via clustering</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Onestimationandorderselectionformultivariateextremesviaclustering.html" rel="alternate" type="text/html" title="On estimation and order selection for multivariate extremes via clustering" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Onestimationandorderselectionformultivariateextremesviaclustering</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Onestimationandorderselectionformultivariateextremesviaclustering.html">&lt;p&gt;We investigate the estimation of multivariate extreme models with a discrete spectral measure using spherical clustering techniques. The primary contribution involves devising a method for selecting the order, that is, the number of clusters. The method consistently identifies the true order, i.e., the number of spectral atoms, and enjoys intuitive implementation in practice. Specifically, we introduce an extra penalty term to the well-known simplified average silhouette width, which penalizes small cluster sizes and small dissimilarities between cluster centers. Consequently, we provide a consistent method for determining the order of a max-linear factor model, where a typical information-based approach is not viable. Our second contribution is a large-deviation-type analysis for estimating the discrete spectral measure through clustering methods, which serves as an assessment of the convergence quality of clustering-based estimation for multivariate extremes. Additionally, as a third contribution, we discuss how estimating the discrete measure can lead to parameter estimations of heavy-tailed factor models. We also present simulations and real-data studies that demonstrate order selection and factor model estimation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14535&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shiyuan Deng, He Tang, Shuyang Bai</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We investigate the estimation of multivariate extreme models with a discrete spectral measure using spherical clustering techniques. The primary contribution involves devising a method for selecting the order, that is, the number of clusters. The method consistently identifies the true order, i.e., the number of spectral atoms, and enjoys intuitive implementation in practice. Specifically, we introduce an extra penalty term to the well-known simplified average silhouette width, which penalizes small cluster sizes and small dissimilarities between cluster centers. Consequently, we provide a consistent method for determining the order of a max-linear factor model, where a typical information-based approach is not viable. Our second contribution is a large-deviation-type analysis for estimating the discrete spectral measure through clustering methods, which serves as an assessment of the convergence quality of clustering-based estimation for multivariate extremes. Additionally, as a third contribution, we discuss how estimating the discrete measure can lead to parameter estimations of heavy-tailed factor models. We also present simulations and real-data studies that demonstrate order selection and factor model estimation.</summary></entry><entry><title type="html">On integral priors for multiple comparison in Bayesian model selection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/OnintegralpriorsformultiplecomparisoninBayesianmodelselection.html" rel="alternate" type="text/html" title="On integral priors for multiple comparison in Bayesian model selection" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/OnintegralpriorsformultiplecomparisoninBayesianmodelselection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/OnintegralpriorsformultiplecomparisoninBayesianmodelselection.html">&lt;p&gt;Noninformative priors constructed for estimation purposes are usually not appropriate for model selection and testing. The methodology of integral priors was developed to get prior distributions for Bayesian model selection when comparing two models, modifying initial improper reference priors. We propose a generalization of this methodology to more than two models. Our approach adds an artificial copy of each model under comparison by compactifying the parametric space and creating an ergodic Markov chain across all models that returns the integral priors as marginals of the stationary distribution. Besides the garantee of their existance and the lack of paradoxes attached to estimation reference priors, an additional advantage of this methodology is that the simulation of this Markov chain is straightforward as it only requires simulations of imaginary training samples for all models and from the corresponding posterior distributions. This renders its implementation automatic and generic, both in the nested case and in the nonnested case.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14184&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Diego Salmerón, Juan Antonio Cano, Christian P. Robert</name></author><category term="stat.ME" /><summary type="html">Noninformative priors constructed for estimation purposes are usually not appropriate for model selection and testing. The methodology of integral priors was developed to get prior distributions for Bayesian model selection when comparing two models, modifying initial improper reference priors. We propose a generalization of this methodology to more than two models. Our approach adds an artificial copy of each model under comparison by compactifying the parametric space and creating an ergodic Markov chain across all models that returns the integral priors as marginals of the stationary distribution. Besides the garantee of their existance and the lack of paradoxes attached to estimation reference priors, an additional advantage of this methodology is that the simulation of this Markov chain is straightforward as it only requires simulations of imaginary training samples for all models and from the corresponding posterior distributions. This renders its implementation automatic and generic, both in the nested case and in the nonnested case.</summary></entry><entry><title type="html">On the distribution of isometric log-ratio transformations under extra-multinomial count data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Onthedistributionofisometriclogratiotransformationsunderextramultinomialcountdata.html" rel="alternate" type="text/html" title="On the distribution of isometric log-ratio transformations under extra-multinomial count data" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Onthedistributionofisometriclogratiotransformationsunderextramultinomialcountdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Onthedistributionofisometriclogratiotransformationsunderextramultinomialcountdata.html">&lt;p&gt;Compositional data arise when count observations are normalised into proportions adding up to unity. To allow use of standard statistical methods, compositional proportions can be mapped from the simplex into the Euclidean space through the isometric log-ratio (ilr) transformation. When the counts follow a multinomial distribution with fixed class-specific probabilities, the distribution of the ensuing ilr coordinates has been shown to be asymptotically multivariate normal. We here derive an asymptotic normal approximation to the distribution of the ilr coordinates when the counts show overdispersion under the Dirichlet-multinomial mixture model. Using a simulation study, we then investigate the practical applicability of the approximation against the empirical distribution of the ilr coordinates under varying levels of extra-multinomial variation and the total count. The approximation works well, except with a small total count or high amount of overdispersion. These empirical results remain even under population-level heterogeneity in the total count. Our work is motivated by microbiome data, which often exhibit considerable extra-multinomial variation and are increasingly treated as compositional through scaling taxon-specific counts into proportions. We conclude that if the analysis of empirical data relies on normality of the ilr coordinates, it may be advisable to choose a taxonomic level where counts are less sparse so that the distribution of taxon-specific class probabilities remains unimodal.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.09956&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Noora Kartiosuo, Joni Virta, Jaakko Nevalainen, Olli Raitakari, Kari Auranen</name></author><category term="stat.ME" /><summary type="html">Compositional data arise when count observations are normalised into proportions adding up to unity. To allow use of standard statistical methods, compositional proportions can be mapped from the simplex into the Euclidean space through the isometric log-ratio (ilr) transformation. When the counts follow a multinomial distribution with fixed class-specific probabilities, the distribution of the ensuing ilr coordinates has been shown to be asymptotically multivariate normal. We here derive an asymptotic normal approximation to the distribution of the ilr coordinates when the counts show overdispersion under the Dirichlet-multinomial mixture model. Using a simulation study, we then investigate the practical applicability of the approximation against the empirical distribution of the ilr coordinates under varying levels of extra-multinomial variation and the total count. The approximation works well, except with a small total count or high amount of overdispersion. These empirical results remain even under population-level heterogeneity in the total count. Our work is motivated by microbiome data, which often exhibit considerable extra-multinomial variation and are increasingly treated as compositional through scaling taxon-specific counts into proportions. We conclude that if the analysis of empirical data relies on normality of the ilr coordinates, it may be advisable to choose a taxonomic level where counts are less sparse so that the distribution of taxon-specific class probabilities remains unimodal.</summary></entry><entry><title type="html">On the use of the Gram matrix for multivariate functional principal components analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/OntheuseoftheGrammatrixformultivariatefunctionalprincipalcomponentsanalysis.html" rel="alternate" type="text/html" title="On the use of the Gram matrix for multivariate functional principal components analysis" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/OntheuseoftheGrammatrixformultivariatefunctionalprincipalcomponentsanalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/OntheuseoftheGrammatrixformultivariatefunctionalprincipalcomponentsanalysis.html">&lt;p&gt;Dimension reduction is crucial in functional data analysis (FDA). The key tool to reduce the dimension of the data is functional principal component analysis. Existing approaches for functional principal component analysis usually involve the diagonalization of the covariance operator. With the increasing size and complexity of functional datasets, estimating the covariance operator has become more challenging. Therefore, there is a growing need for efficient methodologies to estimate the eigencomponents. Using the duality of the space of observations and the space of functional features, we propose to use the inner-product between the curves to estimate the eigenelements of multivariate and multidimensional functional datasets. The relationship between the eigenelements of the covariance operator and those of the inner-product matrix is established. We explore the application of these methodologies in several FDA settings and provide general guidance on their usability.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.12949&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Steven Golovkine, Edward Gunning, Andrew J. Simpkin, Norma Bargary</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Dimension reduction is crucial in functional data analysis (FDA). The key tool to reduce the dimension of the data is functional principal component analysis. Existing approaches for functional principal component analysis usually involve the diagonalization of the covariance operator. With the increasing size and complexity of functional datasets, estimating the covariance operator has become more challenging. Therefore, there is a growing need for efficient methodologies to estimate the eigencomponents. Using the duality of the space of observations and the space of functional features, we propose to use the inner-product between the curves to estimate the eigenelements of multivariate and multidimensional functional datasets. The relationship between the eigenelements of the covariance operator and those of the inner-product matrix is established. We explore the application of these methodologies in several FDA settings and provide general guidance on their usability.</summary></entry><entry><title type="html">Optimal Scaling Results for Moreau-Yosida Metropolis-adjusted Langevin Algorithms</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/OptimalScalingResultsforMoreauYosidaMetropolisadjustedLangevinAlgorithms.html" rel="alternate" type="text/html" title="Optimal Scaling Results for Moreau-Yosida Metropolis-adjusted Langevin Algorithms" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/OptimalScalingResultsforMoreauYosidaMetropolisadjustedLangevinAlgorithms</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/OptimalScalingResultsforMoreauYosidaMetropolisadjustedLangevinAlgorithms.html">&lt;p&gt;We consider a recently proposed class of MCMC methods which uses proximity maps instead of gradients to build proposal mechanisms which can be employed for both differentiable and non-differentiable targets. These methods have been shown to be stable for a wide class of targets, making them a valuable alternative to Metropolis-adjusted Langevin algorithms (MALA); and have found wide application in imaging contexts. The wider stability properties are obtained by building the Moreau-Yosida envelope for the target of interest, which depends on a parameter $\lambda$. In this work, we investigate the optimal scaling problem for this class of algorithms, which encompasses MALA, and provide practical guidelines for the implementation of these methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.02446&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Francesca R. Crucinio, Alain Durmus, Pablo Jiménez, Gareth O. Roberts</name></author><category term="stat.CO," /><category term="stat.TH" /><summary type="html">We consider a recently proposed class of MCMC methods which uses proximity maps instead of gradients to build proposal mechanisms which can be employed for both differentiable and non-differentiable targets. These methods have been shown to be stable for a wide class of targets, making them a valuable alternative to Metropolis-adjusted Langevin algorithms (MALA); and have found wide application in imaging contexts. The wider stability properties are obtained by building the Moreau-Yosida envelope for the target of interest, which depends on a parameter $\lambda$. In this work, we investigate the optimal scaling problem for this class of algorithms, which encompasses MALA, and provide practical guidelines for the implementation of these methods.</summary></entry><entry><title type="html">Polynomial time guarantees for sampling based posterior inference in high-dimensional generalised linear models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Polynomialtimeguaranteesforsamplingbasedposteriorinferenceinhighdimensionalgeneralisedlinearmodels.html" rel="alternate" type="text/html" title="Polynomial time guarantees for sampling based posterior inference in high-dimensional generalised linear models" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Polynomialtimeguaranteesforsamplingbasedposteriorinferenceinhighdimensionalgeneralisedlinearmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Polynomialtimeguaranteesforsamplingbasedposteriorinferenceinhighdimensionalgeneralisedlinearmodels.html">&lt;p&gt;The problem of computing posterior functionals in general high-dimensional statistical models with possibly non-log-concave likelihood functions is considered. Based on the proof strategy of [49], but using only local likelihood conditions and without relying on M-estimation theory, nonasymptotic statistical and computational guarantees are provided for a gradient based MCMC algorithm. Given a suitable initialiser, these guarantees scale polynomially in key algorithmic quantities. The abstract results are applied to several concrete statistical models, including density estimation, nonparametric regression with generalised linear models and a canonical statistical non-linear inverse problem from PDEs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.13296&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Randolf Altmeyer</name></author><category term="stat.CO," /><category term="stat.TH" /><summary type="html">The problem of computing posterior functionals in general high-dimensional statistical models with possibly non-log-concave likelihood functions is considered. Based on the proof strategy of [49], but using only local likelihood conditions and without relying on M-estimation theory, nonasymptotic statistical and computational guarantees are provided for a gradient based MCMC algorithm. Given a suitable initialiser, these guarantees scale polynomially in key algorithmic quantities. The abstract results are applied to several concrete statistical models, including density estimation, nonparametric regression with generalised linear models and a canonical statistical non-linear inverse problem from PDEs.</summary></entry><entry><title type="html">Predicting Progression Events in Multiple Myeloma from Routine Blood Work</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/PredictingProgressionEventsinMultipleMyelomafromRoutineBloodWork.html" rel="alternate" type="text/html" title="Predicting Progression Events in Multiple Myeloma from Routine Blood Work" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/PredictingProgressionEventsinMultipleMyelomafromRoutineBloodWork</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/PredictingProgressionEventsinMultipleMyelomafromRoutineBloodWork.html">&lt;p&gt;The ability to accurately predict disease progression is paramount for optimizing multiple myeloma patient care. This study introduces a hybrid neural network architecture, combining Long Short-Term Memory networks with a Conditional Restricted Boltzmann Machine, to predict future blood work of affected patients from a series of historical laboratory results. We demonstrate that our model can replicate the statistical moments of the time series ($0.95~\pm~0.01~\geq~R^2~\geq~0.83~\pm~0.03$) and forecast future blood work features with high correlation to actual patient data ($0.92\pm0.02~\geq~r~\geq~0.52~\pm~0.09$). Subsequently, a second Long Short-Term Memory network is employed to detect and annotate disease progression events within the forecasted blood work time series. We show that these annotations enable the prediction of progression events with significant reliability (AUROC$~=~0.88~\pm~0.01$), up to 12 months in advance (AUROC($t+12~$mos)$~=0.65~\pm~0.01$). Our system is designed in a modular fashion, featuring separate entities for forecasting and progression event annotation. This structure not only enhances interpretability but also facilitates the integration of additional modules to perform subsequent operations on the generated outputs. Our approach utilizes a minimal set of routine blood work measurements, which avoids the need for expensive or resource-intensive tests and ensures accessibility of the system in clinical routine. This capability allows for individualized risk assessment and making informed treatment decisions tailored to a patient’s unique disease kinetics. The represented approach contributes to the development of a scalable and cost-effective virtual human twin system for optimized healthcare resource utilization and improved patient outcomes in multiple myeloma care.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18051&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maximilian Ferle, Nora Grieb, Markus Kreuz, Uwe Platzbecker, Thomas Neumuth, Kristin Reiche, Alexander Oeser, Maximilian Merz</name></author><category term="stat.AP" /><summary type="html">The ability to accurately predict disease progression is paramount for optimizing multiple myeloma patient care. This study introduces a hybrid neural network architecture, combining Long Short-Term Memory networks with a Conditional Restricted Boltzmann Machine, to predict future blood work of affected patients from a series of historical laboratory results. We demonstrate that our model can replicate the statistical moments of the time series ($0.95~\pm~0.01~\geq~R^2~\geq~0.83~\pm~0.03$) and forecast future blood work features with high correlation to actual patient data ($0.92\pm0.02~\geq~r~\geq~0.52~\pm~0.09$). Subsequently, a second Long Short-Term Memory network is employed to detect and annotate disease progression events within the forecasted blood work time series. We show that these annotations enable the prediction of progression events with significant reliability (AUROC$~=~0.88~\pm~0.01$), up to 12 months in advance (AUROC($t+12~$mos)$~=0.65~\pm~0.01$). Our system is designed in a modular fashion, featuring separate entities for forecasting and progression event annotation. This structure not only enhances interpretability but also facilitates the integration of additional modules to perform subsequent operations on the generated outputs. Our approach utilizes a minimal set of routine blood work measurements, which avoids the need for expensive or resource-intensive tests and ensures accessibility of the system in clinical routine. This capability allows for individualized risk assessment and making informed treatment decisions tailored to a patient’s unique disease kinetics. The represented approach contributes to the development of a scalable and cost-effective virtual human twin system for optimized healthcare resource utilization and improved patient outcomes in multiple myeloma care.</summary></entry><entry><title type="html">Proximal Interacting Particle Langevin Algorithms</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ProximalInteractingParticleLangevinAlgorithms.html" rel="alternate" type="text/html" title="Proximal Interacting Particle Langevin Algorithms" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ProximalInteractingParticleLangevinAlgorithms</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ProximalInteractingParticleLangevinAlgorithms.html">&lt;p&gt;We introduce a class of algorithms, termed Proximal Interacting Particle Langevin Algorithms (PIPLA), for inference and learning in latent variable models whose joint probability density is non-differentiable. Leveraging proximal Markov chain Monte Carlo (MCMC) techniques and the recently introduced interacting particle Langevin algorithm (IPLA), we propose several variants within the novel proximal IPLA family, tailored to the problem of estimating parameters in a non-differentiable statistical model. We prove nonasymptotic bounds for the parameter estimates produced by multiple algorithms in the strongly log-concave setting and provide comprehensive numerical experiments on various models to demonstrate the effectiveness of the proposed methods. In particular, we demonstrate the utility of the proposed family of algorithms on a toy hierarchical example where our assumptions can be checked, as well as on the problems of sparse Bayesian logistic regression, sparse Bayesian neural network, and sparse matrix completion. Our theory and experiments together show that PIPLA family can be the de facto choice for parameter estimation problems in latent variable models for non-differentiable models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14292&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Paula Cordero Encinar, Francesca R. Crucinio, O. Deniz Akyildiz</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">We introduce a class of algorithms, termed Proximal Interacting Particle Langevin Algorithms (PIPLA), for inference and learning in latent variable models whose joint probability density is non-differentiable. Leveraging proximal Markov chain Monte Carlo (MCMC) techniques and the recently introduced interacting particle Langevin algorithm (IPLA), we propose several variants within the novel proximal IPLA family, tailored to the problem of estimating parameters in a non-differentiable statistical model. We prove nonasymptotic bounds for the parameter estimates produced by multiple algorithms in the strongly log-concave setting and provide comprehensive numerical experiments on various models to demonstrate the effectiveness of the proposed methods. In particular, we demonstrate the utility of the proposed family of algorithms on a toy hierarchical example where our assumptions can be checked, as well as on the problems of sparse Bayesian logistic regression, sparse Bayesian neural network, and sparse matrix completion. Our theory and experiments together show that PIPLA family can be the de facto choice for parameter estimation problems in latent variable models for non-differentiable models.</summary></entry><entry><title type="html">Rating Multi-Modal Time-Series Forecasting Models (MM-TSFM) for Robustness Through a Causal Lens</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/RatingMultiModalTimeSeriesForecastingModelsMMTSFMforRobustnessThroughaCausalLens.html" rel="alternate" type="text/html" title="Rating Multi-Modal Time-Series Forecasting Models (MM-TSFM) for Robustness Through a Causal Lens" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/RatingMultiModalTimeSeriesForecastingModelsMMTSFMforRobustnessThroughaCausalLens</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/RatingMultiModalTimeSeriesForecastingModelsMMTSFMforRobustnessThroughaCausalLens.html">&lt;p&gt;AI systems are notorious for their fragility; minor input changes can potentially cause major output swings. When such systems are deployed in critical areas like finance, the consequences of their uncertain behavior could be severe. In this paper, we focus on multi-modal time-series forecasting, where imprecision due to noisy or incorrect data can lead to erroneous predictions, impacting stakeholders such as analysts, investors, and traders. Recently, it has been shown that beyond numeric data, graphical transformations can be used with advanced visual models to achieve better performance. In this context, we introduce a rating methodology to assess the robustness of Multi-Modal Time-Series Forecasting Models (MM-TSFM) through causal analysis, which helps us understand and quantify the isolated impact of various attributes on the forecasting accuracy of MM-TSFM. We apply our novel rating method on a variety of numeric and multi-modal forecasting models in a large experimental setup (six input settings of control and perturbations, ten data distributions, time series from six leading stocks in three industries over a year of data, and five time-series forecasters) to draw insights on robust forecasting models and the context of their strengths. Within the scope of our study, our main result is that multi-modal (numeric + visual) forecasting, which was found to be more accurate than numeric forecasting in previous studies, can also be more robust in diverse settings. Our work will help different stakeholders of time-series forecasting understand the models` behaviors along trust (robustness) and accuracy dimensions to select an appropriate model for forecasting using our rating method, leading to improved decision-making.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12908&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kausik Lakkaraju, Rachneet Kaur, Zhen Zeng, Parisa Zehtabi, Sunandita Patra, Biplav Srivastava, Marco Valtorta</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">AI systems are notorious for their fragility; minor input changes can potentially cause major output swings. When such systems are deployed in critical areas like finance, the consequences of their uncertain behavior could be severe. In this paper, we focus on multi-modal time-series forecasting, where imprecision due to noisy or incorrect data can lead to erroneous predictions, impacting stakeholders such as analysts, investors, and traders. Recently, it has been shown that beyond numeric data, graphical transformations can be used with advanced visual models to achieve better performance. In this context, we introduce a rating methodology to assess the robustness of Multi-Modal Time-Series Forecasting Models (MM-TSFM) through causal analysis, which helps us understand and quantify the isolated impact of various attributes on the forecasting accuracy of MM-TSFM. We apply our novel rating method on a variety of numeric and multi-modal forecasting models in a large experimental setup (six input settings of control and perturbations, ten data distributions, time series from six leading stocks in three industries over a year of data, and five time-series forecasters) to draw insights on robust forecasting models and the context of their strengths. Within the scope of our study, our main result is that multi-modal (numeric + visual) forecasting, which was found to be more accurate than numeric forecasting in previous studies, can also be more robust in diverse settings. Our work will help different stakeholders of time-series forecasting understand the models` behaviors along trust (robustness) and accuracy dimensions to select an appropriate model for forecasting using our rating method, leading to improved decision-making.</summary></entry><entry><title type="html">Representation Transfer Learning for Semiparametric Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/RepresentationTransferLearningforSemiparametricRegression.html" rel="alternate" type="text/html" title="Representation Transfer Learning for Semiparametric Regression" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/RepresentationTransferLearningforSemiparametricRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/RepresentationTransferLearningforSemiparametricRegression.html">&lt;p&gt;We propose a transfer learning method that utilizes data representations in a semiparametric regression model. Our aim is to perform statistical inference on the parameter of primary interest in the target model while accounting for potential nonlinear effects of confounding variables. We leverage knowledge from source domains, assuming that the sample size of the source data is substantially larger than that of the target data. This knowledge transfer is carried out by the sharing of data representations, predicated on the idea that there exists a set of latent representations transferable from the source to the target domain. We address model heterogeneity between the source and target domains by incorporating domain-specific parameters in their respective models. We establish sufficient conditions for the identifiability of the models and demonstrate that the estimator for the primary parameter in the target model is both consistent and asymptotically normal. These results lay the theoretical groundwork for making statistical inferences about the main effects. Our simulation studies highlight the benefits of our method, and we further illustrate its practical applications using real-world data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13197&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Baihua He, Huihang Liu, Xinyu Zhang, Jian Huang</name></author><category term="stat.ME" /><summary type="html">We propose a transfer learning method that utilizes data representations in a semiparametric regression model. Our aim is to perform statistical inference on the parameter of primary interest in the target model while accounting for potential nonlinear effects of confounding variables. We leverage knowledge from source domains, assuming that the sample size of the source data is substantially larger than that of the target data. This knowledge transfer is carried out by the sharing of data representations, predicated on the idea that there exists a set of latent representations transferable from the source to the target domain. We address model heterogeneity between the source and target domains by incorporating domain-specific parameters in their respective models. We establish sufficient conditions for the identifiability of the models and demonstrate that the estimator for the primary parameter in the target model is both consistent and asymptotically normal. These results lay the theoretical groundwork for making statistical inferences about the main effects. Our simulation studies highlight the benefits of our method, and we further illustrate its practical applications using real-world data.</summary></entry><entry><title type="html">Scale-Translation Equivariant Network for Oceanic Internal Solitary Wave Localization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ScaleTranslationEquivariantNetworkforOceanicInternalSolitaryWaveLocalization.html" rel="alternate" type="text/html" title="Scale-Translation Equivariant Network for Oceanic Internal Solitary Wave Localization" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ScaleTranslationEquivariantNetworkforOceanicInternalSolitaryWaveLocalization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/ScaleTranslationEquivariantNetworkforOceanicInternalSolitaryWaveLocalization.html">&lt;p&gt;Internal solitary waves (ISWs) are gravity waves that are often observed in the interior ocean rather than the surface. They hold significant importance due to their capacity to carry substantial energy, thus influence pollutant transport, oil platform operations, submarine navigation, etc. Researchers have studied ISWs through optical images, synthetic aperture radar (SAR) images, and altimeter data from remote sensing instruments. However, cloud cover in optical remote sensing images variably obscures ground information, leading to blurred or missing surface observations. As such, this paper aims at altimeter-based machine learning solutions to automatically locate ISWs. The challenges, however, lie in the following two aspects: 1) the altimeter data has low resolution, which requires a strong machine learner; 2) labeling data is extremely labor-intensive, leading to very limited data for training. In recent years, the grand progress of deep learning demonstrates strong learning capacity given abundant data. Besides, more recent studies on efficient learning and self-supervised learning laid solid foundations to tackle the aforementioned challenges. In this paper, we propose to inject prior knowledge to achieve a strong and efficient learner. Specifically, intrinsic patterns in altimetry data are efficiently captured using a scale-translation equivariant convolutional neural network (ST-ECNN). By considering inherent symmetries in neural network design, ST-ECNN achieves higher efficiency and better performance than baseline models. Furthermore, we also introduce prior knowledge from massive unsupervised data to enhance our solution using the SimCLR framework for pre-training. Our final solution achieves an overall better performance than baselines on our handcrafted altimetry dataset. Data and codes are available at https://github.com/ZhangWan-byte/Internal_Solitary_Wave_Localization .&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13060&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhang Wan, Shuo Wang, Xudong Zhang</name></author><category term="stat.AP" /><summary type="html">Internal solitary waves (ISWs) are gravity waves that are often observed in the interior ocean rather than the surface. They hold significant importance due to their capacity to carry substantial energy, thus influence pollutant transport, oil platform operations, submarine navigation, etc. Researchers have studied ISWs through optical images, synthetic aperture radar (SAR) images, and altimeter data from remote sensing instruments. However, cloud cover in optical remote sensing images variably obscures ground information, leading to blurred or missing surface observations. As such, this paper aims at altimeter-based machine learning solutions to automatically locate ISWs. The challenges, however, lie in the following two aspects: 1) the altimeter data has low resolution, which requires a strong machine learner; 2) labeling data is extremely labor-intensive, leading to very limited data for training. In recent years, the grand progress of deep learning demonstrates strong learning capacity given abundant data. Besides, more recent studies on efficient learning and self-supervised learning laid solid foundations to tackle the aforementioned challenges. In this paper, we propose to inject prior knowledge to achieve a strong and efficient learner. Specifically, intrinsic patterns in altimetry data are efficiently captured using a scale-translation equivariant convolutional neural network (ST-ECNN). By considering inherent symmetries in neural network design, ST-ECNN achieves higher efficiency and better performance than baseline models. Furthermore, we also introduce prior knowledge from massive unsupervised data to enhance our solution using the SimCLR framework for pre-training. Our final solution achieves an overall better performance than baselines on our handcrafted altimetry dataset. Data and codes are available at https://github.com/ZhangWan-byte/Internal_Solitary_Wave_Localization .</summary></entry><entry><title type="html">Semiparametric Localized Principal Stratification Analysis with Continuous Strata</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/SemiparametricLocalizedPrincipalStratificationAnalysiswithContinuousStrata.html" rel="alternate" type="text/html" title="Semiparametric Localized Principal Stratification Analysis with Continuous Strata" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/SemiparametricLocalizedPrincipalStratificationAnalysiswithContinuousStrata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/SemiparametricLocalizedPrincipalStratificationAnalysiswithContinuousStrata.html">&lt;p&gt;Principal stratification is essential for revealing causal mechanisms involving post-treatment intermediate variables. Principal stratification analysis with continuous intermediate variables is increasingly common but challenging due to the infinite principal strata and the nonidentifiability and nonregularity of principal causal effects. Inspired by recent research, we resolve these challenges by first using a flexible copula-based principal score model to identify principal causal effect under weak principal ignorability. We then target the local functional substitute of principal causal effect, which is statistically regular and can accurately approximate principal causal effect with vanishing bandwidth. We simplify the full efficient influence function of the local functional substitute by considering its oracle-scenario alternative. This leads to a computationally efficient and straightforward estimator for the local functional substitute and principal causal effect with vanishing bandwidth. We prove the double robustness and statistical optimality of our proposed estimator, and derive its asymptotic normality for inferential purposes. We illustrate the appealing statistical performance of our proposed estimator in simulations, and apply it to two real datasets with intriguing scientific discoveries.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13478&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yichi Zhang, Shu Yang</name></author><category term="stat.ME" /><summary type="html">Principal stratification is essential for revealing causal mechanisms involving post-treatment intermediate variables. Principal stratification analysis with continuous intermediate variables is increasingly common but challenging due to the infinite principal strata and the nonidentifiability and nonregularity of principal causal effects. Inspired by recent research, we resolve these challenges by first using a flexible copula-based principal score model to identify principal causal effect under weak principal ignorability. We then target the local functional substitute of principal causal effect, which is statistically regular and can accurately approximate principal causal effect with vanishing bandwidth. We simplify the full efficient influence function of the local functional substitute by considering its oracle-scenario alternative. This leads to a computationally efficient and straightforward estimator for the local functional substitute and principal causal effect with vanishing bandwidth. We prove the double robustness and statistical optimality of our proposed estimator, and derive its asymptotic normality for inferential purposes. We illustrate the appealing statistical performance of our proposed estimator in simulations, and apply it to two real datasets with intriguing scientific discoveries.</summary></entry><entry><title type="html">Semi-supervised Regression Analysis with Model Misspecification and High-dimensional Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/SemisupervisedRegressionAnalysiswithModelMisspecificationandHighdimensionalData.html" rel="alternate" type="text/html" title="Semi-supervised Regression Analysis with Model Misspecification and High-dimensional Data" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/SemisupervisedRegressionAnalysiswithModelMisspecificationandHighdimensionalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/SemisupervisedRegressionAnalysiswithModelMisspecificationandHighdimensionalData.html">&lt;p&gt;The accessibility of vast volumes of unlabeled data has sparked growing interest in semi-supervised learning (SSL) and covariate shift transfer learning (CSTL). In this paper, we present an inference framework for estimating regression coefficients in conditional mean models within both SSL and CSTL settings, while allowing for the misspecification of conditional mean models. We develop an augmented inverse probability weighted (AIPW) method, employing regularized calibrated estimators for both propensity score (PS) and outcome regression (OR) nuisance models, with PS and OR models being sequentially dependent. We show that when the PS model is correctly specified, the proposed estimator achieves consistency, asymptotic normality, and valid confidence intervals, even with possible OR model misspecification and high-dimensional data. Moreover, by suppressing detailed technical choices, we demonstrate that previous methods can be unified within our AIPW framework. Our theoretical findings are verified through extensive simulation studies and a real-world data application.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13906&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ye Tian, Peng Wu, Zhiqiang Tan</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">The accessibility of vast volumes of unlabeled data has sparked growing interest in semi-supervised learning (SSL) and covariate shift transfer learning (CSTL). In this paper, we present an inference framework for estimating regression coefficients in conditional mean models within both SSL and CSTL settings, while allowing for the misspecification of conditional mean models. We develop an augmented inverse probability weighted (AIPW) method, employing regularized calibrated estimators for both propensity score (PS) and outcome regression (OR) nuisance models, with PS and OR models being sequentially dependent. We show that when the PS model is correctly specified, the proposed estimator achieves consistency, asymptotic normality, and valid confidence intervals, even with possible OR model misspecification and high-dimensional data. Moreover, by suppressing detailed technical choices, we demonstrate that previous methods can be unified within our AIPW framework. Our theoretical findings are verified through extensive simulation studies and a real-world data application.</summary></entry><entry><title type="html">Separate Exchangeability as Modeling Principle in Bayesian Nonparametrics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/SeparateExchangeabilityasModelingPrincipleinBayesianNonparametrics.html" rel="alternate" type="text/html" title="Separate Exchangeability as Modeling Principle in Bayesian Nonparametrics" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/SeparateExchangeabilityasModelingPrincipleinBayesianNonparametrics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/SeparateExchangeabilityasModelingPrincipleinBayesianNonparametrics.html">&lt;p&gt;We argue for the use of separate exchangeability as a modeling principle in Bayesian nonparametric (BNP) inference. Separate exchangeability is \emph{de facto} widely applied in the Bayesian parametric case, e.g., it naturally arises in simple mixed models. However, while in some areas, such as random graphs, separate and (closely related) joint exchangeability are widely used, it is curiously underused for several other applications in BNP. We briefly review the definition of separate exchangeability focusing on the implications of such a definition in Bayesian modeling. We then discuss two tractable classes of models that implement separate exchangeability that are the natural counterparts of familiar partially exchangeable BNP models.
  The first is nested random partitions for a data matrix, defining a partition of columns and nested partitions of rows, nested within column clusters. Many recent models for nested partitions implement partially exchangeable models related to variations of the well-known nested Dirichlet process. We argue that inference under such models in some cases ignores important features of the experimental setup. We obtain the separately exchangeable counterpart of such partially exchangeable partition structures.
  The second class is about setting up separately exchangeable priors for a nonparametric regression model when multiple sets of experimental units are involved. We highlight how a Dirichlet process mixture of linear models known as ANOVA DDP can naturally implement separate exchangeability in such regression problems. Finally, we illustrate how to perform inference under such models in two real data examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.07755&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Giovanni Rebaudo, Qiaohui Lin, Peter Mueller</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We argue for the use of separate exchangeability as a modeling principle in Bayesian nonparametric (BNP) inference. Separate exchangeability is \emph{de facto} widely applied in the Bayesian parametric case, e.g., it naturally arises in simple mixed models. However, while in some areas, such as random graphs, separate and (closely related) joint exchangeability are widely used, it is curiously underused for several other applications in BNP. We briefly review the definition of separate exchangeability focusing on the implications of such a definition in Bayesian modeling. We then discuss two tractable classes of models that implement separate exchangeability that are the natural counterparts of familiar partially exchangeable BNP models. The first is nested random partitions for a data matrix, defining a partition of columns and nested partitions of rows, nested within column clusters. Many recent models for nested partitions implement partially exchangeable models related to variations of the well-known nested Dirichlet process. We argue that inference under such models in some cases ignores important features of the experimental setup. We obtain the separately exchangeable counterpart of such partially exchangeable partition structures. The second class is about setting up separately exchangeable priors for a nonparametric regression model when multiple sets of experimental units are involved. We highlight how a Dirichlet process mixture of linear models known as ANOVA DDP can naturally implement separate exchangeability in such regression problems. Finally, we illustrate how to perform inference under such models in two real data examples.</summary></entry><entry><title type="html">Sharp detection of low-dimensional structure in probability measures via dimensional logarithmic Sobolev inequalities</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/SharpdetectionoflowdimensionalstructureinprobabilitymeasuresviadimensionallogarithmicSobolevinequalities.html" rel="alternate" type="text/html" title="Sharp detection of low-dimensional structure in probability measures via dimensional logarithmic Sobolev inequalities" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/SharpdetectionoflowdimensionalstructureinprobabilitymeasuresviadimensionallogarithmicSobolevinequalities</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/SharpdetectionoflowdimensionalstructureinprobabilitymeasuresviadimensionallogarithmicSobolevinequalities.html">&lt;p&gt;Identifying low-dimensional structure in high-dimensional probability measures is an essential pre-processing step for efficient sampling. We introduce a method for identifying and approximating a target measure $\pi$ as a perturbation of a given reference measure $\mu$ along a few significant directions of $\mathbb{R}^{d}$. The reference measure can be a Gaussian or a nonlinear transformation of a Gaussian, as commonly arising in generative modeling. Our method extends prior work on minimizing majorizations of the Kullback–Leibler divergence to identify optimal approximations within this class of measures. Our main contribution unveils a connection between the \emph{dimensional} logarithmic Sobolev inequality (LSI) and approximations with this ansatz. Specifically, when the target and reference are both Gaussian, we show that minimizing the dimensional LSI is equivalent to minimizing the KL divergence restricted to this ansatz. For general non-Gaussian measures, the dimensional LSI produces majorants that uniformly improve on previous majorants for gradient-based dimension reduction. We further demonstrate the applicability of this analysis to the squared Hellinger distance, where analogous reasoning shows that the dimensional Poincar&apos;e inequality offers improved bounds.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13036&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matthew T. C. Li, Tiangang Cui, Fengyi Li, Youssef Marzouk, Olivier Zahm</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Identifying low-dimensional structure in high-dimensional probability measures is an essential pre-processing step for efficient sampling. We introduce a method for identifying and approximating a target measure $\pi$ as a perturbation of a given reference measure $\mu$ along a few significant directions of $\mathbb{R}^{d}$. The reference measure can be a Gaussian or a nonlinear transformation of a Gaussian, as commonly arising in generative modeling. Our method extends prior work on minimizing majorizations of the Kullback–Leibler divergence to identify optimal approximations within this class of measures. Our main contribution unveils a connection between the \emph{dimensional} logarithmic Sobolev inequality (LSI) and approximations with this ansatz. Specifically, when the target and reference are both Gaussian, we show that minimizing the dimensional LSI is equivalent to minimizing the KL divergence restricted to this ansatz. For general non-Gaussian measures, the dimensional LSI produces majorants that uniformly improve on previous majorants for gradient-based dimension reduction. We further demonstrate the applicability of this analysis to the squared Hellinger distance, where analogous reasoning shows that the dimensional Poincar&apos;e inequality offers improved bounds.</summary></entry><entry><title type="html">Temperature in the Iberian Peninsula: Trend, seasonality, and heterogeneity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/TemperatureintheIberianPeninsulaTrendseasonalityandheterogeneity.html" rel="alternate" type="text/html" title="Temperature in the Iberian Peninsula: Trend, seasonality, and heterogeneity" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/TemperatureintheIberianPeninsulaTrendseasonalityandheterogeneity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/TemperatureintheIberianPeninsulaTrendseasonalityandheterogeneity.html">&lt;p&gt;In this paper, we propose fitting unobserved component models to represent the dynamic evolution of bivariate systems of centre and log-range temperatures obtained monthly from minimum/maximum temperatures observed at a given location. In doing so, the centre and log-range temperature are decomposed into potentially stochastic trends, seasonal, and transitory components. Since our model encompasses deterministic trends and seasonal components as limiting cases, we contribute to the debate on whether stochastic or deterministic components better represent the trend and seasonal components. The methodology is implemented to centre and log-range temperature observed in four locations in the Iberian Peninsula, namely, Barcelona, Coru~{n}a, Madrid, and Seville. We show that, at each location, the centre temperature can be represented by a smooth integrated random walk with time-varying slope, while a stochastic level better represents the log-range. We also show that centre and log-range temperature are unrelated. The methodology is then extended to simultaneously model centre and log-range temperature observed at several locations in the Iberian Peninsula. We fit a multi-level dynamic factor model to extract potential commonalities among centre (log-range) temperature while also allowing for heterogeneity in different areas in the Iberian Peninsula. We show that, although the commonality in trends of average temperature is considerable, the regional components are also relevant.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14145&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>C. Vladimir Rodríguez-Caballero, Esther Ruiz</name></author><category term="stat.AP" /><summary type="html">In this paper, we propose fitting unobserved component models to represent the dynamic evolution of bivariate systems of centre and log-range temperatures obtained monthly from minimum/maximum temperatures observed at a given location. In doing so, the centre and log-range temperature are decomposed into potentially stochastic trends, seasonal, and transitory components. Since our model encompasses deterministic trends and seasonal components as limiting cases, we contribute to the debate on whether stochastic or deterministic components better represent the trend and seasonal components. The methodology is implemented to centre and log-range temperature observed in four locations in the Iberian Peninsula, namely, Barcelona, Coru~{n}a, Madrid, and Seville. We show that, at each location, the centre temperature can be represented by a smooth integrated random walk with time-varying slope, while a stochastic level better represents the log-range. We also show that centre and log-range temperature are unrelated. The methodology is then extended to simultaneously model centre and log-range temperature observed at several locations in the Iberian Peninsula. We fit a multi-level dynamic factor model to extract potential commonalities among centre (log-range) temperature while also allowing for heterogeneity in different areas in the Iberian Peninsula. We show that, although the commonality in trends of average temperature is considerable, the regional components are also relevant.</summary></entry><entry><title type="html">Temporal label recovery from noisy dynamical data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Temporallabelrecoveryfromnoisydynamicaldata.html" rel="alternate" type="text/html" title="Temporal label recovery from noisy dynamical data" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Temporallabelrecoveryfromnoisydynamicaldata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Temporallabelrecoveryfromnoisydynamicaldata.html">&lt;p&gt;Analyzing dynamical data often requires information of the temporal labels, but such information is unavailable in many applications. Recovery of these temporal labels, closely related to the seriation or sequencing problem, becomes crucial in the study. However, challenges arise due to the nonlinear nature of the data and the complexity of the underlying dynamical system, which may be periodic or non-periodic. Additionally, noise within the feature space complicates the theoretical analysis. Our work develops spectral algorithms that leverage manifold learning concepts to recover temporal labels from noisy data. We first construct the graph Laplacian of the data, and then employ the second (and the third) Fiedler vectors to recover temporal labels. This method can be applied to both periodic and aperiodic cases. It also does not require monotone properties on the similarity matrix, which are commonly assumed in existing spectral seriation algorithms. We develop the $\ell_{\infty}$ error of our estimators for the temporal labels and ranking, without assumptions on the eigen-gap. In numerical analysis, our method outperforms spectral seriation algorithms based on a similarity matrix. The performance of our algorithms is further demonstrated on a synthetic biomolecule data example.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13635&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuehaw Khoo, Xin T. Tong, Wanjie Wang, Yuguan Wang</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.TH" /><summary type="html">Analyzing dynamical data often requires information of the temporal labels, but such information is unavailable in many applications. Recovery of these temporal labels, closely related to the seriation or sequencing problem, becomes crucial in the study. However, challenges arise due to the nonlinear nature of the data and the complexity of the underlying dynamical system, which may be periodic or non-periodic. Additionally, noise within the feature space complicates the theoretical analysis. Our work develops spectral algorithms that leverage manifold learning concepts to recover temporal labels from noisy data. We first construct the graph Laplacian of the data, and then employ the second (and the third) Fiedler vectors to recover temporal labels. This method can be applied to both periodic and aperiodic cases. It also does not require monotone properties on the similarity matrix, which are commonly assumed in existing spectral seriation algorithms. We develop the $\ell_{\infty}$ error of our estimators for the temporal labels and ranking, without assumptions on the eigen-gap. In numerical analysis, our method outperforms spectral seriation algorithms based on a similarity matrix. The performance of our algorithms is further demonstrated on a synthetic biomolecule data example.</summary></entry><entry><title type="html">Testing identification in mediation and dynamic treatment models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Testingidentificationinmediationanddynamictreatmentmodels.html" rel="alternate" type="text/html" title="Testing identification in mediation and dynamic treatment models" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Testingidentificationinmediationanddynamictreatmentmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Testingidentificationinmediationanddynamictreatmentmodels.html">&lt;p&gt;We propose a test for the identification of causal effects in mediation and dynamic treatment models that is based on two sets of observed variables, namely covariates to be controlled for and suspected instruments, building on the test by Huber and Kueck (2022) for single treatment models. We consider models with a sequential assignment of a treatment and a mediator to assess the direct treatment effect (net of the mediator), the indirect treatment effect (via the mediator), or the joint effect of both treatment and mediator. We establish testable conditions for identifying such effects in observational data. These conditions jointly imply (1) the exogeneity of the treatment and the mediator conditional on covariates and (2) the validity of distinct instruments for the treatment and the mediator, meaning that the instruments do not directly affect the outcome (other than through the treatment or mediator) and are unconfounded given the covariates. Our framework extends to post-treatment sample selection or attrition problems when replacing the mediator by a selection indicator for observing the outcome, enabling joint testing of the selectivity of treatment and attrition. We propose a machine learning-based test to control for covariates in a data-driven manner and analyze its finite sample performance in a simulation study. Additionally, we apply our method to Slovak labor market data and find that our testable implications are not rejected for a sequence of training programs typically considered in dynamic treatment evaluations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13826&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Martin Huber, Kevin Kloiber, Lukas Laffers</name></author><category term="stat.ME" /><summary type="html">We propose a test for the identification of causal effects in mediation and dynamic treatment models that is based on two sets of observed variables, namely covariates to be controlled for and suspected instruments, building on the test by Huber and Kueck (2022) for single treatment models. We consider models with a sequential assignment of a treatment and a mediator to assess the direct treatment effect (net of the mediator), the indirect treatment effect (via the mediator), or the joint effect of both treatment and mediator. We establish testable conditions for identifying such effects in observational data. These conditions jointly imply (1) the exogeneity of the treatment and the mediator conditional on covariates and (2) the validity of distinct instruments for the treatment and the mediator, meaning that the instruments do not directly affect the outcome (other than through the treatment or mediator) and are unconfounded given the covariates. Our framework extends to post-treatment sample selection or attrition problems when replacing the mediator by a selection indicator for observing the outcome, enabling joint testing of the selectivity of treatment and attrition. We propose a machine learning-based test to control for covariates in a data-driven manner and analyze its finite sample performance in a simulation study. Additionally, we apply our method to Slovak labor market data and find that our testable implications are not rejected for a sequence of training programs typically considered in dynamic treatment evaluations.</summary></entry><entry><title type="html">Testing the Fairness-Improvability of Algorithms</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/TestingtheFairnessImprovabilityofAlgorithms.html" rel="alternate" type="text/html" title="Testing the Fairness-Improvability of Algorithms" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/TestingtheFairnessImprovabilityofAlgorithms</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/TestingtheFairnessImprovabilityofAlgorithms.html">&lt;p&gt;Many organizations use algorithms that have a disparate impact, i.e., the benefits or harms of the algorithm fall disproportionately on certain social groups. Addressing an algorithm’s disparate impact can be challenging, especially because it is often unclear whether reducing this impact is possible without sacrificing other important objectives of the organization, such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy-maker can formally substantiate, or refute, this necessity defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can be applied under any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and illustrate its practical application by evaluating a healthcare algorithm originally considered by Obermeyer et al 2019. In this application, we reject the null hypothesis that it is not possible to reduce the algorithm’s disparate impact without compromising on the accuracy of its predictions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04816&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Eric Auerbach, Annie Liang, Kyohei Okumura, Max Tabord-Meehan</name></author><category term="stat.AP" /><summary type="html">Many organizations use algorithms that have a disparate impact, i.e., the benefits or harms of the algorithm fall disproportionately on certain social groups. Addressing an algorithm’s disparate impact can be challenging, especially because it is often unclear whether reducing this impact is possible without sacrificing other important objectives of the organization, such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy-maker can formally substantiate, or refute, this necessity defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can be applied under any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and illustrate its practical application by evaluating a healthcare algorithm originally considered by Obermeyer et al 2019. In this application, we reject the null hypothesis that it is not possible to reduce the algorithm’s disparate impact without compromising on the accuracy of its predictions.</summary></entry><entry><title type="html">The Effective Number of Parameters in Kernel Density Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/TheEffectiveNumberofParametersinKernelDensityEstimation.html" rel="alternate" type="text/html" title="The Effective Number of Parameters in Kernel Density Estimation" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/TheEffectiveNumberofParametersinKernelDensityEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/TheEffectiveNumberofParametersinKernelDensityEstimation.html">&lt;p&gt;The quest for a formula that satisfactorily measures the effective degrees of freedom in kernel density estimation (KDE) is a long standing problem with few solutions. Starting from the orthogonal polynomial sequence (OPS) expansion for the ratio of the empirical to the oracle density, we show how convolution with the kernel leads to a new OPS with respect to which one may express the resulting KDE. The expansion coefficients of the two OPS systems can then be related via a kernel sensitivity matrix, and this then naturally leads to a definition of effective parameters by taking the trace of a symmetrized positive semi-definite normalized version. The resulting effective degrees of freedom (EDoF) formula is an oracle-based quantity; the first ever proposed in the literature. Asymptotic properties of the empirical EDoF are worked out through influence functions. Numerical investigations confirm the theoretical insights.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14453&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sofia Guglielmini, Igor Volobouev, Alexandre Trindade</name></author><category term="stat.ME" /><summary type="html">The quest for a formula that satisfactorily measures the effective degrees of freedom in kernel density estimation (KDE) is a long standing problem with few solutions. Starting from the orthogonal polynomial sequence (OPS) expansion for the ratio of the empirical to the oracle density, we show how convolution with the kernel leads to a new OPS with respect to which one may express the resulting KDE. The expansion coefficients of the two OPS systems can then be related via a kernel sensitivity matrix, and this then naturally leads to a definition of effective parameters by taking the trace of a symmetrized positive semi-definite normalized version. The resulting effective degrees of freedom (EDoF) formula is an oracle-based quantity; the first ever proposed in the literature. Asymptotic properties of the empirical EDoF are worked out through influence functions. Numerical investigations confirm the theoretical insights.</summary></entry><entry><title type="html">Transporting treatment effects from difference-in-differences studies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Transportingtreatmenteffectsfromdifferenceindifferencesstudies.html" rel="alternate" type="text/html" title="Transporting treatment effects from difference-in-differences studies" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Transportingtreatmenteffectsfromdifferenceindifferencesstudies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/Transportingtreatmenteffectsfromdifferenceindifferencesstudies.html">&lt;p&gt;Difference-in-differences (DID) is a popular approach to identify the causal effects of treatments and policies in the presence of unmeasured confounding. DID identifies the sample average treatment effect in the treated (SATT). However, a goal of such research is often to inform decision-making in target populations outside the treated sample. Transportability methods have been developed to extend inferences from study samples to external target populations; these methods have primarily been developed and applied in settings where identification is based on conditional independence between the treatment and potential outcomes, such as in a randomized trial. We present a novel approach to identifying and estimating effects in a target population, based on DID conducted in a study sample that differs from the target population. We present a range of assumptions under which one may identify causal effects in the target population and employ causal diagrams to illustrate these assumptions. In most realistic settings, results depend critically on the assumption that any unmeasured confounders are not effect measure modifiers on the scale of the effect of interest (e.g., risk difference, odds ratio). We develop several estimators of transported effects, including g-computation, inverse odds weighting, and a doubly robust estimator based on the efficient influence function. Simulation results support theoretical properties of the proposed estimators. As an example, we apply our approach to study the effects of a 2018 US federal smoke-free public housing law on air quality in public housing across the US, using data from a DID study conducted in New York City alone.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.17806&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Audrey Renson, Ellicott C. Matthay, Kara E. Rudolph</name></author><category term="stat.ME" /><summary type="html">Difference-in-differences (DID) is a popular approach to identify the causal effects of treatments and policies in the presence of unmeasured confounding. DID identifies the sample average treatment effect in the treated (SATT). However, a goal of such research is often to inform decision-making in target populations outside the treated sample. Transportability methods have been developed to extend inferences from study samples to external target populations; these methods have primarily been developed and applied in settings where identification is based on conditional independence between the treatment and potential outcomes, such as in a randomized trial. We present a novel approach to identifying and estimating effects in a target population, based on DID conducted in a study sample that differs from the target population. We present a range of assumptions under which one may identify causal effects in the target population and employ causal diagrams to illustrate these assumptions. In most realistic settings, results depend critically on the assumption that any unmeasured confounders are not effect measure modifiers on the scale of the effect of interest (e.g., risk difference, odds ratio). We develop several estimators of transported effects, including g-computation, inverse odds weighting, and a doubly robust estimator based on the efficient influence function. Simulation results support theoretical properties of the proposed estimators. As an example, we apply our approach to study the effects of a 2018 US federal smoke-free public housing law on air quality in public housing across the US, using data from a DID study conducted in New York City alone.</summary></entry><entry><title type="html">von Mises Quasi-Processes for Bayesian Circular Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/vonMisesQuasiProcessesforBayesianCircularRegression.html" rel="alternate" type="text/html" title="von Mises Quasi-Processes for Bayesian Circular Regression" /><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/vonMisesQuasiProcessesforBayesianCircularRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/vonMisesQuasiProcessesforBayesianCircularRegression.html">&lt;p&gt;The need for regression models to predict circular values arises in many scientific fields. In this work we explore a family of expressive and interpretable distributions over circle-valued random functions related to Gaussian processes targeting two Euclidean dimensions conditioned on the unit circle. The resulting probability model has connections with continuous spin models in statistical physics. Moreover, its density is very simple and has maximum-entropy, unlike previous Gaussian process-based approaches, which use wrapping or radial marginalization. For posterior inference, we introduce a new Stratonovich-like augmentation that lends itself to fast Markov Chain Monte Carlo sampling. We argue that transductive learning in these models favors a Bayesian approach to the parameters. We present experiments applying this model to the prediction of (i) wind directions and (ii) the percentage of the running gait cycle as a function of joint angles.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13151&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yarden Cohen, Alexandre Khae Wu Navarro, Jes Frellsen, Richard E. Turner, Raziel Riemer, Ari Pakman</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">The need for regression models to predict circular values arises in many scientific fields. In this work we explore a family of expressive and interpretable distributions over circle-valued random functions related to Gaussian processes targeting two Euclidean dimensions conditioned on the unit circle. The resulting probability model has connections with continuous spin models in statistical physics. Moreover, its density is very simple and has maximum-entropy, unlike previous Gaussian process-based approaches, which use wrapping or radial marginalization. For posterior inference, we introduce a new Stratonovich-like augmentation that lends itself to fast Markov Chain Monte Carlo sampling. We argue that transductive learning in these models favors a Bayesian approach to the parameters. We present experiments applying this model to the prediction of (i) wind directions and (ii) the percentage of the running gait cycle as a function of joint angles.</summary></entry></feed>