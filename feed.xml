<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-18T07:14:20+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Bayesian Approach to Estimate Causal Peer Influence Accounting for Latent Network Homophily</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ABayesianApproachtoEstimateCausalPeerInfluenceAccountingforLatentNetworkHomophily.html" rel="alternate" type="text/html" title="A Bayesian Approach to Estimate Causal Peer Influence Accounting for Latent Network Homophily" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ABayesianApproachtoEstimateCausalPeerInfluenceAccountingforLatentNetworkHomophily</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ABayesianApproachtoEstimateCausalPeerInfluenceAccountingforLatentNetworkHomophily.html">&lt;p&gt;Researchers have focused on understanding how individual’s behavior is influenced by the behaviors of their peers in observational studies of social networks. Identifying and estimating causal peer influence, however, is challenging due to confounding by homophily, where people tend to connect with those who share similar characteristics with them. Moreover, since all the attributes driving homophily are generally not always observed and act as unobserved confounders, identifying and estimating causal peer influence becomes infeasible using standard causal identification assumptions. In this paper, we address this challenge by leveraging latent locations inferred from the network itself to disentangle homophily from causal peer influence, and we extend this approach to multiple networks by adopting a Bayesian hierarchical modeling framework. To accommodate the nonlinear dependency of peer influence on individual behavior, we employ a Bayesian nonparametric method, specifically Bayesian Additive Regression Trees (BART), and we propose a Bayesian framework that accounts for the uncertainty in inferring latent locations. We assess the operating characteristics of the estimator via extensive simulation study. Finally, we apply our method to estimate causal peer influence in advice-seeking networks of teachers in secondary schools, in order to assess whether the teachers’ belief about mathematics education is influenced by the beliefs of their peers from whom they receive advice. Our results suggest that, overlooking latent homophily can lead to either underestimation or overestimation of causal peer influence, accompanied by considerable estimation uncertainty.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14789&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Seungha Um, Tracy Sweet, Samrachana Adhikari</name></author><category term="stat.AP" /><summary type="html">Researchers have focused on understanding how individual’s behavior is influenced by the behaviors of their peers in observational studies of social networks. Identifying and estimating causal peer influence, however, is challenging due to confounding by homophily, where people tend to connect with those who share similar characteristics with them. Moreover, since all the attributes driving homophily are generally not always observed and act as unobserved confounders, identifying and estimating causal peer influence becomes infeasible using standard causal identification assumptions. In this paper, we address this challenge by leveraging latent locations inferred from the network itself to disentangle homophily from causal peer influence, and we extend this approach to multiple networks by adopting a Bayesian hierarchical modeling framework. To accommodate the nonlinear dependency of peer influence on individual behavior, we employ a Bayesian nonparametric method, specifically Bayesian Additive Regression Trees (BART), and we propose a Bayesian framework that accounts for the uncertainty in inferring latent locations. We assess the operating characteristics of the estimator via extensive simulation study. Finally, we apply our method to estimate causal peer influence in advice-seeking networks of teachers in secondary schools, in order to assess whether the teachers’ belief about mathematics education is influenced by the beliefs of their peers from whom they receive advice. Our results suggest that, overlooking latent homophily can lead to either underestimation or overestimation of causal peer influence, accompanied by considerable estimation uncertainty.</summary></entry><entry><title type="html">A Laplace transform-based test for the equality of positive semidefinite matrix distributions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ALaplacetransformbasedtestfortheequalityofpositivesemidefinitematrixdistributions.html" rel="alternate" type="text/html" title="A Laplace transform-based test for the equality of positive semidefinite matrix distributions" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ALaplacetransformbasedtestfortheequalityofpositivesemidefinitematrixdistributions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ALaplacetransformbasedtestfortheequalityofpositivesemidefinitematrixdistributions.html">&lt;p&gt;In this paper, we present a novel test for determining equality in distribution of matrix distributions. Our approach is based on the integral squared difference of the empirical Laplace transforms with respect to the noncentral Wishart measure. We conduct an extensive power study to assess the performance of the test and determine the optimal choice of parameters. Furthermore, we demonstrate the applicability of the test on financial and non-life insurance data, illustrating its effectiveness in practical scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10733&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Žikica Lukić</name></author><category term="stat.ME" /><summary type="html">In this paper, we present a novel test for determining equality in distribution of matrix distributions. Our approach is based on the integral squared difference of the empirical Laplace transforms with respect to the noncentral Wishart measure. We conduct an extensive power study to assess the performance of the test and determine the optimal choice of parameters. Furthermore, we demonstrate the applicability of the test on financial and non-life insurance data, illustrating its effectiveness in practical scenarios.</summary></entry><entry><title type="html">A connection between Tempering and Entropic Mirror Descent</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AconnectionbetweenTemperingandEntropicMirrorDescent.html" rel="alternate" type="text/html" title="A connection between Tempering and Entropic Mirror Descent" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AconnectionbetweenTemperingandEntropicMirrorDescent</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AconnectionbetweenTemperingandEntropicMirrorDescent.html">&lt;p&gt;This paper explores the connections between tempering (for Sequential Monte Carlo; SMC) and entropic mirror descent to sample from a target probability distribution whose unnormalized density is known. We establish that tempering SMC corresponds to entropic mirror descent applied to the reverse Kullback-Leibler (KL) divergence and obtain convergence rates for the tempering iterates. Our result motivates the tempering iterates from an optimization point of view, showing that tempering can be seen as a descent scheme of the KL divergence with respect to the Fisher-Rao geometry, in contrast to Langevin dynamics that perform descent of the KL with respect to the Wasserstein-2 geometry. We exploit the connection between tempering and mirror descent iterates to justify common practices in SMC and derive adaptive tempering rules that improve over other alternative benchmarks in the literature.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.11914&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nicolas Chopin, Francesca R. Crucinio, Anna Korba</name></author><category term="stat.CO," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">This paper explores the connections between tempering (for Sequential Monte Carlo; SMC) and entropic mirror descent to sample from a target probability distribution whose unnormalized density is known. We establish that tempering SMC corresponds to entropic mirror descent applied to the reverse Kullback-Leibler (KL) divergence and obtain convergence rates for the tempering iterates. Our result motivates the tempering iterates from an optimization point of view, showing that tempering can be seen as a descent scheme of the KL divergence with respect to the Fisher-Rao geometry, in contrast to Langevin dynamics that perform descent of the KL with respect to the Wasserstein-2 geometry. We exploit the connection between tempering and mirror descent iterates to justify common practices in SMC and derive adaptive tempering rules that improve over other alternative benchmarks in the literature.</summary></entry><entry><title type="html">A conservation law for posterior predictive variance</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Aconservationlawforposteriorpredictivevariance.html" rel="alternate" type="text/html" title="A conservation law for posterior predictive variance" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Aconservationlawforposteriorpredictivevariance</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Aconservationlawforposteriorpredictivevariance.html">&lt;p&gt;We use the law of total variance to generate multiple expressions for the posterior predictive variance in Bayesian hierarchical models. These expressions are sums of terms involving conditional expectations and conditional variances. Since the posterior predictive variance is fixed given the hierarchical model, it represents a constant quantity that is conserved over the various expressions for it. The terms in the expressions can be assessed in absolute or relative terms to understand the main contributors to the length of prediction intervals. Also, sometimes these terms can be intepreted in the context of the hierarchical model. We show several examples, closed form and computational, to illustrate the uses of this approach in model assessment.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11806&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bertrand Clarke, Dean Dustin</name></author><category term="stat.ME" /><summary type="html">We use the law of total variance to generate multiple expressions for the posterior predictive variance in Bayesian hierarchical models. These expressions are sums of terms involving conditional expectations and conditional variances. Since the posterior predictive variance is fixed given the hierarchical model, it represents a constant quantity that is conserved over the various expressions for it. The terms in the expressions can be assessed in absolute or relative terms to understand the main contributors to the length of prediction intervals. Also, sometimes these terms can be intepreted in the context of the hierarchical model. We show several examples, closed form and computational, to illustrate the uses of this approach in model assessment.</summary></entry><entry><title type="html">Adaptive Experimentation When You Can’t Experiment</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AdaptiveExperimentationWhenYouCantExperiment.html" rel="alternate" type="text/html" title="Adaptive Experimentation When You Can’t Experiment" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AdaptiveExperimentationWhenYouCantExperiment</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AdaptiveExperimentationWhenYouCantExperiment.html">&lt;p&gt;This paper introduces the \emph{confounded pure exploration transductive linear bandit} (\texttt{CPET-LB}) problem. As a motivating example, often online services cannot directly assign users to specific control or treatment experiences either for business or practical reasons. In these settings, naively comparing treatment and control groups that may result from self-selection can lead to biased estimates of underlying treatment effects. Instead, online services can employ a properly randomized encouragement that incentivizes users toward a specific treatment. Our methodology provides online services with an adaptive experimental design approach for learning the best-performing treatment for such \textit{encouragement designs}. We consider a more general underlying model captured by a linear structural equation and formulate pure exploration linear bandits in this setting. Though pure exploration has been extensively studied in standard adaptive experimental design settings, we believe this is the first work considering a setting where noise is confounded. Elimination-style algorithms using experimental design methods in combination with a novel finite-time confidence interval on an instrumental variable style estimator are presented with sample complexity upper bounds nearly matching a minimax lower bound. Finally, experiments are conducted that demonstrate the efficacy of our approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10738&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yao Zhao, Kwang-Sung Jun, Tanner Fiez, Lalit Jain</name></author><category term="stat.ME" /><summary type="html">This paper introduces the \emph{confounded pure exploration transductive linear bandit} (\texttt{CPET-LB}) problem. As a motivating example, often online services cannot directly assign users to specific control or treatment experiences either for business or practical reasons. In these settings, naively comparing treatment and control groups that may result from self-selection can lead to biased estimates of underlying treatment effects. Instead, online services can employ a properly randomized encouragement that incentivizes users toward a specific treatment. Our methodology provides online services with an adaptive experimental design approach for learning the best-performing treatment for such \textit{encouragement designs}. We consider a more general underlying model captured by a linear structural equation and formulate pure exploration linear bandits in this setting. Though pure exploration has been extensively studied in standard adaptive experimental design settings, we believe this is the first work considering a setting where noise is confounded. Elimination-style algorithms using experimental design methods in combination with a novel finite-time confidence interval on an instrumental variable style estimator are presented with sample complexity upper bounds nearly matching a minimax lower bound. Finally, experiments are conducted that demonstrate the efficacy of our approach.</summary></entry><entry><title type="html">Additive Density-on-Scalar Regression in Bayes Hilbert Spaces with an Application to Gender Economics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AdditiveDensityonScalarRegressioninBayesHilbertSpaceswithanApplicationtoGenderEconomics.html" rel="alternate" type="text/html" title="Additive Density-on-Scalar Regression in Bayes Hilbert Spaces with an Application to Gender Economics" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AdditiveDensityonScalarRegressioninBayesHilbertSpaceswithanApplicationtoGenderEconomics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AdditiveDensityonScalarRegressioninBayesHilbertSpaceswithanApplicationtoGenderEconomics.html">&lt;p&gt;Motivated by research on gender identity norms and the distribution of the woman’s share in a couple’s total labor income, we consider functional additive regression models for probability density functions as responses with scalar covariates. To preserve nonnegativity and integration to one under vector space operations, we formulate the model for densities in a Bayes Hilbert space, which allows to not only consider continuous densities, but also, e.g., discrete or mixed densities. Mixed ones occur in our application, as the woman’s income share is a continuous variable having discrete point masses at zero and one for single-earner couples. Estimation is based on a gradient boosting algorithm, allowing for potentially numerous flexible covariate effects and model selection. We develop properties of Bayes Hilbert spaces related to subcompositional coherence, yielding (odds-ratio) interpretation of effect functions and simplified estimation for mixed densities via an orthogonal decomposition. Applying our approach to data from the German Socio-Economic Panel Study (SOEP) shows a more symmetric distribution in East German than in West German couples after reunification and a smaller child penalty comparing couples with and without minor children. These West-East differences become smaller, but are persistent over time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.11771&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Eva-Maria Maier, Almond Stöcker, Bernd Fitzenberger, Sonja Greven</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Motivated by research on gender identity norms and the distribution of the woman’s share in a couple’s total labor income, we consider functional additive regression models for probability density functions as responses with scalar covariates. To preserve nonnegativity and integration to one under vector space operations, we formulate the model for densities in a Bayes Hilbert space, which allows to not only consider continuous densities, but also, e.g., discrete or mixed densities. Mixed ones occur in our application, as the woman’s income share is a continuous variable having discrete point masses at zero and one for single-earner couples. Estimation is based on a gradient boosting algorithm, allowing for potentially numerous flexible covariate effects and model selection. We develop properties of Bayes Hilbert spaces related to subcompositional coherence, yielding (odds-ratio) interpretation of effect functions and simplified estimation for mixed densities via an orthogonal decomposition. Applying our approach to data from the German Socio-Economic Panel Study (SOEP) shows a more symmetric distribution in East German than in West German couples after reunification and a smaller child penalty comparing couples with and without minor children. These West-East differences become smaller, but are persistent over time.</summary></entry><entry><title type="html">Alternative Approaches for Estimating Highest-Density Regions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AlternativeApproachesforEstimatingHighestDensityRegions.html" rel="alternate" type="text/html" title="Alternative Approaches for Estimating Highest-Density Regions" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AlternativeApproachesforEstimatingHighestDensityRegions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AlternativeApproachesforEstimatingHighestDensityRegions.html">&lt;p&gt;Among the variety of statistical intervals, highest-density regions (HDRs) stand out for their ability to effectively summarize a distribution or sample, unveiling its distinctive and salient features. An HDR represents the minimum size set that satisfies a certain probability coverage, and current methods for their computation require knowledge or estimation of the underlying probability distribution or density $f$. In this work, we illustrate a broader framework for computing HDRs, which generalizes the classical density quantile method introduced in the seminal paper of Hyndman (1996). The framework is based on neighbourhood measures, i.e., measures that preserve the order induced in the sample by $f$, and include the density $f$ as a special case. We explore a number of suitable distance-based measures, such as the $k$-nearest neighborhood distance, and some probabilistic variants based on copula models. An extensive comparison is provided, showing the advantages of the copula-based strategy, especially in those scenarios that exhibit complex structures (e.g., multimodalities or particular dependencies). Finally, we discuss the practical implications of our findings for estimating HDRs in real-world applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.00245&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nina Deliu, Brunero Liseo</name></author><category term="stat.ME" /><summary type="html">Among the variety of statistical intervals, highest-density regions (HDRs) stand out for their ability to effectively summarize a distribution or sample, unveiling its distinctive and salient features. An HDR represents the minimum size set that satisfies a certain probability coverage, and current methods for their computation require knowledge or estimation of the underlying probability distribution or density $f$. In this work, we illustrate a broader framework for computing HDRs, which generalizes the classical density quantile method introduced in the seminal paper of Hyndman (1996). The framework is based on neighbourhood measures, i.e., measures that preserve the order induced in the sample by $f$, and include the density $f$ as a special case. We explore a number of suitable distance-based measures, such as the $k$-nearest neighborhood distance, and some probabilistic variants based on copula models. An extensive comparison is provided, showing the advantages of the copula-based strategy, especially in those scenarios that exhibit complex structures (e.g., multimodalities or particular dependencies). Finally, we discuss the practical implications of our findings for estimating HDRs in real-world applications.</summary></entry><entry><title type="html">An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed Bandits</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AnExperimentalDesignforAnytimeValidCausalInferenceonMultiArmedBandits.html" rel="alternate" type="text/html" title="An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed Bandits" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AnExperimentalDesignforAnytimeValidCausalInferenceonMultiArmedBandits</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AnExperimentalDesignforAnytimeValidCausalInferenceonMultiArmedBandits.html">&lt;p&gt;Experimentation is crucial for managers to rigorously quantify the value of a change and determine if it leads to a statistically significant improvement over the status quo, thus augmenting their decision-making. Many companies now mandate that all changes undergo experimentation, presenting two challenges: (1) reducing the risk/cost of experimentation by minimizing the proportion of customers assigned to the inferior treatment and (2) increasing the experimentation velocity by enabling managers to stop experiments as soon as results are statistically significant. This paper simultaneously addresses both challenges by proposing the Mixture Adaptive Design (MAD), a new experimental design for multi-armed bandit (MAB) algorithms that enables anytime valid inference on the Average Treatment Effect (ATE) for any MAB algorithm. Intuitively, the MAB “mixes” any bandit algorithm with a Bernoulli design such that at each time step, the probability that a customer is assigned via the Bernoulli design is controlled by a user-specified deterministic sequence that can converge to zero. The sequence enables managers to directly and interpretably control the trade-off between regret minimization and inferential precision. Under mild conditions on the rate the sequence converges to zero, we provide a confidence sequence that is asymptotically anytime valid and demonstrate that the MAD is guaranteed to have a finite stopping time in the presence of a true non-zero ATE. Hence, the MAD allows managers to stop experiments early when a significant ATE is detected while ensuring valid inference, enhancing both the efficiency and reliability of adaptive experiments. Empirically, we demonstrate that the MAD achieves finite-sample anytime-validity while accurately and precisely estimating the ATE, all without incurring significant losses in reward compared to standard bandit designs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.05794&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Biyonka Liang, Iavor Bojinov</name></author><category term="stat.ME" /><summary type="html">Experimentation is crucial for managers to rigorously quantify the value of a change and determine if it leads to a statistically significant improvement over the status quo, thus augmenting their decision-making. Many companies now mandate that all changes undergo experimentation, presenting two challenges: (1) reducing the risk/cost of experimentation by minimizing the proportion of customers assigned to the inferior treatment and (2) increasing the experimentation velocity by enabling managers to stop experiments as soon as results are statistically significant. This paper simultaneously addresses both challenges by proposing the Mixture Adaptive Design (MAD), a new experimental design for multi-armed bandit (MAB) algorithms that enables anytime valid inference on the Average Treatment Effect (ATE) for any MAB algorithm. Intuitively, the MAB “mixes” any bandit algorithm with a Bernoulli design such that at each time step, the probability that a customer is assigned via the Bernoulli design is controlled by a user-specified deterministic sequence that can converge to zero. The sequence enables managers to directly and interpretably control the trade-off between regret minimization and inferential precision. Under mild conditions on the rate the sequence converges to zero, we provide a confidence sequence that is asymptotically anytime valid and demonstrate that the MAD is guaranteed to have a finite stopping time in the presence of a true non-zero ATE. Hence, the MAD allows managers to stop experiments early when a significant ATE is detected while ensuring valid inference, enhancing both the efficiency and reliability of adaptive experiments. Empirically, we demonstrate that the MAD achieves finite-sample anytime-validity while accurately and precisely estimating the ATE, all without incurring significant losses in reward compared to standard bandit designs.</summary></entry><entry><title type="html">An extended generalized Pareto regression model for count data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AnextendedgeneralizedParetoregressionmodelforcountdata.html" rel="alternate" type="text/html" title="An extended generalized Pareto regression model for count data" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AnextendedgeneralizedParetoregressionmodelforcountdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AnextendedgeneralizedParetoregressionmodelforcountdata.html">&lt;p&gt;The statistical modeling of discrete extremes has received less attention than their continuous counterparts in the Extreme Value Theory (EVT) literature. One approach to the transition from continuous to discrete extremes is the modeling of threshold exceedances of integer random variables by the discrete version of the generalized Pareto distribution. However, the optimal choice of thresholds defining exceedances remains a problematic issue. Moreover, in a regression framework, the treatment of the majority of non-extreme data below the selected threshold is either ignored or separated from the extremes. To tackle these issues, we expand on the concept of employing a smooth transition between the bulk and the upper tail of the distribution. In the case of zero inflation, we also develop models with an additional parameter. To incorporate possible predictors, we relate the parameters to additive smoothed predictors via an appropriate link, as in the generalized additive model (GAM) framework. A penalized maximum likelihood estimation procedure is implemented. We illustrate our modeling proposal with a real dataset of avalanche activity in the French Alps. With the advantage of bypassing the threshold selection step, our results indicate that the proposed models are more flexible and robust than competing models, such as the negative binomial distribution&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.15253&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Touqeer Ahmad, Carlo Gaetan, Philippe Naveau</name></author><category term="stat.ME" /><summary type="html">The statistical modeling of discrete extremes has received less attention than their continuous counterparts in the Extreme Value Theory (EVT) literature. One approach to the transition from continuous to discrete extremes is the modeling of threshold exceedances of integer random variables by the discrete version of the generalized Pareto distribution. However, the optimal choice of thresholds defining exceedances remains a problematic issue. Moreover, in a regression framework, the treatment of the majority of non-extreme data below the selected threshold is either ignored or separated from the extremes. To tackle these issues, we expand on the concept of employing a smooth transition between the bulk and the upper tail of the distribution. In the case of zero inflation, we also develop models with an additional parameter. To incorporate possible predictors, we relate the parameters to additive smoothed predictors via an appropriate link, as in the generalized additive model (GAM) framework. A penalized maximum likelihood estimation procedure is implemented. We illustrate our modeling proposal with a real dataset of avalanche activity in the French Alps. With the advantage of bypassing the threshold selection step, our results indicate that the proposed models are more flexible and robust than competing models, such as the negative binomial distribution</summary></entry><entry><title type="html">Asymptotic Properties of Matthews Correlation Coefficient</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AsymptoticPropertiesofMatthewsCorrelationCoefficient.html" rel="alternate" type="text/html" title="Asymptotic Properties of Matthews Correlation Coefficient" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AsymptoticPropertiesofMatthewsCorrelationCoefficient</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/AsymptoticPropertiesofMatthewsCorrelationCoefficient.html">&lt;p&gt;Evaluating classifications is crucial in statistics and machine learning, as it influences decision-making across various fields, such as patient prognosis and therapy in critical conditions. The Matthews correlation coefficient (MCC) is recognized as a performance metric with high reliability, offering a balanced measurement even in the presence of class imbalances. Despite its importance, there remains a notable lack of comprehensive research on the statistical inference of MCC. This deficiency often leads to studies merely validating and comparing MCC point estimates, a practice that, while common, overlooks the statistical significance and reliability of results. Addressing this research gap, our paper introduces and evaluates several methods to construct asymptotic confidence intervals for the single MCC and the differences between MCCs in paired designs. Through simulations across various scenarios, we evaluate the finite-sample behavior of these methods and compare their performances. Furthermore, through real data analysis, we illustrate the potential utility of our findings in comparing binary classifiers, highlighting the possible contributions of our research in this field.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12622&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuki Itaya, Jun Tamura, Kenichi Hayashi, Kouji Yamamoto</name></author><category term="stat.ME" /><summary type="html">Evaluating classifications is crucial in statistics and machine learning, as it influences decision-making across various fields, such as patient prognosis and therapy in critical conditions. The Matthews correlation coefficient (MCC) is recognized as a performance metric with high reliability, offering a balanced measurement even in the presence of class imbalances. Despite its importance, there remains a notable lack of comprehensive research on the statistical inference of MCC. This deficiency often leads to studies merely validating and comparing MCC point estimates, a practice that, while common, overlooks the statistical significance and reliability of results. Addressing this research gap, our paper introduces and evaluates several methods to construct asymptotic confidence intervals for the single MCC and the differences between MCCs in paired designs. Through simulations across various scenarios, we evaluate the finite-sample behavior of these methods and compare their performances. Furthermore, through real data analysis, we illustrate the potential utility of our findings in comparing binary classifiers, highlighting the possible contributions of our research in this field.</summary></entry><entry><title type="html">Background Modeling for Double Higgs Boson Production: Density Ratios and Optimal Transport</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BackgroundModelingforDoubleHiggsBosonProductionDensityRatiosandOptimalTransport.html" rel="alternate" type="text/html" title="Background Modeling for Double Higgs Boson Production: Density Ratios and Optimal Transport" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BackgroundModelingforDoubleHiggsBosonProductionDensityRatiosandOptimalTransport</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BackgroundModelingforDoubleHiggsBosonProductionDensityRatiosandOptimalTransport.html">&lt;p&gt;We study the problem of data-driven background estimation, arising in the search of physics signals predicted by the Standard Model at the Large Hadron Collider. Our work is motivated by the search for the production of pairs of Higgs bosons decaying into four bottom quarks. A number of other physical processes, known as background, also share the same final state. The data arising in this problem is therefore a mixture of unlabeled background and signal events, and the primary aim of the analysis is to determine whether the proportion of unlabeled signal events is nonzero. A challenging but necessary first step is to estimate the distribution of background events. Past work in this area has determined regions of the space of collider events where signal is unlikely to appear, and where the background distribution is therefore identifiable. The background distribution can be estimated in these regions, and extrapolated into the region of primary interest using transfer learning with a multivariate classifier. We build upon this existing approach in two ways. First, we revisit this method by developing a customized residual neural network which is tailored to the structure and symmetries of collider data. Second, we develop a new method for background estimation, based on the optimal transport problem, which relies on modeling assumptions distinct from earlier work. These two methods can serve as cross-checks for each other in particle physics analyses, due to the complementarity of their underlying assumptions. We compare their performance on simulated double Higgs boson data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.02807&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tudor Manole, Patrick Bryant, John Alison, Mikael Kuusela, Larry Wasserman</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">We study the problem of data-driven background estimation, arising in the search of physics signals predicted by the Standard Model at the Large Hadron Collider. Our work is motivated by the search for the production of pairs of Higgs bosons decaying into four bottom quarks. A number of other physical processes, known as background, also share the same final state. The data arising in this problem is therefore a mixture of unlabeled background and signal events, and the primary aim of the analysis is to determine whether the proportion of unlabeled signal events is nonzero. A challenging but necessary first step is to estimate the distribution of background events. Past work in this area has determined regions of the space of collider events where signal is unlikely to appear, and where the background distribution is therefore identifiable. The background distribution can be estimated in these regions, and extrapolated into the region of primary interest using transfer learning with a multivariate classifier. We build upon this existing approach in two ways. First, we revisit this method by developing a customized residual neural network which is tailored to the structure and symmetries of collider data. Second, we develop a new method for background estimation, based on the optimal transport problem, which relies on modeling assumptions distinct from earlier work. These two methods can serve as cross-checks for each other in particle physics analyses, due to the complementarity of their underlying assumptions. We compare their performance on simulated double Higgs boson data.</summary></entry><entry><title type="html">Bayesian Hierarchical Modelling of Noisy Gamma Processes: Model Formulation, Identifiability, Model Fitting, and Extensions to Unit-to-Unit Variability</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianHierarchicalModellingofNoisyGammaProcessesModelFormulationIdentifiabilityModelFittingandExtensionstoUnittoUnitVariability.html" rel="alternate" type="text/html" title="Bayesian Hierarchical Modelling of Noisy Gamma Processes: Model Formulation, Identifiability, Model Fitting, and Extensions to Unit-to-Unit Variability" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianHierarchicalModellingofNoisyGammaProcessesModelFormulationIdentifiabilityModelFittingandExtensionstoUnittoUnitVariability</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianHierarchicalModellingofNoisyGammaProcessesModelFormulationIdentifiabilityModelFittingandExtensionstoUnittoUnitVariability.html">&lt;p&gt;The gamma process is a natural model for monotonic degradation processes. In practice, it is desirable to extend the single gamma process to incorporate measurement error and to construct models for the degradation of several nominally identical units. In this paper, we show how these extensions are easily facilitated through the Bayesian hierarchical modelling framework. Following the precepts of the Bayesian statistical workflow, we show the principled construction of a noisy gamma process model. We also reparameterise the gamma process to simplify the specification of priors and make it obvious how the single gamma process model can be extended to include unit-to-unit variability or covariates. We first fit the noisy gamma process model to a single simulated degradation trace. In doing so, we find an identifiability problem between the volatility of the gamma process and the measurement error when there are only a few noisy degradation observations. However, this lack of identifiability can be resolved by including extra information in the analysis through a stronger prior or extra data that informs one of the non-identifiable parameters, or by borrowing information from multiple units. We then explore extensions of the model to account for unit-to-unit variability and demonstrate them using a crack-propagation data set with added measurement error. Lastly, we perform model selection in a fully Bayesian framework by using cross-validation to approximate the expected log probability density of new observation. We also show how failure time distributions with uncertainty intervals can be calculated for new units or units that are currently under test but are yet to fail.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11216&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ryan Leadbetter, Gabriel Gonzalez Caceres, Aloke Phatak</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The gamma process is a natural model for monotonic degradation processes. In practice, it is desirable to extend the single gamma process to incorporate measurement error and to construct models for the degradation of several nominally identical units. In this paper, we show how these extensions are easily facilitated through the Bayesian hierarchical modelling framework. Following the precepts of the Bayesian statistical workflow, we show the principled construction of a noisy gamma process model. We also reparameterise the gamma process to simplify the specification of priors and make it obvious how the single gamma process model can be extended to include unit-to-unit variability or covariates. We first fit the noisy gamma process model to a single simulated degradation trace. In doing so, we find an identifiability problem between the volatility of the gamma process and the measurement error when there are only a few noisy degradation observations. However, this lack of identifiability can be resolved by including extra information in the analysis through a stronger prior or extra data that informs one of the non-identifiable parameters, or by borrowing information from multiple units. We then explore extensions of the model to account for unit-to-unit variability and demonstrate them using a crack-propagation data set with added measurement error. Lastly, we perform model selection in a fully Bayesian framework by using cross-validation to approximate the expected log probability density of new observation. We also show how failure time distributions with uncertainty intervals can be calculated for new units or units that are currently under test but are yet to fail.</summary></entry><entry><title type="html">Bayesian Inference for Consistent Predictions in Overparameterized Nonlinear Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianInferenceforConsistentPredictionsinOverparameterizedNonlinearRegression.html" rel="alternate" type="text/html" title="Bayesian Inference for Consistent Predictions in Overparameterized Nonlinear Regression" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianInferenceforConsistentPredictionsinOverparameterizedNonlinearRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianInferenceforConsistentPredictionsinOverparameterizedNonlinearRegression.html">&lt;p&gt;The remarkable generalization performance of large-scale models has been challenging the conventional wisdom of the statistical learning theory. Although recent theoretical studies have shed light on this behavior in linear models and nonlinear classifiers, a comprehensive understanding of overparameterization in nonlinear regression models is still lacking. This study explores the predictive properties of overparameterized nonlinear regression within the Bayesian framework, extending the methodology of the adaptive prior considering the intrinsic spectral structure of the data. Posterior contraction is established for generalized linear and single-neuron models with Lipschitz continuous activation functions, demonstrating the consistency in the predictions of the proposed approach. Moreover, the Bayesian framework enables uncertainty estimation of the predictions. The proposed method was validated via numerical simulations and a real data application, showing its ability to achieve accurate predictions and reliable uncertainty estimates. This work provides a theoretical understanding of the advantages of overparameterization and a principled Bayesian approach to large nonlinear models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.04498&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tomoya Wakayama</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">The remarkable generalization performance of large-scale models has been challenging the conventional wisdom of the statistical learning theory. Although recent theoretical studies have shed light on this behavior in linear models and nonlinear classifiers, a comprehensive understanding of overparameterization in nonlinear regression models is still lacking. This study explores the predictive properties of overparameterized nonlinear regression within the Bayesian framework, extending the methodology of the adaptive prior considering the intrinsic spectral structure of the data. Posterior contraction is established for generalized linear and single-neuron models with Lipschitz continuous activation functions, demonstrating the consistency in the predictions of the proposed approach. Moreover, the Bayesian framework enables uncertainty estimation of the predictions. The proposed method was validated via numerical simulations and a real data application, showing its ability to achieve accurate predictions and reliable uncertainty estimates. This work provides a theoretical understanding of the advantages of overparameterization and a principled Bayesian approach to large nonlinear models.</summary></entry><entry><title type="html">Bayesian Networks and Machine Learning for COVID-19 Severity Explanation and Demographic Symptom Classification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianNetworksandMachineLearningforCOVID19SeverityExplanationandDemographicSymptomClassification.html" rel="alternate" type="text/html" title="Bayesian Networks and Machine Learning for COVID-19 Severity Explanation and Demographic Symptom Classification" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianNetworksandMachineLearningforCOVID19SeverityExplanationandDemographicSymptomClassification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianNetworksandMachineLearningforCOVID19SeverityExplanationandDemographicSymptomClassification.html">&lt;p&gt;With the prevailing efforts to combat the coronavirus disease 2019 (COVID-19) pandemic, there are still uncertainties that are yet to be discovered about its spread, future impact, and resurgence. In this paper, we present a three-stage data-driven approach to distill the hidden information about COVID-19. The first stage employs a Bayesian network structure learning method to identify the causal relationships among COVID-19 symptoms and their intrinsic demographic variables. As a second stage, the output from the Bayesian network structure learning, serves as a useful guide to train an unsupervised machine learning (ML) algorithm that uncovers the similarities in patients’ symptoms through clustering. The final stage then leverages the labels obtained from clustering to train a demographic symptom identification (DSID) model which predicts a patient’s symptom class and the corresponding demographic probability distribution. We applied our method on the COVID-19 dataset obtained from the Centers for Disease Control and Prevention (CDC) in the United States. Results from the experiments show a testing accuracy of 99.99\%, as against the 41.15\% accuracy of a heuristic ML method. This strongly reveals the viability of our Bayesian network and ML approach in understanding the relationship between the virus symptoms, and providing insights on patients’ stratification towards reducing the severity of the virus.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10807&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Oluwaseun T. Ajayi, Yu Cheng</name></author><category term="stat.ML," /><category term="stat.AP" /><summary type="html">With the prevailing efforts to combat the coronavirus disease 2019 (COVID-19) pandemic, there are still uncertainties that are yet to be discovered about its spread, future impact, and resurgence. In this paper, we present a three-stage data-driven approach to distill the hidden information about COVID-19. The first stage employs a Bayesian network structure learning method to identify the causal relationships among COVID-19 symptoms and their intrinsic demographic variables. As a second stage, the output from the Bayesian network structure learning, serves as a useful guide to train an unsupervised machine learning (ML) algorithm that uncovers the similarities in patients’ symptoms through clustering. The final stage then leverages the labels obtained from clustering to train a demographic symptom identification (DSID) model which predicts a patient’s symptom class and the corresponding demographic probability distribution. We applied our method on the COVID-19 dataset obtained from the Centers for Disease Control and Prevention (CDC) in the United States. Results from the experiments show a testing accuracy of 99.99\%, as against the 41.15\% accuracy of a heuristic ML method. This strongly reveals the viability of our Bayesian network and ML approach in understanding the relationship between the virus symptoms, and providing insights on patients’ stratification towards reducing the severity of the virus.</summary></entry><entry><title type="html">Bayesian Outcome Weighted Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianOutcomeWeightedLearning.html" rel="alternate" type="text/html" title="Bayesian Outcome Weighted Learning" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianOutcomeWeightedLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianOutcomeWeightedLearning.html">&lt;p&gt;One of the primary goals of statistical precision medicine is to learn optimal individualized treatment rules (ITRs). The classification-based, or machine learning-based, approach to estimating optimal ITRs was first introduced in outcome-weighted learning (OWL). OWL recasts the optimal ITR learning problem into a weighted classification problem, which can be solved using machine learning methods, e.g., support vector machines. In this paper, we introduce a Bayesian formulation of OWL. Starting from the OWL objective function, we generate a pseudo-likelihood which can be expressed as a scale mixture of normal distributions. A Gibbs sampling algorithm is developed to sample the posterior distribution of the parameters. In addition to providing a strategy for learning an optimal ITR, Bayesian OWL provides a natural, probabilistic approach to estimate uncertainty in ITR treatment recommendations themselves. We demonstrate the performance of our method through several simulation studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11573&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sophia Yazzourh, Nikki L. B. Freeman</name></author><category term="stat.ME" /><summary type="html">One of the primary goals of statistical precision medicine is to learn optimal individualized treatment rules (ITRs). The classification-based, or machine learning-based, approach to estimating optimal ITRs was first introduced in outcome-weighted learning (OWL). OWL recasts the optimal ITR learning problem into a weighted classification problem, which can be solved using machine learning methods, e.g., support vector machines. In this paper, we introduce a Bayesian formulation of OWL. Starting from the OWL objective function, we generate a pseudo-likelihood which can be expressed as a scale mixture of normal distributions. A Gibbs sampling algorithm is developed to sample the posterior distribution of the parameters. In addition to providing a strategy for learning an optimal ITR, Bayesian OWL provides a natural, probabilistic approach to estimate uncertainty in ITR treatment recommendations themselves. We demonstrate the performance of our method through several simulation studies.</summary></entry><entry><title type="html">Bayesian Variable Selection via Hierarchical Gaussian Process Model in Computer Experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianVariableSelectionviaHierarchicalGaussianProcessModelinComputerExperiments.html" rel="alternate" type="text/html" title="Bayesian Variable Selection via Hierarchical Gaussian Process Model in Computer Experiments" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianVariableSelectionviaHierarchicalGaussianProcessModelinComputerExperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianVariableSelectionviaHierarchicalGaussianProcessModelinComputerExperiments.html">&lt;p&gt;Identifying the active factors that have significant impacts on the output of the complex system is an important but challenging variable selection problem in computer experiments. In this paper, a Bayesian hierarchical Gaussian process model is developed and some latent indicator variables are embedded into this setting for the sake of labelling the important variables. The parameter estimation and variable selection can be processed simultaneously in a full Bayesian framework through an efficient Markov Chain Monte Carlo (MCMC) method – Metropolis-within-Gibbs sampler. The much better performances of the proposed method compared with the related competitors are evaluated by the analysis of simulated examples and a practical application.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11306&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xiao Yao, Ning Jianhui, Qin Hong</name></author><category term="stat.ME" /><summary type="html">Identifying the active factors that have significant impacts on the output of the complex system is an important but challenging variable selection problem in computer experiments. In this paper, a Bayesian hierarchical Gaussian process model is developed and some latent indicator variables are embedded into this setting for the sake of labelling the important variables. The parameter estimation and variable selection can be processed simultaneously in a full Bayesian framework through an efficient Markov Chain Monte Carlo (MCMC) method – Metropolis-within-Gibbs sampler. The much better performances of the proposed method compared with the related competitors are evaluated by the analysis of simulated examples and a practical application.</summary></entry><entry><title type="html">Bayesian inference for aggregated Hawkes processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianinferenceforaggregatedHawkesprocesses.html" rel="alternate" type="text/html" title="Bayesian inference for aggregated Hawkes processes" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianinferenceforaggregatedHawkesprocesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianinferenceforaggregatedHawkesprocesses.html">&lt;p&gt;The Hawkes process, a self-exciting point process, has a wide range of applications in modeling earthquakes, social networks and stock markets. The established estimation process requires that researchers have access to the exact time stamps and spatial information. However, available data are often rounded or aggregated. We develop a Bayesian estimation procedure for the parameters of a Hawkes process based on aggregated data. Our approach is developed for temporal, spatio-temporal, and mutually exciting Hawkes processes where data are available over discrete time periods and regions. We show theoretically that the parameters of the Hawkes process are identifiable from aggregated data under general specifications. We demonstrate the method on simulated temporal and spatio-temporal data with various model specifications in the presence of one or more interacting processes, and under varying coarseness of data aggregation. Finally, we examine the internal and cross-excitation effects of airstrikes and insurgent violence events from February 2007 to June 2008, with some data aggregated by day.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.16552&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lingxiao Zhou, Georgia Papadogeorgou</name></author><category term="stat.ME" /><summary type="html">The Hawkes process, a self-exciting point process, has a wide range of applications in modeling earthquakes, social networks and stock markets. The established estimation process requires that researchers have access to the exact time stamps and spatial information. However, available data are often rounded or aggregated. We develop a Bayesian estimation procedure for the parameters of a Hawkes process based on aggregated data. Our approach is developed for temporal, spatio-temporal, and mutually exciting Hawkes processes where data are available over discrete time periods and regions. We show theoretically that the parameters of the Hawkes process are identifiable from aggregated data under general specifications. We demonstrate the method on simulated temporal and spatio-temporal data with various model specifications in the presence of one or more interacting processes, and under varying coarseness of data aggregation. Finally, we examine the internal and cross-excitation effects of airstrikes and insurgent violence events from February 2007 to June 2008, with some data aggregated by day.</summary></entry><entry><title type="html">Bayesian regression discontinuity design with unknown cutoff</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Bayesianregressiondiscontinuitydesignwithunknowncutoff.html" rel="alternate" type="text/html" title="Bayesian regression discontinuity design with unknown cutoff" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Bayesianregressiondiscontinuitydesignwithunknowncutoff</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Bayesianregressiondiscontinuitydesignwithunknowncutoff.html">&lt;p&gt;Regression discontinuity design (RDD) is a quasi-experimental approach used to estimate the causal effects of an intervention assigned based on a cutoff criterion. RDD exploits the idea that close to the cutoff units below and above are similar; hence, they can be meaningfully compared. Consequently, the causal effect can be estimated only locally at the cutoff point. This makes the cutoff point an essential element of RDD. However, especially in medical applications, the exact cutoff location may not always be disclosed to the researcher, and even when it is, the actual location may deviate from the official one. As we illustrate on the application of RDD to the HIV treatment eligibility data, estimating the causal effect at an incorrect cutoff point leads to meaningless results. Moreover, since the cutoff criterion often acts as a guideline rather than as a strict rule, the location of the cutoff may be unclear from the data. The method we present can be applied both as an estimation and validation tool in RDD. We use a Bayesian approach to incorporate prior knowledge and uncertainty about the cutoff location in the causal effect estimation. At the same time, our Bayesian model LoTTA is fitted globally to the whole data, whereas RDD is a local, boundary point estimation problem. In this work we address a natural question that arises: how to make Bayesian inference more local to render a meaningful and powerful estimate of the treatment effect?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11585&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Julia Kowalska, Mark van de Wiel, Stéphanie van der Pas</name></author><category term="stat.ME" /><summary type="html">Regression discontinuity design (RDD) is a quasi-experimental approach used to estimate the causal effects of an intervention assigned based on a cutoff criterion. RDD exploits the idea that close to the cutoff units below and above are similar; hence, they can be meaningfully compared. Consequently, the causal effect can be estimated only locally at the cutoff point. This makes the cutoff point an essential element of RDD. However, especially in medical applications, the exact cutoff location may not always be disclosed to the researcher, and even when it is, the actual location may deviate from the official one. As we illustrate on the application of RDD to the HIV treatment eligibility data, estimating the causal effect at an incorrect cutoff point leads to meaningless results. Moreover, since the cutoff criterion often acts as a guideline rather than as a strict rule, the location of the cutoff may be unclear from the data. The method we present can be applied both as an estimation and validation tool in RDD. We use a Bayesian approach to incorporate prior knowledge and uncertainty about the cutoff location in the causal effect estimation. At the same time, our Bayesian model LoTTA is fitted globally to the whole data, whereas RDD is a local, boundary point estimation problem. In this work we address a natural question that arises: how to make Bayesian inference more local to render a meaningful and powerful estimate of the treatment effect?</summary></entry><entry><title type="html">Causal Inference with Outcomes Truncated by Death and Missing Not at Random</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/CausalInferencewithOutcomesTruncatedbyDeathandMissingNotatRandom.html" rel="alternate" type="text/html" title="Causal Inference with Outcomes Truncated by Death and Missing Not at Random" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/CausalInferencewithOutcomesTruncatedbyDeathandMissingNotatRandom</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/CausalInferencewithOutcomesTruncatedbyDeathandMissingNotatRandom.html">&lt;p&gt;In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10554&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wei Li, Yuan Liu, Shanshan Luo, Zhi Geng</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study.</summary></entry><entry><title type="html">Causal inference for N-of-1 trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/CausalinferenceforNof1trials.html" rel="alternate" type="text/html" title="Causal inference for N-of-1 trials" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/CausalinferenceforNof1trials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/CausalinferenceforNof1trials.html">&lt;p&gt;The aim of personalized medicine is to tailor treatment decisions to individuals’ characteristics. N-of-1 trials are within-person crossover trials that hold the promise of targeting individual-specific effects. While the idea behind N-of-1 trials might seem simple, analyzing and interpreting N-of-1 trials is not straightforward. In particular, there exists confusion about the role of randomization in this design, the (implicit) target estimand, and the need for covariate adjustment. Here we ground N-of-1 trials in a formal causal inference framework and formalize intuitive claims from the N-of-1 trial literature. We focus on causal inference from a single N-of-1 trial and define a conditional average treatment effect (CATE) that represents a target in this setting, which we call the U-CATE. We discuss the assumptions sufficient for identification and estimation of the U-CATE under different causal models in which the treatment schedule is assigned at baseline. A simple mean difference is shown to be an unbiased, asymptotically normal estimator of the U-CATE in simple settings, such as when participants have stable conditions (e.g., chronic pain) and interventions have effects limited in time (no carryover). We also consider settings where carryover effects, trends over time, time-varying common causes of the outcome, and outcome-outcome effects are present. In these more complex settings, we show that a time-varying g-formula identifies the U-CATE under explicit assumptions. Finally, we analyze data from N-of-1 trials about acne symptoms. Using this example, we show how different assumptions about the underlying data generating process can lead to different analytical strategies in N-of-1 trials.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10360&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marco Piccininni, Mats J. Stensrud, Zachary Shahn, Stefan Konigorski</name></author><category term="stat.ME" /><summary type="html">The aim of personalized medicine is to tailor treatment decisions to individuals’ characteristics. N-of-1 trials are within-person crossover trials that hold the promise of targeting individual-specific effects. While the idea behind N-of-1 trials might seem simple, analyzing and interpreting N-of-1 trials is not straightforward. In particular, there exists confusion about the role of randomization in this design, the (implicit) target estimand, and the need for covariate adjustment. Here we ground N-of-1 trials in a formal causal inference framework and formalize intuitive claims from the N-of-1 trial literature. We focus on causal inference from a single N-of-1 trial and define a conditional average treatment effect (CATE) that represents a target in this setting, which we call the U-CATE. We discuss the assumptions sufficient for identification and estimation of the U-CATE under different causal models in which the treatment schedule is assigned at baseline. A simple mean difference is shown to be an unbiased, asymptotically normal estimator of the U-CATE in simple settings, such as when participants have stable conditions (e.g., chronic pain) and interventions have effects limited in time (no carryover). We also consider settings where carryover effects, trends over time, time-varying common causes of the outcome, and outcome-outcome effects are present. In these more complex settings, we show that a time-varying g-formula identifies the U-CATE under explicit assumptions. Finally, we analyze data from N-of-1 trials about acne symptoms. Using this example, we show how different assumptions about the underlying data generating process can lead to different analytical strategies in N-of-1 trials.</summary></entry><entry><title type="html">Choosing a Proxy Metric from Past Experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ChoosingaProxyMetricfromPastExperiments.html" rel="alternate" type="text/html" title="Choosing a Proxy Metric from Past Experiments" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ChoosingaProxyMetricfromPastExperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ChoosingaProxyMetricfromPastExperiments.html">&lt;p&gt;In many randomized experiments, the treatment effect of the long-term metric (i.e. the primary outcome of interest) is often difficult or infeasible to measure. Such long-term metrics are often slow to react to changes and sufficiently noisy they are challenging to faithfully estimate in short-horizon experiments. A common alternative is to measure several short-term proxy metrics in the hope they closely track the long-term metric – so they can be used to effectively guide decision-making in the near-term. We introduce a new statistical framework to both define and construct an optimal proxy metric for use in a homogeneous population of randomized experiments. Our procedure first reduces the construction of an optimal proxy metric in a given experiment to a portfolio optimization problem which depends on the true latent treatment effects and noise level of experiment under consideration. We then denoise the observed treatment effects of the long-term metric and a set of proxies in a historical corpus of randomized experiments to extract estimates of the latent treatment effects for use in the optimization problem. One key insight derived from our approach is that the optimal proxy metric for a given experiment is not apriori fixed; rather it should depend on the sample size (or effective noise level) of the randomized experiment for which it is deployed. To instantiate and evaluate our framework, we employ our methodology in a large corpus of randomized experiments from an industrial recommendation system and construct proxy metrics that perform favorably relative to several baselines.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.07893&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nilesh Tripuraneni, Lee Richardson, Alexander D&apos;Amour, Jacopo Soriano, Steve Yadlowsky</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">In many randomized experiments, the treatment effect of the long-term metric (i.e. the primary outcome of interest) is often difficult or infeasible to measure. Such long-term metrics are often slow to react to changes and sufficiently noisy they are challenging to faithfully estimate in short-horizon experiments. A common alternative is to measure several short-term proxy metrics in the hope they closely track the long-term metric – so they can be used to effectively guide decision-making in the near-term. We introduce a new statistical framework to both define and construct an optimal proxy metric for use in a homogeneous population of randomized experiments. Our procedure first reduces the construction of an optimal proxy metric in a given experiment to a portfolio optimization problem which depends on the true latent treatment effects and noise level of experiment under consideration. We then denoise the observed treatment effects of the long-term metric and a set of proxies in a historical corpus of randomized experiments to extract estimates of the latent treatment effects for use in the optimization problem. One key insight derived from our approach is that the optimal proxy metric for a given experiment is not apriori fixed; rather it should depend on the sample size (or effective noise level) of the randomized experiment for which it is deployed. To instantiate and evaluate our framework, we employ our methodology in a large corpus of randomized experiments from an industrial recommendation system and construct proxy metrics that perform favorably relative to several baselines.</summary></entry><entry><title type="html">Conformal prediction with local weights: randomization enables local guarantees</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Conformalpredictionwithlocalweightsrandomizationenableslocalguarantees.html" rel="alternate" type="text/html" title="Conformal prediction with local weights: randomization enables local guarantees" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Conformalpredictionwithlocalweightsrandomizationenableslocalguarantees</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Conformalpredictionwithlocalweightsrandomizationenableslocalguarantees.html">&lt;p&gt;In this work, we consider the problem of building distribution-free prediction intervals with finite-sample conditional coverage guarantees. Conformal prediction (CP) is an increasingly popular framework for building prediction intervals with distribution-free guarantees, but these guarantees only ensure marginal coverage: the probability of coverage is averaged over a random draw of both the training and test data, meaning that there might be substantial undercoverage within certain subpopulations. Instead, ideally, we would want to have local coverage guarantees that hold for each possible value of the test point’s features. While the impossibility of achieving pointwise local coverage is well established in the literature, many variants of conformal prediction algorithm show favorable local coverage properties empirically. Relaxing the definition of local coverage can allow for a theoretical understanding of this empirical phenomenon. We aim to bridge this gap between theoretical validation and empirical performance by proving achievable and interpretable guarantees for a relaxed notion of local coverage. Building on the localized CP method of Guan (2023) and the weighted CP framework of Tibshirani et al. (2019), we propose a new method, randomly-localized conformal prediction (RLCP), which returns prediction intervals that are not only marginally valid but also achieve a relaxed local coverage guarantee and guarantees under covariate shift. Through a series of simulations and real data experiments, we validate these coverage guarantees of RLCP while comparing it with the other local conformal prediction methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.07850&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rohan Hore, Rina Foygel Barber</name></author><category term="stat.ME" /><summary type="html">In this work, we consider the problem of building distribution-free prediction intervals with finite-sample conditional coverage guarantees. Conformal prediction (CP) is an increasingly popular framework for building prediction intervals with distribution-free guarantees, but these guarantees only ensure marginal coverage: the probability of coverage is averaged over a random draw of both the training and test data, meaning that there might be substantial undercoverage within certain subpopulations. Instead, ideally, we would want to have local coverage guarantees that hold for each possible value of the test point’s features. While the impossibility of achieving pointwise local coverage is well established in the literature, many variants of conformal prediction algorithm show favorable local coverage properties empirically. Relaxing the definition of local coverage can allow for a theoretical understanding of this empirical phenomenon. We aim to bridge this gap between theoretical validation and empirical performance by proving achievable and interpretable guarantees for a relaxed notion of local coverage. Building on the localized CP method of Guan (2023) and the weighted CP framework of Tibshirani et al. (2019), we propose a new method, randomly-localized conformal prediction (RLCP), which returns prediction intervals that are not only marginally valid but also achieve a relaxed local coverage guarantee and guarantees under covariate shift. Through a series of simulations and real data experiments, we validate these coverage guarantees of RLCP while comparing it with the other local conformal prediction methods.</summary></entry><entry><title type="html">Consider or Choose? The Role and Power of Consideration Sets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ConsiderorChooseTheRoleandPowerofConsiderationSets.html" rel="alternate" type="text/html" title="Consider or Choose? The Role and Power of Consideration Sets" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ConsiderorChooseTheRoleandPowerofConsiderationSets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ConsiderorChooseTheRoleandPowerofConsiderationSets.html">&lt;p&gt;Consideration sets play a crucial role in discrete choice modeling, where customers are commonly assumed to go through a two-stage decision making process. Specifically, customers are assumed to form consideration sets in the first stage and then use a second-stage choice mechanism to pick the product with the highest utility from the consideration sets. Recent studies mostly aim to propose more powerful choice mechanisms based on advanced non-parametric models to improve prediction accuracy. In contrast, this paper takes a step back from exploring more complex second-stage choice mechanisms and instead focus on how effectively we can model customer choice relying only on the first-stage consideration set formation. To this end, we study a class of nonparametric choice models that is only specified by a distribution over consideration sets and has a bounded rationality interpretation. We denote it as the consideration set model. Intriguingly, we show that this class of choice models can be characterized by the axiom of symmetric demand cannibalization, which enables complete statistical identification. We further consider the model’s downstream assortment planning as an application. We first present an exact description of the optimal assortment, proving that it is revenue-ordered based on the blocks defined by the consideration sets. Despite this compelling structure, we establish that the assortment optimization problem under this model is NP-hard even to approximate. This result shows that accounting for consideration sets in the model inevitably results in inapproximability in assortment planning, even though the consideration set model uses the simplest possible uniform second-stage choice mechanism. Finally, using a real-world dataset, we show the tremendous power of the first-stage consideration sets when modeling customers’ decision-making processes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.04354&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yi-Chun Akchen, Dmitry Mitrofanov</name></author><category term="stat.ME" /><summary type="html">Consideration sets play a crucial role in discrete choice modeling, where customers are commonly assumed to go through a two-stage decision making process. Specifically, customers are assumed to form consideration sets in the first stage and then use a second-stage choice mechanism to pick the product with the highest utility from the consideration sets. Recent studies mostly aim to propose more powerful choice mechanisms based on advanced non-parametric models to improve prediction accuracy. In contrast, this paper takes a step back from exploring more complex second-stage choice mechanisms and instead focus on how effectively we can model customer choice relying only on the first-stage consideration set formation. To this end, we study a class of nonparametric choice models that is only specified by a distribution over consideration sets and has a bounded rationality interpretation. We denote it as the consideration set model. Intriguingly, we show that this class of choice models can be characterized by the axiom of symmetric demand cannibalization, which enables complete statistical identification. We further consider the model’s downstream assortment planning as an application. We first present an exact description of the optimal assortment, proving that it is revenue-ordered based on the blocks defined by the consideration sets. Despite this compelling structure, we establish that the assortment optimization problem under this model is NP-hard even to approximate. This result shows that accounting for consideration sets in the model inevitably results in inapproximability in assortment planning, even though the consideration set model uses the simplest possible uniform second-stage choice mechanism. Finally, using a real-world dataset, we show the tremendous power of the first-stage consideration sets when modeling customers’ decision-making processes.</summary></entry><entry><title type="html">Copula-like inference for discrete bivariate distributions with rectangular supports</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Copulalikeinferencefordiscretebivariatedistributionswithrectangularsupports.html" rel="alternate" type="text/html" title="Copula-like inference for discrete bivariate distributions with rectangular supports" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Copulalikeinferencefordiscretebivariatedistributionswithrectangularsupports</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Copulalikeinferencefordiscretebivariatedistributionswithrectangularsupports.html">&lt;p&gt;After reviewing a large body of literature on the modeling of bivariate discrete distributions with finite support, \cite{Gee20} made a compelling case for the use of $I$-projections in the sense of \cite{Csi75} as a sound way to attempt to decompose a bivariate probability mass function (p.m.f.) into its two univariate margins and a bivariate p.m.f.\ with uniform margins playing the role of a discrete copula. From a practical perspective, the necessary $I$-projections on Fr&apos;echet classes can be carried out using the iterative proportional fitting procedure (IPFP), also known as Sinkhorn’s algorithm or matrix scaling in the literature. After providing conditions under which a bivariate p.m.f.\ can be decomposed in the aforementioned sense, we investigate, for starting bivariate p.m.f.s with rectangular supports, nonparametric and parametric estimation procedures as well as goodness-of-fit tests for the underlying discrete copula. Related asymptotic results are provided and build upon a differentiability result for $I$-projections on Fr&apos;echet classes which can be of independent interest. Theoretical results are complemented by finite-sample experiments and a data example.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.04225&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ivan Kojadinovic, Tommaso Martini</name></author><category term="stat.ME" /><summary type="html">After reviewing a large body of literature on the modeling of bivariate discrete distributions with finite support, \cite{Gee20} made a compelling case for the use of $I$-projections in the sense of \cite{Csi75} as a sound way to attempt to decompose a bivariate probability mass function (p.m.f.) into its two univariate margins and a bivariate p.m.f.\ with uniform margins playing the role of a discrete copula. From a practical perspective, the necessary $I$-projections on Fr&apos;echet classes can be carried out using the iterative proportional fitting procedure (IPFP), also known as Sinkhorn’s algorithm or matrix scaling in the literature. After providing conditions under which a bivariate p.m.f.\ can be decomposed in the aforementioned sense, we investigate, for starting bivariate p.m.f.s with rectangular supports, nonparametric and parametric estimation procedures as well as goodness-of-fit tests for the underlying discrete copula. Related asymptotic results are provided and build upon a differentiability result for $I$-projections on Fr&apos;echet classes which can be of independent interest. Theoretical results are complemented by finite-sample experiments and a data example.</summary></entry><entry><title type="html">Counterfactual Generative Models for Time-Varying Treatments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/CounterfactualGenerativeModelsforTimeVaryingTreatments.html" rel="alternate" type="text/html" title="Counterfactual Generative Models for Time-Varying Treatments" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/CounterfactualGenerativeModelsforTimeVaryingTreatments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/CounterfactualGenerativeModelsforTimeVaryingTreatments.html">&lt;p&gt;Estimating the counterfactual outcome of treatment is essential for decision-making in public health and clinical science, among others. Often, treatments are administered in a sequential, time-varying manner, leading to an exponentially increased number of possible counterfactual outcomes. Furthermore, in modern applications, the outcomes are high-dimensional and conventional average treatment effect estimation fails to capture disparities in individuals. To tackle these challenges, we propose a novel conditional generative framework capable of producing counterfactual samples under time-varying treatment, without the need for explicit density estimation. Our method carefully addresses the distribution mismatch between the observed and counterfactual distributions via a loss function based on inverse probability re-weighting, and supports integration with state-of-the-art conditional generative models such as the guided diffusion and conditional variational autoencoder. We present a thorough evaluation of our method using both synthetic and real-world data. Our results demonstrate that our method is capable of generating high-quality counterfactual samples and outperforms the state-of-the-art baselines.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.15742&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shenghao Wu, Wenbin Zhou, Minshuo Chen, Shixiang Zhu</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Estimating the counterfactual outcome of treatment is essential for decision-making in public health and clinical science, among others. Often, treatments are administered in a sequential, time-varying manner, leading to an exponentially increased number of possible counterfactual outcomes. Furthermore, in modern applications, the outcomes are high-dimensional and conventional average treatment effect estimation fails to capture disparities in individuals. To tackle these challenges, we propose a novel conditional generative framework capable of producing counterfactual samples under time-varying treatment, without the need for explicit density estimation. Our method carefully addresses the distribution mismatch between the observed and counterfactual distributions via a loss function based on inverse probability re-weighting, and supports integration with state-of-the-art conditional generative models such as the guided diffusion and conditional variational autoencoder. We present a thorough evaluation of our method using both synthetic and real-world data. Our results demonstrate that our method is capable of generating high-quality counterfactual samples and outperforms the state-of-the-art baselines.</summary></entry><entry><title type="html">DCDILP: a distributed learning method for large-scale causal structure learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/DCDILPadistributedlearningmethodforlargescalecausalstructurelearning.html" rel="alternate" type="text/html" title="DCDILP: a distributed learning method for large-scale causal structure learning" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/DCDILPadistributedlearningmethodforlargescalecausalstructurelearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/DCDILPadistributedlearningmethodforlargescalecausalstructurelearning.html">&lt;p&gt;This paper presents a novel approach to causal discovery through a divide-and-conquer framework. By decomposing the problem into smaller subproblems defined on Markov blankets, the proposed DCDILP method first explores in parallel the local causal graphs of these subproblems. However, this local discovery phase encounters systematic challenges due to the presence of hidden confounders (variables within each Markov blanket may be influenced by external variables). Moreover, aggregating these local causal graphs in a consistent global graph defines a large size combinatorial optimization problem. DCDILP addresses these challenges by: i) restricting the local subgraphs to causal links only related with the central variable of the Markov blanket; ii) formulating the reconciliation of local causal graphs as an integer linear programming method. The merits of the approach, in both terms of causal discovery accuracy and scalability in the size of the problem, are showcased by experiments and comparisons with the state of the art.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10481&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shuyu Dong, Michèle Sebag, Kento Uemura, Akito Fujii, Shuang Chang, Yusuke Koyanagi, Koji Maruhashi</name></author><category term="stat.ME" /><summary type="html">This paper presents a novel approach to causal discovery through a divide-and-conquer framework. By decomposing the problem into smaller subproblems defined on Markov blankets, the proposed DCDILP method first explores in parallel the local causal graphs of these subproblems. However, this local discovery phase encounters systematic challenges due to the presence of hidden confounders (variables within each Markov blanket may be influenced by external variables). Moreover, aggregating these local causal graphs in a consistent global graph defines a large size combinatorial optimization problem. DCDILP addresses these challenges by: i) restricting the local subgraphs to causal links only related with the central variable of the Markov blanket; ii) formulating the reconciliation of local causal graphs as an integer linear programming method. The merits of the approach, in both terms of causal discovery accuracy and scalability in the size of the problem, are showcased by experiments and comparisons with the state of the art.</summary></entry><entry><title type="html">Data-Adaptive Identification of Subpopulations Vulnerable to Chemical Exposures using Stochastic Interventions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/DataAdaptiveIdentificationofSubpopulationsVulnerabletoChemicalExposuresusingStochasticInterventions.html" rel="alternate" type="text/html" title="Data-Adaptive Identification of Subpopulations Vulnerable to Chemical Exposures using Stochastic Interventions" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/DataAdaptiveIdentificationofSubpopulationsVulnerabletoChemicalExposuresusingStochasticInterventions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/DataAdaptiveIdentificationofSubpopulationsVulnerabletoChemicalExposuresusingStochasticInterventions.html">&lt;p&gt;In environmental epidemiology, identifying subpopulations vulnerable to chemical exposures and those who may benefit differently from exposure-reducing policies is essential. For instance, sex-specific vulnerabilities, age, and pregnancy are critical factors for policymakers when setting regulatory guidelines. However, current semi-parametric methods for heterogeneous treatment effects are often limited to binary exposures and function as black boxes, lacking clear, interpretable rules for subpopulation-specific policy interventions. This study introduces a novel method using cross-validated targeted minimum loss-based estimation (TMLE) paired with a data-adaptive target parameter strategy to identify subpopulations with the most significant differential impact from simulated policy interventions that reduce exposure. Our approach is assumption-lean, allowing for the integration of machine learning while still yielding valid confidence intervals. We demonstrate the robustness of our methodology through simulations and application to NHANES data. Our analysis of NHANES data for persistent organic pollutants on leukocyte telomere length (LTL) identified age as the maximum effect modifier. Specifically, we found that exposure to 3,3’,4,4’,5-pentachlorobiphenyl (pcnb) consistently had a differential impact on LTL, with a one standard deviation reduction in exposure leading to a more pronounced increase in LTL among younger populations compared to older ones. We offer our method as an open-source software package, \texttt{EffectXshift}, enabling researchers to investigate the effect modification of continuous exposures. The \texttt{EffectXshift} package provides clear and interpretable results, informing targeted public health interventions and policy decisions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10792&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David McCoy, Wenxin Zhang, Alan Hubbard, Mark van der Laan, Alejandro Schuler</name></author><category term="stat.ME" /><summary type="html">In environmental epidemiology, identifying subpopulations vulnerable to chemical exposures and those who may benefit differently from exposure-reducing policies is essential. For instance, sex-specific vulnerabilities, age, and pregnancy are critical factors for policymakers when setting regulatory guidelines. However, current semi-parametric methods for heterogeneous treatment effects are often limited to binary exposures and function as black boxes, lacking clear, interpretable rules for subpopulation-specific policy interventions. This study introduces a novel method using cross-validated targeted minimum loss-based estimation (TMLE) paired with a data-adaptive target parameter strategy to identify subpopulations with the most significant differential impact from simulated policy interventions that reduce exposure. Our approach is assumption-lean, allowing for the integration of machine learning while still yielding valid confidence intervals. We demonstrate the robustness of our methodology through simulations and application to NHANES data. Our analysis of NHANES data for persistent organic pollutants on leukocyte telomere length (LTL) identified age as the maximum effect modifier. Specifically, we found that exposure to 3,3’,4,4’,5-pentachlorobiphenyl (pcnb) consistently had a differential impact on LTL, with a one standard deviation reduction in exposure leading to a more pronounced increase in LTL among younger populations compared to older ones. We offer our method as an open-source software package, \texttt{EffectXshift}, enabling researchers to investigate the effect modification of continuous exposures. The \texttt{EffectXshift} package provides clear and interpretable results, informing targeted public health interventions and policy decisions.</summary></entry><entry><title type="html">Data-driven uncertainty quantification for constrained stochastic differential equations and application to solar photovoltaic power forecast data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Datadrivenuncertaintyquantificationforconstrainedstochasticdifferentialequationsandapplicationtosolarphotovoltaicpowerforecastdata.html" rel="alternate" type="text/html" title="Data-driven uncertainty quantification for constrained stochastic differential equations and application to solar photovoltaic power forecast data" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Datadrivenuncertaintyquantificationforconstrainedstochasticdifferentialequationsandapplicationtosolarphotovoltaicpowerforecastdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Datadrivenuncertaintyquantificationforconstrainedstochasticdifferentialequationsandapplicationtosolarphotovoltaicpowerforecastdata.html">&lt;p&gt;In this work, we extend the data-driven It\^{o} stochastic differential equation (SDE) framework for the pathwise assessment of short-term forecast errors to account for the time-dependent upper bound that naturally constrains the observable historical data and forecast. We propose a new nonlinear and time-inhomogeneous SDE model with a Jacobi-type diffusion term for the phenomenon of interest, simultaneously driven by the forecast and the constraining upper bound. We rigorously demonstrate the existence and uniqueness of a strong solution to the SDE model by imposing a condition for the time-varying mean-reversion parameter appearing in the drift term. The normalized forecast function is thresholded to keep such mean-reversion parameters bounded. The SDE model parameter calibration is applied to user-selected approximations of the likelihood function. Another novel contribution is estimating the unknown transition density of the forecast error process with a tailored kernel smoothing technique with the control variate method, coupling an adequate SDE to the original one. We provide a theoretical study about how to choose the optimal bandwidth. We fit the model to the 2019 photovoltaic (PV) solar power daily production and forecast data in Uruguay, computing the daily maximum solar PV production estimation. Two statistical versions of the constrained SDE model are fit, with the beta and truncated normal distributions as proxies for the transition density. Empirical results include simulations of the normalized solar PV power production and pathwise confidence bands generated through an indirect inference method. An objective comparison of optimal parametric points associated with the two selected statistical approximations is provided by applying our innovative kernel smoothing estimation technique of the transition function of the forecast error process.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.13133&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Khaoula Ben Chaabane, Ahmed Kebaier, Marco Scavino, Raúl Tempone</name></author><category term="stat.ME" /><summary type="html">In this work, we extend the data-driven It\^{o} stochastic differential equation (SDE) framework for the pathwise assessment of short-term forecast errors to account for the time-dependent upper bound that naturally constrains the observable historical data and forecast. We propose a new nonlinear and time-inhomogeneous SDE model with a Jacobi-type diffusion term for the phenomenon of interest, simultaneously driven by the forecast and the constraining upper bound. We rigorously demonstrate the existence and uniqueness of a strong solution to the SDE model by imposing a condition for the time-varying mean-reversion parameter appearing in the drift term. The normalized forecast function is thresholded to keep such mean-reversion parameters bounded. The SDE model parameter calibration is applied to user-selected approximations of the likelihood function. Another novel contribution is estimating the unknown transition density of the forecast error process with a tailored kernel smoothing technique with the control variate method, coupling an adequate SDE to the original one. We provide a theoretical study about how to choose the optimal bandwidth. We fit the model to the 2019 photovoltaic (PV) solar power daily production and forecast data in Uruguay, computing the daily maximum solar PV production estimation. Two statistical versions of the constrained SDE model are fit, with the beta and truncated normal distributions as proxies for the transition density. Empirical results include simulations of the normalized solar PV power production and pathwise confidence bands generated through an indirect inference method. An objective comparison of optimal parametric points associated with the two selected statistical approximations is provided by applying our innovative kernel smoothing estimation technique of the transition function of the forecast error process.</summary></entry><entry><title type="html">Data integration of non-probability and probability samples with predictive mean matching</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Dataintegrationofnonprobabilityandprobabilitysampleswithpredictivemeanmatching.html" rel="alternate" type="text/html" title="Data integration of non-probability and probability samples with predictive mean matching" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Dataintegrationofnonprobabilityandprobabilitysampleswithpredictivemeanmatching</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Dataintegrationofnonprobabilityandprobabilitysampleswithpredictivemeanmatching.html">&lt;p&gt;In this paper we study predictive mean matching mass imputation estimators to integrate data from probability and non-probability samples. We consider two approaches: matching predicted to predicted ($\hat{y}-\hat{y}$~matching; PMM A) and predicted to observed ($\hat{y}-y$~matching; PMM B) values. We prove the consistency of two semi-parametric mass imputation estimators based on these approaches and derive their variance and estimators of variance. We underline the differences of our approach with the nearest neighbour approach proposed by Yang et al. (2021) and prove consistency of the PMM A estimator under model mis-specification. Our approach can be employed with non-parametric regression techniques, such as kernel regression, and the analytical expression for variance can also be applied in nearest neighbour matching for non-probability samples. We conduct extensive simulation studies in order to compare the properties of this estimator with existing approaches, discuss the selection of $k$-nearest neighbours, and study the effects of model mis-specification. The paper finishes with empirical study in integration of job vacancy survey and vacancies submitted to public employment offices (admin and online data). Open source software is available for the proposed approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.13750&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Piotr Chlebicki, {\L}ukasz Chrostowski, Maciej Beręsewicz</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In this paper we study predictive mean matching mass imputation estimators to integrate data from probability and non-probability samples. We consider two approaches: matching predicted to predicted ($\hat{y}-\hat{y}$~matching; PMM A) and predicted to observed ($\hat{y}-y$~matching; PMM B) values. We prove the consistency of two semi-parametric mass imputation estimators based on these approaches and derive their variance and estimators of variance. We underline the differences of our approach with the nearest neighbour approach proposed by Yang et al. (2021) and prove consistency of the PMM A estimator under model mis-specification. Our approach can be employed with non-parametric regression techniques, such as kernel regression, and the analytical expression for variance can also be applied in nearest neighbour matching for non-probability samples. We conduct extensive simulation studies in order to compare the properties of this estimator with existing approaches, discuss the selection of $k$-nearest neighbours, and study the effects of model mis-specification. The paper finishes with empirical study in integration of job vacancy survey and vacancies submitted to public employment offices (admin and online data). Open source software is available for the proposed approaches.</summary></entry><entry><title type="html">Design-based variance estimation of the Hájek effect estimator in stratified and clustered experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/DesignbasedvarianceestimationoftheH%C3%A1jekeffectestimatorinstratifiedandclusteredexperiments.html" rel="alternate" type="text/html" title="Design-based variance estimation of the Hájek effect estimator in stratified and clustered experiments" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/DesignbasedvarianceestimationoftheH%C3%A1jekeffectestimatorinstratifiedandclusteredexperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/DesignbasedvarianceestimationoftheH%C3%A1jekeffectestimatorinstratifiedandclusteredexperiments.html">&lt;p&gt;Randomized controlled trials (RCTs) are used to evaluate treatment effects. When individuals are grouped together, clustered RCTs are conducted. Stratification is recommended to reduce imbalance of baseline covariates between treatment and control. In practice, this can lead to comparisons between clusters of very different sizes. As a result, direct adjustment estimators that average differences of means within the strata may be inconsistent. We study differences of inverse probability weighted means of a treatment and a control group – H&apos;ajek effect estimators – under two common forms of stratification: small strata that increase in number; or larger strata with growing numbers of clusters in each. Under either scenario, mild conditions give consistency and asymptotic Normality. We propose a variance estimator applicable to designs with any number of strata and strata of any size. We describe a special use of the variance estimator that improves small sample performance of Wald-type confidence intervals. The H&apos;ajek effect estimator lends itself to covariance adjustment, and our variance estimator remains applicable. Simulations and real-world applications in children’s nutrition and education confirm favorable operating characteristics, demonstrating advantages of the H&apos;ajek effect estimator beyond its simplicity and ease of use.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10473&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinhe Wang, Ben B. Hansen</name></author><category term="stat.ME" /><summary type="html">Randomized controlled trials (RCTs) are used to evaluate treatment effects. When individuals are grouped together, clustered RCTs are conducted. Stratification is recommended to reduce imbalance of baseline covariates between treatment and control. In practice, this can lead to comparisons between clusters of very different sizes. As a result, direct adjustment estimators that average differences of means within the strata may be inconsistent. We study differences of inverse probability weighted means of a treatment and a control group – H&apos;ajek effect estimators – under two common forms of stratification: small strata that increase in number; or larger strata with growing numbers of clusters in each. Under either scenario, mild conditions give consistency and asymptotic Normality. We propose a variance estimator applicable to designs with any number of strata and strata of any size. We describe a special use of the variance estimator that improves small sample performance of Wald-type confidence intervals. The H&apos;ajek effect estimator lends itself to covariance adjustment, and our variance estimator remains applicable. Simulations and real-world applications in children’s nutrition and education confirm favorable operating characteristics, demonstrating advantages of the H&apos;ajek effect estimator beyond its simplicity and ease of use.</summary></entry><entry><title type="html">Diffusion Generative Modelling for Divide-and-Conquer MCMC</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/DiffusionGenerativeModellingforDivideandConquerMCMC.html" rel="alternate" type="text/html" title="Diffusion Generative Modelling for Divide-and-Conquer MCMC" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/DiffusionGenerativeModellingforDivideandConquerMCMC</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/DiffusionGenerativeModellingforDivideandConquerMCMC.html">&lt;p&gt;Divide-and-conquer MCMC is a strategy for parallelising Markov Chain Monte Carlo sampling by running independent samplers on disjoint subsets of a dataset and merging their output. An ongoing challenge in the literature is to efficiently perform this merging without imposing distributional assumptions on the posteriors. We propose using diffusion generative modelling to fit density approximations to the subposterior distributions. This approach outperforms existing methods on challenging merging problems, while its computational cost scales more efficiently to high dimensional problems than existing density estimation approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11664&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>C. Trojan, P. Fearnhead, C. Nemeth</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Divide-and-conquer MCMC is a strategy for parallelising Markov Chain Monte Carlo sampling by running independent samplers on disjoint subsets of a dataset and merging their output. An ongoing challenge in the literature is to efficiently perform this merging without imposing distributional assumptions on the posteriors. We propose using diffusion generative modelling to fit density approximations to the subposterior distributions. This approach outperforms existing methods on challenging merging problems, while its computational cost scales more efficiently to high dimensional problems than existing density estimation approaches.</summary></entry><entry><title type="html">Estimating Heterogeneous Causal Effects of High-Dimensional Treatments: Application to Conjoint Analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/EstimatingHeterogeneousCausalEffectsofHighDimensionalTreatmentsApplicationtoConjointAnalysis.html" rel="alternate" type="text/html" title="Estimating Heterogeneous Causal Effects of High-Dimensional Treatments: Application to Conjoint Analysis" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/EstimatingHeterogeneousCausalEffectsofHighDimensionalTreatmentsApplicationtoConjointAnalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/EstimatingHeterogeneousCausalEffectsofHighDimensionalTreatmentsApplicationtoConjointAnalysis.html">&lt;p&gt;Estimation of heterogeneous treatment effects is an active area of research. Most of the existing methods, however, focus on estimating the conditional average treatment effects of a single, binary treatment given a set of pre-treatment covariates. In this paper, we propose a method to estimate the heterogeneous causal effects of high-dimensional treatments, which poses unique challenges in terms of estimation and interpretation. The proposed approach finds maximally heterogeneous groups and uses a Bayesian mixture of regularized logistic regressions to identify groups of units who exhibit similar patterns of treatment effects. By directly modeling group membership with covariates, the proposed methodology allows one to explore the unit characteristics that are associated with different patterns of treatment effects. Our motivating application is conjoint analysis, which is a popular type of survey experiment in social science and marketing research and is based on a high-dimensional factorial design. We apply the proposed methodology to the conjoint data, where survey respondents are asked to select one of two immigrant profiles with randomly selected attributes. We find that a group of respondents with a relatively high degree of prejudice appears to discriminate against immigrants from non-European countries like Iraq. An open-source software package is available for implementing the proposed methodology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2201.01357&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Max Goplerud, Kosuke Imai, Nicole E. Pashley</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Estimation of heterogeneous treatment effects is an active area of research. Most of the existing methods, however, focus on estimating the conditional average treatment effects of a single, binary treatment given a set of pre-treatment covariates. In this paper, we propose a method to estimate the heterogeneous causal effects of high-dimensional treatments, which poses unique challenges in terms of estimation and interpretation. The proposed approach finds maximally heterogeneous groups and uses a Bayesian mixture of regularized logistic regressions to identify groups of units who exhibit similar patterns of treatment effects. By directly modeling group membership with covariates, the proposed methodology allows one to explore the unit characteristics that are associated with different patterns of treatment effects. Our motivating application is conjoint analysis, which is a popular type of survey experiment in social science and marketing research and is based on a high-dimensional factorial design. We apply the proposed methodology to the conjoint data, where survey respondents are asked to select one of two immigrant profiles with randomly selected attributes. We find that a group of respondents with a relatively high degree of prejudice appears to discriminate against immigrants from non-European countries like Iraq. An open-source software package is available for implementing the proposed methodology.</summary></entry><entry><title type="html">False Discovery Rate Control For Structured Multiple Testing: Asymmetric Rules And Conformal Q-values</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/FalseDiscoveryRateControlForStructuredMultipleTestingAsymmetricRulesAndConformalQvalues.html" rel="alternate" type="text/html" title="False Discovery Rate Control For Structured Multiple Testing: Asymmetric Rules And Conformal Q-values" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/FalseDiscoveryRateControlForStructuredMultipleTestingAsymmetricRulesAndConformalQvalues</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/FalseDiscoveryRateControlForStructuredMultipleTestingAsymmetricRulesAndConformalQvalues.html">&lt;p&gt;The effective utilization of structural information in data while ensuring statistical validity poses a significant challenge in false discovery rate (FDR) analyses. Conformal inference provides rigorous theory for grounding complex machine learning methods without relying on strong assumptions or highly idealized models. However, existing conformal methods have limitations in handling structured multiple testing. This is because their validity requires the deployment of symmetric rules, which assume the exchangeability of data points and permutation-invariance of fitting algorithms. To overcome these limitations, we introduce the pseudo local index of significance (PLIS) procedure, which is capable of accommodating asymmetric rules and requires only pairwise exchangeability between the null conformity scores. We demonstrate that PLIS offers finite-sample guarantees in FDR control and the ability to assign higher weights to relevant data points. Numerical results confirm the effectiveness and robustness of PLIS and show improvements in power compared to existing model-free methods in various scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.15322&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zinan Zhao, Wenguang Sun</name></author><category term="stat.ME" /><summary type="html">The effective utilization of structural information in data while ensuring statistical validity poses a significant challenge in false discovery rate (FDR) analyses. Conformal inference provides rigorous theory for grounding complex machine learning methods without relying on strong assumptions or highly idealized models. However, existing conformal methods have limitations in handling structured multiple testing. This is because their validity requires the deployment of symmetric rules, which assume the exchangeability of data points and permutation-invariance of fitting algorithms. To overcome these limitations, we introduce the pseudo local index of significance (PLIS) procedure, which is capable of accommodating asymmetric rules and requires only pairwise exchangeability between the null conformity scores. We demonstrate that PLIS offers finite-sample guarantees in FDR control and the ability to assign higher weights to relevant data points. Numerical results confirm the effectiveness and robustness of PLIS and show improvements in power compared to existing model-free methods in various scenarios.</summary></entry><entry><title type="html">Fast solution to the fair ranking problem using the Sinkhorn algorithm</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/FastsolutiontothefairrankingproblemusingtheSinkhornalgorithm.html" rel="alternate" type="text/html" title="Fast solution to the fair ranking problem using the Sinkhorn algorithm" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/FastsolutiontothefairrankingproblemusingtheSinkhornalgorithm</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/FastsolutiontothefairrankingproblemusingtheSinkhornalgorithm.html">&lt;p&gt;In two-sided marketplaces such as online flea markets, recommender systems for providing consumers with personalized item rankings play a key role in promoting transactions between providers and consumers. Meanwhile, two-sided marketplaces face the problem of balancing consumer satisfaction and fairness among items to stimulate activity of item providers. Saito and Joachims (2022) devised an impact-based fair ranking method for maximizing the Nash social welfare based on fair division; however, this method, which requires solving a large-scale constrained nonlinear optimization problem, is very difficult to apply to practical-scale recommender systems. We thus propose a fast solution to the impact-based fair ranking problem. We first transform the fair ranking problem into an unconstrained optimization problem and then design a gradient ascent method that repeatedly executes the Sinkhorn algorithm. Experimental results demonstrate that our algorithm provides fair rankings of high quality and is about 1000 times faster than application of commercial optimization software.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10262&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuki Uehara, Shunnosuke Ikeda, Naoki Nishimura, Koya Ohashi, Yilin Li, Jie Yang, Deddy Jobson, Xingxia Zha, Takeshi Matsumoto, Noriyoshi Sukegawa, Yuichi Takano</name></author><category term="stat.CO" /><summary type="html">In two-sided marketplaces such as online flea markets, recommender systems for providing consumers with personalized item rankings play a key role in promoting transactions between providers and consumers. Meanwhile, two-sided marketplaces face the problem of balancing consumer satisfaction and fairness among items to stimulate activity of item providers. Saito and Joachims (2022) devised an impact-based fair ranking method for maximizing the Nash social welfare based on fair division; however, this method, which requires solving a large-scale constrained nonlinear optimization problem, is very difficult to apply to practical-scale recommender systems. We thus propose a fast solution to the impact-based fair ranking problem. We first transform the fair ranking problem into an unconstrained optimization problem and then design a gradient ascent method that repeatedly executes the Sinkhorn algorithm. Experimental results demonstrate that our algorithm provides fair rankings of high quality and is about 1000 times faster than application of commercial optimization software.</summary></entry><entry><title type="html">Functional Clustering for Longitudinal Associations between County-Level Social Determinants of Health and Stroke Mortality in the US</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/FunctionalClusteringforLongitudinalAssociationsbetweenCountyLevelSocialDeterminantsofHealthandStrokeMortalityintheUS.html" rel="alternate" type="text/html" title="Functional Clustering for Longitudinal Associations between County-Level Social Determinants of Health and Stroke Mortality in the US" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/FunctionalClusteringforLongitudinalAssociationsbetweenCountyLevelSocialDeterminantsofHealthandStrokeMortalityintheUS</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/FunctionalClusteringforLongitudinalAssociationsbetweenCountyLevelSocialDeterminantsofHealthandStrokeMortalityintheUS.html">&lt;p&gt;Understanding longitudinally changing associations between Social determinants of health (SDOH) and stroke mortality is crucial for timely stroke management. Previous studies have revealed a significant regional disparity in the SDOH – stroke mortality associations. However, they do not develop data-driven methods based on these longitudinal associations for regional division in stroke control. To fill this gap, we propose a novel clustering method for SDOH – stroke mortality associations in the US counties. To enhance interpretability and statistical efficiency of the clustering outcomes, we introduce a new class of smoothness-sparsity pursued penalties for simultaneous clustering and variable selection in the longitudinal associations. As a result, we can identify important SDOH that contribute to longitudinal changes in the stroke mortality, facilitating clustering of US counties into several regions based on how these SDOH relate to stroke mortality. The effectiveness of our proposed method is demonstrated through extensive numerical studies. By applying our method to a county-level SDOH and stroke mortality longitudinal data, we identify 18 important SDOH for stroke mortality and divide the US counties into two clusters based on these selected SDOH. Our findings unveil complex regional heterogeneity in the longitudinal associations between SDOH and stroke mortality, providing valuable insights in region-specific SDOH adjustments for mitigating stroke mortality.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10499&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fangzhi Luo, Jianbin Tan, Donglan Zhang, Hui Huang, Ye Shen</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Understanding longitudinally changing associations between Social determinants of health (SDOH) and stroke mortality is crucial for timely stroke management. Previous studies have revealed a significant regional disparity in the SDOH – stroke mortality associations. However, they do not develop data-driven methods based on these longitudinal associations for regional division in stroke control. To fill this gap, we propose a novel clustering method for SDOH – stroke mortality associations in the US counties. To enhance interpretability and statistical efficiency of the clustering outcomes, we introduce a new class of smoothness-sparsity pursued penalties for simultaneous clustering and variable selection in the longitudinal associations. As a result, we can identify important SDOH that contribute to longitudinal changes in the stroke mortality, facilitating clustering of US counties into several regions based on how these SDOH relate to stroke mortality. The effectiveness of our proposed method is demonstrated through extensive numerical studies. By applying our method to a county-level SDOH and stroke mortality longitudinal data, we identify 18 important SDOH for stroke mortality and divide the US counties into two clusters based on these selected SDOH. Our findings unveil complex regional heterogeneity in the longitudinal associations between SDOH and stroke mortality, providing valuable insights in region-specific SDOH adjustments for mitigating stroke mortality.</summary></entry><entry><title type="html">Generating contingency tables with fixed marginal probabilities and dependence structures described by loglinear models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Generatingcontingencytableswithfixedmarginalprobabilitiesanddependencestructuresdescribedbyloglinearmodels.html" rel="alternate" type="text/html" title="Generating contingency tables with fixed marginal probabilities and dependence structures described by loglinear models" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Generatingcontingencytableswithfixedmarginalprobabilitiesanddependencestructuresdescribedbyloglinearmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Generatingcontingencytableswithfixedmarginalprobabilitiesanddependencestructuresdescribedbyloglinearmodels.html">&lt;p&gt;We present a method to generate contingency tables that follow loglinear models with prescribed marginal probabilities and dependence structures. We make use of (loglinear) Poisson regression, where the dependence structures, described using odds ratios, are implemented using an offset term. We apply this methodology to carry out simulation studies in the context of population size estimation using dual system and triple system estimators, popular in official statistics. These estimators use contingency tables that summarise the counts of elements enumerated or captured within lists that are linked. The simulation is used to investigate these estimators in the situation that the model assumptions are fulfilled, and the situation that the model assumptions are violated.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.08568&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ceejay Hammond, Peter G. M. van der Heijden, Paul A. Smith</name></author><category term="stat.ME" /><summary type="html">We present a method to generate contingency tables that follow loglinear models with prescribed marginal probabilities and dependence structures. We make use of (loglinear) Poisson regression, where the dependence structures, described using odds ratios, are implemented using an offset term. We apply this methodology to carry out simulation studies in the context of population size estimation using dual system and triple system estimators, popular in official statistics. These estimators use contingency tables that summarise the counts of elements enumerated or captured within lists that are linked. The simulation is used to investigate these estimators in the situation that the model assumptions are fulfilled, and the situation that the model assumptions are violated.</summary></entry><entry><title type="html">HEDE: Heritability estimation in high dimensions by Ensembling Debiased Estimators</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/HEDEHeritabilityestimationinhighdimensionsbyEnsemblingDebiasedEstimators.html" rel="alternate" type="text/html" title="HEDE: Heritability estimation in high dimensions by Ensembling Debiased Estimators" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/HEDEHeritabilityestimationinhighdimensionsbyEnsemblingDebiasedEstimators</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/HEDEHeritabilityestimationinhighdimensionsbyEnsemblingDebiasedEstimators.html">&lt;p&gt;Estimating heritability remains a significant challenge in statistical genetics. Diverse approaches have emerged over the years that are broadly categorized as either random effects or fixed effects heritability methods. In this work, we focus on the latter. We propose HEDE, an ensemble approach to estimate heritability or the signal-to-noise ratio in high-dimensional linear models where the sample size and the dimension grow proportionally. Our method ensembles post-processed versions of the debiased lasso and debiased ridge estimators, and incorporates a data-driven strategy for hyperparameter selection that significantly boosts estimation performance. We establish rigorous consistency guarantees that hold despite adaptive tuning. Extensive simulations demonstrate our method’s superiority over existing state-of-the-art methods across various signal structures and genetic architectures, ranging from sparse to relatively dense and from evenly to unevenly distributed signals. Furthermore, we discuss the advantages of fixed effects heritability estimation compared to random effects estimation. Our theoretical guarantees hold for realistic genotype distributions observed in genetic studies, where genotypes typically take on discrete values and are often well-modeled by sub-Gaussian distributed random variables. We establish our theoretical results by deriving uniform bounds, built upon the convex Gaussian min-max theorem, and leveraging universality results. Finally, we showcase the efficacy of our approach in estimating height and BMI heritability using the UK Biobank.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11184&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yanke Song, Xihong Lin, Pragya Sur</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Estimating heritability remains a significant challenge in statistical genetics. Diverse approaches have emerged over the years that are broadly categorized as either random effects or fixed effects heritability methods. In this work, we focus on the latter. We propose HEDE, an ensemble approach to estimate heritability or the signal-to-noise ratio in high-dimensional linear models where the sample size and the dimension grow proportionally. Our method ensembles post-processed versions of the debiased lasso and debiased ridge estimators, and incorporates a data-driven strategy for hyperparameter selection that significantly boosts estimation performance. We establish rigorous consistency guarantees that hold despite adaptive tuning. Extensive simulations demonstrate our method’s superiority over existing state-of-the-art methods across various signal structures and genetic architectures, ranging from sparse to relatively dense and from evenly to unevenly distributed signals. Furthermore, we discuss the advantages of fixed effects heritability estimation compared to random effects estimation. Our theoretical guarantees hold for realistic genotype distributions observed in genetic studies, where genotypes typically take on discrete values and are often well-modeled by sub-Gaussian distributed random variables. We establish our theoretical results by deriving uniform bounds, built upon the convex Gaussian min-max theorem, and leveraging universality results. Finally, we showcase the efficacy of our approach in estimating height and BMI heritability using the UK Biobank.</summary></entry><entry><title type="html">Heterogeneous Entity Representation for Medicinal Synergy Prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/HeterogeneousEntityRepresentationforMedicinalSynergyPrediction.html" rel="alternate" type="text/html" title="Heterogeneous Entity Representation for Medicinal Synergy Prediction" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/HeterogeneousEntityRepresentationforMedicinalSynergyPrediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/HeterogeneousEntityRepresentationforMedicinalSynergyPrediction.html">&lt;p&gt;Medicinal synergy prediction is a powerful tool in drug discovery and development that harnesses the principles of combination therapy to enhance therapeutic outcomes by improving efficacy, reducing toxicity, and preventing drug resistance. While a myriad of computational methods has emerged for predicting synergistic drug combinations, a large portion of them may overlook the intricate, yet critical relationships between various entities in drug interaction networks, such as drugs, cell lines, and diseases. These relationships are complex and multidimensional, requiring sophisticated modeling to capture nuanced interplay that can significantly influence therapeutic efficacy. We introduce a salient deep hypergraph learning method, namely, Heterogeneous Entity Representation for MEdicinal Synergy prediction (HERMES), to predict anti-cancer drug synergy. HERMES integrates heterogeneous data sources, encompassing drug, cell line, and disease information, to provide a comprehensive understanding of the interactions involved. By leveraging advanced hypergraph neural networks with gated residual mechanisms, HERMES can effectively learn complex relationships/interactions within the data. Our results show HERMES demonstrates state-of-the-art performance, particularly in forecasting new drug combinations, significantly surpassing previous methods. This advancement underscores the potential of HERMES to facilitate more effective and precise drug combination predictions, thereby enhancing the development of novel therapeutic strategies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10778&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiawei Wu, Jun Wen, Mingyuan Yan, Anqi Dong, Can Chen</name></author><category term="stat.AP" /><summary type="html">Medicinal synergy prediction is a powerful tool in drug discovery and development that harnesses the principles of combination therapy to enhance therapeutic outcomes by improving efficacy, reducing toxicity, and preventing drug resistance. While a myriad of computational methods has emerged for predicting synergistic drug combinations, a large portion of them may overlook the intricate, yet critical relationships between various entities in drug interaction networks, such as drugs, cell lines, and diseases. These relationships are complex and multidimensional, requiring sophisticated modeling to capture nuanced interplay that can significantly influence therapeutic efficacy. We introduce a salient deep hypergraph learning method, namely, Heterogeneous Entity Representation for MEdicinal Synergy prediction (HERMES), to predict anti-cancer drug synergy. HERMES integrates heterogeneous data sources, encompassing drug, cell line, and disease information, to provide a comprehensive understanding of the interactions involved. By leveraging advanced hypergraph neural networks with gated residual mechanisms, HERMES can effectively learn complex relationships/interactions within the data. Our results show HERMES demonstrates state-of-the-art performance, particularly in forecasting new drug combinations, significantly surpassing previous methods. This advancement underscores the potential of HERMES to facilitate more effective and precise drug combination predictions, thereby enhancing the development of novel therapeutic strategies.</summary></entry><entry><title type="html">Heterogeneous Treatment Effects and Causal Mechanisms</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/HeterogeneousTreatmentEffectsandCausalMechanisms.html" rel="alternate" type="text/html" title="Heterogeneous Treatment Effects and Causal Mechanisms" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/HeterogeneousTreatmentEffectsandCausalMechanisms</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/HeterogeneousTreatmentEffectsandCausalMechanisms.html">&lt;p&gt;The credibility revolution advances the use of research designs that permit identification and estimation of causal effects. However, understanding which mechanisms produce measured causal effects remains a challenge. A dominant current approach to the quantitative evaluation of mechanisms relies on the detection of heterogeneous treatment effects with respect to pre-treatment covariates. This paper develops a framework to understand when the existence of such heterogeneous treatment effects can support inferences about the activation of a mechanism. We show first that this design cannot provide evidence of mechanism activation without an additional, generally implicit, assumption. Further, even when this assumption is satisfied, if a measured outcome is produced by a non-linear transformation of a directly-affected outcome of theoretical interest, heterogeneous treatment effects are not informative of mechanism activation. We provide novel guidance for interpretation and research design in light of these findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.01566&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiawei Fu, Tara Slough</name></author><category term="stat.ME" /><summary type="html">The credibility revolution advances the use of research designs that permit identification and estimation of causal effects. However, understanding which mechanisms produce measured causal effects remains a challenge. A dominant current approach to the quantitative evaluation of mechanisms relies on the detection of heterogeneous treatment effects with respect to pre-treatment covariates. This paper develops a framework to understand when the existence of such heterogeneous treatment effects can support inferences about the activation of a mechanism. We show first that this design cannot provide evidence of mechanism activation without an additional, generally implicit, assumption. Further, even when this assumption is satisfied, if a measured outcome is produced by a non-linear transformation of a directly-affected outcome of theoretical interest, heterogeneous treatment effects are not informative of mechanism activation. We provide novel guidance for interpretation and research design in light of these findings.</summary></entry><entry><title type="html">Identifying Sample Size and Accuracy and Precision of the Estimators in Case-Crossover Designs with Distributed Lags of Heteroskedastic Time-Varying Continuous Exposures Measured with Simple or Complex Error</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/IdentifyingSampleSizeandAccuracyandPrecisionoftheEstimatorsinCaseCrossoverDesignswithDistributedLagsofHeteroskedasticTimeVaryingContinuousExposuresMeasuredwithSimpleorComplexError.html" rel="alternate" type="text/html" title="Identifying Sample Size and Accuracy and Precision of the Estimators in Case-Crossover Designs with Distributed Lags of Heteroskedastic Time-Varying Continuous Exposures Measured with Simple or Complex Error" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/IdentifyingSampleSizeandAccuracyandPrecisionoftheEstimatorsinCaseCrossoverDesignswithDistributedLagsofHeteroskedasticTimeVaryingContinuousExposuresMeasuredwithSimpleorComplexError</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/IdentifyingSampleSizeandAccuracyandPrecisionoftheEstimatorsinCaseCrossoverDesignswithDistributedLagsofHeteroskedasticTimeVaryingContinuousExposuresMeasuredwithSimpleorComplexError.html">&lt;p&gt;Understanding of sample size, statistical power, and the accuracy and precision of the estimator in epidemiological research can facilitate power and bias analyses. However, such understanding can become complicated for several reasons. First, exposures varying spatiotemporally may be heteroskedastic. Second, distributed lags of exposures may be used to identify critical exposure time-windows. Third, exposure measurement error may exist, impacting the accuracy and/or precision of the estimator that consequently affects sample size and statistical power. Fourth, research may rely on different study designs, so understanding may differ. For example, case-crossover designs as matched case-control designs, are used to estimate health effects of short-term exposures. To address these gaps, I developed approximation equations for sample size, estimates of the estimators and standard errors, including polynomials for non-linear effect estimation. With air pollution exposure estimates, I examined approximations using statistical simulations. Overall, sample size, the accuracy and precision of the estimators can be approximated based on external information about validation, without validation data in hand. For distributed lags, approximations may perform well if residual confounding due to covariate measurement errors is not severe. This condition may be difficult to identify without validation data, so validation research is recommended in identifying critical exposure time-windows.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02369&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Honghyok Kim</name></author><category term="stat.ME" /><summary type="html">Understanding of sample size, statistical power, and the accuracy and precision of the estimator in epidemiological research can facilitate power and bias analyses. However, such understanding can become complicated for several reasons. First, exposures varying spatiotemporally may be heteroskedastic. Second, distributed lags of exposures may be used to identify critical exposure time-windows. Third, exposure measurement error may exist, impacting the accuracy and/or precision of the estimator that consequently affects sample size and statistical power. Fourth, research may rely on different study designs, so understanding may differ. For example, case-crossover designs as matched case-control designs, are used to estimate health effects of short-term exposures. To address these gaps, I developed approximation equations for sample size, estimates of the estimators and standard errors, including polynomials for non-linear effect estimation. With air pollution exposure estimates, I examined approximations using statistical simulations. Overall, sample size, the accuracy and precision of the estimators can be approximated based on external information about validation, without validation data in hand. For distributed lags, approximations may perform well if residual confounding due to covariate measurement errors is not severe. This condition may be difficult to identify without validation data, so validation research is recommended in identifying critical exposure time-windows.</summary></entry><entry><title type="html">Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ImprovingtheValidityandPracticalUsefulnessofAIMLEvaluationsUsinganEstimandsFramework.html" rel="alternate" type="text/html" title="Improving the Validity and Practical Usefulness of AI/ML Evaluations Using an Estimands Framework" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ImprovingtheValidityandPracticalUsefulnessofAIMLEvaluationsUsinganEstimandsFramework</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ImprovingtheValidityandPracticalUsefulnessofAIMLEvaluationsUsinganEstimandsFramework.html">&lt;p&gt;Commonly, AI or machine learning (ML) models are evaluated on benchmark datasets. This practice supports innovative methodological research, but benchmark performance can be poorly correlated with performance in real-world applications – a construct validity issue. To improve the validity and practical usefulness of evaluations, we propose using an estimands framework adapted from international clinical trials guidelines. This framework provides a systematic structure for inference and reporting in evaluations, emphasizing the importance of a well-defined estimation target. We illustrate our proposal on examples of commonly used evaluation methodologies - involving cross-validation, clustering evaluation, and LLM benchmarking - that can lead to incorrect rankings of competing models (rank reversals) with high probability, even when performance differences are large. We demonstrate how the estimands framework can help uncover underlying issues, their causes, and potential solutions. Ultimately, we believe this framework can improve the validity of evaluations through better-aligned inference, and help decision-makers and model users interpret reported results more effectively.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10366&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Olivier Binette, Jerome P. Reiter</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Commonly, AI or machine learning (ML) models are evaluated on benchmark datasets. This practice supports innovative methodological research, but benchmark performance can be poorly correlated with performance in real-world applications – a construct validity issue. To improve the validity and practical usefulness of evaluations, we propose using an estimands framework adapted from international clinical trials guidelines. This framework provides a systematic structure for inference and reporting in evaluations, emphasizing the importance of a well-defined estimation target. We illustrate our proposal on examples of commonly used evaluation methodologies - involving cross-validation, clustering evaluation, and LLM benchmarking - that can lead to incorrect rankings of competing models (rank reversals) with high probability, even when performance differences are large. We demonstrate how the estimands framework can help uncover underlying issues, their causes, and potential solutions. Ultimately, we believe this framework can improve the validity of evaluations through better-aligned inference, and help decision-makers and model users interpret reported results more effectively.</summary></entry><entry><title type="html">Information-theoretic evaluation of covariate distributions models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Informationtheoreticevaluationofcovariatedistributionsmodels.html" rel="alternate" type="text/html" title="Information-theoretic evaluation of covariate distributions models" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Informationtheoreticevaluationofcovariatedistributionsmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Informationtheoreticevaluationofcovariatedistributionsmodels.html">&lt;p&gt;Statistical modelling of covariate distributions allows to generate virtual populations or to impute missing values in a covariate dataset. Covariate distributions typically have non-Gaussian margins and show nonlinear correlation structures, which simple multivariate Gaussian distributions fail to represent. Prominent non-Gaussian frameworks for covariate distribution modelling are copula-based models and models based on multiple imputation by chained equations (MICE). While both frameworks have already found applications in the life sciences, a systematic investigation of their goodness-of-fit to the theoretical underlying distribution, indicating strengths and weaknesses under different conditions, is still lacking. To bridge this gap, we thoroughly evaluated covariate distribution models in terms of Kullback-Leibler divergence (KL-D), a scale-invariant information-theoretic goodness-of-fit criterion for distributions. Methodologically, we proposed a new approach to construct confidence intervals for KL-D by combining nearest neighbour-based KL-D estimators with subsampling-based uncertainty quantification. In relevant data sets of different sizes and dimensionalities with both continuous and discrete covariates, non-Gaussian models showed consistent improvements in KL-D, compared to simpler Gaussian or scale transform approximations. KL-D estimates were also robust to the inclusion of latent variables and large fractions of missing values. While good generalization behaviour to new data could be seen in copula-based models, MICE shows a trend for overfitting and its performance should always be evaluated on separate test data. Parametric copula models and MICE were found to scale much better with the dataset dimension than nonparametric copula models. These findings corroborate the potential of non-Gaussian models for modelling realistic life science covariate distributions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10611&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Niklas Hartung, Aleksandra Khatova</name></author><category term="stat.AP" /><summary type="html">Statistical modelling of covariate distributions allows to generate virtual populations or to impute missing values in a covariate dataset. Covariate distributions typically have non-Gaussian margins and show nonlinear correlation structures, which simple multivariate Gaussian distributions fail to represent. Prominent non-Gaussian frameworks for covariate distribution modelling are copula-based models and models based on multiple imputation by chained equations (MICE). While both frameworks have already found applications in the life sciences, a systematic investigation of their goodness-of-fit to the theoretical underlying distribution, indicating strengths and weaknesses under different conditions, is still lacking. To bridge this gap, we thoroughly evaluated covariate distribution models in terms of Kullback-Leibler divergence (KL-D), a scale-invariant information-theoretic goodness-of-fit criterion for distributions. Methodologically, we proposed a new approach to construct confidence intervals for KL-D by combining nearest neighbour-based KL-D estimators with subsampling-based uncertainty quantification. In relevant data sets of different sizes and dimensionalities with both continuous and discrete covariates, non-Gaussian models showed consistent improvements in KL-D, compared to simpler Gaussian or scale transform approximations. KL-D estimates were also robust to the inclusion of latent variables and large fractions of missing values. While good generalization behaviour to new data could be seen in copula-based models, MICE shows a trend for overfitting and its performance should always be evaluated on separate test data. Parametric copula models and MICE were found to scale much better with the dataset dimension than nonparametric copula models. These findings corroborate the potential of non-Gaussian models for modelling realistic life science covariate distributions.</summary></entry><entry><title type="html">Interventional Imbalanced Multi-Modal Representation Learning via $\beta$-Generalization Front-Door Criterion</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/InterventionalImbalancedMultiModalRepresentationLearningviabetaGeneralizationFrontDoorCriterion.html" rel="alternate" type="text/html" title="Interventional Imbalanced Multi-Modal Representation Learning via $\beta$-Generalization Front-Door Criterion" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/InterventionalImbalancedMultiModalRepresentationLearningviabetaGeneralizationFrontDoorCriterion</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/InterventionalImbalancedMultiModalRepresentationLearningviabetaGeneralizationFrontDoorCriterion.html">&lt;p&gt;Multi-modal methods establish comprehensive superiority over uni-modal methods. However, the imbalanced contributions of different modalities to task-dependent predictions constantly degrade the discriminative performance of canonical multi-modal methods. Based on the contribution to task-dependent predictions, modalities can be identified as predominant and auxiliary modalities. Benchmark methods raise a tractable solution: augmenting the auxiliary modality with a minor contribution during training. However, our empirical explorations challenge the fundamental idea behind such behavior, and we further conclude that benchmark approaches suffer from certain defects: insufficient theoretical interpretability and limited exploration capability of discriminative knowledge. To this end, we revisit multi-modal representation learning from a causal perspective and build the Structural Causal Model. Following the empirical explorations, we determine to capture the true causality between the discriminative knowledge of predominant modality and predictive label while considering the auxiliary modality. Thus, we introduce the $\beta$-generalization front-door criterion. Furthermore, we propose a novel network for sufficiently exploring multi-modal discriminative knowledge. Rigorous theoretical analyses and various empirical evaluations are provided to support the effectiveness of the innate mechanism behind our proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11490&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yi Li, Jiangmeng Li, Fei Song, Qingmeng Zhu, Changwen Zheng, Wenwen Qiang</name></author><category term="stat.ME" /><summary type="html">Multi-modal methods establish comprehensive superiority over uni-modal methods. However, the imbalanced contributions of different modalities to task-dependent predictions constantly degrade the discriminative performance of canonical multi-modal methods. Based on the contribution to task-dependent predictions, modalities can be identified as predominant and auxiliary modalities. Benchmark methods raise a tractable solution: augmenting the auxiliary modality with a minor contribution during training. However, our empirical explorations challenge the fundamental idea behind such behavior, and we further conclude that benchmark approaches suffer from certain defects: insufficient theoretical interpretability and limited exploration capability of discriminative knowledge. To this end, we revisit multi-modal representation learning from a causal perspective and build the Structural Causal Model. Following the empirical explorations, we determine to capture the true causality between the discriminative knowledge of predominant modality and predictive label while considering the auxiliary modality. Thus, we introduce the $\beta$-generalization front-door criterion. Furthermore, we propose a novel network for sufficiently exploring multi-modal discriminative knowledge. Rigorous theoretical analyses and various empirical evaluations are provided to support the effectiveness of the innate mechanism behind our proposed method.</summary></entry><entry><title type="html">Invariant Probabilistic Prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/InvariantProbabilisticPrediction.html" rel="alternate" type="text/html" title="Invariant Probabilistic Prediction" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/InvariantProbabilisticPrediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/InvariantProbabilisticPrediction.html">&lt;p&gt;In recent years, there has been a growing interest in statistical methods that exhibit robust performance under distribution changes between training and test data. While most of the related research focuses on point predictions with the squared error loss, this article turns the focus towards probabilistic predictions, which aim to comprehensively quantify the uncertainty of an outcome variable given covariates. Within a causality-inspired framework, we investigate the invariance and robustness of probabilistic predictions with respect to proper scoring rules. We show that arbitrary distribution shifts do not, in general, admit invariant and robust probabilistic predictions, in contrast to the setting of point prediction. We illustrate how to choose evaluation metrics and restrict the class of distribution shifts to allow for identifiability and invariance in the prototypical Gaussian heteroscedastic linear model. Motivated by these findings, we propose a method to yield invariant probabilistic predictions, called IPP, and study the consistency of the underlying parameters. Finally, we demonstrate the empirical performance of our proposed procedure on simulated as well as on single-cell data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.10083&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexander Henzi, Xinwei Shen, Michael Law, Peter Bühlmann</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">In recent years, there has been a growing interest in statistical methods that exhibit robust performance under distribution changes between training and test data. While most of the related research focuses on point predictions with the squared error loss, this article turns the focus towards probabilistic predictions, which aim to comprehensively quantify the uncertainty of an outcome variable given covariates. Within a causality-inspired framework, we investigate the invariance and robustness of probabilistic predictions with respect to proper scoring rules. We show that arbitrary distribution shifts do not, in general, admit invariant and robust probabilistic predictions, in contrast to the setting of point prediction. We illustrate how to choose evaluation metrics and restrict the class of distribution shifts to allow for identifiability and invariance in the prototypical Gaussian heteroscedastic linear model. Motivated by these findings, we propose a method to yield invariant probabilistic predictions, called IPP, and study the consistency of the underlying parameters. Finally, we demonstrate the empirical performance of our proposed procedure on simulated as well as on single-cell data.</summary></entry><entry><title type="html">Joint Linked Component Analysis for Multiview Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/JointLinkedComponentAnalysisforMultiviewData.html" rel="alternate" type="text/html" title="Joint Linked Component Analysis for Multiview Data" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/JointLinkedComponentAnalysisforMultiviewData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/JointLinkedComponentAnalysisforMultiviewData.html">&lt;p&gt;In this work, we propose the joint linked component analysis (joint_LCA) for multiview data. Unlike classic methods which extract the shared components in a sequential manner, the objective of joint_LCA is to identify the view-specific loading matrices and the rank of the common latent subspace simultaneously. We formulate a matrix decomposition model where a joint structure and an individual structure are present in each data view, which enables us to arrive at a clean svd representation for the cross covariance between any pair of data views. An objective function with a novel penalty term is then proposed to achieve simultaneous estimation and rank selection. In addition, a refitting procedure is employed as a remedy to reduce the shrinkage bias caused by the penalization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11761&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lin Xiao, Luo Xiao</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">In this work, we propose the joint linked component analysis (joint_LCA) for multiview data. Unlike classic methods which extract the shared components in a sequential manner, the objective of joint_LCA is to identify the view-specific loading matrices and the rank of the common latent subspace simultaneously. We formulate a matrix decomposition model where a joint structure and an individual structure are present in each data view, which enables us to arrive at a clean svd representation for the cross covariance between any pair of data views. An objective function with a novel penalty term is then proposed to achieve simultaneous estimation and rank selection. In addition, a refitting procedure is employed as a remedy to reduce the shrinkage bias caused by the penalization.</summary></entry><entry><title type="html">L0-regularized compressed sensing with Mean-field Coherent Ising Machines</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/L0regularizedcompressedsensingwithMeanfieldCoherentIsingMachines.html" rel="alternate" type="text/html" title="L0-regularized compressed sensing with Mean-field Coherent Ising Machines" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/L0regularizedcompressedsensingwithMeanfieldCoherentIsingMachines</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/L0regularizedcompressedsensingwithMeanfieldCoherentIsingMachines.html">&lt;p&gt;Coherent Ising Machine (CIM) is a network of optical parametric oscillators that solves combinatorial optimization problems by finding the ground state of an Ising Hamiltonian. As a practical application of CIM, Aonishi et al. proposed a quantum-classical hybrid system to solve optimization problems of L0-regularization-based compressed sensing (L0RBCS). Gunathilaka et al. has further enhanced the accuracy of the system. However, the computationally expensive CIM’s stochastic differential equations (SDEs) limit the use of digital hardware implementations. As an alternative to Gunathilaka et al.’s CIM SDEs used previously, we propose using the mean-field CIM (MF-CIM) model, which is a physics-inspired heuristic solver without quantum noise. MF-CIM surmounts the high computational cost due to the simple nature of the differential equations (DEs). Furthermore, our results indicate that the proposed model has similar performance to physically accurate SDEs in both artificial and magnetic resonance imaging data, paving the way for implementing CIM-based L0RBCS on digital hardware such as Field Programmable Gate Arrays (FPGAs).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00366&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mastiyage Don Sudeera Hasaranga Gunathilaka, Yoshitaka Inui, Satoshi Kako, Kazushi Mimura, Masato Okada, Yoshihisa Yamamoto, Toru Aonishi</name></author><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Coherent Ising Machine (CIM) is a network of optical parametric oscillators that solves combinatorial optimization problems by finding the ground state of an Ising Hamiltonian. As a practical application of CIM, Aonishi et al. proposed a quantum-classical hybrid system to solve optimization problems of L0-regularization-based compressed sensing (L0RBCS). Gunathilaka et al. has further enhanced the accuracy of the system. However, the computationally expensive CIM’s stochastic differential equations (SDEs) limit the use of digital hardware implementations. As an alternative to Gunathilaka et al.’s CIM SDEs used previously, we propose using the mean-field CIM (MF-CIM) model, which is a physics-inspired heuristic solver without quantum noise. MF-CIM surmounts the high computational cost due to the simple nature of the differential equations (DEs). Furthermore, our results indicate that the proposed model has similar performance to physically accurate SDEs in both artificial and magnetic resonance imaging data, paving the way for implementing CIM-based L0RBCS on digital hardware such as Field Programmable Gate Arrays (FPGAs).</summary></entry><entry><title type="html">Mixed-integer linear programming for computing optimal experimental designs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Mixedintegerlinearprogrammingforcomputingoptimalexperimentaldesigns.html" rel="alternate" type="text/html" title="Mixed-integer linear programming for computing optimal experimental designs" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Mixedintegerlinearprogrammingforcomputingoptimalexperimentaldesigns</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Mixedintegerlinearprogrammingforcomputingoptimalexperimentaldesigns.html">&lt;p&gt;The problem of computing an exact experimental design that is optimal for the least-squares estimation of the parameters of a regression model is considered. We show that this problem can be solved via mixed-integer linear programming (MILP) for a wide class of optimality criteria, including the criteria of A-, I-, G- and MV-optimality. This approach improves upon the current state-of-the-art mathematical programming formulation, which uses mixed-integer second-order cone programming. The key idea underlying the MILP formulation is McCormick relaxation, which critically depends on finite interval bounds for the elements of the covariance matrix of the least-squares estimator corresponding to an optimal exact design. We provide both analytic and algorithmic methods for constructing these bounds. We also demonstrate the unique advantages of the MILP approach, such as the possibility of incorporating multiple design constraints into the optimization problem, including constraints on the variances and covariances of the least-squares estimator.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.17562&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Radoslav Harman, Samuel Rosa</name></author><category term="stat.CO" /><summary type="html">The problem of computing an exact experimental design that is optimal for the least-squares estimation of the parameters of a regression model is considered. We show that this problem can be solved via mixed-integer linear programming (MILP) for a wide class of optimality criteria, including the criteria of A-, I-, G- and MV-optimality. This approach improves upon the current state-of-the-art mathematical programming formulation, which uses mixed-integer second-order cone programming. The key idea underlying the MILP formulation is McCormick relaxation, which critically depends on finite interval bounds for the elements of the covariance matrix of the least-squares estimator corresponding to an optimal exact design. We provide both analytic and algorithmic methods for constructing these bounds. We also demonstrate the unique advantages of the MILP approach, such as the possibility of incorporating multiple design constraints into the optimization problem, including constraints on the variances and covariances of the least-squares estimator.</summary></entry><entry><title type="html">On the Implicit Bias of Adam</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/OntheImplicitBiasofAdam.html" rel="alternate" type="text/html" title="On the Implicit Bias of Adam" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/OntheImplicitBiasofAdam</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/OntheImplicitBiasofAdam.html">&lt;p&gt;In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different “norm” involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, conversely, impede its reduction (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.00079&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matias D. Cattaneo, Jason M. Klusowski, Boris Shigida</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">In previous literature, backward error analysis was used to find ordinary differential equations (ODEs) approximating the gradient descent trajectory. It was found that finite step sizes implicitly regularize solutions because terms appearing in the ODEs penalize the two-norm of the loss gradients. We prove that the existence of similar implicit regularization in RMSProp and Adam depends on their hyperparameters and the training stage, but with a different “norm” involved: the corresponding ODE terms either penalize the (perturbed) one-norm of the loss gradients or, conversely, impede its reduction (the latter case being typical). We also conduct numerical experiments and discuss how the proven facts can influence generalization.</summary></entry><entry><title type="html">Parallelizing MCMC with Machine Learning Classifier and Its Criterion Based on Kullback-Leibler Divergence</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ParallelizingMCMCwithMachineLearningClassifierandItsCriterionBasedonKullbackLeiblerDivergence.html" rel="alternate" type="text/html" title="Parallelizing MCMC with Machine Learning Classifier and Its Criterion Based on Kullback-Leibler Divergence" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ParallelizingMCMCwithMachineLearningClassifierandItsCriterionBasedonKullbackLeiblerDivergence</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ParallelizingMCMCwithMachineLearningClassifierandItsCriterionBasedonKullbackLeiblerDivergence.html">&lt;p&gt;In the era of Big Data, analyzing high-dimensional and large datasets presents significant computational challenges. Although Bayesian statistics is well-suited for these complex data structures, Markov chain Monte Carlo (MCMC) method, which are essential for Bayesian estimation, suffers from computation cost because of its sequential nature. For faster and more effective computation, this paper introduces an algorithm to enhance a parallelizing MCMC method to handle this computation problem. We highlight the critical role of the overlapped area of posterior distributions after data partitioning, and propose a method using a machine learning classifier to effectively identify and extract MCMC draws from the area to approximate the actual posterior distribution. Our main contribution is the development of a Kullback-Leibler (KL) divergence-based criterion that simplifies hyperparameter tuning in training a classifier and makes the method nearly hyperparameter-free. Simulation studies validate the efficacy of our proposed methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11246&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tomoki Matsumoto, Yuichiro Kanazawa</name></author><category term="stat.CO" /><summary type="html">In the era of Big Data, analyzing high-dimensional and large datasets presents significant computational challenges. Although Bayesian statistics is well-suited for these complex data structures, Markov chain Monte Carlo (MCMC) method, which are essential for Bayesian estimation, suffers from computation cost because of its sequential nature. For faster and more effective computation, this paper introduces an algorithm to enhance a parallelizing MCMC method to handle this computation problem. We highlight the critical role of the overlapped area of posterior distributions after data partitioning, and propose a method using a machine learning classifier to effectively identify and extract MCMC draws from the area to approximate the actual posterior distribution. Our main contribution is the development of a Kullback-Leibler (KL) divergence-based criterion that simplifies hyperparameter tuning in training a classifier and makes the method nearly hyperparameter-free. Simulation studies validate the efficacy of our proposed methods.</summary></entry><entry><title type="html">Particle Denoising Diffusion Sampler</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ParticleDenoisingDiffusionSampler.html" rel="alternate" type="text/html" title="Particle Denoising Diffusion Sampler" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ParticleDenoisingDiffusionSampler</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ParticleDenoisingDiffusionSampler.html">&lt;p&gt;Denoising diffusion models have become ubiquitous for generative modeling. The core idea is to transport the data distribution to a Gaussian by using a diffusion. Approximate samples from the data distribution are then obtained by estimating the time-reversal of this diffusion using score matching ideas. We follow here a similar strategy to sample from unnormalized probability densities and compute their normalizing constants. However, the time-reversed diffusion is here simulated by using an original iterative particle scheme relying on a novel score matching loss. Contrary to standard denoising diffusion models, the resulting Particle Denoising Diffusion Sampler (PDDS) provides asymptotically consistent estimates under mild assumptions. We demonstrate PDDS on multimodal and high dimensional sampling tasks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.06320&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Angus Phillips, Hai-Dang Dau, Michael John Hutchinson, Valentin De Bortoli, George Deligiannidis, Arnaud Doucet</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Denoising diffusion models have become ubiquitous for generative modeling. The core idea is to transport the data distribution to a Gaussian by using a diffusion. Approximate samples from the data distribution are then obtained by estimating the time-reversal of this diffusion using score matching ideas. We follow here a similar strategy to sample from unnormalized probability densities and compute their normalizing constants. However, the time-reversed diffusion is here simulated by using an original iterative particle scheme relying on a novel score matching loss. Contrary to standard denoising diffusion models, the resulting Particle Denoising Diffusion Sampler (PDDS) provides asymptotically consistent estimates under mild assumptions. We demonstrate PDDS on multimodal and high dimensional sampling tasks.</summary></entry><entry><title type="html">Predictive Probabilities Made Simple: A Fast and Accurate Method for Clinical Trial Decision Making</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/PredictiveProbabilitiesMadeSimpleAFastandAccurateMethodforClinicalTrialDecisionMaking.html" rel="alternate" type="text/html" title="Predictive Probabilities Made Simple: A Fast and Accurate Method for Clinical Trial Decision Making" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/PredictiveProbabilitiesMadeSimpleAFastandAccurateMethodforClinicalTrialDecisionMaking</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/PredictiveProbabilitiesMadeSimpleAFastandAccurateMethodforClinicalTrialDecisionMaking.html">&lt;p&gt;Bayesian predictive probabilities are commonly used for interim monitoring of clinical trials through efficacy and futility stopping rules. Despite their usefulness, calculation of predictive probabilities, particularly in pre-experiment trial simulation, can be a significant challenge. We introduce an approximation for computing predictive probabilities using either a p-value or a posterior probability that significantly reduces this burden. We show the approximation has a high degree of concordance with standard Monte Carlo imputation methods for computing predictive probabilities, and present five simulation studies comparing the approximation to the full predictive probability for a range of primary analysis strategies: dichotomous, time-to-event, and ordinal endpoints, as well as historical borrowing and longitudinal modeling. We find that this faster method of predictive probability approximation works well in all five applications, thus significantly reducing the computational burden of trial simulation, allowing more virtual trials to be simulated to achieve greater precision in estimating trial operating characteristics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11406&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joe Marion, Liz Lorenzi, Cora Allen-Savietta, Scott Berry, Kert Viele</name></author><category term="stat.AP" /><summary type="html">Bayesian predictive probabilities are commonly used for interim monitoring of clinical trials through efficacy and futility stopping rules. Despite their usefulness, calculation of predictive probabilities, particularly in pre-experiment trial simulation, can be a significant challenge. We introduce an approximation for computing predictive probabilities using either a p-value or a posterior probability that significantly reduces this burden. We show the approximation has a high degree of concordance with standard Monte Carlo imputation methods for computing predictive probabilities, and present five simulation studies comparing the approximation to the full predictive probability for a range of primary analysis strategies: dichotomous, time-to-event, and ordinal endpoints, as well as historical borrowing and longitudinal modeling. We find that this faster method of predictive probability approximation works well in all five applications, thus significantly reducing the computational burden of trial simulation, allowing more virtual trials to be simulated to achieve greater precision in estimating trial operating characteristics.</summary></entry><entry><title type="html">Producing treatment hierarchies in network meta-analysis using probabilistic models and treatment-choice criteria</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Producingtreatmenthierarchiesinnetworkmetaanalysisusingprobabilisticmodelsandtreatmentchoicecriteria.html" rel="alternate" type="text/html" title="Producing treatment hierarchies in network meta-analysis using probabilistic models and treatment-choice criteria" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Producingtreatmenthierarchiesinnetworkmetaanalysisusingprobabilisticmodelsandtreatmentchoicecriteria</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Producingtreatmenthierarchiesinnetworkmetaanalysisusingprobabilisticmodelsandtreatmentchoicecriteria.html">&lt;p&gt;A key output of network meta-analysis (NMA) is the relative ranking of the treatments; nevertheless, it has attracted a lot of criticism. This is mainly due to the fact that ranking is an influential output and prone to over-interpretations even when relative effects imply small differences between treatments. To date, common ranking methods rely on metrics that lack a straightforward interpretation, while it is still unclear how to measure their uncertainty. We introduce a novel framework for estimating treatment hierarchies in NMA. At first, we formulate a mathematical expression that defines a treatment choice criterion (TCC) based on clinically important values. This TCC is applied to the study treatment effects to generate paired data indicating treatment preferences or ties. Then, we synthesize the paired data across studies using an extension of the so-called “Bradley-Terry” model. We assign to each treatment a latent variable interpreted as the treatment “ability” and we estimate the ability parameters within a regression model. Higher ability estimates correspond to higher positions in the final ranking. We further extend our model to adjust for covariates that may affect treatment selection. We illustrate the proposed approach and compare it with alternatives in two datasets: a network comparing 18 antidepressants for major depression and a network comparing 6 antihypertensives for the incidence of diabetes. Our approach provides a robust and interpretable treatment hierarchy which accounts for clinically important values and is presented alongside with uncertainty measures. Overall, the proposed framework offers a novel approach for ranking in NMA based on concrete criteria and preserves from over-interpretation of unimportant differences between treatments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10612&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Theodoros Evrenoglou, Adriani Nikolakopoulou, Guido Schwarzer, Gerta Rücker, Anna Chaimani</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.OT" /><summary type="html">A key output of network meta-analysis (NMA) is the relative ranking of the treatments; nevertheless, it has attracted a lot of criticism. This is mainly due to the fact that ranking is an influential output and prone to over-interpretations even when relative effects imply small differences between treatments. To date, common ranking methods rely on metrics that lack a straightforward interpretation, while it is still unclear how to measure their uncertainty. We introduce a novel framework for estimating treatment hierarchies in NMA. At first, we formulate a mathematical expression that defines a treatment choice criterion (TCC) based on clinically important values. This TCC is applied to the study treatment effects to generate paired data indicating treatment preferences or ties. Then, we synthesize the paired data across studies using an extension of the so-called “Bradley-Terry” model. We assign to each treatment a latent variable interpreted as the treatment “ability” and we estimate the ability parameters within a regression model. Higher ability estimates correspond to higher positions in the final ranking. We further extend our model to adjust for covariates that may affect treatment selection. We illustrate the proposed approach and compare it with alternatives in two datasets: a network comparing 18 antidepressants for major depression and a network comparing 6 antihypertensives for the incidence of diabetes. Our approach provides a robust and interpretable treatment hierarchy which accounts for clinically important values and is presented alongside with uncertainty measures. Overall, the proposed framework offers a novel approach for ranking in NMA based on concrete criteria and preserves from over-interpretation of unimportant differences between treatments.</summary></entry><entry><title type="html">Quick and Simple Kernel Differential Equation Regression Estimators for Data with Sparse Design</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/QuickandSimpleKernelDifferentialEquationRegressionEstimatorsforDatawithSparseDesign.html" rel="alternate" type="text/html" title="Quick and Simple Kernel Differential Equation Regression Estimators for Data with Sparse Design" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/QuickandSimpleKernelDifferentialEquationRegressionEstimatorsforDatawithSparseDesign</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/QuickandSimpleKernelDifferentialEquationRegressionEstimatorsforDatawithSparseDesign.html">&lt;p&gt;Local polynomial regression of order at least one often performs poorly in regions of sparse data. Local constant regression is exceptional in this regard, though it is the least accurate method in general, especially at the boundaries of the data. Incorporating information from differential equations which may approximately or exactly hold is one way of extending the sparse design capacity of local constant regression while reducing bias and variance. A nonparametric regression method that exploits first order differential equations is introduced in this paper and applied to noisy mouse tumour growth data. Asymptotic biases and variances of kernel estimators using Taylor polynomials with different degrees are discussed. Model comparison is performed for different estimators through simulation studies under various scenarios which simulate exponential-type growth.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10308&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chunlei Ge, W. John Braun</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Local polynomial regression of order at least one often performs poorly in regions of sparse data. Local constant regression is exceptional in this regard, though it is the least accurate method in general, especially at the boundaries of the data. Incorporating information from differential equations which may approximately or exactly hold is one way of extending the sparse design capacity of local constant regression while reducing bias and variance. A nonparametric regression method that exploits first order differential equations is introduced in this paper and applied to noisy mouse tumour growth data. Asymptotic biases and variances of kernel estimators using Taylor polynomials with different degrees are discussed. Model comparison is performed for different estimators through simulation studies under various scenarios which simulate exponential-type growth.</summary></entry><entry><title type="html">Regularization of the ensemble Kalman filter using a non-parametric, non-stationary spatial model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/RegularizationoftheensembleKalmanfilterusinganonparametricnonstationaryspatialmodel.html" rel="alternate" type="text/html" title="Regularization of the ensemble Kalman filter using a non-parametric, non-stationary spatial model" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/RegularizationoftheensembleKalmanfilterusinganonparametricnonstationaryspatialmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/RegularizationoftheensembleKalmanfilterusinganonparametricnonstationaryspatialmodel.html">&lt;p&gt;The sample covariance matrix of a random vector is a good estimate of the true covariance matrix if the sample size is much larger than the length of the vector. In high-dimensional problems, this condition is never met. As a result, in high dimensions the Ensemble Kalman Filter’s (EnKF) ensemble does not contain enough information to specify the prior covariance matrix accurately. This necessitates the need for regularization of the analysis (observation update) problem. We propose a regularization technique based on a new spatial model. The model is a constrained version of the general Gaussian process convolution model. The constraints include local stationarity and smoothness of local spectra. We regularize EnKF by postulating that its prior covariances obey the spatial model. Placing a hyperprior distribution on the model parameters and using the likelihood of the prior ensemble data allows for an optimized use of both the ensemble and the hyperprior. The respective estimator is shown to be consistent. Its neural Bayes implementation proved to be both accurate and computationally efficient. In simulation experiments, the new technique led to substantially better EnKF performance than several existing techniques.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.14318&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael Tsyrulnikov, Arseniy Sotskiy</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">The sample covariance matrix of a random vector is a good estimate of the true covariance matrix if the sample size is much larger than the length of the vector. In high-dimensional problems, this condition is never met. As a result, in high dimensions the Ensemble Kalman Filter’s (EnKF) ensemble does not contain enough information to specify the prior covariance matrix accurately. This necessitates the need for regularization of the analysis (observation update) problem. We propose a regularization technique based on a new spatial model. The model is a constrained version of the general Gaussian process convolution model. The constraints include local stationarity and smoothness of local spectra. We regularize EnKF by postulating that its prior covariances obey the spatial model. Placing a hyperprior distribution on the model parameters and using the likelihood of the prior ensemble data allows for an optimized use of both the ensemble and the hyperprior. The respective estimator is shown to be consistent. Its neural Bayes implementation proved to be both accurate and computationally efficient. In simulation experiments, the new technique led to substantially better EnKF performance than several existing techniques.</summary></entry><entry><title type="html">Ride-sharing Determinants: Spatial and Spatio-temporal Bayesian Analysis for Chicago Service in 2022</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/RidesharingDeterminantsSpatialandSpatiotemporalBayesianAnalysisforChicagoServicein2022.html" rel="alternate" type="text/html" title="Ride-sharing Determinants: Spatial and Spatio-temporal Bayesian Analysis for Chicago Service in 2022" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/RidesharingDeterminantsSpatialandSpatiotemporalBayesianAnalysisforChicagoServicein2022</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/RidesharingDeterminantsSpatialandSpatiotemporalBayesianAnalysisforChicagoServicein2022.html">&lt;p&gt;The rapid expansion of ride-sharing services has caused significant disruptions in the transpor-tation industry and fundamentally altered the way individuals move from one place to another. Accurate estimation of ride-sharing improves service utilization and reliability and reduces travel time and traffic congestion. In this study, we employ two Bayesian models to estimate ride-sharing demand in the 77 Chicago community areas. We consider demographic, scoio-economic, transportation factors as well as land-use characteristics as explanatory variables. Our models assume conditional autoregression (CAR) prior for the explanatory variables. Moreover, the Bayesian frameworks estimate both the unstructured random error and the struc-tured errors for the spatial and the spatiotemporal correlation. We assessed the performance of the estimated models and the residuals of the spatial regression model have no left-over spatial structure. For the spatiotemporal model, the squared correlation between actual ride-shares and the fitted values is 0.95. Our analysis revealed that the demographic factors (populations size and registered crimes) positively impact the ride-sharing demand. Additionally, the ride-sharing demand increases with higher income and increase in the economically active propor-tion of the population as well as the residents with no cars. Moreover, the transit availability and the walkability indices are crucial determinants for the ridesharing in Chicago.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11590&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mohamed Elkhouly, Taqwa Alhadidi</name></author><category term="stat.AP" /><summary type="html">The rapid expansion of ride-sharing services has caused significant disruptions in the transpor-tation industry and fundamentally altered the way individuals move from one place to another. Accurate estimation of ride-sharing improves service utilization and reliability and reduces travel time and traffic congestion. In this study, we employ two Bayesian models to estimate ride-sharing demand in the 77 Chicago community areas. We consider demographic, scoio-economic, transportation factors as well as land-use characteristics as explanatory variables. Our models assume conditional autoregression (CAR) prior for the explanatory variables. Moreover, the Bayesian frameworks estimate both the unstructured random error and the struc-tured errors for the spatial and the spatiotemporal correlation. We assessed the performance of the estimated models and the residuals of the spatial regression model have no left-over spatial structure. For the spatiotemporal model, the squared correlation between actual ride-shares and the fitted values is 0.95. Our analysis revealed that the demographic factors (populations size and registered crimes) positively impact the ride-sharing demand. Additionally, the ride-sharing demand increases with higher income and increase in the economically active propor-tion of the population as well as the residents with no cars. Moreover, the transit availability and the walkability indices are crucial determinants for the ridesharing in Chicago.</summary></entry><entry><title type="html">R-miss-tastic: a unified platform for missing values methods and workflows</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Rmisstasticaunifiedplatformformissingvaluesmethodsandworkflows.html" rel="alternate" type="text/html" title="R-miss-tastic: a unified platform for missing values methods and workflows" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Rmisstasticaunifiedplatformformissingvaluesmethodsandworkflows</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Rmisstasticaunifiedplatformformissingvaluesmethodsandworkflows.html">&lt;p&gt;Missing values are unavoidable when working with data. Their occurrence is exacerbated as more data from different sources become available. However, most statistical models and visualization methods require complete data, and improper handling of missing data results in information loss or biased analyses. Since the seminal work of Rubin (1976), a burgeoning literature on missing values has arisen, with heterogeneous aims and motivations. This led to the development of various methods, formalizations, and tools. For practitioners, it remains nevertheless challenging to decide which method is most suited for their problem, partially due to a lack of systematic covering of this topic in statistics or data science curricula.
  To help address this challenge, we have launched the “R-miss-tastic” platform, which aims to provide an overview of standard missing values problems, methods, and relevant implementations of methodologies. Beyond gathering and organizing a large majority of the material on missing data (bibliography, courses, tutorials, implementations), “R-miss-tastic” covers the development of standardized analysis workflows. Indeed, we have developed several pipelines in R and Python to allow for hands-on illustration of and recommendations on missing values handling in various statistical tasks such as matrix completion, estimation and prediction, while ensuring reproducibility of the analyses. Finally, the platform is dedicated to users who analyze incomplete data, researchers who want to compare their methods and search for an up-to-date bibliography, and also teachers who are looking for didactic materials (notebooks, video, slides).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.04822&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Imke Mayer, Aude Sportisse, Julie Josse, Nicholas Tierney, Nathalie Vialaneix</name></author><category term="stat.ME" /><summary type="html">Missing values are unavoidable when working with data. Their occurrence is exacerbated as more data from different sources become available. However, most statistical models and visualization methods require complete data, and improper handling of missing data results in information loss or biased analyses. Since the seminal work of Rubin (1976), a burgeoning literature on missing values has arisen, with heterogeneous aims and motivations. This led to the development of various methods, formalizations, and tools. For practitioners, it remains nevertheless challenging to decide which method is most suited for their problem, partially due to a lack of systematic covering of this topic in statistics or data science curricula. To help address this challenge, we have launched the “R-miss-tastic” platform, which aims to provide an overview of standard missing values problems, methods, and relevant implementations of methodologies. Beyond gathering and organizing a large majority of the material on missing data (bibliography, courses, tutorials, implementations), “R-miss-tastic” covers the development of standardized analysis workflows. Indeed, we have developed several pipelines in R and Python to allow for hands-on illustration of and recommendations on missing values handling in various statistical tasks such as matrix completion, estimation and prediction, while ensuring reproducibility of the analyses. Finally, the platform is dedicated to users who analyze incomplete data, researchers who want to compare their methods and search for an up-to-date bibliography, and also teachers who are looking for didactic materials (notebooks, video, slides).</summary></entry><entry><title type="html">Robust Functional Data Analysis for Stochastic Evolution Equations in Infinite Dimensions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/RobustFunctionalDataAnalysisforStochasticEvolutionEquationsinInfiniteDimensions.html" rel="alternate" type="text/html" title="Robust Functional Data Analysis for Stochastic Evolution Equations in Infinite Dimensions" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/RobustFunctionalDataAnalysisforStochasticEvolutionEquationsinInfiniteDimensions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/RobustFunctionalDataAnalysisforStochasticEvolutionEquationsinInfiniteDimensions.html">&lt;p&gt;We develop an asymptotic theory for the jump robust measurement of covariations in the context of stochastic evolution equation in infinite dimensions. Namely, we identify scaling limits for realized covariations of solution processes with the quadratic covariation of the latent random process that drives the evolution equation which is assumed to be a Hilbert space-valued semimartingale. We discuss applications to dynamically consistent and outlier-robust dimension reduction in the spirit of functional principal components and the estimation of infinite-dimensional stochastic volatility models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.16286&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Dennis Schroers</name></author><category term="stat.ME" /><summary type="html">We develop an asymptotic theory for the jump robust measurement of covariations in the context of stochastic evolution equation in infinite dimensions. Namely, we identify scaling limits for realized covariations of solution processes with the quadratic covariation of the latent random process that drives the evolution equation which is assumed to be a Hilbert space-valued semimartingale. We discuss applications to dynamically consistent and outlier-robust dimension reduction in the spirit of functional principal components and the estimation of infinite-dimensional stochastic volatility models.</summary></entry><entry><title type="html">Sampling and estimation on manifolds using the Langevin diffusion</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SamplingandestimationonmanifoldsusingtheLangevindiffusion.html" rel="alternate" type="text/html" title="Sampling and estimation on manifolds using the Langevin diffusion" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SamplingandestimationonmanifoldsusingtheLangevindiffusion</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SamplingandestimationonmanifoldsusingtheLangevindiffusion.html">&lt;p&gt;Error bounds are derived for sampling and estimation using a discretization of an intrinsically defined Langevin diffusion with invariant measure $\text{d}\mu_\phi \propto e^{-\phi} \mathrm{dvol}&lt;em&gt;g $ on a compact Riemannian manifold. Two estimators of linear functionals of $\mu&lt;/em&gt;\phi $ based on the discretized Markov process are considered: a time-averaging estimator based on a single trajectory and an ensemble-averaging estimator based on multiple independent trajectories. Imposing no restrictions beyond a nominal level of smoothness on $\phi$, first-order error bounds, in discretization step size, on the bias and variance/mean-square error of both estimators are derived. The order of error matches the optimal rate in Euclidean and flat spaces, and leads to a first-order bound on distance between the invariant measure $\mu_\phi$ and a stationary measure of the discretized Markov process. This order is preserved even upon using retractions when exponential maps are unavailable in closed form, thus enhancing practicality of the proposed algorithms. Generality of the proof techniques, which exploit links between two partial differential equations and the semigroup of operators corresponding to the Langevin diffusion, renders them amenable for the study of a more general class of sampling algorithms related to the Langevin diffusion. Conditions for extending analysis to the case of non-compact manifolds are discussed. Numerical illustrations with distributions, log-concave and otherwise, on the manifolds of positive and negative curvature elucidate on the derived bounds and demonstrate practical utility of the sampling algorithm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.14882&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Karthik Bharath, Alexander Lewis, Akash Sharma, Michael V Tretyakov</name></author><category term="stat.CO," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Error bounds are derived for sampling and estimation using a discretization of an intrinsically defined Langevin diffusion with invariant measure $\text{d}\mu_\phi \propto e^{-\phi} \mathrm{dvol}g $ on a compact Riemannian manifold. Two estimators of linear functionals of $\mu\phi $ based on the discretized Markov process are considered: a time-averaging estimator based on a single trajectory and an ensemble-averaging estimator based on multiple independent trajectories. Imposing no restrictions beyond a nominal level of smoothness on $\phi$, first-order error bounds, in discretization step size, on the bias and variance/mean-square error of both estimators are derived. The order of error matches the optimal rate in Euclidean and flat spaces, and leads to a first-order bound on distance between the invariant measure $\mu_\phi$ and a stationary measure of the discretized Markov process. This order is preserved even upon using retractions when exponential maps are unavailable in closed form, thus enhancing practicality of the proposed algorithms. Generality of the proof techniques, which exploit links between two partial differential equations and the semigroup of operators corresponding to the Langevin diffusion, renders them amenable for the study of a more general class of sampling algorithms related to the Langevin diffusion. Conditions for extending analysis to the case of non-compact manifolds are discussed. Numerical illustrations with distributions, log-concave and otherwise, on the manifolds of positive and negative curvature elucidate on the derived bounds and demonstrate practical utility of the sampling algorithm.</summary></entry><entry><title type="html">Sampling metastable systems using collective variables and Jarzynski-Crooks paths</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SamplingmetastablesystemsusingcollectivevariablesandJarzynskiCrookspaths.html" rel="alternate" type="text/html" title="Sampling metastable systems using collective variables and Jarzynski-Crooks paths" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SamplingmetastablesystemsusingcollectivevariablesandJarzynskiCrookspaths</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SamplingmetastablesystemsusingcollectivevariablesandJarzynskiCrookspaths.html">&lt;p&gt;We consider the problem of sampling a high dimensional multimodal target probability measure. We assume that a good proposal kernel to move only a subset of the degrees of freedoms (also known as collective variables) is known a priori. This proposal kernel can for example be built using normalizing flows. We show how to extend the move from the collective variable space to the full space and how to implement an accept-reject step in order to get a reversible chain with respect to a target probability measure. The accept-reject step does not require to know the marginal of the original measure in the collective variable (namely to know the free energy). The obtained algorithm admits several variants, some of them being very close to methods which have been proposed previously in the literature. We show how the obtained acceptance ratio can be expressed in terms of the work which appears in the Jarzynski-Crooks equality, at least for some variants. Numerical illustrations demonstrate the efficiency of the approach on various simple test cases, and allow us to compare the variants of the algorithm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18160&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Christoph Schönle, Marylou Gabrié, Tony Lelièvre, Gabriel Stoltz</name></author><category term="cond-mat.stat-mech," /><category term="stat.CO" /><summary type="html">We consider the problem of sampling a high dimensional multimodal target probability measure. We assume that a good proposal kernel to move only a subset of the degrees of freedoms (also known as collective variables) is known a priori. This proposal kernel can for example be built using normalizing flows. We show how to extend the move from the collective variable space to the full space and how to implement an accept-reject step in order to get a reversible chain with respect to a target probability measure. The accept-reject step does not require to know the marginal of the original measure in the collective variable (namely to know the free energy). The obtained algorithm admits several variants, some of them being very close to methods which have been proposed previously in the literature. We show how the obtained acceptance ratio can be expressed in terms of the work which appears in the Jarzynski-Crooks equality, at least for some variants. Numerical illustrations demonstrate the efficiency of the approach on various simple test cases, and allow us to compare the variants of the algorithm.</summary></entry><entry><title type="html">Scalable Bayesian Image-on-Scalar Regression for Population-Scale Neuroimaging Data Analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ScalableBayesianImageonScalarRegressionforPopulationScaleNeuroimagingDataAnalysis.html" rel="alternate" type="text/html" title="Scalable Bayesian Image-on-Scalar Regression for Population-Scale Neuroimaging Data Analysis" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ScalableBayesianImageonScalarRegressionforPopulationScaleNeuroimagingDataAnalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/ScalableBayesianImageonScalarRegressionforPopulationScaleNeuroimagingDataAnalysis.html">&lt;p&gt;Bayesian Image-on-Scalar Regression (ISR) offers significant advantages for neuroimaging data analysis, including flexibility and the ability to quantify uncertainty. However, its application to large-scale imaging datasets, such as found in the UK Biobank, is hindered by the computational demands of traditional posterior computation methods, as well as the challenge of individual-specific brain masks that deviate from the common mask typically used in standard ISR approaches. To address these challenges, we introduce a novel Bayesian ISR model that is scalable and accommodates inconsistent brain masks across subjects in large-scale imaging studies. Our model leverages Gaussian process priors and integrates salience area indicators to facilitate ISR. We develop a cutting-edge scalable posterior computation algorithm that employs stochastic gradient Langevin dynamics coupled with memory mapping techniques, ensuring that computation time scales linearly with subsample size and memory usage is constrained only by the batch size. Our approach uniquely enables direct spatial posterior inferences on brain activation regions. The efficacy of our method is demonstrated through simulations and analysis of the UK Biobank task fMRI data, encompassing 38,639 subjects and over 120,000 voxels per image, showing that it can achieve a speed increase of 4 to 11 times and enhance statistical power by 8% to 18% compared to traditional Gibbs sampling with zero-imputation in various simulation scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.13204&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuliang Xu, Timothy D. Johnson, Thomas E. Nichols, Jian Kang</name></author><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Bayesian Image-on-Scalar Regression (ISR) offers significant advantages for neuroimaging data analysis, including flexibility and the ability to quantify uncertainty. However, its application to large-scale imaging datasets, such as found in the UK Biobank, is hindered by the computational demands of traditional posterior computation methods, as well as the challenge of individual-specific brain masks that deviate from the common mask typically used in standard ISR approaches. To address these challenges, we introduce a novel Bayesian ISR model that is scalable and accommodates inconsistent brain masks across subjects in large-scale imaging studies. Our model leverages Gaussian process priors and integrates salience area indicators to facilitate ISR. We develop a cutting-edge scalable posterior computation algorithm that employs stochastic gradient Langevin dynamics coupled with memory mapping techniques, ensuring that computation time scales linearly with subsample size and memory usage is constrained only by the batch size. Our approach uniquely enables direct spatial posterior inferences on brain activation regions. The efficacy of our method is demonstrated through simulations and analysis of the UK Biobank task fMRI data, encompassing 38,639 subjects and over 120,000 voxels per image, showing that it can achieve a speed increase of 4 to 11 times and enhance statistical power by 8% to 18% compared to traditional Gibbs sampling with zero-imputation in various simulation scenarios.</summary></entry><entry><title type="html">Scaling-aware rating of count forecasts</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Scalingawareratingofcountforecasts.html" rel="alternate" type="text/html" title="Scaling-aware rating of count forecasts" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Scalingawareratingofcountforecasts</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Scalingawareratingofcountforecasts.html">&lt;p&gt;Forecast quality should be assessed in the context of what is possible in theory and what is reasonable to expect in practice. Often, one can identify an approximate upper bound to a probabilistic forecast’s sharpness, which sets a lower, not necessarily achievable, limit to error metrics. In retail forecasting, a simple, but often unconquerable sharpness limit is given by the Poisson distribution. When evaluating forecasts using traditional metrics such as Mean Absolute Error, it is hard to judge whether a certain achieved value reflects unavoidable Poisson noise or truly indicates an overdispersed prediction model. Moreover, every evaluation metric suffers from precision scaling: Perhaps surprisingly, the metric’s value is mostly defined by the selling rate and by the resulting rate-dependent Poisson noise, and only secondarily by the forecast quality. For any metric, comparing two groups of forecasted products often yields “the slow movers are performing worse than the fast movers” or vice versa, the na&quot;ive scaling trap. To distill the intrinsic quality of a forecast, we stratify predictions into buckets of approximately equal predicted value and evaluate metrics separately per bucket. By comparing the achieved value per bucket to benchmarks, we obtain an intuitive visualization of forecast quality, which can be summarized into a single rating that makes forecast quality comparable among different products or even industries. The thereby developed scaling-aware forecast rating is applied to forecasting models used on the M5 competition dataset as well as to real-life forecasts provided by Blue Yonder’s Demand Edge for Retail solution for grocery products in Sainsbury’s supermarkets in the United Kingdom. The results permit a clear interpretation and high-level understanding of model quality by non-experts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.16313&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Malte C. Tichy, Illia Babounikau, Nikolas Wolke, Stefan Ulbrich, Michael Feindt</name></author><category term="stat.AP" /><summary type="html">Forecast quality should be assessed in the context of what is possible in theory and what is reasonable to expect in practice. Often, one can identify an approximate upper bound to a probabilistic forecast’s sharpness, which sets a lower, not necessarily achievable, limit to error metrics. In retail forecasting, a simple, but often unconquerable sharpness limit is given by the Poisson distribution. When evaluating forecasts using traditional metrics such as Mean Absolute Error, it is hard to judge whether a certain achieved value reflects unavoidable Poisson noise or truly indicates an overdispersed prediction model. Moreover, every evaluation metric suffers from precision scaling: Perhaps surprisingly, the metric’s value is mostly defined by the selling rate and by the resulting rate-dependent Poisson noise, and only secondarily by the forecast quality. For any metric, comparing two groups of forecasted products often yields “the slow movers are performing worse than the fast movers” or vice versa, the na&quot;ive scaling trap. To distill the intrinsic quality of a forecast, we stratify predictions into buckets of approximately equal predicted value and evaluate metrics separately per bucket. By comparing the achieved value per bucket to benchmarks, we obtain an intuitive visualization of forecast quality, which can be summarized into a single rating that makes forecast quality comparable among different products or even industries. The thereby developed scaling-aware forecast rating is applied to forecasting models used on the M5 competition dataset as well as to real-life forecasts provided by Blue Yonder’s Demand Edge for Retail solution for grocery products in Sainsbury’s supermarkets in the United Kingdom. The results permit a clear interpretation and high-level understanding of model quality by non-experts.</summary></entry><entry><title type="html">Sparse and Integrative Principal Component Analysis for Multiview Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SparseandIntegrativePrincipalComponentAnalysisforMultiviewData.html" rel="alternate" type="text/html" title="Sparse and Integrative Principal Component Analysis for Multiview Data" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SparseandIntegrativePrincipalComponentAnalysisforMultiviewData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SparseandIntegrativePrincipalComponentAnalysisforMultiviewData.html">&lt;p&gt;We consider dimension reduction of multiview data, which are emerging in scientific studies. Formulating multiview data as multi-variate data with block structures corresponding to the different views, or views of data, we estimate top eigenvectors from multiview data that have two-fold sparsity, elementwise sparsity and blockwise sparsity. We propose a Fantope-based optimization criterion with multiple penalties to enforce the desired sparsity patterns and a denoising step is employed to handle potential presence of heteroskedastic noise across different data views. An alternating direction method of multipliers (ADMM) algorithm is used for optimization. We derive the l2 convergence of the estimated top eigenvectors and establish their sparsity and support recovery properties. Numerical studies are used to illustrate the proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.06718&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lin Xiao, Luo Xiao</name></author><category term="stat.ME" /><summary type="html">We consider dimension reduction of multiview data, which are emerging in scientific studies. Formulating multiview data as multi-variate data with block structures corresponding to the different views, or views of data, we estimate top eigenvectors from multiview data that have two-fold sparsity, elementwise sparsity and blockwise sparsity. We propose a Fantope-based optimization criterion with multiple penalties to enforce the desired sparsity patterns and a denoising step is employed to handle potential presence of heteroskedastic noise across different data views. An alternating direction method of multipliers (ADMM) algorithm is used for optimization. We derive the l2 convergence of the estimated top eigenvectors and establish their sparsity and support recovery properties. Numerical studies are used to illustrate the proposed method.</summary></entry><entry><title type="html">Spectral co-Clustering in Multi-layer Directed Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SpectralcoClusteringinMultilayerDirectedNetworks.html" rel="alternate" type="text/html" title="Spectral co-Clustering in Multi-layer Directed Networks" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SpectralcoClusteringinMultilayerDirectedNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SpectralcoClusteringinMultilayerDirectedNetworks.html">&lt;p&gt;Modern network analysis often involves multi-layer network data in which the nodes are aligned, and the edges on each layer represent one of the multiple relations among the nodes. Current literature on multi-layer network data is mostly limited to undirected relations. However, direct relations are more common and may introduce extra information. This study focuses on community detection (or clustering) in multi-layer directed networks. To take into account the asymmetry, a novel spectral-co-clustering-based algorithm is developed to detect co-clusters, which capture the sending patterns and receiving patterns of nodes, respectively. Specifically, the eigendecomposition of the debiased sum of Gram matrices over the layer-wise adjacency matrices is computed, followed by the k-means, where the sum of Gram matrices is used to avoid possible cancellation of clusters caused by direct summation. Theoretical analysis of the algorithm under the multi-layer stochastic co-block model is provided, where the common assumption that the cluster number is coupled with the rank of the model is relaxed. After a systematic analysis of the eigenvectors of the population version algorithm, the misclassification rates are derived, which show that multi-layers would bring benefits to the clustering performance. The experimental results of simulated data corroborate the theoretical predictions, and the analysis of a real-world trade network dataset provides interpretable results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.10572&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wenqing Su, Xiao Guo, Xiangyu Chang, Ying Yang</name></author><category term="stat.AP," /><category term="stat.TH" /><summary type="html">Modern network analysis often involves multi-layer network data in which the nodes are aligned, and the edges on each layer represent one of the multiple relations among the nodes. Current literature on multi-layer network data is mostly limited to undirected relations. However, direct relations are more common and may introduce extra information. This study focuses on community detection (or clustering) in multi-layer directed networks. To take into account the asymmetry, a novel spectral-co-clustering-based algorithm is developed to detect co-clusters, which capture the sending patterns and receiving patterns of nodes, respectively. Specifically, the eigendecomposition of the debiased sum of Gram matrices over the layer-wise adjacency matrices is computed, followed by the k-means, where the sum of Gram matrices is used to avoid possible cancellation of clusters caused by direct summation. Theoretical analysis of the algorithm under the multi-layer stochastic co-block model is provided, where the common assumption that the cluster number is coupled with the rank of the model is relaxed. After a systematic analysis of the eigenvectors of the population version algorithm, the misclassification rates are derived, which show that multi-layers would bring benefits to the clustering performance. The experimental results of simulated data corroborate the theoretical predictions, and the analysis of a real-world trade network dataset provides interpretable results.</summary></entry><entry><title type="html">Spillover Detection for Donor Selection in Synthetic Control Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SpilloverDetectionforDonorSelectioninSyntheticControlModels.html" rel="alternate" type="text/html" title="Spillover Detection for Donor Selection in Synthetic Control Models" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SpilloverDetectionforDonorSelectioninSyntheticControlModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SpilloverDetectionforDonorSelectioninSyntheticControlModels.html">&lt;p&gt;Synthetic control (SC) models are widely used to estimate causal effects in settings with observational time-series data. To identify the causal effect on a target unit, SC requires the existence of correlated units that are not impacted by the intervention. Given one of these potential donor units, how can we decide whether it is in fact a valid donor - that is, one not subject to spillover effects from the intervention? Such a decision typically requires appealing to strong a priori domain knowledge specifying the units, which becomes infeasible in situations with large pools of potential donors. In this paper, we introduce a practical, theoretically-grounded donor selection procedure, aiming to weaken this domain knowledge requirement. Our main result is a Theorem that yields the assumptions required to identify donor values at post-intervention time points using only pre-intervention data. We show how this Theorem - and the assumptions underpinning it - can be turned into a practical method for detecting potential spillover effects and excluding invalid donors when constructing SCs. Importantly, we employ sensitivity analysis to formally bound the bias in our SC causal estimate in situations where an excluded donor was indeed valid, or where a selected donor was invalid. Using ideas from the proximal causal inference and instrumental variables literature, we show that the excluded donors can nevertheless be leveraged to further debias causal effect estimates. Finally, we illustrate our donor selection procedure on both simulated and real-world datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11399&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael O&apos;Riordan, Ciarán M. Gilligan-Lee</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Synthetic control (SC) models are widely used to estimate causal effects in settings with observational time-series data. To identify the causal effect on a target unit, SC requires the existence of correlated units that are not impacted by the intervention. Given one of these potential donor units, how can we decide whether it is in fact a valid donor - that is, one not subject to spillover effects from the intervention? Such a decision typically requires appealing to strong a priori domain knowledge specifying the units, which becomes infeasible in situations with large pools of potential donors. In this paper, we introduce a practical, theoretically-grounded donor selection procedure, aiming to weaken this domain knowledge requirement. Our main result is a Theorem that yields the assumptions required to identify donor values at post-intervention time points using only pre-intervention data. We show how this Theorem - and the assumptions underpinning it - can be turned into a practical method for detecting potential spillover effects and excluding invalid donors when constructing SCs. Importantly, we employ sensitivity analysis to formally bound the bias in our SC causal estimate in situations where an excluded donor was indeed valid, or where a selected donor was invalid. Using ideas from the proximal causal inference and instrumental variables literature, we show that the excluded donors can nevertheless be leveraged to further debias causal effect estimates. Finally, we illustrate our donor selection procedure on both simulated and real-world datasets.</summary></entry><entry><title type="html">Statistical Considerations for Evaluating Treatment Effect under Various Non-proportional Hazard Scenarios</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/StatisticalConsiderationsforEvaluatingTreatmentEffectunderVariousNonproportionalHazardScenarios.html" rel="alternate" type="text/html" title="Statistical Considerations for Evaluating Treatment Effect under Various Non-proportional Hazard Scenarios" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/StatisticalConsiderationsforEvaluatingTreatmentEffectunderVariousNonproportionalHazardScenarios</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/StatisticalConsiderationsforEvaluatingTreatmentEffectunderVariousNonproportionalHazardScenarios.html">&lt;p&gt;We conducted a systematic comparison of statistical methods used for the analysis of time-to-event outcomes under various proportional and nonproportional hazard (NPH) scenarios. Our study used data from recently published oncology trials to compare the Log-rank test, still by far the most widely used option, against some available alternatives, including the MaxCombo test, the Restricted Mean Survival Time Difference (dRMST) test, the Generalized Gamma Model (GGM) and the Generalized F Model (GFM). Power, type I error rate, and time-dependent bias with respect to the RMST difference, survival probability difference, and median survival time were used to evaluate and compare the performance of these methods. In addition to the real data, we simulated three hypothetical scenarios with crossing hazards chosen so that the early and late effects ‘cancel out’ and used them to evaluate the ability of the aforementioned methods to detect time-specific and overall treatment effects. We implemented novel metrics for assessing the time-dependent bias in treatment effect estimates to provide a more comprehensive evaluation in NPH scenarios. Recommendations under each NPH scenario are provided by examining the type I error rate, power, and time-dependent bias associated with each statistical approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11043&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinyu Zhang, Erich J. Greene, Ondrej Blaha, Wei Wei</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">We conducted a systematic comparison of statistical methods used for the analysis of time-to-event outcomes under various proportional and nonproportional hazard (NPH) scenarios. Our study used data from recently published oncology trials to compare the Log-rank test, still by far the most widely used option, against some available alternatives, including the MaxCombo test, the Restricted Mean Survival Time Difference (dRMST) test, the Generalized Gamma Model (GGM) and the Generalized F Model (GFM). Power, type I error rate, and time-dependent bias with respect to the RMST difference, survival probability difference, and median survival time were used to evaluate and compare the performance of these methods. In addition to the real data, we simulated three hypothetical scenarios with crossing hazards chosen so that the early and late effects ‘cancel out’ and used them to evaluate the ability of the aforementioned methods to detect time-specific and overall treatment effects. We implemented novel metrics for assessing the time-dependent bias in treatment effect estimates to provide a more comprehensive evaluation in NPH scenarios. Recommendations under each NPH scenario are provided by examining the type I error rate, power, and time-dependent bias associated with each statistical approach.</summary></entry><entry><title type="html">Statistical Evolution of ODI Cricket: Analyzing Performance Trends and Effect Sizes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/StatisticalEvolutionofODICricketAnalyzingPerformanceTrendsandEffectSizes.html" rel="alternate" type="text/html" title="Statistical Evolution of ODI Cricket: Analyzing Performance Trends and Effect Sizes" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/StatisticalEvolutionofODICricketAnalyzingPerformanceTrendsandEffectSizes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/StatisticalEvolutionofODICricketAnalyzingPerformanceTrendsandEffectSizes.html">&lt;p&gt;In the dynamic realm of One Day International (ODI) cricket, the sport has undergone significant transformations over the past four decades. This study digs into the intricate evolution of ODI cricket from 1987 to 2023, analyzing about 4000 matches to uncover pivotal performance indicators such as batting prowess, bowling efficiency, and partnership dynamics. Employing statistical methodologies, including Cohen’s effect size, we scrutinize the observed changes that have shaped ODI cricket’s landscape. Our findings reveal nuanced trends: while first innings scores have shown stability with sporadic high outliers in recent years, the impact of achieving scores exceeding 300 has notably increased. Furthermore, batting depth and early wickets lost in the first innings continue to significantly influence match outcomes, highlighting strategic shifts in team approaches. We also observe improvements in second innings bowling effectiveness, particularly in wicket-taking ability, underscoring evolving defensive strategies. This research contributes a statistical foundation to comprehensively understand the evolving dynamics of ODI cricket, offering insights crucial for strategic decision-making and further analysis in sports analytics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11652&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Pratik Mullick</name></author><category term="stat.AP" /><summary type="html">In the dynamic realm of One Day International (ODI) cricket, the sport has undergone significant transformations over the past four decades. This study digs into the intricate evolution of ODI cricket from 1987 to 2023, analyzing about 4000 matches to uncover pivotal performance indicators such as batting prowess, bowling efficiency, and partnership dynamics. Employing statistical methodologies, including Cohen’s effect size, we scrutinize the observed changes that have shaped ODI cricket’s landscape. Our findings reveal nuanced trends: while first innings scores have shown stability with sporadic high outliers in recent years, the impact of achieving scores exceeding 300 has notably increased. Furthermore, batting depth and early wickets lost in the first innings continue to significantly influence match outcomes, highlighting strategic shifts in team approaches. We also observe improvements in second innings bowling effectiveness, particularly in wicket-taking ability, underscoring evolving defensive strategies. This research contributes a statistical foundation to comprehensively understand the evolving dynamics of ODI cricket, offering insights crucial for strategic decision-making and further analysis in sports analytics.</summary></entry><entry><title type="html">Symmetry-driven embedding of networks in hyperbolic space</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Symmetrydrivenembeddingofnetworksinhyperbolicspace.html" rel="alternate" type="text/html" title="Symmetry-driven embedding of networks in hyperbolic space" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Symmetrydrivenembeddingofnetworksinhyperbolicspace</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Symmetrydrivenembeddingofnetworksinhyperbolicspace.html">&lt;p&gt;Hyperbolic models can reproduce the heavy-tailed degree distribution, high clustering, and hierarchical structure of empirical networks. Current algorithms for finding the hyperbolic coordinates of networks, however, do not quantify uncertainty in the inferred coordinates. We present BIGUE, a Markov chain Monte Carlo (MCMC) algorithm that samples the posterior distribution of a Bayesian hyperbolic random graph model. We show that combining random walk and random cluster transformations significantly improves mixing compared to the commonly used and state-of-the-art dynamic Hamiltonian Monte Carlo algorithm. Using this algorithm, we also provide evidence that the posterior distribution cannot be approximated by a multivariate normal distribution, thereby justifying the use of MCMC to quantify the uncertainty of the inferred parameters.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10711&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Simon Lizotte, Jean-Gabriel Young, Antoine Allard</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">Hyperbolic models can reproduce the heavy-tailed degree distribution, high clustering, and hierarchical structure of empirical networks. Current algorithms for finding the hyperbolic coordinates of networks, however, do not quantify uncertainty in the inferred coordinates. We present BIGUE, a Markov chain Monte Carlo (MCMC) algorithm that samples the posterior distribution of a Bayesian hyperbolic random graph model. We show that combining random walk and random cluster transformations significantly improves mixing compared to the commonly used and state-of-the-art dynamic Hamiltonian Monte Carlo algorithm. Using this algorithm, we also provide evidence that the posterior distribution cannot be approximated by a multivariate normal distribution, thereby justifying the use of MCMC to quantify the uncertainty of the inferred parameters.</summary></entry><entry><title type="html">SynthTree: Co-supervised Local Model Synthesis for Explainable Prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SynthTreeCosupervisedLocalModelSynthesisforExplainablePrediction.html" rel="alternate" type="text/html" title="SynthTree: Co-supervised Local Model Synthesis for Explainable Prediction" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SynthTreeCosupervisedLocalModelSynthesisforExplainablePrediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/SynthTreeCosupervisedLocalModelSynthesisforExplainablePrediction.html">&lt;p&gt;Explainable machine learning (XML) has emerged as a major challenge in artificial intelligence (AI). Although black-box models such as Deep Neural Networks and Gradient Boosting often exhibit exceptional predictive accuracy, their lack of interpretability is a notable drawback, particularly in domains requiring transparency and trust. This paper tackles this core AI problem by proposing a novel method to enhance explainability with minimal accuracy loss, using a Mixture of Linear Models (MLM) estimated under the co-supervision of black-box models. We have developed novel methods for estimating MLM by leveraging AI techniques. Specifically, we explore two approaches for partitioning the input space: agglomerative clustering and decision trees. The agglomerative clustering approach provides greater flexibility in model construction, while the decision tree approach further enhances explainability, yielding a decision tree model with linear or logistic regression models at its leaf nodes. Comparative analyses with widely-used and state-of-the-art predictive models demonstrate the effectiveness of our proposed methods. Experimental results show that statistical models can significantly enhance the explainability of AI, thereby broadening their potential for real-world applications. Our findings highlight the critical role that statistical methodologies can play in advancing explainable AI.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10962&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Evgenii Kuriabov, Jia Li</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Explainable machine learning (XML) has emerged as a major challenge in artificial intelligence (AI). Although black-box models such as Deep Neural Networks and Gradient Boosting often exhibit exceptional predictive accuracy, their lack of interpretability is a notable drawback, particularly in domains requiring transparency and trust. This paper tackles this core AI problem by proposing a novel method to enhance explainability with minimal accuracy loss, using a Mixture of Linear Models (MLM) estimated under the co-supervision of black-box models. We have developed novel methods for estimating MLM by leveraging AI techniques. Specifically, we explore two approaches for partitioning the input space: agglomerative clustering and decision trees. The agglomerative clustering approach provides greater flexibility in model construction, while the decision tree approach further enhances explainability, yielding a decision tree model with linear or logistic regression models at its leaf nodes. Comparative analyses with widely-used and state-of-the-art predictive models demonstrate the effectiveness of our proposed methods. Experimental results show that statistical models can significantly enhance the explainability of AI, thereby broadening their potential for real-world applications. Our findings highlight the critical role that statistical methodologies can play in advancing explainable AI.</summary></entry><entry><title type="html">Teleporter Theory: A General and Simple Approach for Modeling Cross-World Counterfactual Causality</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/TeleporterTheoryAGeneralandSimpleApproachforModelingCrossWorldCounterfactualCausality.html" rel="alternate" type="text/html" title="Teleporter Theory: A General and Simple Approach for Modeling Cross-World Counterfactual Causality" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/TeleporterTheoryAGeneralandSimpleApproachforModelingCrossWorldCounterfactualCausality</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/TeleporterTheoryAGeneralandSimpleApproachforModelingCrossWorldCounterfactualCausality.html">&lt;p&gt;Leveraging the development of structural causal model (SCM), researchers can establish graphical models for exploring the causal mechanisms behind machine learning techniques. As the complexity of machine learning applications rises, single-world interventionism causal analysis encounters theoretical adaptation limitations. Accordingly, cross-world counterfactual approach extends our understanding of causality beyond observed data, enabling hypothetical reasoning about alternative scenarios. However, the joint involvement of cross-world variables, encompassing counterfactual variables and real-world variables, challenges the construction of the graphical model. Twin network is a subtle attempt, establishing a symbiotic relationship, to bridge the gap between graphical modeling and the introduction of counterfactuals albeit with room for improvement in generalization. In this regard, we demonstrate the theoretical breakdowns of twin networks in certain cross-world counterfactual scenarios. To this end, we propose a novel teleporter theory to establish a general and simple graphical representation of counterfactuals, which provides criteria for determining teleporter variables to connect multiple worlds. In theoretical application, we determine that introducing the proposed teleporter theory can directly obtain the conditional independence between counterfactual variables and real-world variables from the cross-world SCM without requiring complex algebraic derivations. Accordingly, we can further identify counterfactual causal effects through cross-world symbolic derivation. We demonstrate the generality of the teleporter theory to the practical application. Adhering to the proposed theory, we build a plug-and-play module, and the effectiveness of which are substantiated by experiments on benchmarks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11501&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiangmeng Li, Bin Qin, Qirui Ji, Yi Li, Wenwen Qiang, Jianwen Cao, Fanjiang Xu</name></author><category term="stat.ME" /><summary type="html">Leveraging the development of structural causal model (SCM), researchers can establish graphical models for exploring the causal mechanisms behind machine learning techniques. As the complexity of machine learning applications rises, single-world interventionism causal analysis encounters theoretical adaptation limitations. Accordingly, cross-world counterfactual approach extends our understanding of causality beyond observed data, enabling hypothetical reasoning about alternative scenarios. However, the joint involvement of cross-world variables, encompassing counterfactual variables and real-world variables, challenges the construction of the graphical model. Twin network is a subtle attempt, establishing a symbiotic relationship, to bridge the gap between graphical modeling and the introduction of counterfactuals albeit with room for improvement in generalization. In this regard, we demonstrate the theoretical breakdowns of twin networks in certain cross-world counterfactual scenarios. To this end, we propose a novel teleporter theory to establish a general and simple graphical representation of counterfactuals, which provides criteria for determining teleporter variables to connect multiple worlds. In theoretical application, we determine that introducing the proposed teleporter theory can directly obtain the conditional independence between counterfactual variables and real-world variables from the cross-world SCM without requiring complex algebraic derivations. Accordingly, we can further identify counterfactual causal effects through cross-world symbolic derivation. We demonstrate the generality of the teleporter theory to the practical application. Adhering to the proposed theory, we build a plug-and-play module, and the effectiveness of which are substantiated by experiments on benchmarks.</summary></entry><entry><title type="html">The analysis of paired comparison data in the presence of cyclicality and intransitivity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Theanalysisofpairedcomparisondatainthepresenceofcyclicalityandintransitivity.html" rel="alternate" type="text/html" title="The analysis of paired comparison data in the presence of cyclicality and intransitivity" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Theanalysisofpairedcomparisondatainthepresenceofcyclicalityandintransitivity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Theanalysisofpairedcomparisondatainthepresenceofcyclicalityandintransitivity.html">&lt;p&gt;A principled approach to cyclicality and intransitivity in cardinal paired comparison data is developed within the framework of graphical linear models. Fundamental to our developments is a detailed understanding and study of the parameter space which accommodates cyclicality and intransitivity. In particular, the relationships between the reduced, completely transitive model, the full, not necessarily transitive model, and all manner of intermediate models are explored for both complete and incomplete paired comparison graphs. It is shown that identifying cyclicality and intransitivity reduces to a model selection problem and a new method for model selection employing geometrical insights, unique to the problem at hand, is proposed. The large sample properties of the estimators as well as guarantees on the selected model are provided. It is thus shown that in large samples all cyclicalities and intransitivities can be identified. The method is exemplified using simulations and the analysis of an illustrative example.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11584&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rahul Singh, Ori Davidov</name></author><category term="stat.ME" /><summary type="html">A principled approach to cyclicality and intransitivity in cardinal paired comparison data is developed within the framework of graphical linear models. Fundamental to our developments is a detailed understanding and study of the parameter space which accommodates cyclicality and intransitivity. In particular, the relationships between the reduced, completely transitive model, the full, not necessarily transitive model, and all manner of intermediate models are explored for both complete and incomplete paired comparison graphs. It is shown that identifying cyclicality and intransitivity reduces to a model selection problem and a new method for model selection employing geometrical insights, unique to the problem at hand, is proposed. The large sample properties of the estimators as well as guarantees on the selected model are provided. It is thus shown that in large samples all cyclicalities and intransitivities can be identified. The method is exemplified using simulations and the analysis of an illustrative example.</summary></entry><entry><title type="html">The data augmentation algorithm</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Thedataaugmentationalgorithm.html" rel="alternate" type="text/html" title="The data augmentation algorithm" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Thedataaugmentationalgorithm</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Thedataaugmentationalgorithm.html">&lt;p&gt;The data augmentation (DA) algorithms are popular Markov chain Monte Carlo (MCMC) algorithms often used for sampling from intractable probability distributions. This review article comprehensively surveys DA MCMC algorithms, highlighting their theoretical foundations, methodological implementations, and diverse applications in frequentist and Bayesian statistics. The article discusses tools for studying the convergence properties of DA algorithms. Furthermore, it contains various strategies for accelerating the speed of convergence of the DA algorithms, different extensions of DA algorithms and outlines promising directions for future research. This paper aims to serve as a resource for researchers and practitioners seeking to leverage data augmentation techniques in MCMC algorithms by providing key insights and synthesizing recent developments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10464&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vivekananda Roy, Kshitij Khare, James P. Hobert</name></author><category term="stat.CO," /><category term="stat.ME," /><category term="stat.ML" /><summary type="html">The data augmentation (DA) algorithms are popular Markov chain Monte Carlo (MCMC) algorithms often used for sampling from intractable probability distributions. This review article comprehensively surveys DA MCMC algorithms, highlighting their theoretical foundations, methodological implementations, and diverse applications in frequentist and Bayesian statistics. The article discusses tools for studying the convergence properties of DA algorithms. Furthermore, it contains various strategies for accelerating the speed of convergence of the DA algorithms, different extensions of DA algorithms and outlines promising directions for future research. This paper aims to serve as a resource for researchers and practitioners seeking to leverage data augmentation techniques in MCMC algorithms by providing key insights and synthesizing recent developments.</summary></entry><entry><title type="html">Treatment Effects in Market Equilibrium</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/TreatmentEffectsinMarketEquilibrium.html" rel="alternate" type="text/html" title="Treatment Effects in Market Equilibrium" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/TreatmentEffectsinMarketEquilibrium</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/TreatmentEffectsinMarketEquilibrium.html">&lt;p&gt;Policy-relevant treatment effect estimation in a marketplace setting requires taking into account both the direct benefit of the treatment and any spillovers induced by changes to the market equilibrium. The standard way to address these challenges is to evaluate interventions via cluster-randomized experiments, where each cluster corresponds to an isolated market. This approach, however, cannot be used when we only have access to a single market (or a small number of markets). Here, we show how to identify and estimate policy-relevant treatment effects using a unit-level randomized trial run within a single large market. A standard Bernoulli-randomized trial allows consistent estimation of direct effects, and of treatment heterogeneity measures that can be used for welfare-improving targeting. Estimating spillovers - as well as providing confidence intervals for the direct effect - requires estimates of price elasticities, which we provide using an augmented experimental design. Our results rely on all spillovers being mediated via the (observed) prices of a finite number of traded goods, and the market power of any single unit decaying as the market gets large. We illustrate our results using a simulation calibrated to a conditional cash transfer experiment in the Philippines.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2109.11647&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Evan Munro, Xu Kuang, Stefan Wager</name></author><category term="stat.ME" /><summary type="html">Policy-relevant treatment effect estimation in a marketplace setting requires taking into account both the direct benefit of the treatment and any spillovers induced by changes to the market equilibrium. The standard way to address these challenges is to evaluate interventions via cluster-randomized experiments, where each cluster corresponds to an isolated market. This approach, however, cannot be used when we only have access to a single market (or a small number of markets). Here, we show how to identify and estimate policy-relevant treatment effects using a unit-level randomized trial run within a single large market. A standard Bernoulli-randomized trial allows consistent estimation of direct effects, and of treatment heterogeneity measures that can be used for welfare-improving targeting. Estimating spillovers - as well as providing confidence intervals for the direct effect - requires estimates of price elasticities, which we provide using an augmented experimental design. Our results rely on all spillovers being mediated via the (observed) prices of a finite number of traded goods, and the market power of any single unit decaying as the market gets large. We illustrate our results using a simulation calibrated to a conditional cash transfer experiment in the Philippines.</summary></entry><entry><title type="html">When Graph Neural Network Meets Causality: Opportunities, Methodologies and An Outlook</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/WhenGraphNeuralNetworkMeetsCausalityOpportunitiesMethodologiesandAnOutlook.html" rel="alternate" type="text/html" title="When Graph Neural Network Meets Causality: Opportunities, Methodologies and An Outlook" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/WhenGraphNeuralNetworkMeetsCausalityOpportunitiesMethodologiesandAnOutlook</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/WhenGraphNeuralNetworkMeetsCausalityOpportunitiesMethodologiesandAnOutlook.html">&lt;p&gt;Graph Neural Networks (GNNs) have emerged as powerful representation learning tools for capturing complex dependencies within diverse graph-structured data. Despite their success in a wide range of graph mining tasks, GNNs have raised serious concerns regarding their trustworthiness, including susceptibility to distribution shift, biases towards certain populations, and lack of explainability. Recently, integrating causal learning techniques into GNNs has sparked numerous ground-breaking studies since many GNN trustworthiness issues can be alleviated by capturing the underlying data causality rather than superficial correlations. In this survey, we comprehensively review recent research efforts on Causality-Inspired GNNs (CIGNNs). Specifically, we first employ causal tools to analyze the primary trustworthiness risks of existing GNNs, underscoring the necessity for GNNs to comprehend the causal mechanisms within graph data. Moreover, we introduce a taxonomy of CIGNNs based on the type of causal learning capability they are equipped with, i.e., causal reasoning and causal representation learning. Besides, we systematically introduce typical methods within each category and discuss how they mitigate trustworthiness risks. Finally, we summarize useful resources and discuss several future directions, hoping to shed light on new research opportunities in this emerging field. The representative papers, along with open-source data and codes, are available in https://github.com/usail-hkust/Causality-Inspired-GNNs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.12477&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wenzhao Jiang, Hao Liu, Hui Xiong</name></author><category term="stat.ME" /><summary type="html">Graph Neural Networks (GNNs) have emerged as powerful representation learning tools for capturing complex dependencies within diverse graph-structured data. Despite their success in a wide range of graph mining tasks, GNNs have raised serious concerns regarding their trustworthiness, including susceptibility to distribution shift, biases towards certain populations, and lack of explainability. Recently, integrating causal learning techniques into GNNs has sparked numerous ground-breaking studies since many GNN trustworthiness issues can be alleviated by capturing the underlying data causality rather than superficial correlations. In this survey, we comprehensively review recent research efforts on Causality-Inspired GNNs (CIGNNs). Specifically, we first employ causal tools to analyze the primary trustworthiness risks of existing GNNs, underscoring the necessity for GNNs to comprehend the causal mechanisms within graph data. Moreover, we introduce a taxonomy of CIGNNs based on the type of causal learning capability they are equipped with, i.e., causal reasoning and causal representation learning. Besides, we systematically introduce typical methods within each category and discuss how they mitigate trustworthiness risks. Finally, we summarize useful resources and discuss several future directions, hoping to shed light on new research opportunities in this emerging field. The representative papers, along with open-source data and codes, are available in https://github.com/usail-hkust/Causality-Inspired-GNNs.</summary></entry><entry><title type="html">When Pearson $\chi^2$ and other divisible statistics are not goodness-of-fit tests</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/WhenPearsonchi2andotherdivisiblestatisticsarenotgoodnessoffittests.html" rel="alternate" type="text/html" title="When Pearson $\chi^2$ and other divisible statistics are not goodness-of-fit tests" /><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/WhenPearsonchi2andotherdivisiblestatisticsarenotgoodnessoffittests</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/WhenPearsonchi2andotherdivisiblestatisticsarenotgoodnessoffittests.html">&lt;p&gt;Thousands of experiments are analyzed and papers are published each year involving the statistical analysis of grouped data. While this area of statistics is often perceived - somewhat naively - as saturated, several misconceptions still affect everyday practice, and new frontiers have so far remained unexplored. Researchers must be aware of the limitations affecting their analyses and what are the new possibilities in their hands.
  Motivated by this need, the article introduces a unifying approach to the analysis of grouped data which allows us to study the class of divisible statistics - that includes Pearson’s $\chi^2$, the likelihood ratio as special cases - with a fresh perspective. The contributions collected in this manuscript span from modeling and estimation to distribution-free goodness-of-fit tests.
  Perhaps the most surprising result presented here is that, in a sparse regime, all tests proposed in the literature are dominated by a class of weighted linear statistics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09195&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sara Algeri, Estate V. Khmaladze</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Thousands of experiments are analyzed and papers are published each year involving the statistical analysis of grouped data. While this area of statistics is often perceived - somewhat naively - as saturated, several misconceptions still affect everyday practice, and new frontiers have so far remained unexplored. Researchers must be aware of the limitations affecting their analyses and what are the new possibilities in their hands. Motivated by this need, the article introduces a unifying approach to the analysis of grouped data which allows us to study the class of divisible statistics - that includes Pearson’s $\chi^2$, the likelihood ratio as special cases - with a fresh perspective. The contributions collected in this manuscript span from modeling and estimation to distribution-free goodness-of-fit tests. Perhaps the most surprising result presented here is that, in a sparse regime, all tests proposed in the literature are dominated by a class of weighted linear statistics.</summary></entry></feed>