<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-24T07:13:22+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Bayesian Approach to Estimate Causal Peer Influence Accounting for Latent Network Homophily</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ABayesianApproachtoEstimateCausalPeerInfluenceAccountingforLatentNetworkHomophily.html" rel="alternate" type="text/html" title="A Bayesian Approach to Estimate Causal Peer Influence Accounting for Latent Network Homophily" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ABayesianApproachtoEstimateCausalPeerInfluenceAccountingforLatentNetworkHomophily</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ABayesianApproachtoEstimateCausalPeerInfluenceAccountingforLatentNetworkHomophily.html">&lt;p&gt;Researchers have focused on understanding how individual’s behavior is influenced by the behaviors of their peers in observational studies of social networks. Identifying and estimating causal peer influence, however, is challenging due to confounding by homophily, where people tend to connect with those who share similar characteristics with them. Moreover, since all the attributes driving homophily are generally not always observed and act as unobserved confounders, identifying and estimating causal peer influence becomes infeasible using standard causal identification assumptions. In this paper, we address this challenge by leveraging latent locations inferred from the network itself to disentangle homophily from causal peer influence, and we extend this approach to multiple networks by adopting a Bayesian hierarchical modeling framework. To accommodate the nonlinear dependency of peer influence on individual behavior, we employ a Bayesian nonparametric method, specifically Bayesian Additive Regression Trees (BART), and we propose a Bayesian framework that accounts for the uncertainty in inferring latent locations. We assess the operating characteristics of the estimator via extensive simulation study. Finally, we apply our method to estimate causal peer influence in advice-seeking networks of teachers in secondary schools, in order to assess whether the teachers’ belief about mathematics education is influenced by the beliefs of their peers from whom they receive advice. Our results suggest that, overlooking latent homophily can lead to either underestimation or overestimation of causal peer influence, accompanied by considerable estimation uncertainty.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14789&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Seungha Um, Tracy Sweet, Samrachana Adhikari</name></author><category term="stat.AP" /><summary type="html">Researchers have focused on understanding how individual’s behavior is influenced by the behaviors of their peers in observational studies of social networks. Identifying and estimating causal peer influence, however, is challenging due to confounding by homophily, where people tend to connect with those who share similar characteristics with them. Moreover, since all the attributes driving homophily are generally not always observed and act as unobserved confounders, identifying and estimating causal peer influence becomes infeasible using standard causal identification assumptions. In this paper, we address this challenge by leveraging latent locations inferred from the network itself to disentangle homophily from causal peer influence, and we extend this approach to multiple networks by adopting a Bayesian hierarchical modeling framework. To accommodate the nonlinear dependency of peer influence on individual behavior, we employ a Bayesian nonparametric method, specifically Bayesian Additive Regression Trees (BART), and we propose a Bayesian framework that accounts for the uncertainty in inferring latent locations. We assess the operating characteristics of the estimator via extensive simulation study. Finally, we apply our method to estimate causal peer influence in advice-seeking networks of teachers in secondary schools, in order to assess whether the teachers’ belief about mathematics education is influenced by the beliefs of their peers from whom they receive advice. Our results suggest that, overlooking latent homophily can lead to either underestimation or overestimation of causal peer influence, accompanied by considerable estimation uncertainty.</summary></entry><entry><title type="html">A Bayesian Convolutional Neural Network-based Generalized Linear Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ABayesianConvolutionalNeuralNetworkbasedGeneralizedLinearModel.html" rel="alternate" type="text/html" title="A Bayesian Convolutional Neural Network-based Generalized Linear Model" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ABayesianConvolutionalNeuralNetworkbasedGeneralizedLinearModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ABayesianConvolutionalNeuralNetworkbasedGeneralizedLinearModel.html">&lt;p&gt;Convolutional neural networks (CNNs) provide flexible function approximations for a wide variety of applications when the input variables are in the form of images or spatial data. Although CNNs often outperform traditional statistical models in prediction accuracy, statistical inference, such as estimating the effects of covariates and quantifying the prediction uncertainty, is not trivial due to the highly complicated model structure and overparameterization. To address this challenge, we propose a new Bayesian approach by embedding CNNs within the generalized linear models (GLMs) framework. We use extracted nodes from the last hidden layer of CNN with Monte Carlo (MC) dropout as informative covariates in GLM. This improves accuracy in prediction and regression coefficient inference, allowing for the interpretation of coefficients and uncertainty quantification. By fitting ensemble GLMs across multiple realizations from MC dropout, we can account for uncertainties in extracting the features. We apply our methods to biological and epidemiological problems, which have both high-dimensional correlated inputs and vector covariates. Specifically, we consider malaria incidence data, brain tumor image data, and fMRI data. By extracting information from correlated inputs, the proposed method can provide an interpretable Bayesian analysis. The algorithm can be broadly applicable to image regressions or correlated data analysis by enabling accurate Bayesian inference quickly.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.09560&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yeseul Jeon, Won Chang, Seonghyun Jeong, Sanghoon Han, Jaewoo Park</name></author><category term="stat.ME" /><summary type="html">Convolutional neural networks (CNNs) provide flexible function approximations for a wide variety of applications when the input variables are in the form of images or spatial data. Although CNNs often outperform traditional statistical models in prediction accuracy, statistical inference, such as estimating the effects of covariates and quantifying the prediction uncertainty, is not trivial due to the highly complicated model structure and overparameterization. To address this challenge, we propose a new Bayesian approach by embedding CNNs within the generalized linear models (GLMs) framework. We use extracted nodes from the last hidden layer of CNN with Monte Carlo (MC) dropout as informative covariates in GLM. This improves accuracy in prediction and regression coefficient inference, allowing for the interpretation of coefficients and uncertainty quantification. By fitting ensemble GLMs across multiple realizations from MC dropout, we can account for uncertainties in extracting the features. We apply our methods to biological and epidemiological problems, which have both high-dimensional correlated inputs and vector covariates. Specifically, we consider malaria incidence data, brain tumor image data, and fMRI data. By extracting information from correlated inputs, the proposed method can provide an interpretable Bayesian analysis. The algorithm can be broadly applicable to image regressions or correlated data analysis by enabling accurate Bayesian inference quickly.</summary></entry><entry><title type="html">A Deep Learning Approach to Multi-Fiber Parameter Estimation and Uncertainty Quantification in Diffusion MRI</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ADeepLearningApproachtoMultiFiberParameterEstimationandUncertaintyQuantificationinDiffusionMRI.html" rel="alternate" type="text/html" title="A Deep Learning Approach to Multi-Fiber Parameter Estimation and Uncertainty Quantification in Diffusion MRI" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ADeepLearningApproachtoMultiFiberParameterEstimationandUncertaintyQuantificationinDiffusionMRI</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ADeepLearningApproachtoMultiFiberParameterEstimationandUncertaintyQuantificationinDiffusionMRI.html">&lt;p&gt;Diffusion MRI (dMRI) is the primary imaging modality used to study brain microstructure in vivo. Reliable and computationally efficient parameter inference for common dMRI biophysical models is a challenging inverse problem, due to factors such as variable dimensionalities (reflecting the unknown number of distinct white matter fiber populations in a voxel), low signal-to-noise ratios, and non-linear forward models. These challenges have led many existing methods to use biologically implausible simplified models to stabilize estimation, for instance, assuming shared microstructure across all fiber populations within a voxel. In this work, we introduce a novel sequential method for multi-fiber parameter inference that decomposes the task into a series of manageable subproblems. These subproblems are solved using deep neural networks tailored to problem-specific structure and symmetry, and trained via simulation. The resulting inference procedure is largely amortized, enabling scalable parameter estimation and uncertainty quantification across all model parameters. Simulation studies and real imaging data analysis using the Human Connectome Project (HCP) demonstrate the advantages of our method over standard alternatives. In the case of the standard model of diffusion, our results show that under HCP-like acquisition schemes, estimates for extra-cellular parallel diffusivity are highly uncertain, while those for the intra-cellular volume fraction can be estimated with relatively high precision.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13655&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>William Consagra, Lipeng Ning, Yogesh Rathi</name></author><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Diffusion MRI (dMRI) is the primary imaging modality used to study brain microstructure in vivo. Reliable and computationally efficient parameter inference for common dMRI biophysical models is a challenging inverse problem, due to factors such as variable dimensionalities (reflecting the unknown number of distinct white matter fiber populations in a voxel), low signal-to-noise ratios, and non-linear forward models. These challenges have led many existing methods to use biologically implausible simplified models to stabilize estimation, for instance, assuming shared microstructure across all fiber populations within a voxel. In this work, we introduce a novel sequential method for multi-fiber parameter inference that decomposes the task into a series of manageable subproblems. These subproblems are solved using deep neural networks tailored to problem-specific structure and symmetry, and trained via simulation. The resulting inference procedure is largely amortized, enabling scalable parameter estimation and uncertainty quantification across all model parameters. Simulation studies and real imaging data analysis using the Human Connectome Project (HCP) demonstrate the advantages of our method over standard alternatives. In the case of the standard model of diffusion, our results show that under HCP-like acquisition schemes, estimates for extra-cellular parallel diffusivity are highly uncertain, while those for the intra-cellular volume fraction can be estimated with relatively high precision.</summary></entry><entry><title type="html">A Direct Importance Sampling-based Framework for Rare Event Uncertainty Quantification in Non-Gaussian Spaces</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ADirectImportanceSamplingbasedFrameworkforRareEventUncertaintyQuantificationinNonGaussianSpaces.html" rel="alternate" type="text/html" title="A Direct Importance Sampling-based Framework for Rare Event Uncertainty Quantification in Non-Gaussian Spaces" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ADirectImportanceSamplingbasedFrameworkforRareEventUncertaintyQuantificationinNonGaussianSpaces</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ADirectImportanceSamplingbasedFrameworkforRareEventUncertaintyQuantificationinNonGaussianSpaces.html">&lt;p&gt;This work introduces a novel framework for precisely and efficiently estimating rare event probabilities in complex, high-dimensional non-Gaussian spaces, building on our foundational Approximate Sampling Target with Post-processing Adjustment (ASTPA) approach. An unnormalized sampling target is first constructed and sampled, relaxing the optimal importance sampling distribution and appropriately designed for non-Gaussian spaces. Post-sampling, its normalizing constant is estimated using a stable inverse importance sampling procedure, employing an importance sampling density based on the already available samples. The sought probability is then computed based on the estimates evaluated in these two stages. The proposed estimator is theoretically analyzed, proving its unbiasedness and deriving its analytical coefficient of variation. To sample the constructed target, we resort to our developed Quasi-Newton mass preconditioned Hamiltonian MCMC (QNp-HMCMC) and we prove that it converges to the correct stationary target distribution. To avoid the challenging task of tuning the trajectory length in complex spaces, QNp-HMCMC is effectively utilized in this work with a single-step integration. We thus show the equivalence of QNp-HMCMC with single-step implementation to a unique and efficient preconditioned Metropolis-adjusted Langevin algorithm (MALA). An optimization approach is also leveraged to initiate QNp-HMCMC effectively, and the implementation of the developed framework in bounded spaces is eventually discussed. A series of diverse problems involving high dimensionality (several hundred inputs), strong nonlinearity, and non-Gaussianity is presented, showcasing the capabilities and efficiency of the suggested framework and demonstrating its advantages compared to relevant state-of-the-art sampling methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14149&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elsayed Eshra, Konstantinos G. Papakonstantinou, Hamed Nikbakht</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">This work introduces a novel framework for precisely and efficiently estimating rare event probabilities in complex, high-dimensional non-Gaussian spaces, building on our foundational Approximate Sampling Target with Post-processing Adjustment (ASTPA) approach. An unnormalized sampling target is first constructed and sampled, relaxing the optimal importance sampling distribution and appropriately designed for non-Gaussian spaces. Post-sampling, its normalizing constant is estimated using a stable inverse importance sampling procedure, employing an importance sampling density based on the already available samples. The sought probability is then computed based on the estimates evaluated in these two stages. The proposed estimator is theoretically analyzed, proving its unbiasedness and deriving its analytical coefficient of variation. To sample the constructed target, we resort to our developed Quasi-Newton mass preconditioned Hamiltonian MCMC (QNp-HMCMC) and we prove that it converges to the correct stationary target distribution. To avoid the challenging task of tuning the trajectory length in complex spaces, QNp-HMCMC is effectively utilized in this work with a single-step integration. We thus show the equivalence of QNp-HMCMC with single-step implementation to a unique and efficient preconditioned Metropolis-adjusted Langevin algorithm (MALA). An optimization approach is also leveraged to initiate QNp-HMCMC effectively, and the implementation of the developed framework in bounded spaces is eventually discussed. A series of diverse problems involving high dimensionality (several hundred inputs), strong nonlinearity, and non-Gaussianity is presented, showcasing the capabilities and efficiency of the suggested framework and demonstrating its advantages compared to relevant state-of-the-art sampling methods.</summary></entry><entry><title type="html">A Physics-Informed, Deep Double Reservoir Network for Forecasting Boundary Layer Velocity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/APhysicsInformedDeepDoubleReservoirNetworkforForecastingBoundaryLayerVelocity.html" rel="alternate" type="text/html" title="A Physics-Informed, Deep Double Reservoir Network for Forecasting Boundary Layer Velocity" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/APhysicsInformedDeepDoubleReservoirNetworkforForecastingBoundaryLayerVelocity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/APhysicsInformedDeepDoubleReservoirNetworkforForecastingBoundaryLayerVelocity.html">&lt;p&gt;When a fluid flows over a solid surface, it creates a thin boundary layer where the flow velocity is influenced by the surface through viscosity, and can transition from laminar to turbulent at sufficiently high speeds. Understanding and forecasting the fluid dynamics under these conditions is one of the most challenging scientific problems in fluid dynamics. It is therefore of high interest to formulate models able to capture the nonlinear spatio-temporal velocity structure as well as produce forecasts in a computationally efficient manner. Traditional statistical approaches are limited in their ability to produce timely forecasts of complex, nonlinear spatio-temporal structures which are at the same time able to incorporate the underlying flow physics. In this work, we propose a model to accurately forecast boundary layer velocities with a deep double reservoir computing network which is capable of capturing the complex, nonlinear dynamics of the boundary layer while at the same time incorporating physical constraints via a penalty obtained by a Partial Differential Equation (PDE). Simulation studies on a one-dimensional viscous fluid demonstrate how the proposed model is able to produce accurate forecasts while simultaneously accounting for energy loss. The application focuses on boundary layer data on a water tunnel with a PDE penalty derived from an appropriate simplification of the Navier-Stokes equations, showing forecasts improved by 33.7% and 80.0% in terms of mass conservation and variability of velocity fluctuation, respectfully, against non physics-informed methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.05728&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matthew Bonas, David H. Richter, Stefano Castruccio</name></author><category term="stat.AP" /><summary type="html">When a fluid flows over a solid surface, it creates a thin boundary layer where the flow velocity is influenced by the surface through viscosity, and can transition from laminar to turbulent at sufficiently high speeds. Understanding and forecasting the fluid dynamics under these conditions is one of the most challenging scientific problems in fluid dynamics. It is therefore of high interest to formulate models able to capture the nonlinear spatio-temporal velocity structure as well as produce forecasts in a computationally efficient manner. Traditional statistical approaches are limited in their ability to produce timely forecasts of complex, nonlinear spatio-temporal structures which are at the same time able to incorporate the underlying flow physics. In this work, we propose a model to accurately forecast boundary layer velocities with a deep double reservoir computing network which is capable of capturing the complex, nonlinear dynamics of the boundary layer while at the same time incorporating physical constraints via a penalty obtained by a Partial Differential Equation (PDE). Simulation studies on a one-dimensional viscous fluid demonstrate how the proposed model is able to produce accurate forecasts while simultaneously accounting for energy loss. The application focuses on boundary layer data on a water tunnel with a PDE penalty derived from an appropriate simplification of the Navier-Stokes equations, showing forecasts improved by 33.7% and 80.0% in terms of mass conservation and variability of velocity fluctuation, respectfully, against non physics-informed methods.</summary></entry><entry><title type="html">Adaptive Bayesian Multivariate Spline Knot Inference with Prior Specifications on Model Complexity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/AdaptiveBayesianMultivariateSplineKnotInferencewithPriorSpecificationsonModelComplexity.html" rel="alternate" type="text/html" title="Adaptive Bayesian Multivariate Spline Knot Inference with Prior Specifications on Model Complexity" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/AdaptiveBayesianMultivariateSplineKnotInferencewithPriorSpecificationsonModelComplexity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/AdaptiveBayesianMultivariateSplineKnotInferencewithPriorSpecificationsonModelComplexity.html">&lt;p&gt;In multivariate spline regression, the number and locations of knots influence the performance and interpretability significantly. However, due to non-differentiability and varying dimensions, there is no desirable frequentist method to make inference on knots. In this article, we propose a fully Bayesian approach for knot inference in multivariate spline regression. The existing Bayesian method often uses BIC to calculate the posterior, but BIC is too liberal and it will heavily overestimate the knot number when the candidate model space is large. We specify a new prior on the knot number to take into account the complexity of the model space and derive an analytic formula in the normal model. In the non-normal cases, we utilize the extended Bayesian information criterion to approximate the posterior density. The samples are simulated in the space with differing dimensions via reversible jump Markov chain Monte Carlo. We apply the proposed method in knot inference and manifold denoising. Experiments demonstrate the splendid capability of the algorithm, especially in function fitting with jumping discontinuity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13353&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Junhui He, Ying Yang, Jian Kang</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">In multivariate spline regression, the number and locations of knots influence the performance and interpretability significantly. However, due to non-differentiability and varying dimensions, there is no desirable frequentist method to make inference on knots. In this article, we propose a fully Bayesian approach for knot inference in multivariate spline regression. The existing Bayesian method often uses BIC to calculate the posterior, but BIC is too liberal and it will heavily overestimate the knot number when the candidate model space is large. We specify a new prior on the knot number to take into account the complexity of the model space and derive an analytic formula in the normal model. In the non-normal cases, we utilize the extended Bayesian information criterion to approximate the posterior density. The samples are simulated in the space with differing dimensions via reversible jump Markov chain Monte Carlo. We apply the proposed method in knot inference and manifold denoising. Experiments demonstrate the splendid capability of the algorithm, especially in function fitting with jumping discontinuity.</summary></entry><entry><title type="html">Adaptive tempering schedules with approximative intermediate measures for filtering problems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Adaptivetemperingscheduleswithapproximativeintermediatemeasuresforfilteringproblems.html" rel="alternate" type="text/html" title="Adaptive tempering schedules with approximative intermediate measures for filtering problems" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Adaptivetemperingscheduleswithapproximativeintermediatemeasuresforfilteringproblems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Adaptivetemperingscheduleswithapproximativeintermediatemeasuresforfilteringproblems.html">&lt;p&gt;Data assimilation algorithms integrate prior information from numerical model simulations with observed data. Ensemble-based filters, regarded as state-of-the-art, are widely employed for large-scale estimation tasks in disciplines such as geoscience and meteorology. Despite their inability to produce the true posterior distribution for nonlinear systems, their robustness and capacity for state tracking are noteworthy. In contrast, Particle filters yield the correct distribution in the ensemble limit but require substantially larger ensemble sizes than ensemble-based filters to maintain stability in higher-dimensional spaces. It is essential to transcend traditional Gaussian assumptions to achieve realistic quantification of uncertainties. One approach involves the hybridisation of filters, facilitated by tempering, to harness the complementary strengths of different filters. A new adaptive tempering method is proposed to tune the underlying schedule, aiming to systematically surpass the performance previously achieved. Although promising numerical results for certain filter combinations in toy examples exist in the literature, the tuning of hyperparameters presents a considerable challenge. A deeper understanding of these interactions is crucial for practical applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14408&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Iris Rammelmüller, Gottfried Hastermann, Jana de Wiljes</name></author><category term="stat.CO" /><summary type="html">Data assimilation algorithms integrate prior information from numerical model simulations with observed data. Ensemble-based filters, regarded as state-of-the-art, are widely employed for large-scale estimation tasks in disciplines such as geoscience and meteorology. Despite their inability to produce the true posterior distribution for nonlinear systems, their robustness and capacity for state tracking are noteworthy. In contrast, Particle filters yield the correct distribution in the ensemble limit but require substantially larger ensemble sizes than ensemble-based filters to maintain stability in higher-dimensional spaces. It is essential to transcend traditional Gaussian assumptions to achieve realistic quantification of uncertainties. One approach involves the hybridisation of filters, facilitated by tempering, to harness the complementary strengths of different filters. A new adaptive tempering method is proposed to tune the underlying schedule, aiming to systematically surpass the performance previously achieved. Although promising numerical results for certain filter combinations in toy examples exist in the literature, the tuning of hyperparameters presents a considerable challenge. A deeper understanding of these interactions is crucial for practical applications.</summary></entry><entry><title type="html">A marginal structural model for normal tissue complication probability</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Amarginalstructuralmodelfornormaltissuecomplicationprobability.html" rel="alternate" type="text/html" title="A marginal structural model for normal tissue complication probability" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Amarginalstructuralmodelfornormaltissuecomplicationprobability</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Amarginalstructuralmodelfornormaltissuecomplicationprobability.html">&lt;p&gt;The goal of radiation therapy for cancer is to deliver prescribed radiation dose to the tumor while minimizing dose to the surrounding healthy tissues. To evaluate treatment plans, the dose distribution to healthy organs is commonly summarized as dose-volume histograms (DVHs). Normal tissue complication probability (NTCP) modelling has centered around making patient-level risk predictions with features extracted from the DVHs, but few have considered adapting a causal framework to evaluate the safety of alternative treatment plans. We propose causal estimands for NTCP based on deterministic and stochastic interventions, as well as propose estimators based on marginal structural models that impose bivariable monotonicity between dose, volume, and toxicity risk. The properties of these estimators are studied through simulations, and their use is illustrated in the context of radiotherapy treatment of anal canal cancer patients.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.05659&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thai-Son Tang, Zhihui Liu, Ali Hosni, John Kim, Olli Saarela</name></author><category term="stat.ME" /><summary type="html">The goal of radiation therapy for cancer is to deliver prescribed radiation dose to the tumor while minimizing dose to the surrounding healthy tissues. To evaluate treatment plans, the dose distribution to healthy organs is commonly summarized as dose-volume histograms (DVHs). Normal tissue complication probability (NTCP) modelling has centered around making patient-level risk predictions with features extracted from the DVHs, but few have considered adapting a causal framework to evaluate the safety of alternative treatment plans. We propose causal estimands for NTCP based on deterministic and stochastic interventions, as well as propose estimators based on marginal structural models that impose bivariable monotonicity between dose, volume, and toxicity risk. The properties of these estimators are studied through simulations, and their use is illustrated in the context of radiotherapy treatment of anal canal cancer patients.</summary></entry><entry><title type="html">An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/AnEmbeddedDiachronicSenseChangeModelwithaCaseStudyfromAncientGreek.html" rel="alternate" type="text/html" title="An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/AnEmbeddedDiachronicSenseChangeModelwithaCaseStudyfromAncientGreek</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/AnEmbeddedDiachronicSenseChangeModelwithaCaseStudyfromAncientGreek.html">&lt;p&gt;Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small and sparse, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC (Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as ``kosmos’’ (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal changes in these representations. This paper introduces EDiSC, an Embedded DiSC model, which combines word embeddings with DiSC to provide superior model performance. It is shown empirically that EDiSC offers improved predictive accuracy, ground-truth recovery and uncertainty quantification, as well as better sampling efficiency and scalability properties with MCMC methods. The challenges of fitting these models are also discussed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.00541&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Schyan Zafar, Geoff K. Nicholls</name></author><category term="stat.ME" /><summary type="html">Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small and sparse, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC (Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as ``kosmos’’ (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal changes in these representations. This paper introduces EDiSC, an Embedded DiSC model, which combines word embeddings with DiSC to provide superior model performance. It is shown empirically that EDiSC offers improved predictive accuracy, ground-truth recovery and uncertainty quantification, as well as better sampling efficiency and scalability properties with MCMC methods. The challenges of fitting these models are also discussed.</summary></entry><entry><title type="html">An Empirical Comparison of Methods to Produce Business Statistics Using Non-Probability Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/AnEmpiricalComparisonofMethodstoProduceBusinessStatisticsUsingNonProbabilityData.html" rel="alternate" type="text/html" title="An Empirical Comparison of Methods to Produce Business Statistics Using Non-Probability Data" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/AnEmpiricalComparisonofMethodstoProduceBusinessStatisticsUsingNonProbabilityData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/AnEmpiricalComparisonofMethodstoProduceBusinessStatisticsUsingNonProbabilityData.html">&lt;p&gt;There is a growing trend among statistical agencies to explore non-probability data sources for producing more timely and detailed statistics, while reducing costs and respondent burden. Coverage and measurement error are two issues that may be present in such data. The imperfections may be corrected using available information relating to the population of interest, such as a census or a reference probability sample.
  In this paper, we compare a wide range of existing methods for producing population estimates using a non-probability dataset through a simulation study based on a realistic business population. The study was conducted to examine the performance of the methods under different missingness and data quality assumptions. The results confirm the ability of the methods examined to address selection bias. When no measurement error is present in the non-probability dataset, a screening dual-frame approach for the probability sample tends to yield lower sample size and mean squared error results. The presence of measurement error and/or nonignorable missingness increases mean squared errors for estimators that depend heavily on the non-probability data. In this case, the best approach tends to be to fall back to a model-assisted estimator based on the probability sample.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14208&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lyndon Ang, Robert Clark, Bronwyn Loong, Anders Holmberg</name></author><category term="stat.ME" /><summary type="html">There is a growing trend among statistical agencies to explore non-probability data sources for producing more timely and detailed statistics, while reducing costs and respondent burden. Coverage and measurement error are two issues that may be present in such data. The imperfections may be corrected using available information relating to the population of interest, such as a census or a reference probability sample. In this paper, we compare a wide range of existing methods for producing population estimates using a non-probability dataset through a simulation study based on a realistic business population. The study was conducted to examine the performance of the methods under different missingness and data quality assumptions. The results confirm the ability of the methods examined to address selection bias. When no measurement error is present in the non-probability dataset, a screening dual-frame approach for the probability sample tends to yield lower sample size and mean squared error results. The presence of measurement error and/or nonignorable missingness increases mean squared errors for estimators that depend heavily on the non-probability data. In this case, the best approach tends to be to fall back to a model-assisted estimator based on the probability sample.</summary></entry><entry><title type="html">An assessment of racial disparities in pretrial decision-making using misclassification models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Anassessmentofracialdisparitiesinpretrialdecisionmakingusingmisclassificationmodels.html" rel="alternate" type="text/html" title="An assessment of racial disparities in pretrial decision-making using misclassification models" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Anassessmentofracialdisparitiesinpretrialdecisionmakingusingmisclassificationmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Anassessmentofracialdisparitiesinpretrialdecisionmakingusingmisclassificationmodels.html">&lt;p&gt;Pretrial risk assessment tools are used in jurisdictions across the country to assess the likelihood of “pretrial failure,” the event where defendants either fail to appear for court or reoffend. Judicial officers, in turn, use these assessments to determine whether to release or detain defendants during trial. While algorithmic risk assessment tools were designed to predict pretrial failure with greater accuracy relative to judges, there is still concern that both risk assessment recommendations and pretrial decisions are biased against minority groups. In this paper, we develop methods to investigate the association between risk factors and pretrial failure, while simultaneously estimating misclassification rates of pretrial risk assessments and of judicial decisions as a function of defendant race. This approach adds to a growing literature that makes use of outcome misclassification methods to answer questions about fairness in pretrial decision-making. We give a detailed simulation study for our proposed methodology and apply these methods to data from the Virginia Department of Criminal Justice Services. We estimate that the VPRAI algorithm has near-perfect specificity, but its sensitivity differs by defendant race. Judicial decisions also display evidence of bias; we estimate wrongful detention rates of 39.7% and 51.4% among white and Black defendants, respectively.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.08599&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kimberly A. Hochstedler Webb, Sarah A. Riley, Martin T. Wells</name></author><category term="stat.AP" /><summary type="html">Pretrial risk assessment tools are used in jurisdictions across the country to assess the likelihood of “pretrial failure,” the event where defendants either fail to appear for court or reoffend. Judicial officers, in turn, use these assessments to determine whether to release or detain defendants during trial. While algorithmic risk assessment tools were designed to predict pretrial failure with greater accuracy relative to judges, there is still concern that both risk assessment recommendations and pretrial decisions are biased against minority groups. In this paper, we develop methods to investigate the association between risk factors and pretrial failure, while simultaneously estimating misclassification rates of pretrial risk assessments and of judicial decisions as a function of defendant race. This approach adds to a growing literature that makes use of outcome misclassification methods to answer questions about fairness in pretrial decision-making. We give a detailed simulation study for our proposed methodology and apply these methods to data from the Virginia Department of Criminal Justice Services. We estimate that the VPRAI algorithm has near-perfect specificity, but its sensitivity differs by defendant race. Judicial decisions also display evidence of bias; we estimate wrongful detention rates of 39.7% and 51.4% among white and Black defendants, respectively.</summary></entry><entry><title type="html">Anytime-Valid Tests of Group Invariance through Conformal Prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/AnytimeValidTestsofGroupInvariancethroughConformalPrediction.html" rel="alternate" type="text/html" title="Anytime-Valid Tests of Group Invariance through Conformal Prediction" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/AnytimeValidTestsofGroupInvariancethroughConformalPrediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/AnytimeValidTestsofGroupInvariancethroughConformalPrediction.html">&lt;p&gt;We develop anytime-valid tests of invariance under the action of compact groups. The resulting test statistics are optimal in a logarithmic-growth sense. We apply our method to extend recent anytime-valid tests of independence and to construct tests of normality.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.15461&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tyron Lardy, Muriel Felipe Pérez-Ortiz</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We develop anytime-valid tests of invariance under the action of compact groups. The resulting test statistics are optimal in a logarithmic-growth sense. We apply our method to extend recent anytime-valid tests of independence and to construct tests of normality.</summary></entry><entry><title type="html">A spatial interference approach to account for mobility in air pollution studies with multivariate continuous treatments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Aspatialinterferenceapproachtoaccountformobilityinairpollutionstudieswithmultivariatecontinuoustreatments.html" rel="alternate" type="text/html" title="A spatial interference approach to account for mobility in air pollution studies with multivariate continuous treatments" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Aspatialinterferenceapproachtoaccountformobilityinairpollutionstudieswithmultivariatecontinuoustreatments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Aspatialinterferenceapproachtoaccountformobilityinairpollutionstudieswithmultivariatecontinuoustreatments.html">&lt;p&gt;We develop new methodology to improve our understanding of the causal effects of multivariate air pollution exposures on public health. Typically, exposure to air pollution for an individual is measured at their home geographic region, though people travel to different regions with potentially different levels of air pollution. To account for this, we incorporate estimates of the mobility of individuals from cell phone mobility data to get an improved estimate of their exposure to air pollution. We treat this as an interference problem, where individuals in one geographic region can be affected by exposures in other regions due to mobility into those areas. We propose policy-relevant estimands and derive expressions showing the extent of bias one would obtain by ignoring this mobility. We additionally highlight the benefits of the proposed interference framework relative to a measurement error framework for accounting for mobility. We develop novel estimation strategies to estimate causal effects that account for this spatial spillover utilizing flexible Bayesian methodology. Lastly, we use the proposed methodology to study the health effects of ambient air pollution on mortality among Medicare enrollees in the United States.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.14194&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Heejun Shin, Danielle Braun, Kezia Irene, Michelle Audirac, Joseph Antonelli</name></author><category term="stat.ME" /><summary type="html">We develop new methodology to improve our understanding of the causal effects of multivariate air pollution exposures on public health. Typically, exposure to air pollution for an individual is measured at their home geographic region, though people travel to different regions with potentially different levels of air pollution. To account for this, we incorporate estimates of the mobility of individuals from cell phone mobility data to get an improved estimate of their exposure to air pollution. We treat this as an interference problem, where individuals in one geographic region can be affected by exposures in other regions due to mobility into those areas. We propose policy-relevant estimands and derive expressions showing the extent of bias one would obtain by ignoring this mobility. We additionally highlight the benefits of the proposed interference framework relative to a measurement error framework for accounting for mobility. We develop novel estimation strategies to estimate causal effects that account for this spatial spillover utilizing flexible Bayesian methodology. Lastly, we use the proposed methodology to study the health effects of ambient air pollution on mortality among Medicare enrollees in the United States.</summary></entry><entry><title type="html">Bayesian Inference Under Differential Privacy: Prior Selection Considerations with Application to Univariate Gaussian Data and Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/BayesianInferenceUnderDifferentialPrivacyPriorSelectionConsiderationswithApplicationtoUnivariateGaussianDataandRegression.html" rel="alternate" type="text/html" title="Bayesian Inference Under Differential Privacy: Prior Selection Considerations with Application to Univariate Gaussian Data and Regression" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/BayesianInferenceUnderDifferentialPrivacyPriorSelectionConsiderationswithApplicationtoUnivariateGaussianDataandRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/BayesianInferenceUnderDifferentialPrivacyPriorSelectionConsiderationswithApplicationtoUnivariateGaussianDataandRegression.html">&lt;p&gt;We describe Bayesian inference for the mean and variance of bounded data protected by differential privacy and modeled as Gaussian. Using this setting, we demonstrate that analysts can and should take the constraints imposed by the bounds into account when specifying prior distributions. Additionally, we provide theoretical and empirical results regarding what classes of default priors produce valid inference for a differentially private release in settings where substantial prior information is not available. We discuss how these results can be applied to Bayesian inference for regression with differentially private data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13801&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zeki Kazan, Jerome P. Reiter</name></author><category term="stat.ME" /><summary type="html">We describe Bayesian inference for the mean and variance of bounded data protected by differential privacy and modeled as Gaussian. Using this setting, we demonstrate that analysts can and should take the constraints imposed by the bounds into account when specifying prior distributions. Additionally, we provide theoretical and empirical results regarding what classes of default priors produce valid inference for a differentially private release in settings where substantial prior information is not available. We discuss how these results can be applied to Bayesian inference for regression with differentially private data.</summary></entry><entry><title type="html">Better Simulations for Validating Causal Discovery with the DAG-Adaptation of the Onion Method</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/BetterSimulationsforValidatingCausalDiscoverywiththeDAGAdaptationoftheOnionMethod.html" rel="alternate" type="text/html" title="Better Simulations for Validating Causal Discovery with the DAG-Adaptation of the Onion Method" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/BetterSimulationsforValidatingCausalDiscoverywiththeDAGAdaptationoftheOnionMethod</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/BetterSimulationsforValidatingCausalDiscoverywiththeDAGAdaptationoftheOnionMethod.html">&lt;p&gt;The number of artificial intelligence algorithms for learning causal models from data is growing rapidly. Most &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;causal discovery&apos;&apos; or&lt;/code&gt;causal structure learning’’ algorithms are primarily validated through simulation studies. However, no widely accepted simulation standards exist and publications often report conflicting performance statistics – even when only considering publications that simulate data from linear models. In response, several manuscripts have criticized a popular simulation design for validating algorithms in the linear case.
  We propose a new simulation design for generating linear models for directed acyclic graphs (DAGs): the DAG-adaptation of the Onion (DaO) method. DaO simulations are fundamentally different from existing simulations because they prioritize the distribution of correlation matrices rather than the distribution of linear effects. Specifically, the DaO method uniformly samples the space of all correlation matrices consistent with (i.e. Markov to) a DAG. We also discuss how to sample DAGs and present methods for generating DAGs with scale-free in-degree or out-degree. We compare the DaO method against two alternative simulation designs and provide implementations of the DaO method in Python and R: https://github.com/bja43/DaO_simulation. We advocate for others to adopt DaO simulations as a fair universal benchmark.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13100&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bryan Andrews, Erich Kummerfeld</name></author><category term="stat.ME" /><summary type="html">The number of artificial intelligence algorithms for learning causal models from data is growing rapidly. Most causal discovery&apos;&apos; orcausal structure learning’’ algorithms are primarily validated through simulation studies. However, no widely accepted simulation standards exist and publications often report conflicting performance statistics – even when only considering publications that simulate data from linear models. In response, several manuscripts have criticized a popular simulation design for validating algorithms in the linear case. We propose a new simulation design for generating linear models for directed acyclic graphs (DAGs): the DAG-adaptation of the Onion (DaO) method. DaO simulations are fundamentally different from existing simulations because they prioritize the distribution of correlation matrices rather than the distribution of linear effects. Specifically, the DaO method uniformly samples the space of all correlation matrices consistent with (i.e. Markov to) a DAG. We also discuss how to sample DAGs and present methods for generating DAGs with scale-free in-degree or out-degree. We compare the DaO method against two alternative simulation designs and provide implementations of the DaO method in Python and R: https://github.com/bja43/DaO_simulation. We advocate for others to adopt DaO simulations as a fair universal benchmark.</summary></entry><entry><title type="html">Biarchetype analysis: simultaneous learning of observations and features based on extremes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Biarchetypeanalysissimultaneouslearningofobservationsandfeaturesbasedonextremes.html" rel="alternate" type="text/html" title="Biarchetype analysis: simultaneous learning of observations and features based on extremes" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Biarchetypeanalysissimultaneouslearningofobservationsandfeaturesbasedonextremes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Biarchetypeanalysissimultaneouslearningofobservationsandfeaturesbasedonextremes.html">&lt;p&gt;We introduce a novel exploratory technique, termed biarchetype analysis, which extends archetype analysis to simultaneously identify archetypes of both observations and features. This innovative unsupervised machine learning tool aims to represent observations and features through instances of pure types, or biarchetypes, which are easily interpretable as they embody mixtures of observations and features. Furthermore, the observations and features are expressed as mixtures of the biarchetypes, which makes the structure of the data easier to understand. We propose an algorithm to solve biarchetype analysis. Although clustering is not the primary aim of this technique, biarchetype analysis is demonstrated to offer significant advantages over biclustering methods, particularly in terms of interpretability. This is attributed to biarchetypes being extreme instances, in contrast to the centroids produced by biclustering, which inherently enhances human comprehension. The application of biarchetype analysis across various machine learning challenges underscores its value, and both the source code and examples are readily accessible in R and Python at https://github.com/aleixalcacer/JA-BIAA.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.11153&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aleix Alcacer, Irene Epifanio, Ximo Gual-Arnau</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.ML" /><summary type="html">We introduce a novel exploratory technique, termed biarchetype analysis, which extends archetype analysis to simultaneously identify archetypes of both observations and features. This innovative unsupervised machine learning tool aims to represent observations and features through instances of pure types, or biarchetypes, which are easily interpretable as they embody mixtures of observations and features. Furthermore, the observations and features are expressed as mixtures of the biarchetypes, which makes the structure of the data easier to understand. We propose an algorithm to solve biarchetype analysis. Although clustering is not the primary aim of this technique, biarchetype analysis is demonstrated to offer significant advantages over biclustering methods, particularly in terms of interpretability. This is attributed to biarchetypes being extreme instances, in contrast to the centroids produced by biclustering, which inherently enhances human comprehension. The application of biarchetype analysis across various machine learning challenges underscores its value, and both the source code and examples are readily accessible in R and Python at https://github.com/aleixalcacer/JA-BIAA.</summary></entry><entry><title type="html">Calibrated Model Criticism Using Split Predictive Checks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/CalibratedModelCriticismUsingSplitPredictiveChecks.html" rel="alternate" type="text/html" title="Calibrated Model Criticism Using Split Predictive Checks" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/CalibratedModelCriticismUsingSplitPredictiveChecks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/CalibratedModelCriticismUsingSplitPredictiveChecks.html">&lt;p&gt;Checking how well a fitted model explains the data is one of the most fundamental parts of a Bayesian data analysis. However, existing model checking methods suffer from trade-offs between being well-calibrated, automated, and computationally efficient. To overcome these limitations, we propose split predictive checks (SPCs), which combine the ease-of-use and speed of posterior predictive checks with the good calibration properties of predictive checks that rely on model-specific derivations or inference schemes. We develop an asymptotic theory for two types of SPCs: single SPCs and the divided SPCs. Our results demonstrate that they offer complementary strengths. Single SPCs work well with smaller datasets and provide excellent power when there is substantial misspecification, such as when the estimate uncertainty in the test statistic is significantly underestimated. When the sample size is large, divided SPCs can provide better power and are able to detect more subtle form of misspecification. We validate the finite-sample utility of SPCs through extensive simulation experiments in exponential family and hierarchical models, and provide three real-data examples where SPCs offer novel insights and additional flexibility beyond what is available when using posterior predictive checks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.15897&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiawei Li, Jonathan H. Huggins</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Checking how well a fitted model explains the data is one of the most fundamental parts of a Bayesian data analysis. However, existing model checking methods suffer from trade-offs between being well-calibrated, automated, and computationally efficient. To overcome these limitations, we propose split predictive checks (SPCs), which combine the ease-of-use and speed of posterior predictive checks with the good calibration properties of predictive checks that rely on model-specific derivations or inference schemes. We develop an asymptotic theory for two types of SPCs: single SPCs and the divided SPCs. Our results demonstrate that they offer complementary strengths. Single SPCs work well with smaller datasets and provide excellent power when there is substantial misspecification, such as when the estimate uncertainty in the test statistic is significantly underestimated. When the sample size is large, divided SPCs can provide better power and are able to detect more subtle form of misspecification. We validate the finite-sample utility of SPCs through extensive simulation experiments in exponential family and hierarchical models, and provide three real-data examples where SPCs offer novel insights and additional flexibility beyond what is available when using posterior predictive checks.</summary></entry><entry><title type="html">Causal Discovery in Multivariate Extremes with a Hydrological Analysis of Swiss River Discharges</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/CausalDiscoveryinMultivariateExtremeswithaHydrologicalAnalysisofSwissRiverDischarges.html" rel="alternate" type="text/html" title="Causal Discovery in Multivariate Extremes with a Hydrological Analysis of Swiss River Discharges" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/CausalDiscoveryinMultivariateExtremeswithaHydrologicalAnalysisofSwissRiverDischarges</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/CausalDiscoveryinMultivariateExtremeswithaHydrologicalAnalysisofSwissRiverDischarges.html">&lt;p&gt;Causal asymmetry is based on the principle that an event is a cause only if its absence would not have been a cause. From there, uncovering causal effects becomes a matter of comparing a well-defined score in both directions. Motivated by studying causal effects at extreme levels of a multivariate random vector, we propose to construct a model-agnostic causal score relying solely on the assumption of the existence of a max-domain of attraction. Based on a representation of a Generalized Pareto random vector, we construct the causal score as the Wasserstein distance between the margins and a well-specified random variable. The proposed methodology is illustrated on a hydrologically simulated dataset of different characteristics of catchments in Switzerland: discharge, precipitation, and snowmelt.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10371&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Linda Mhalla, Valérie Chavez-Demoulin, Philippe Naveau</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Causal asymmetry is based on the principle that an event is a cause only if its absence would not have been a cause. From there, uncovering causal effects becomes a matter of comparing a well-defined score in both directions. Motivated by studying causal effects at extreme levels of a multivariate random vector, we propose to construct a model-agnostic causal score relying solely on the assumption of the existence of a max-domain of attraction. Based on a representation of a Generalized Pareto random vector, we construct the causal score as the Wasserstein distance between the margins and a well-specified random variable. The proposed methodology is illustrated on a hydrologically simulated dataset of different characteristics of catchments in Switzerland: discharge, precipitation, and snowmelt.</summary></entry><entry><title type="html">Causal Inference with Cocycles</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/CausalInferencewithCocycles.html" rel="alternate" type="text/html" title="Causal Inference with Cocycles" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/CausalInferencewithCocycles</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/CausalInferencewithCocycles.html">&lt;p&gt;Many interventions in causal inference can be represented as transformations. We identify a local symmetry property satisfied by a large class of causal models under such interventions. Where present, this symmetry can be characterized by a type of map called a cocycle, an object that is central to dynamical systems theory. We show that such cocycles exist under general conditions and are sufficient to identify interventional and counterfactual distributions. We use these results to derive cocycle-based estimators for causal estimands and show they achieve semiparametric efficiency under typical conditions. Since (infinitely) many distributions can share the same cocycle, these estimators make causal inference robust to mis-specification by sidestepping superfluous modelling assumptions. We demonstrate both robustness and state-of-the-art performance in several simulations, and apply our method to estimate the effects of 401(k) pension plan eligibility on asset accumulation using a real dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13844&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hugh Dance, Benjamin Bloem-Reddy</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Many interventions in causal inference can be represented as transformations. We identify a local symmetry property satisfied by a large class of causal models under such interventions. Where present, this symmetry can be characterized by a type of map called a cocycle, an object that is central to dynamical systems theory. We show that such cocycles exist under general conditions and are sufficient to identify interventional and counterfactual distributions. We use these results to derive cocycle-based estimators for causal estimands and show they achieve semiparametric efficiency under typical conditions. Since (infinitely) many distributions can share the same cocycle, these estimators make causal inference robust to mis-specification by sidestepping superfluous modelling assumptions. We demonstrate both robustness and state-of-the-art performance in several simulations, and apply our method to estimate the effects of 401(k) pension plan eligibility on asset accumulation using a real dataset.</summary></entry><entry><title type="html">Closed-form estimators for an exponential family derived from likelihood equations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Closedformestimatorsforanexponentialfamilyderivedfromlikelihoodequations.html" rel="alternate" type="text/html" title="Closed-form estimators for an exponential family derived from likelihood equations" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Closedformestimatorsforanexponentialfamilyderivedfromlikelihoodequations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Closedformestimatorsforanexponentialfamilyderivedfromlikelihoodequations.html">&lt;p&gt;In this paper, we derive closed-form estimators for the parameters of some probability distributions belonging to the exponential family. A bootstrap bias-reduced version of these proposed closed-form estimators are also derived. A Monte Carlo simulation is performed for the assessment of the estimators. The results are seen to be quite favorable to the proposed bootstrap bias-reduce estimators.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14509&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roberto Vila, Eduardo Nakano, Helton Saulo</name></author><category term="stat.ME" /><summary type="html">In this paper, we derive closed-form estimators for the parameters of some probability distributions belonging to the exponential family. A bootstrap bias-reduced version of these proposed closed-form estimators are also derived. A Monte Carlo simulation is performed for the assessment of the estimators. The results are seen to be quite favorable to the proposed bootstrap bias-reduce estimators.</summary></entry><entry><title type="html">Computing the Instantaneous Collision Probability between Satellites using Characteristic Function Inversion</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ComputingtheInstantaneousCollisionProbabilitybetweenSatellitesusingCharacteristicFunctionInversion.html" rel="alternate" type="text/html" title="Computing the Instantaneous Collision Probability between Satellites using Characteristic Function Inversion" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ComputingtheInstantaneousCollisionProbabilitybetweenSatellitesusingCharacteristicFunctionInversion</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ComputingtheInstantaneousCollisionProbabilitybetweenSatellitesusingCharacteristicFunctionInversion.html">&lt;p&gt;The probability that two satellites overlap in space at a specified instant of time is called their instantaneous collision probability. Assuming Gaussian uncertainties and spherical satellites, this probability is the integral of a Gaussian distribution over a sphere. This paper shows how to compute the probability using an established numerical procedure called characteristic function inversion. The collision probability in the short-term encounter scenario is also evaluated with this approach, where the instant at which the probability is computed is the time of closest approach between the objects. Python and R code is provided to evaluate the probability in practice. Overall, the approach has been established for over fifty years, is implemented in existing software, does not rely on analytical approximations, and can be used to evaluate two and three dimensional collision probabilities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12230&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jason Bernstein</name></author><category term="stat.AP" /><summary type="html">The probability that two satellites overlap in space at a specified instant of time is called their instantaneous collision probability. Assuming Gaussian uncertainties and spherical satellites, this probability is the integral of a Gaussian distribution over a sphere. This paper shows how to compute the probability using an established numerical procedure called characteristic function inversion. The collision probability in the short-term encounter scenario is also evaluated with this approach, where the instant at which the probability is computed is the time of closest approach between the objects. Python and R code is provided to evaluate the probability in practice. Overall, the approach has been established for over fifty years, is implemented in existing software, does not rely on analytical approximations, and can be used to evaluate two and three dimensional collision probabilities.</summary></entry><entry><title type="html">Conditioning diffusion models by explicit forward-backward bridging</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Conditioningdiffusionmodelsbyexplicitforwardbackwardbridging.html" rel="alternate" type="text/html" title="Conditioning diffusion models by explicit forward-backward bridging" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Conditioningdiffusionmodelsbyexplicitforwardbackwardbridging</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Conditioningdiffusionmodelsbyexplicitforwardbackwardbridging.html">&lt;p&gt;Given an unconditional diffusion model $\pi(x, y)$, using it to perform conditional simulation $\pi(x \mid y)$ is still largely an open question and is typically achieved by learning conditional drifts to the denoising SDE after the fact. In this work, we express conditional simulation as an inference problem on an augmented space corresponding to a partial SDE bridge. This perspective allows us to implement efficient and principled particle Gibbs and pseudo-marginal samplers marginally targeting the conditional distribution $\pi(x \mid y)$. Contrary to existing methodology, our methods do not introduce any additional approximation to the unconditional diffusion model aside from the Monte Carlo error. We showcase the benefits and drawbacks of our approach on a series of synthetic and real data examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13794&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Adrien Corenflos, Zheng Zhao, Simo Särkkä, Jens Sjölund, Thomas B. Schön</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Given an unconditional diffusion model $\pi(x, y)$, using it to perform conditional simulation $\pi(x \mid y)$ is still largely an open question and is typically achieved by learning conditional drifts to the denoising SDE after the fact. In this work, we express conditional simulation as an inference problem on an augmented space corresponding to a partial SDE bridge. This perspective allows us to implement efficient and principled particle Gibbs and pseudo-marginal samplers marginally targeting the conditional distribution $\pi(x \mid y)$. Contrary to existing methodology, our methods do not introduce any additional approximation to the unconditional diffusion model aside from the Monte Carlo error. We showcase the benefits and drawbacks of our approach on a series of synthetic and real data examples.</summary></entry><entry><title type="html">Conformal uncertainty quantification using kernel depth measures in separable Hilbert spaces</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ConformaluncertaintyquantificationusingkerneldepthmeasuresinseparableHilbertspaces.html" rel="alternate" type="text/html" title="Conformal uncertainty quantification using kernel depth measures in separable Hilbert spaces" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ConformaluncertaintyquantificationusingkerneldepthmeasuresinseparableHilbertspaces</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ConformaluncertaintyquantificationusingkerneldepthmeasuresinseparableHilbertspaces.html">&lt;p&gt;Depth measures have gained popularity in the statistical literature for defining level sets in complex data structures like multivariate data, functional data, and graphs. Despite their versatility, integrating depth measures into regression modeling for establishing prediction regions remains underexplored. To address this gap, we propose a novel method utilizing a model-free uncertainty quantification algorithm based on conditional depth measures and conditional kernel mean embeddings. This enables the creation of tailored prediction and tolerance regions in regression models handling complex statistical responses and predictors in separable Hilbert spaces. Our focus in this paper is exclusively on examples where the response is a functional data object. To enhance practicality, we introduce a conformal prediction algorithm, providing non-asymptotic guarantees in the derived prediction region. Additionally, we establish both conditional and unconditional consistency results and fast convergence rates in some special homoscedastic cases. We evaluate the model finite sample performance in extensive simulation studies with different function objects as probability distributions and functional data. Finally, we apply the approach in a digital health application related to physical activity, aiming to offer personalized recommendations in the US. population based on individuals’ characteristics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13970&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marcos Matabuena, Rahul Ghosal, Pavlo Mozharovskyi, Oscar Hernan Madrid Padilla, Jukka-Pekka Onnela</name></author><category term="stat.ME" /><summary type="html">Depth measures have gained popularity in the statistical literature for defining level sets in complex data structures like multivariate data, functional data, and graphs. Despite their versatility, integrating depth measures into regression modeling for establishing prediction regions remains underexplored. To address this gap, we propose a novel method utilizing a model-free uncertainty quantification algorithm based on conditional depth measures and conditional kernel mean embeddings. This enables the creation of tailored prediction and tolerance regions in regression models handling complex statistical responses and predictors in separable Hilbert spaces. Our focus in this paper is exclusively on examples where the response is a functional data object. To enhance practicality, we introduce a conformal prediction algorithm, providing non-asymptotic guarantees in the derived prediction region. Additionally, we establish both conditional and unconditional consistency results and fast convergence rates in some special homoscedastic cases. We evaluate the model finite sample performance in extensive simulation studies with different function objects as probability distributions and functional data. Finally, we apply the approach in a digital health application related to physical activity, aiming to offer personalized recommendations in the US. population based on individuals’ characteristics.</summary></entry><entry><title type="html">Continuous Treatment Effects with Surrogate Outcomes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ContinuousTreatmentEffectswithSurrogateOutcomes.html" rel="alternate" type="text/html" title="Continuous Treatment Effects with Surrogate Outcomes" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ContinuousTreatmentEffectswithSurrogateOutcomes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ContinuousTreatmentEffectswithSurrogateOutcomes.html">&lt;p&gt;In many real-world causal inference applications, the primary outcomes (labels) are often partially missing, especially if they are expensive or difficult to collect. If the missingness depends on covariates (i.e., missingness is not completely at random), analyses based on fully observed samples alone may be biased. Incorporating surrogates, which are fully observed post-treatment variables related to the primary outcome, can improve estimation in this case. In this paper, we study the role of surrogates in estimating continuous treatment effects and propose a doubly robust method to efficiently incorporate surrogates in the analysis, which uses both labeled and unlabeled data and does not suffer from the above selection bias problem. Importantly, we establish the asymptotic normality of the proposed estimator and show possible improvements on the variance compared with methods that solely use labeled data. Extensive simulations show our methods enjoy appealing empirical performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.00168&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhenghao Zeng, David Arbour, Avi Feller, Raghavendra Addanki, Ryan Rossi, Ritwik Sinha, Edward H. Kennedy</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">In many real-world causal inference applications, the primary outcomes (labels) are often partially missing, especially if they are expensive or difficult to collect. If the missingness depends on covariates (i.e., missingness is not completely at random), analyses based on fully observed samples alone may be biased. Incorporating surrogates, which are fully observed post-treatment variables related to the primary outcome, can improve estimation in this case. In this paper, we study the role of surrogates in estimating continuous treatment effects and propose a doubly robust method to efficiently incorporate surrogates in the analysis, which uses both labeled and unlabeled data and does not suffer from the above selection bias problem. Importantly, we establish the asymptotic normality of the proposed estimator and show possible improvements on the variance compared with methods that solely use labeled data. Extensive simulations show our methods enjoy appealing empirical performance.</summary></entry><entry><title type="html">Contraction and Convergence Rates for Discretized Kinetic Langevin Dynamics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ContractionandConvergenceRatesforDiscretizedKineticLangevinDynamics.html" rel="alternate" type="text/html" title="Contraction and Convergence Rates for Discretized Kinetic Langevin Dynamics" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ContractionandConvergenceRatesforDiscretizedKineticLangevinDynamics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ContractionandConvergenceRatesforDiscretizedKineticLangevinDynamics.html">&lt;p&gt;We provide a framework to analyze the convergence of discretized kinetic Langevin dynamics for $M$-$\nabla$Lipschitz, $m$-convex potentials. Our approach gives convergence rates of $\mathcal{O}(m/M)$, with explicit stepsize restrictions, which are of the same order as the stability threshold for Gaussian targets and are valid for a large interval of the friction parameter. We apply this methodology to various integration schemes which are popular in the molecular dynamics and machine learning communities. Further, we introduce the property ``$\gamma$-limit convergent” (GLC) to characterize underdamped Langevin schemes that converge to overdamped dynamics in the high-friction limit and which have stepsize restrictions that are independent of the friction parameter; we show that this property is not generic by exhibiting methods from both the class and its complement. Finally, we provide asymptotic bias estimates for the BAOAB scheme, which remain accurate in the high-friction limit by comparison to a modified stochastic dynamics which preserves the invariant measure.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.10684&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Benedict Leimkuhler, Daniel Paulin, Peter A. Whalley</name></author><category term="stat.CO" /><summary type="html">We provide a framework to analyze the convergence of discretized kinetic Langevin dynamics for $M$-$\nabla$Lipschitz, $m$-convex potentials. Our approach gives convergence rates of $\mathcal{O}(m/M)$, with explicit stepsize restrictions, which are of the same order as the stability threshold for Gaussian targets and are valid for a large interval of the friction parameter. We apply this methodology to various integration schemes which are popular in the molecular dynamics and machine learning communities. Further, we introduce the property ``$\gamma$-limit convergent” (GLC) to characterize underdamped Langevin schemes that converge to overdamped dynamics in the high-friction limit and which have stepsize restrictions that are independent of the friction parameter; we show that this property is not generic by exhibiting methods from both the class and its complement. Finally, we provide asymptotic bias estimates for the BAOAB scheme, which remain accurate in the high-friction limit by comparison to a modified stochastic dynamics which preserves the invariant measure.</summary></entry><entry><title type="html">Control, Transport and Sampling: Towards Better Loss Design</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ControlTransportandSamplingTowardsBetterLossDesign.html" rel="alternate" type="text/html" title="Control, Transport and Sampling: Towards Better Loss Design" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ControlTransportandSamplingTowardsBetterLossDesign</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ControlTransportandSamplingTowardsBetterLossDesign.html">&lt;p&gt;Leveraging connections between diffusion-based sampling, optimal transport, and optimal stochastic control through their shared links to the Schr&quot;odinger bridge problem, we propose novel objective functions that can be used to transport $\nu$ to $\mu$, consequently sample from the target $\mu$, via optimally controlled dynamics. We highlight the importance of the pathwise perspective and the role various optimality conditions on the path measure can play for the design of valid training losses, the careful choice of which offer numerical advantages in practical implementation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13731&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Qijia Jiang, David Nabergoj</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Leveraging connections between diffusion-based sampling, optimal transport, and optimal stochastic control through their shared links to the Schr&quot;odinger bridge problem, we propose novel objective functions that can be used to transport $\nu$ to $\mu$, consequently sample from the target $\mu$, via optimally controlled dynamics. We highlight the importance of the pathwise perspective and the role various optimality conditions on the path measure can play for the design of valid training losses, the careful choice of which offer numerical advantages in practical implementation.</summary></entry><entry><title type="html">Cumulant-based approximation for fast and efficient prediction for species distribution</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Cumulantbasedapproximationforfastandefficientpredictionforspeciesdistribution.html" rel="alternate" type="text/html" title="Cumulant-based approximation for fast and efficient prediction for species distribution" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Cumulantbasedapproximationforfastandefficientpredictionforspeciesdistribution</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Cumulantbasedapproximationforfastandefficientpredictionforspeciesdistribution.html">&lt;p&gt;Species distribution modeling plays an important role in estimating the habitat suitability of species using environmental variables. For this purpose, Maxent and the Poisson point process are popular and powerful methods extensively employed across various ecological and biological sciences. However, the computational speed becomes prohibitively slow when using huge background datasets, which is often the case with fine-resolution data or global-scale estimations. To address this problem, we propose a computationally efficient species distribution model using a cumulant-based approximation (CBA) applied to the loss function of $\gamma$-divergence. Additionally, we introduce a sequential estimating algorithm with an $L_1$ penalty to select important environmental variables closely associated with species distribution. The regularized geometric-mean method, derived from the CBA, demonstrates high computational efficiency and estimation accuracy. Moreover, by applying CBA to Maxent, we establish that Maxent and Fisher linear discriminant analysis are equivalent under a normality assumption. This equivalence leads to an highly efficient computational method for estimating species distribution. The effectiveness of our proposed methods is illustrated through simulation studies and by analyzing data on 226 species from the National Centre for Ecological Analysis and Synthesis and 709 Japanese vascular plant species. The computational efficiency of the proposed methods is significantly improved compared to Maxent, while maintaining comparable estimation accuracy. A R package {\tt CBA} is also prepared to provide all programming codes used in simulation studies and real data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14456&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Osamu Komori, Yusuke Saigusa, Shinto Eguchi, Yasuhiro Kubota</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Species distribution modeling plays an important role in estimating the habitat suitability of species using environmental variables. For this purpose, Maxent and the Poisson point process are popular and powerful methods extensively employed across various ecological and biological sciences. However, the computational speed becomes prohibitively slow when using huge background datasets, which is often the case with fine-resolution data or global-scale estimations. To address this problem, we propose a computationally efficient species distribution model using a cumulant-based approximation (CBA) applied to the loss function of $\gamma$-divergence. Additionally, we introduce a sequential estimating algorithm with an $L_1$ penalty to select important environmental variables closely associated with species distribution. The regularized geometric-mean method, derived from the CBA, demonstrates high computational efficiency and estimation accuracy. Moreover, by applying CBA to Maxent, we establish that Maxent and Fisher linear discriminant analysis are equivalent under a normality assumption. This equivalence leads to an highly efficient computational method for estimating species distribution. The effectiveness of our proposed methods is illustrated through simulation studies and by analyzing data on 226 species from the National Centre for Ecological Analysis and Synthesis and 709 Japanese vascular plant species. The computational efficiency of the proposed methods is significantly improved compared to Maxent, while maintaining comparable estimation accuracy. A R package {\tt CBA} is also prepared to provide all programming codes used in simulation studies and real data analysis.</summary></entry><entry><title type="html">Data Assimilation with Machine Learning Surrogate Models: A Case Study with FourCastNet</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/DataAssimilationwithMachineLearningSurrogateModelsACaseStudywithFourCastNet.html" rel="alternate" type="text/html" title="Data Assimilation with Machine Learning Surrogate Models: A Case Study with FourCastNet" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/DataAssimilationwithMachineLearningSurrogateModelsACaseStudywithFourCastNet</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/DataAssimilationwithMachineLearningSurrogateModelsACaseStudywithFourCastNet.html">&lt;p&gt;Modern data-driven surrogate models for weather forecasting provide accurate short-term predictions but inaccurate and nonphysical long-term forecasts. This paper investigates online weather prediction using machine learning surrogates supplemented with partial and noisy observations. We empirically demonstrate and theoretically justify that, despite the long-time instability of the surrogates and the sparsity of the observations, filtering estimates can remain accurate in the long-time horizon. As a case study, we integrate FourCastNet, a state-of-the-art weather surrogate model, within a variational data assimilation framework using partial, noisy ERA5 data. Our results show that filtering estimates remain accurate over a year-long assimilation window and provide effective initial conditions for forecasting tasks, including extreme event prediction.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13180&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Melissa Adrian, Daniel Sanz-Alonso, Rebecca Willett</name></author><category term="stat.AP" /><summary type="html">Modern data-driven surrogate models for weather forecasting provide accurate short-term predictions but inaccurate and nonphysical long-term forecasts. This paper investigates online weather prediction using machine learning surrogates supplemented with partial and noisy observations. We empirically demonstrate and theoretically justify that, despite the long-time instability of the surrogates and the sparsity of the observations, filtering estimates can remain accurate in the long-time horizon. As a case study, we integrate FourCastNet, a state-of-the-art weather surrogate model, within a variational data assimilation framework using partial, noisy ERA5 data. Our results show that filtering estimates remain accurate over a year-long assimilation window and provide effective initial conditions for forecasting tasks, including extreme event prediction.</summary></entry><entry><title type="html">Decoding Social Sentiment in DAO: A Comparative Analysis of Blockchain Governance Communities</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/DecodingSocialSentimentinDAOAComparativeAnalysisofBlockchainGovernanceCommunities.html" rel="alternate" type="text/html" title="Decoding Social Sentiment in DAO: A Comparative Analysis of Blockchain Governance Communities" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/DecodingSocialSentimentinDAOAComparativeAnalysisofBlockchainGovernanceCommunities</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/DecodingSocialSentimentinDAOAComparativeAnalysisofBlockchainGovernanceCommunities.html">&lt;p&gt;Blockchain technology is leading a revolutionary transformation across diverse industries, with effective governance being critical for the success and sustainability of blockchain projects. Community forums, pivotal in engaging decentralized autonomous organizations (DAOs), significantly impact blockchain governance decisions. Concurrently, Natural Language Processing (NLP), particularly sentiment analysis, provides powerful insights from textual data. While prior research has explored the potential of NLP tools in social media sentiment analysis, there is a gap in understanding the sentiment landscape of blockchain governance communities. The evolving discourse and sentiment dynamics on the forums of top DAOs remain largely unknown. This paper delves deep into the evolving discourse and sentiment dynamics on the public forums of leading DeFi projects: Aave, Uniswap, Curve DAO, Yearn.finance, Merit Circle, and Balancer, focusing primarily on discussions related to governance issues. Our study shows that participants in decentralized communities generally express positive sentiments during Discord discussions. Furthermore, there is a potential interaction between discussion intensity and sentiment dynamics; higher discussion volume may contribute to a more stable sentiment from code analysis. The insights gained from this study are valuable for decision-makers in blockchain governance, underscoring the pivotal role of sentiment analysis in interpreting community emotions and its evolving impact on the landscape of blockchain governance. This research significantly contributes to the interdisciplinary exploration of the intersection of blockchain and society, specifically emphasizing the decentralized blockchain governance ecosystem. We provide our data and code for replicability as open access on GitHub.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.14676&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yutong Quan, Xintong Wu, Wanlin Deng, Luyao Zhang</name></author><category term="stat.AP" /><summary type="html">Blockchain technology is leading a revolutionary transformation across diverse industries, with effective governance being critical for the success and sustainability of blockchain projects. Community forums, pivotal in engaging decentralized autonomous organizations (DAOs), significantly impact blockchain governance decisions. Concurrently, Natural Language Processing (NLP), particularly sentiment analysis, provides powerful insights from textual data. While prior research has explored the potential of NLP tools in social media sentiment analysis, there is a gap in understanding the sentiment landscape of blockchain governance communities. The evolving discourse and sentiment dynamics on the forums of top DAOs remain largely unknown. This paper delves deep into the evolving discourse and sentiment dynamics on the public forums of leading DeFi projects: Aave, Uniswap, Curve DAO, Yearn.finance, Merit Circle, and Balancer, focusing primarily on discussions related to governance issues. Our study shows that participants in decentralized communities generally express positive sentiments during Discord discussions. Furthermore, there is a potential interaction between discussion intensity and sentiment dynamics; higher discussion volume may contribute to a more stable sentiment from code analysis. The insights gained from this study are valuable for decision-makers in blockchain governance, underscoring the pivotal role of sentiment analysis in interpreting community emotions and its evolving impact on the landscape of blockchain governance. This research significantly contributes to the interdisciplinary exploration of the intersection of blockchain and society, specifically emphasizing the decentralized blockchain governance ecosystem. We provide our data and code for replicability as open access on GitHub.</summary></entry><entry><title type="html">Distributed High-Dimensional Quantile Regression: Estimation Efficiency and Support Recovery</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/DistributedHighDimensionalQuantileRegressionEstimationEfficiencyandSupportRecovery.html" rel="alternate" type="text/html" title="Distributed High-Dimensional Quantile Regression: Estimation Efficiency and Support Recovery" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/DistributedHighDimensionalQuantileRegressionEstimationEfficiencyandSupportRecovery</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/DistributedHighDimensionalQuantileRegressionEstimationEfficiencyandSupportRecovery.html">&lt;p&gt;In this paper, we focus on distributed estimation and support recovery for high-dimensional linear quantile regression. Quantile regression is a popular alternative tool to the least squares regression for robustness against outliers and data heterogeneity. However, the non-smoothness of the check loss function poses big challenges to both computation and theory in the distributed setting. To tackle these problems, we transform the original quantile regression into the least-squares optimization. By applying a double-smoothing approach, we extend a previous Newton-type distributed approach without the restrictive independent assumption between the error term and covariates. An efficient algorithm is developed, which enjoys high computation and communication efficiency. Theoretically, the proposed distributed estimator achieves a near-oracle convergence rate and high support recovery accuracy after a constant number of iterations. Extensive experiments on synthetic examples and a real data application further demonstrate the effectiveness of the proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07552&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Caixing Wang, Ziliang Shen</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">In this paper, we focus on distributed estimation and support recovery for high-dimensional linear quantile regression. Quantile regression is a popular alternative tool to the least squares regression for robustness against outliers and data heterogeneity. However, the non-smoothness of the check loss function poses big challenges to both computation and theory in the distributed setting. To tackle these problems, we transform the original quantile regression into the least-squares optimization. By applying a double-smoothing approach, we extend a previous Newton-type distributed approach without the restrictive independent assumption between the error term and covariates. An efficient algorithm is developed, which enjoys high computation and communication efficiency. Theoretically, the proposed distributed estimator achieves a near-oracle convergence rate and high support recovery accuracy after a constant number of iterations. Extensive experiments on synthetic examples and a real data application further demonstrate the effectiveness of the proposed method.</summary></entry><entry><title type="html">Efficient Algorithms for the Sensitivities of the Pearson Correlation Coefficient and Its Statistical Significance to Online Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/EfficientAlgorithmsfortheSensitivitiesofthePearsonCorrelationCoefficientandItsStatisticalSignificancetoOnlineData.html" rel="alternate" type="text/html" title="Efficient Algorithms for the Sensitivities of the Pearson Correlation Coefficient and Its Statistical Significance to Online Data" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/EfficientAlgorithmsfortheSensitivitiesofthePearsonCorrelationCoefficientandItsStatisticalSignificancetoOnlineData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/EfficientAlgorithmsfortheSensitivitiesofthePearsonCorrelationCoefficientandItsStatisticalSignificancetoOnlineData.html">&lt;p&gt;Reliably measuring the collinearity of bivariate data is crucial in statistics, particularly for time-series analysis or ongoing studies in which incoming observations can significantly impact current collinearity estimates. Leveraging identities from Welford’s online algorithm for sample variance, we develop a rigorous theoretical framework for analyzing the maximal change to the Pearson correlation coefficient and its p-value that can be induced by additional data. Further, we show that the resulting optimization problems yield elegant closed-form solutions that can be accurately computed by linear- and constant-time algorithms. Our work not only creates new theoretical avenues for robust correlation measures, but also has broad practical implications for disciplines that span econometrics, operations research, clinical trials, climatology, differential privacy, and bioinformatics. Software implementations of our algorithms in Cython-wrapped C are made available at https://github.com/marc-harary/sensitivity for reproducibility, practical deployment, and future theoretical development.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14686&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marc Harary</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Reliably measuring the collinearity of bivariate data is crucial in statistics, particularly for time-series analysis or ongoing studies in which incoming observations can significantly impact current collinearity estimates. Leveraging identities from Welford’s online algorithm for sample variance, we develop a rigorous theoretical framework for analyzing the maximal change to the Pearson correlation coefficient and its p-value that can be induced by additional data. Further, we show that the resulting optimization problems yield elegant closed-form solutions that can be accurately computed by linear- and constant-time algorithms. Our work not only creates new theoretical avenues for robust correlation measures, but also has broad practical implications for disciplines that span econometrics, operations research, clinical trials, climatology, differential privacy, and bioinformatics. Software implementations of our algorithms in Cython-wrapped C are made available at https://github.com/marc-harary/sensitivity for reproducibility, practical deployment, and future theoretical development.</summary></entry><entry><title type="html">Efficient Shapley Performance Attribution for Least-Squares Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/EfficientShapleyPerformanceAttributionforLeastSquaresRegression.html" rel="alternate" type="text/html" title="Efficient Shapley Performance Attribution for Least-Squares Regression" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/EfficientShapleyPerformanceAttributionforLeastSquaresRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/EfficientShapleyPerformanceAttributionforLeastSquaresRegression.html">&lt;p&gt;We consider the performance of a least-squares regression model, as judged by out-of-sample $R^2$. Shapley values give a fair attribution of the performance of a model to its input features, taking into account interdependencies between features. Evaluating the Shapley values exactly requires solving a number of regression problems that is exponential in the number of features, so a Monte Carlo-type approximation is typically used. We focus on the special case of least-squares regression models, where several tricks can be used to compute and evaluate regression models efficiently. These tricks give a substantial speed up, allowing many more Monte Carlo samples to be evaluated, achieving better accuracy. We refer to our method as least-squares Shapley performance attribution (LS-SPA), and describe our open-source implementation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.19245&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Logan Bell, Nikhil Devanathan, Stephen Boyd</name></author><category term="stat.CO" /><summary type="html">We consider the performance of a least-squares regression model, as judged by out-of-sample $R^2$. Shapley values give a fair attribution of the performance of a model to its input features, taking into account interdependencies between features. Evaluating the Shapley values exactly requires solving a number of regression problems that is exponential in the number of features, so a Monte Carlo-type approximation is typically used. We focus on the special case of least-squares regression models, where several tricks can be used to compute and evaluate regression models efficiently. These tricks give a substantial speed up, allowing many more Monte Carlo samples to be evaluated, achieving better accuracy. We refer to our method as least-squares Shapley performance attribution (LS-SPA), and describe our open-source implementation.</summary></entry><entry><title type="html">Enhancing Dose Selection in Phase I Cancer Trials: Extending the Bayesian Logistic Regression Model with Non-DLT Adverse Events Integration</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/EnhancingDoseSelectioninPhaseICancerTrialsExtendingtheBayesianLogisticRegressionModelwithNonDLTAdverseEventsIntegration.html" rel="alternate" type="text/html" title="Enhancing Dose Selection in Phase I Cancer Trials: Extending the Bayesian Logistic Regression Model with Non-DLT Adverse Events Integration" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/EnhancingDoseSelectioninPhaseICancerTrialsExtendingtheBayesianLogisticRegressionModelwithNonDLTAdverseEventsIntegration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/EnhancingDoseSelectioninPhaseICancerTrialsExtendingtheBayesianLogisticRegressionModelwithNonDLTAdverseEventsIntegration.html">&lt;p&gt;This paper presents the Burdened Bayesian Logistic Regression Model (BBLRM), an enhancement to the Bayesian Logistic Regression Model (BLRM) for dose-finding in phase I oncology trials. Traditionally, the BLRM determines the maximum tolerated dose (MTD) based on dose-limiting toxicities (DLTs). However, clinicians often perceive model-based designs like BLRM as complex and less conservative than rule-based designs, such as the widely used 3+3 method. To address these concerns, the BBLRM incorporates non-DLT adverse events (nDLTAEs) into the model. These events, although not severe enough to qualify as DLTs, provide additional information suggesting that higher doses might result in DLTs. In the BBLRM, an additional parameter $\delta$ is introduced to account for nDLTAEs. This parameter adjusts the toxicity probability estimates, making the model more conservative in dose escalation. The $\delta$ parameter is derived from the proportion of patients experiencing nDLTAEs within each cohort and is tuned to balance the model’s conservatism. This approach aims to reduce the likelihood of assigning toxic doses as MTD while involving clinicians more directly in the decision-making process. The paper includes a simulation study comparing BBLRM with the traditional BLRM across various scenarios. The simulations demonstrate that BBLRM significantly reduces the selection of toxic doses as MTD without compromising, and sometimes even increasing, the accuracy of MTD identification. These results suggest that integrating nDLTAEs into the dose-finding process can enhance the safety and acceptance of model-based designs in phase I oncology trials.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13767&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrea Nizzardo, Luca Genetti, Marco Pergher</name></author><category term="stat.ME" /><summary type="html">This paper presents the Burdened Bayesian Logistic Regression Model (BBLRM), an enhancement to the Bayesian Logistic Regression Model (BLRM) for dose-finding in phase I oncology trials. Traditionally, the BLRM determines the maximum tolerated dose (MTD) based on dose-limiting toxicities (DLTs). However, clinicians often perceive model-based designs like BLRM as complex and less conservative than rule-based designs, such as the widely used 3+3 method. To address these concerns, the BBLRM incorporates non-DLT adverse events (nDLTAEs) into the model. These events, although not severe enough to qualify as DLTs, provide additional information suggesting that higher doses might result in DLTs. In the BBLRM, an additional parameter $\delta$ is introduced to account for nDLTAEs. This parameter adjusts the toxicity probability estimates, making the model more conservative in dose escalation. The $\delta$ parameter is derived from the proportion of patients experiencing nDLTAEs within each cohort and is tuned to balance the model’s conservatism. This approach aims to reduce the likelihood of assigning toxic doses as MTD while involving clinicians more directly in the decision-making process. The paper includes a simulation study comparing BBLRM with the traditional BLRM across various scenarios. The simulations demonstrate that BBLRM significantly reduces the selection of toxic doses as MTD without compromising, and sometimes even increasing, the accuracy of MTD identification. These results suggest that integrating nDLTAEs into the dose-finding process can enhance the safety and acceptance of model-based designs in phase I oncology trials.</summary></entry><entry><title type="html">Ensemble size dependence of the logarithmic score for forecasts issued as multivariate normal distributions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Ensemblesizedependenceofthelogarithmicscoreforforecastsissuedasmultivariatenormaldistributions.html" rel="alternate" type="text/html" title="Ensemble size dependence of the logarithmic score for forecasts issued as multivariate normal distributions" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Ensemblesizedependenceofthelogarithmicscoreforforecastsissuedasmultivariatenormaldistributions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Ensemblesizedependenceofthelogarithmicscoreforforecastsissuedasmultivariatenormaldistributions.html">&lt;p&gt;Multivariate probabilistic verification is concerned with the evaluation of joint probability distributions of vector quantities such as a weather variable at multiple locations or a wind vector for instance. The logarithmic score is a proper score that is useful in this context. In order to apply this score to ensemble forecasts, a choice for the density is required. Here, we are interested in the specific case when the density is multivariate normal with mean and covariance given by the ensemble mean and ensemble covariance, respectively. Under the assumptions of multivariate normality and exchangeability of the ensemble members, a relationship is derived which describes how the logarithmic score depends on ensemble size. It permits to estimate the score in the limit of infinite ensemble size from a small ensemble and thus produces a fair logarithmic score for multivariate ensemble forecasts under the assumption of normality. This generalises a study from 2018 which derived the ensemble size adjustment of the logarithmic score in the univariate case.
  An application to medium-range forecasts examines the usefulness of the ensemble size adjustments when multivariate normality is only an approximation. Predictions of vectors consisting of several different combinations of upper air variables are considered. Logarithmic scores are calculated for these vectors using ECMWF’s daily extended-range forecasts which consist of a 100-member ensemble. The probabilistic forecasts of these vectors are verified against operational ECMWF analyses in the Northern mid-latitudes in autumn 2023. Scores are computed for ensemble sizes from 8 to 100. The fair logarithmic scores of ensembles with different cardinalities are very close, in contrast to the unadjusted scores which decrease considerably with ensemble size. This provides evidence for the practical usefulness of the derived relationships.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13400&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Martin Leutbecher, Sándor Baran</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Multivariate probabilistic verification is concerned with the evaluation of joint probability distributions of vector quantities such as a weather variable at multiple locations or a wind vector for instance. The logarithmic score is a proper score that is useful in this context. In order to apply this score to ensemble forecasts, a choice for the density is required. Here, we are interested in the specific case when the density is multivariate normal with mean and covariance given by the ensemble mean and ensemble covariance, respectively. Under the assumptions of multivariate normality and exchangeability of the ensemble members, a relationship is derived which describes how the logarithmic score depends on ensemble size. It permits to estimate the score in the limit of infinite ensemble size from a small ensemble and thus produces a fair logarithmic score for multivariate ensemble forecasts under the assumption of normality. This generalises a study from 2018 which derived the ensemble size adjustment of the logarithmic score in the univariate case. An application to medium-range forecasts examines the usefulness of the ensemble size adjustments when multivariate normality is only an approximation. Predictions of vectors consisting of several different combinations of upper air variables are considered. Logarithmic scores are calculated for these vectors using ECMWF’s daily extended-range forecasts which consist of a 100-member ensemble. The probabilistic forecasts of these vectors are verified against operational ECMWF analyses in the Northern mid-latitudes in autumn 2023. Scores are computed for ensemble sizes from 8 to 100. The fair logarithmic scores of ensembles with different cardinalities are very close, in contrast to the unadjusted scores which decrease considerably with ensemble size. This provides evidence for the practical usefulness of the derived relationships.</summary></entry><entry><title type="html">Extending Kernel Testing To General Designs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ExtendingKernelTestingToGeneralDesigns.html" rel="alternate" type="text/html" title="Extending Kernel Testing To General Designs" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ExtendingKernelTestingToGeneralDesigns</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ExtendingKernelTestingToGeneralDesigns.html">&lt;p&gt;Kernel-based testing has revolutionized the field of non-parametric tests through the embedding of distributions in an RKHS. This strategy has proven to be powerful and flexible, yet its applicability has been limited to the standard two-sample case, while practical situations often involve more complex experimental designs. To extend kernel testing to any design, we propose a linear model in the RKHS that allows for the decomposition of mean embeddings into additive functional effects. We then introduce a truncated kernel Hotelling-Lawley statistic to test the effects of the model, demonstrating that its asymptotic distribution is chi-square, which remains valid with its Nystrom approximation. We discuss a homoscedasticity assumption that, although absent in the standard two-sample case, is necessary for general designs. Finally, we illustrate our framework using a single-cell RNA sequencing dataset and provide kernel-based generalizations of classical diagnostic and exploration tools to broaden the scope of kernel testing in any experimental design.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13799&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anthony Ozier-Lafontaine, Franck Picard, Bertrand Michel</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Kernel-based testing has revolutionized the field of non-parametric tests through the embedding of distributions in an RKHS. This strategy has proven to be powerful and flexible, yet its applicability has been limited to the standard two-sample case, while practical situations often involve more complex experimental designs. To extend kernel testing to any design, we propose a linear model in the RKHS that allows for the decomposition of mean embeddings into additive functional effects. We then introduce a truncated kernel Hotelling-Lawley statistic to test the effects of the model, demonstrating that its asymptotic distribution is chi-square, which remains valid with its Nystrom approximation. We discuss a homoscedasticity assumption that, although absent in the standard two-sample case, is necessary for general designs. Finally, we illustrate our framework using a single-cell RNA sequencing dataset and provide kernel-based generalizations of classical diagnostic and exploration tools to broaden the scope of kernel testing in any experimental design.</summary></entry><entry><title type="html">Extreme value statistics of nerve transmission delay</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Extremevaluestatisticsofnervetransmissiondelay.html" rel="alternate" type="text/html" title="Extreme value statistics of nerve transmission delay" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Extremevaluestatisticsofnervetransmissiondelay</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Extremevaluestatisticsofnervetransmissiondelay.html">&lt;p&gt;Nerve transmission delay is an important topic in neuroscience. Spike signals fired or received at the dendrites of a neuron travel from the axon to the presynaptic cell. The spike signal triggers a chemical reaction at the synapse, wherein a presynaptic cell transfers neurotransmitters to the postsynaptic cell, and regenerates electrical signals by a chemical reaction process through ion channels and transmits it to neighboring neurons. In the context of describing the complex physiological reaction process as a stochastic process, this study aimed to show that the distribution of the maximum time interval of spike signals follows extreme order statistics. By considering the statistical variance in the time constant of the Leaky Integrate-and-Fire model, which is a deterministic time evolution model of spike signals, we enabled randomness in the time interval of spike signals. When the time constant follows an exponential distribution function, the time interval of the spike signal also follows an exponential distribution. In this case, our theory and simulations confirmed that the histogram of the maximum time interval follows the Gumbel distribution, which is one of the three types of extreme value statistics. We also confirmed that the histogram of the maximum time interval follows a Fr&apos;{e}chet distribution when the time interval of the spike signal follows a Pareto distribution. These findings confirm that nerve transmission delay can be described using extreme value statistics and could, therefore, be used as a new indicator for transmission delay.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.00484&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Satori Tsuzuki</name></author><category term="stat.AP" /><summary type="html">Nerve transmission delay is an important topic in neuroscience. Spike signals fired or received at the dendrites of a neuron travel from the axon to the presynaptic cell. The spike signal triggers a chemical reaction at the synapse, wherein a presynaptic cell transfers neurotransmitters to the postsynaptic cell, and regenerates electrical signals by a chemical reaction process through ion channels and transmits it to neighboring neurons. In the context of describing the complex physiological reaction process as a stochastic process, this study aimed to show that the distribution of the maximum time interval of spike signals follows extreme order statistics. By considering the statistical variance in the time constant of the Leaky Integrate-and-Fire model, which is a deterministic time evolution model of spike signals, we enabled randomness in the time interval of spike signals. When the time constant follows an exponential distribution function, the time interval of the spike signal also follows an exponential distribution. In this case, our theory and simulations confirmed that the histogram of the maximum time interval follows the Gumbel distribution, which is one of the three types of extreme value statistics. We also confirmed that the histogram of the maximum time interval follows a Fr&apos;{e}chet distribution when the time interval of the spike signal follows a Pareto distribution. These findings confirm that nerve transmission delay can be described using extreme value statistics and could, therefore, be used as a new indicator for transmission delay.</summary></entry><entry><title type="html">False Discovery Rate Control For Structured Multiple Testing: Asymmetric Rules And Conformal Q-values</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/FalseDiscoveryRateControlForStructuredMultipleTestingAsymmetricRulesAndConformalQvalues.html" rel="alternate" type="text/html" title="False Discovery Rate Control For Structured Multiple Testing: Asymmetric Rules And Conformal Q-values" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/FalseDiscoveryRateControlForStructuredMultipleTestingAsymmetricRulesAndConformalQvalues</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/FalseDiscoveryRateControlForStructuredMultipleTestingAsymmetricRulesAndConformalQvalues.html">&lt;p&gt;The effective utilization of structural information in data while ensuring statistical validity poses a significant challenge in false discovery rate (FDR) analyses. Conformal inference provides rigorous theory for grounding complex machine learning methods without relying on strong assumptions or highly idealized models. However, existing conformal methods have limitations in handling structured multiple testing. This is because their validity requires the deployment of symmetric rules, which assume the exchangeability of data points and permutation-invariance of fitting algorithms. To overcome these limitations, we introduce the pseudo local index of significance (PLIS) procedure, which is capable of accommodating asymmetric rules and requires only pairwise exchangeability between the null conformity scores. We demonstrate that PLIS offers finite-sample guarantees in FDR control and the ability to assign higher weights to relevant data points. Numerical results confirm the effectiveness and robustness of PLIS and show improvements in power compared to existing model-free methods in various scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.15322&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zinan Zhao, Wenguang Sun</name></author><category term="stat.ME" /><summary type="html">The effective utilization of structural information in data while ensuring statistical validity poses a significant challenge in false discovery rate (FDR) analyses. Conformal inference provides rigorous theory for grounding complex machine learning methods without relying on strong assumptions or highly idealized models. However, existing conformal methods have limitations in handling structured multiple testing. This is because their validity requires the deployment of symmetric rules, which assume the exchangeability of data points and permutation-invariance of fitting algorithms. To overcome these limitations, we introduce the pseudo local index of significance (PLIS) procedure, which is capable of accommodating asymmetric rules and requires only pairwise exchangeability between the null conformity scores. We demonstrate that PLIS offers finite-sample guarantees in FDR control and the ability to assign higher weights to relevant data points. Numerical results confirm the effectiveness and robustness of PLIS and show improvements in power compared to existing model-free methods in various scenarios.</summary></entry><entry><title type="html">ForLion: A New Algorithm for D-optimal Designs under General Parametric Statistical Models with Mixed Factors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ForLionANewAlgorithmforDoptimalDesignsunderGeneralParametricStatisticalModelswithMixedFactors.html" rel="alternate" type="text/html" title="ForLion: A New Algorithm for D-optimal Designs under General Parametric Statistical Models with Mixed Factors" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ForLionANewAlgorithmforDoptimalDesignsunderGeneralParametricStatisticalModelswithMixedFactors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ForLionANewAlgorithmforDoptimalDesignsunderGeneralParametricStatisticalModelswithMixedFactors.html">&lt;p&gt;In this paper, we address the problem of designing an experimental plan with both discrete and continuous factors under fairly general parametric statistical models. We propose a new algorithm, named ForLion, to search for locally optimal approximate designs under the D-criterion. The algorithm performs an exhaustive search in a design space with mixed factors while keeping high efficiency and reducing the number of distinct experimental settings. Its optimality is guaranteed by the general equivalence theorem. We present the relevant theoretical results for multinomial logit models (MLM) and generalized linear models (GLM), and demonstrate the superiority of our algorithm over state-of-the-art design algorithms using real-life experiments under MLM and GLM. Our simulation studies show that the ForLion algorithm could reduce the number of experimental settings by 25% or improve the relative efficiency of the designs by 17.5% on average. Our algorithm can help the experimenters reduce the time cost, the usage of experimental devices, and thus the total cost of their experiments while preserving high efficiencies of the designs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.09367&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yifei Huang, Keren Li, Abhyuday Mandal, Jie Yang</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">In this paper, we address the problem of designing an experimental plan with both discrete and continuous factors under fairly general parametric statistical models. We propose a new algorithm, named ForLion, to search for locally optimal approximate designs under the D-criterion. The algorithm performs an exhaustive search in a design space with mixed factors while keeping high efficiency and reducing the number of distinct experimental settings. Its optimality is guaranteed by the general equivalence theorem. We present the relevant theoretical results for multinomial logit models (MLM) and generalized linear models (GLM), and demonstrate the superiority of our algorithm over state-of-the-art design algorithms using real-life experiments under MLM and GLM. Our simulation studies show that the ForLion algorithm could reduce the number of experimental settings by 25% or improve the relative efficiency of the designs by 17.5% on average. Our algorithm can help the experimenters reduce the time cost, the usage of experimental devices, and thus the total cost of their experiments while preserving high efficiencies of the designs.</summary></entry><entry><title type="html">Gaussian Measures Conditioned on Nonlinear Observations: Consistency, MAP Estimators, and Simulation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/GaussianMeasuresConditionedonNonlinearObservationsConsistencyMAPEstimatorsandSimulation.html" rel="alternate" type="text/html" title="Gaussian Measures Conditioned on Nonlinear Observations: Consistency, MAP Estimators, and Simulation" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/GaussianMeasuresConditionedonNonlinearObservationsConsistencyMAPEstimatorsandSimulation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/GaussianMeasuresConditionedonNonlinearObservationsConsistencyMAPEstimatorsandSimulation.html">&lt;p&gt;The article presents a systematic study of the problem of conditioning a Gaussian random variable $\xi$ on nonlinear observations of the form $F \circ \phi(\xi)$ where $\phi: \mathcal{X} \to \mathbb{R}^N$ is a bounded linear operator and $F$ is nonlinear. Such problems arise in the context of Bayesian inference and recent machine learning-inspired PDE solvers. We give a representer theorem for the conditioned random variable $\xi \mid F\circ \phi(\xi)$, stating that it decomposes as the sum of an infinite-dimensional Gaussian (which is identified analytically) as well as a finite-dimensional non-Gaussian measure. We also introduce a novel notion of the mode of a conditional measure by taking the limit of the natural relaxation of the problem, to which we can apply the existing notion of maximum a posteriori estimators of posterior measures. Finally, we introduce a variant of the Laplace approximation for the efficient simulation of the aforementioned conditioned Gaussian random variables towards uncertainty quantification.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13149&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yifan Chen, Bamdad Hosseini, Houman Owhadi, Andrew M Stuart</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">The article presents a systematic study of the problem of conditioning a Gaussian random variable $\xi$ on nonlinear observations of the form $F \circ \phi(\xi)$ where $\phi: \mathcal{X} \to \mathbb{R}^N$ is a bounded linear operator and $F$ is nonlinear. Such problems arise in the context of Bayesian inference and recent machine learning-inspired PDE solvers. We give a representer theorem for the conditioned random variable $\xi \mid F\circ \phi(\xi)$, stating that it decomposes as the sum of an infinite-dimensional Gaussian (which is identified analytically) as well as a finite-dimensional non-Gaussian measure. We also introduce a novel notion of the mode of a conditional measure by taking the limit of the natural relaxation of the problem, to which we can apply the existing notion of maximum a posteriori estimators of posterior measures. Finally, we introduce a variant of the Laplace approximation for the efficient simulation of the aforementioned conditioned Gaussian random variables towards uncertainty quantification.</summary></entry><entry><title type="html">Generalised Bayes Linear Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/GeneralisedBayesLinearInference.html" rel="alternate" type="text/html" title="Generalised Bayes Linear Inference" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/GeneralisedBayesLinearInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/GeneralisedBayesLinearInference.html">&lt;p&gt;Motivated by big data and the vast parameter spaces in modern machine learning models, optimisation approaches to Bayesian inference have seen a surge in popularity in recent years. In this paper, we address the connection between the popular new methods termed generalised Bayesian inference and Bayes linear methods. We propose a further generalisation to Bayesian inference that unifies these and other recent approaches by considering the Bayesian inference problem as one of finding the closest point in a particular solution space to a data generating process, where these notions differ depending on user-specified geometries and foundational belief systems. Motivated by this framework, we propose a generalisation to Bayes linear approaches that enables fast and principled inferences that obey the coherence requirements implied by domain restrictions on random quantities. We demonstrate the efficacy of generalised Bayes linear inference on a number of examples, including monotonic regression and inference for spatial counts. This paper is accompanied by an R package available at github.com/astfalckl/bayeslinear.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14145&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lachlan Astfalck, Cassandra Bird, Daniel Williamson</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Motivated by big data and the vast parameter spaces in modern machine learning models, optimisation approaches to Bayesian inference have seen a surge in popularity in recent years. In this paper, we address the connection between the popular new methods termed generalised Bayesian inference and Bayes linear methods. We propose a further generalisation to Bayesian inference that unifies these and other recent approaches by considering the Bayesian inference problem as one of finding the closest point in a particular solution space to a data generating process, where these notions differ depending on user-specified geometries and foundational belief systems. Motivated by this framework, we propose a generalisation to Bayes linear approaches that enables fast and principled inferences that obey the coherence requirements implied by domain restrictions on random quantities. We demonstrate the efficacy of generalised Bayes linear inference on a number of examples, including monotonic regression and inference for spatial counts. This paper is accompanied by an R package available at github.com/astfalckl/bayeslinear.</summary></entry><entry><title type="html">Hidden semi-Markov models with inhomogeneous state dwell-time distributions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/HiddensemiMarkovmodelswithinhomogeneousstatedwelltimedistributions.html" rel="alternate" type="text/html" title="Hidden semi-Markov models with inhomogeneous state dwell-time distributions" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/HiddensemiMarkovmodelswithinhomogeneousstatedwelltimedistributions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/HiddensemiMarkovmodelswithinhomogeneousstatedwelltimedistributions.html">&lt;p&gt;The well-established methodology for the estimation of hidden semi-Markov models (HSMMs) as hidden Markov models (HMMs) with extended state spaces is further developed to incorporate covariate influences across all aspects of the state process model, in particular, regarding the distributions governing the state dwell time. The special case of periodically varying covariate effects on the state dwell-time distributions - and possibly the conditional transition probabilities - is examined in detail to derive important properties of such models, namely the periodically varying unconditional state distribution as well as the overall state dwell-time distribution. Through simulation studies, we ascertain key properties of these models and develop recommendations for hyperparameter settings. Furthermore, we provide a case study involving an HSMM with periodically varying dwell-time distributions to analyse the movement trajectory of an arctic muskox, demonstrating the practical relevance of the developed methodology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13553&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jan-Ole Koslik</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The well-established methodology for the estimation of hidden semi-Markov models (HSMMs) as hidden Markov models (HMMs) with extended state spaces is further developed to incorporate covariate influences across all aspects of the state process model, in particular, regarding the distributions governing the state dwell time. The special case of periodically varying covariate effects on the state dwell-time distributions - and possibly the conditional transition probabilities - is examined in detail to derive important properties of such models, namely the periodically varying unconditional state distribution as well as the overall state dwell-time distribution. Through simulation studies, we ascertain key properties of these models and develop recommendations for hyperparameter settings. Furthermore, we provide a case study involving an HSMM with periodically varying dwell-time distributions to analyse the movement trajectory of an arctic muskox, demonstrating the practical relevance of the developed methodology.</summary></entry><entry><title type="html">Identifying Causal Effects Under Functional Dependencies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/IdentifyingCausalEffectsUnderFunctionalDependencies.html" rel="alternate" type="text/html" title="Identifying Causal Effects Under Functional Dependencies" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/IdentifyingCausalEffectsUnderFunctionalDependencies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/IdentifyingCausalEffectsUnderFunctionalDependencies.html">&lt;p&gt;We study the identification of causal effects, motivated by two improvements to identifiability which can be attained if one knows that some variables in a causal graph are functionally determined by their parents (without needing to know the specific functions). First, an unidentifiable causal effect may become identifiable when certain variables are functional. Second, certain functional variables can be excluded from being observed without affecting the identifiability of a causal effect, which may significantly reduce the number of needed variables in observational data. Our results are largely based on an elimination procedure which removes functional variables from a causal graph while preserving key properties in the resulting causal graph, including the identifiability of causal effects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.04919&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yizuo Chen, Adnan Darwiche</name></author><category term="stat.ME" /><summary type="html">We study the identification of causal effects, motivated by two improvements to identifiability which can be attained if one knows that some variables in a causal graph are functionally determined by their parents (without needing to know the specific functions). First, an unidentifiable causal effect may become identifiable when certain variables are functional. Second, certain functional variables can be excluded from being observed without affecting the identifiability of a causal effect, which may significantly reduce the number of needed variables in observational data. Our results are largely based on an elimination procedure which removes functional variables from a causal graph while preserving key properties in the resulting causal graph, including the identifiability of causal effects.</summary></entry><entry><title type="html">Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/IntegratingLargeLanguageModelsinCausalDiscoveryAStatisticalCausalApproach.html" rel="alternate" type="text/html" title="Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/IntegratingLargeLanguageModelsinCausalDiscoveryAStatisticalCausalApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/IntegratingLargeLanguageModelsinCausalDiscoveryAStatisticalCausalApproach.html">&lt;p&gt;In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is significant for creating consistent meaningful causal models, despite the challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)’’ for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, by using an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve SCD on this dataset, even if this dataset has never been included in the training data of the LLM. The proposed approach can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.01454&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Masayuki Takayama, Tadahisa Okuda, Thong Pham, Tatsuyoshi Ikenoue, Shingo Fukuma, Shohei Shimizu, Akiyoshi Sannai</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is significant for creating consistent meaningful causal models, despite the challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)’’ for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, by using an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve SCD on this dataset, even if this dataset has never been included in the training data of the LLM. The proposed approach can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains.</summary></entry><entry><title type="html">Interval identification of natural effects in the presence of outcome-related unmeasured confounding</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Intervalidentificationofnaturaleffectsinthepresenceofoutcomerelatedunmeasuredconfounding.html" rel="alternate" type="text/html" title="Interval identification of natural effects in the presence of outcome-related unmeasured confounding" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Intervalidentificationofnaturaleffectsinthepresenceofoutcomerelatedunmeasuredconfounding</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Intervalidentificationofnaturaleffectsinthepresenceofoutcomerelatedunmeasuredconfounding.html">&lt;p&gt;With reference to a binary outcome and a binary mediator, we derive identification bounds for natural effects under a reduced set of assumptions. Specifically, no assumptions about confounding are made that involve the outcome; we only assume no unobserved exposure-mediator confounding as well as a condition termed partially constant cross-world dependence (PC-CWD), which poses fewer constraints on the counterfactual probabilities than the usual cross-world independence assumption. The proposed strategy can be used also to achieve interval identification of the total effect, which is no longer point identified under the considered set of assumptions. Our derivations are based on postulating a logistic regression model for the mediator as well as for the outcome. However, in both cases the functional form governing the dependence on the explanatory variables is allowed to be arbitrary, thereby resulting in a semi-parametric approach. To account for sampling variability, we provide delta-method approximations of standard errors in order to build uncertainty intervals from identification bounds. The proposed method is applied to a dataset gathered from a Spanish prospective cohort study. The aim is to evaluate whether the effect of smoking on lung cancer risk is mediated by the onset of pulmonary emphysema.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13621&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marco Doretti, Elena Stanghellini</name></author><category term="stat.ME" /><summary type="html">With reference to a binary outcome and a binary mediator, we derive identification bounds for natural effects under a reduced set of assumptions. Specifically, no assumptions about confounding are made that involve the outcome; we only assume no unobserved exposure-mediator confounding as well as a condition termed partially constant cross-world dependence (PC-CWD), which poses fewer constraints on the counterfactual probabilities than the usual cross-world independence assumption. The proposed strategy can be used also to achieve interval identification of the total effect, which is no longer point identified under the considered set of assumptions. Our derivations are based on postulating a logistic regression model for the mediator as well as for the outcome. However, in both cases the functional form governing the dependence on the explanatory variables is allowed to be arbitrary, thereby resulting in a semi-parametric approach. To account for sampling variability, we provide delta-method approximations of standard errors in order to build uncertainty intervals from identification bounds. The proposed method is applied to a dataset gathered from a Spanish prospective cohort study. The aim is to evaluate whether the effect of smoking on lung cancer risk is mediated by the onset of pulmonary emphysema.</summary></entry><entry><title type="html">Iterative Methods for Full-Scale Gaussian Process Approximations for Large Spatial Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/IterativeMethodsforFullScaleGaussianProcessApproximationsforLargeSpatialData.html" rel="alternate" type="text/html" title="Iterative Methods for Full-Scale Gaussian Process Approximations for Large Spatial Data" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/IterativeMethodsforFullScaleGaussianProcessApproximationsforLargeSpatialData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/IterativeMethodsforFullScaleGaussianProcessApproximationsforLargeSpatialData.html">&lt;p&gt;Gaussian processes are flexible probabilistic regression models which are widely used in statistics and machine learning. However, a drawback is their limited scalability to large data sets. To alleviate this, we consider full-scale approximations (FSAs) that combine predictive process methods and covariance tapering, thus approximating both global and local structures. We show how iterative methods can be used to reduce the computational costs for calculating likelihoods, gradients, and predictive distributions with FSAs. We introduce a novel preconditioner and show that it accelerates the conjugate gradient method’s convergence speed and mitigates its sensitivity with respect to the FSA parameters and the eigenvalue structure of the original covariance matrix, and we demonstrate empirically that it outperforms a state-of-the-art pivoted Cholesky preconditioner. Further, we present a novel, accurate, and fast way to calculate predictive variances relying on stochastic estimations and iterative methods. In both simulated and real-world data experiments, we find that our proposed methodology achieves the same accuracy as Cholesky-based computations with a substantial reduction in computational time. Finally, we also compare different approaches for determining inducing points in predictive process and FSA models. All methods are implemented in a free C++ software library with high-level Python and R packages.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14492&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tim Gyger, Reinhard Furrer, Fabio Sigrist</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Gaussian processes are flexible probabilistic regression models which are widely used in statistics and machine learning. However, a drawback is their limited scalability to large data sets. To alleviate this, we consider full-scale approximations (FSAs) that combine predictive process methods and covariance tapering, thus approximating both global and local structures. We show how iterative methods can be used to reduce the computational costs for calculating likelihoods, gradients, and predictive distributions with FSAs. We introduce a novel preconditioner and show that it accelerates the conjugate gradient method’s convergence speed and mitigates its sensitivity with respect to the FSA parameters and the eigenvalue structure of the original covariance matrix, and we demonstrate empirically that it outperforms a state-of-the-art pivoted Cholesky preconditioner. Further, we present a novel, accurate, and fast way to calculate predictive variances relying on stochastic estimations and iterative methods. In both simulated and real-world data experiments, we find that our proposed methodology achieves the same accuracy as Cholesky-based computations with a substantial reduction in computational time. Finally, we also compare different approaches for determining inducing points in predictive process and FSA models. All methods are implemented in a free C++ software library with high-level Python and R packages.</summary></entry><entry><title type="html">Large Skew-t Copula Models and Asymmetric Dependence in Intraday Equity Returns</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/LargeSkewtCopulaModelsandAsymmetricDependenceinIntradayEquityReturns.html" rel="alternate" type="text/html" title="Large Skew-t Copula Models and Asymmetric Dependence in Intraday Equity Returns" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/LargeSkewtCopulaModelsandAsymmetricDependenceinIntradayEquityReturns</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/LargeSkewtCopulaModelsandAsymmetricDependenceinIntradayEquityReturns.html">&lt;p&gt;Skew-t copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio (2003) allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A stochastic gradient ascent algorithm is used to solve the variational optimization. The methodology is used to estimate skew-t factor copula models with up to 15 factors for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise correlations. In a moving window study we show that the asymmetric dependencies also vary over time, and that intraday predictive densities from the skew-t copula are more accurate than those from benchmark copula models. Portfolio selection strategies based on the estimated pairwise asymmetric dependencies improve performance relative to the index.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.05564&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lin Deng, Michael Stanley Smith, Worapree Maneesoonthorn</name></author><category term="stat.CO" /><summary type="html">Skew-t copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio (2003) allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A stochastic gradient ascent algorithm is used to solve the variational optimization. The methodology is used to estimate skew-t factor copula models with up to 15 factors for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise correlations. In a moving window study we show that the asymmetric dependencies also vary over time, and that intraday predictive densities from the skew-t copula are more accurate than those from benchmark copula models. Portfolio selection strategies based on the estimated pairwise asymmetric dependencies improve performance relative to the index.</summary></entry><entry><title type="html">Long-range Ising model for regional-scale seismic risk analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/LongrangeIsingmodelforregionalscaleseismicriskanalysis.html" rel="alternate" type="text/html" title="Long-range Ising model for regional-scale seismic risk analysis" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/LongrangeIsingmodelforregionalscaleseismicriskanalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/LongrangeIsingmodelforregionalscaleseismicriskanalysis.html">&lt;p&gt;This study introduces the long-range Ising model from statistical mechanics to the Performance-Based Earthquake Engineering (PBEE) framework for regional seismic damage analysis. The application of the PBEE framework at a regional scale involves estimating the damage states of numerous structures, typically performed using fragility function-based stochastic simulations. However, these simulations often assume conditional independence or employ simplistic dependency models among the damage states of structures, leading to significant misrepresentation of regional risk. The Ising model addresses this issue by converting the available information on binary damage states (safe or failure) into a joint probability mass function, leveraging the principle of maximum entropy. The Ising model offers two main benefits: (1) it requires only the first- and second-order cross-moments, enabling seamless integration with the existing PBEE framework, and (2) it provides meaningful physical interpretations of the model parameters, facilitating the uncovering of insights not apparent from data. To demonstrate the proposed method, we applied the Ising model to 156 buildings in Antakya, Turkey, using post-hazard damage evaluation data, and to 182 buildings in Pacific Heights, San Francisco, using simulated data from the Regional Resilience Determination (R2D) tool. In both instances, the Ising model accurately reproduces the provided information and generates meaningful insights into regional damage. The study also investigates the change in Ising model parameters under varying earthquake magnitudes, along with the mean-field approximation, further facilitating the applicability of the proposed approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.11429&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sebin Oh, Sang-ri Yi, Ziqi Wang</name></author><category term="stat.AP" /><summary type="html">This study introduces the long-range Ising model from statistical mechanics to the Performance-Based Earthquake Engineering (PBEE) framework for regional seismic damage analysis. The application of the PBEE framework at a regional scale involves estimating the damage states of numerous structures, typically performed using fragility function-based stochastic simulations. However, these simulations often assume conditional independence or employ simplistic dependency models among the damage states of structures, leading to significant misrepresentation of regional risk. The Ising model addresses this issue by converting the available information on binary damage states (safe or failure) into a joint probability mass function, leveraging the principle of maximum entropy. The Ising model offers two main benefits: (1) it requires only the first- and second-order cross-moments, enabling seamless integration with the existing PBEE framework, and (2) it provides meaningful physical interpretations of the model parameters, facilitating the uncovering of insights not apparent from data. To demonstrate the proposed method, we applied the Ising model to 156 buildings in Antakya, Turkey, using post-hazard damage evaluation data, and to 182 buildings in Pacific Heights, San Francisco, using simulated data from the Regional Resilience Determination (R2D) tool. In both instances, the Ising model accurately reproduces the provided information and generates meaningful insights into regional damage. The study also investigates the change in Ising model parameters under varying earthquake magnitudes, along with the mean-field approximation, further facilitating the applicability of the proposed approach.</summary></entry><entry><title type="html">MCMC for Bayesian nonparametric mixture modeling under differential privacy</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/MCMCforBayesiannonparametricmixturemodelingunderdifferentialprivacy.html" rel="alternate" type="text/html" title="MCMC for Bayesian nonparametric mixture modeling under differential privacy" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/MCMCforBayesiannonparametricmixturemodelingunderdifferentialprivacy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/MCMCforBayesiannonparametricmixturemodelingunderdifferentialprivacy.html">&lt;p&gt;Estimating the probability density of a population while preserving the privacy of individuals in that population is an important and challenging problem that has received considerable attention in recent years. While the previous literature focused on frequentist approaches, in this paper, we propose a Bayesian nonparametric mixture model under differential privacy (DP) and present two Markov chain Monte Carlo (MCMC) algorithms for posterior inference. One is a marginal approach, resembling Neal’s algorithm 5 with a pseudo-marginal Metropolis-Hastings move, and the other is a conditional approach. Although our focus is primarily on local DP, we show that our MCMC algorithms can be easily extended to deal with global differential privacy mechanisms. Moreover, for some carefully chosen mechanisms and mixture kernels, we show how auxiliary parameters can be analytically marginalized, allowing standard MCMC algorithms (i.e., non-privatized, such as Neal’s Algorithm 2) to be efficiently employed. Our approach is general and applicable to any mixture model and privacy mechanism. In several simulations and a real case study, we discuss the performance of our algorithms and evaluate different privacy mechanisms proposed in the frequentist literature.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.09818&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mario Beraha, Stefano Favaro, Vinayak Rao</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Estimating the probability density of a population while preserving the privacy of individuals in that population is an important and challenging problem that has received considerable attention in recent years. While the previous literature focused on frequentist approaches, in this paper, we propose a Bayesian nonparametric mixture model under differential privacy (DP) and present two Markov chain Monte Carlo (MCMC) algorithms for posterior inference. One is a marginal approach, resembling Neal’s algorithm 5 with a pseudo-marginal Metropolis-Hastings move, and the other is a conditional approach. Although our focus is primarily on local DP, we show that our MCMC algorithms can be easily extended to deal with global differential privacy mechanisms. Moreover, for some carefully chosen mechanisms and mixture kernels, we show how auxiliary parameters can be analytically marginalized, allowing standard MCMC algorithms (i.e., non-privatized, such as Neal’s Algorithm 2) to be efficiently employed. Our approach is general and applicable to any mixture model and privacy mechanism. In several simulations and a real case study, we discuss the performance of our algorithms and evaluate different privacy mechanisms proposed in the frequentist literature.</summary></entry><entry><title type="html">Machine Learning for Exoplanet Detection in High-Contrast Spectroscopy: Revealing Exoplanets by Leveraging Hidden Molecular Signatures in Cross-Correlated Spectra with Convolutional Neural Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/MachineLearningforExoplanetDetectioninHighContrastSpectroscopyRevealingExoplanetsbyLeveragingHiddenMolecularSignaturesinCrossCorrelatedSpectrawithConvolutionalNeuralNetworks.html" rel="alternate" type="text/html" title="Machine Learning for Exoplanet Detection in High-Contrast Spectroscopy: Revealing Exoplanets by Leveraging Hidden Molecular Signatures in Cross-Correlated Spectra with Convolutional Neural Networks" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/MachineLearningforExoplanetDetectioninHighContrastSpectroscopyRevealingExoplanetsbyLeveragingHiddenMolecularSignaturesinCrossCorrelatedSpectrawithConvolutionalNeuralNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/MachineLearningforExoplanetDetectioninHighContrastSpectroscopyRevealingExoplanetsbyLeveragingHiddenMolecularSignaturesinCrossCorrelatedSpectrawithConvolutionalNeuralNetworks.html">&lt;p&gt;The new generation of observatories and instruments (VLT/ERIS, JWST, ELT) motivate the development of robust methods to detect and characterise faint and close-in exoplanets. Molecular mapping and cross-correlation for spectroscopy use molecular templates to isolate a planet’s spectrum from its host star. However, reliance on signal-to-noise ratio (S/N) metrics can lead to missed discoveries, due to strong assumptions of Gaussian independent and identically distributed noise. We introduce machine learning for cross-correlation spectroscopy (MLCCS); the method aims to leverage weak assumptions on exoplanet characterisation, such as the presence of specific molecules in atmospheres, to improve detection sensitivity for exoplanets. MLCCS methods, including a perceptron and unidimensional convolutional neural networks, operate in the cross-correlated spectral dimension, in which patterns from molecules can be identified. We test on mock datasets of synthetic planets inserted into real noise from SINFONI at K-band. The results from MLCCS show outstanding improvements. The outcome on a grid of faint synthetic gas giants shows that for a false discovery rate up to 5%, a perceptron can detect about 26 times the amount of planets compared to an S/N metric. This factor increases up to 77 times with convolutional neural networks, with a statistical sensitivity shift from 0.7% to 55.5%. In addition, MLCCS methods show a drastic improvement in detection confidence and conspicuity on imaging spectroscopy. Once trained, MLCCS methods offer sensitive and rapid detection of exoplanets and their molecular species in the spectral dimension. They handle systematic noise and challenging seeing conditions, can adapt to many spectroscopic instruments and modes, and are versatile regarding atmospheric characteristics, which can enable identification of various planets in archival and future data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13469&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emily O. Garvin, Markus J. Bonse, Jean Hayoz, Gabriele Cugno, Jonas Spiller, Polychronis A. Patapis, Dominique Petit Dit de la Roche, Rakesh Nath-Ranga, Olivier Absil, Nicolai F. Meinshausen, Sascha P. Quanz</name></author><category term="stat.AP" /><summary type="html">The new generation of observatories and instruments (VLT/ERIS, JWST, ELT) motivate the development of robust methods to detect and characterise faint and close-in exoplanets. Molecular mapping and cross-correlation for spectroscopy use molecular templates to isolate a planet’s spectrum from its host star. However, reliance on signal-to-noise ratio (S/N) metrics can lead to missed discoveries, due to strong assumptions of Gaussian independent and identically distributed noise. We introduce machine learning for cross-correlation spectroscopy (MLCCS); the method aims to leverage weak assumptions on exoplanet characterisation, such as the presence of specific molecules in atmospheres, to improve detection sensitivity for exoplanets. MLCCS methods, including a perceptron and unidimensional convolutional neural networks, operate in the cross-correlated spectral dimension, in which patterns from molecules can be identified. We test on mock datasets of synthetic planets inserted into real noise from SINFONI at K-band. The results from MLCCS show outstanding improvements. The outcome on a grid of faint synthetic gas giants shows that for a false discovery rate up to 5%, a perceptron can detect about 26 times the amount of planets compared to an S/N metric. This factor increases up to 77 times with convolutional neural networks, with a statistical sensitivity shift from 0.7% to 55.5%. In addition, MLCCS methods show a drastic improvement in detection confidence and conspicuity on imaging spectroscopy. Once trained, MLCCS methods offer sensitive and rapid detection of exoplanets and their molecular species in the spectral dimension. They handle systematic noise and challenging seeing conditions, can adapt to many spectroscopic instruments and modes, and are versatile regarding atmospheric characteristics, which can enable identification of various planets in archival and future data.</summary></entry><entry><title type="html">Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing Flows</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/MarkovianFlowMatchingAcceleratingMCMCwithContinuousNormalizingFlows.html" rel="alternate" type="text/html" title="Markovian Flow Matching: Accelerating MCMC with Continuous Normalizing Flows" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/MarkovianFlowMatchingAcceleratingMCMCwithContinuousNormalizingFlows</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/MarkovianFlowMatchingAcceleratingMCMCwithContinuousNormalizingFlows.html">&lt;p&gt;Continuous normalizing flows (CNFs) learn the probability path between a reference and a target density by modeling the vector field generating said path using neural networks. Recently, Lipman et al. (2022) introduced a simple and inexpensive method for training CNFs in generative modeling, termed flow matching (FM). In this paper, we re-purpose this method for probabilistic inference by incorporating Markovian sampling methods in evaluating the FM objective and using the learned probability path to improve Monte Carlo sampling. We propose a sequential method, which uses samples from a Markov chain to fix the probability path defining the FM objective. We augment this scheme with an adaptive tempering mechanism that allows the discovery of multiple modes in the target. Under mild assumptions, we establish convergence to a local optimum of the FM objective, discuss improvements in the convergence rate, and illustrate our methods on synthetic and real-world examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14392&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alberto Cabezas, Louis Sharrock, Christopher Nemeth</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Continuous normalizing flows (CNFs) learn the probability path between a reference and a target density by modeling the vector field generating said path using neural networks. Recently, Lipman et al. (2022) introduced a simple and inexpensive method for training CNFs in generative modeling, termed flow matching (FM). In this paper, we re-purpose this method for probabilistic inference by incorporating Markovian sampling methods in evaluating the FM objective and using the learned probability path to improve Monte Carlo sampling. We propose a sequential method, which uses samples from a Markov chain to fix the probability path defining the FM objective. We augment this scheme with an adaptive tempering mechanism that allows the discovery of multiple modes in the target. Under mild assumptions, we establish convergence to a local optimum of the FM objective, discuss improvements in the convergence rate, and illustrate our methods on synthetic and real-world examples.</summary></entry><entry><title type="html">Multilevel functional data analysis modeling of human glucose response to meal intake</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Multilevelfunctionaldataanalysismodelingofhumanglucoseresponsetomealintake.html" rel="alternate" type="text/html" title="Multilevel functional data analysis modeling of human glucose response to meal intake" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Multilevelfunctionaldataanalysismodelingofhumanglucoseresponsetomealintake</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Multilevelfunctionaldataanalysismodelingofhumanglucoseresponsetomealintake.html">&lt;p&gt;Glucose meal response information collected via Continuous Glucose Monitoring (CGM) is relevant to the assessment of individual metabolic status and the support of personalized diet prescriptions. However, the complexity of the data produced by CGM monitors pushes the limits of existing analytic methods. CGM data often exhibits substantial within-person variability and has a natural multilevel structure. This research is motivated by the analysis of CGM data from individuals without diabetes in the AEGIS study. The dataset includes detailed information on meal timing and nutrition for each individual over different days. The primary focus of this study is to examine CGM glucose responses following patients’ meals and explore the time-dependent associations with dietary and patient characteristics. Motivated by this problem, we propose a new analytical framework based on multilevel functional models, including a new functional mixed R-square coefficient. The use of these models illustrates 3 key points: (i) The importance of analyzing glucose responses across the entire functional domain when making diet recommendations; (ii) The differential metabolic responses between normoglycemic and prediabetic patients, particularly with regards to lipid intake; (iii) The importance of including random, person-level effects when modelling this scientific problem.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14690&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marcos Matabuena, Joe Sartini, Francisco Gude</name></author><category term="stat.AP" /><summary type="html">Glucose meal response information collected via Continuous Glucose Monitoring (CGM) is relevant to the assessment of individual metabolic status and the support of personalized diet prescriptions. However, the complexity of the data produced by CGM monitors pushes the limits of existing analytic methods. CGM data often exhibits substantial within-person variability and has a natural multilevel structure. This research is motivated by the analysis of CGM data from individuals without diabetes in the AEGIS study. The dataset includes detailed information on meal timing and nutrition for each individual over different days. The primary focus of this study is to examine CGM glucose responses following patients’ meals and explore the time-dependent associations with dietary and patient characteristics. Motivated by this problem, we propose a new analytical framework based on multilevel functional models, including a new functional mixed R-square coefficient. The use of these models illustrates 3 key points: (i) The importance of analyzing glucose responses across the entire functional domain when making diet recommendations; (ii) The differential metabolic responses between normoglycemic and prediabetic patients, particularly with regards to lipid intake; (iii) The importance of including random, person-level effects when modelling this scientific problem.</summary></entry><entry><title type="html">Nonparametric quantile regression for spatio-temporal processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Nonparametricquantileregressionforspatiotemporalprocesses.html" rel="alternate" type="text/html" title="Nonparametric quantile regression for spatio-temporal processes" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Nonparametricquantileregressionforspatiotemporalprocesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Nonparametricquantileregressionforspatiotemporalprocesses.html">&lt;p&gt;In this paper, we develop a new and effective approach to nonparametric quantile regression that accommodates ultrahigh-dimensional data arising from spatio-temporal processes. This approach proves advantageous in staving off computational challenges that constitute known hindrances to existing nonparametric quantile regression methods when the number of predictors is much larger than the available sample size. We investigate conditions under which estimation is feasible and of good overall quality and obtain sharp approximations that we employ to devising statistical inference methodology. These include simultaneous confidence intervals and tests of hypotheses, whose asymptotics is borne by a non-trivial functional central limit theorem tailored to martingale differences. Additionally, we provide finite-sample results through various simulations which, accompanied by an illustrative application to real-worldesque data (on electricity demand), offer guarantees on the performance of the proposed methodology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13783&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Soudeep Deb, Claudia Neves, Subhrajyoty Roy</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In this paper, we develop a new and effective approach to nonparametric quantile regression that accommodates ultrahigh-dimensional data arising from spatio-temporal processes. This approach proves advantageous in staving off computational challenges that constitute known hindrances to existing nonparametric quantile regression methods when the number of predictors is much larger than the available sample size. We investigate conditions under which estimation is feasible and of good overall quality and obtain sharp approximations that we employ to devising statistical inference methodology. These include simultaneous confidence intervals and tests of hypotheses, whose asymptotics is borne by a non-trivial functional central limit theorem tailored to martingale differences. Additionally, we provide finite-sample results through various simulations which, accompanied by an illustrative application to real-worldesque data (on electricity demand), offer guarantees on the performance of the proposed methodology.</summary></entry><entry><title type="html">Normalizing Basis Functions: Approximate Stationary Models for Large Spatial Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/NormalizingBasisFunctionsApproximateStationaryModelsforLargeSpatialData.html" rel="alternate" type="text/html" title="Normalizing Basis Functions: Approximate Stationary Models for Large Spatial Data" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/NormalizingBasisFunctionsApproximateStationaryModelsforLargeSpatialData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/NormalizingBasisFunctionsApproximateStationaryModelsforLargeSpatialData.html">&lt;p&gt;In geostatistics, traditional spatial models often rely on the Gaussian Process (GP) to fit stationary covariances to data. It is well known that this approach becomes computationally infeasible when dealing with large data volumes, necessitating the use of approximate methods. A powerful class of methods approximate the GP as a sum of basis functions with random coefficients. Although this technique offers computational efficiency, it does not inherently guarantee a stationary covariance. To mitigate this issue, the basis functions can be “normalized” to maintain a constant marginal variance, avoiding unwanted artifacts and edge effects. This allows for the fitting of nearly stationary models to large, potentially non-stationary datasets, providing a rigorous base to extend to more complex problems. Unfortunately, the process of normalizing these basis functions is computationally demanding. To address this, we introduce two fast and accurate algorithms to the normalization step, allowing for efficient prediction on fine grids. The practical value of these algorithms is showcased in the context of a spatial analysis on a large dataset, where significant computational speedups are achieved. While implementation and testing are done specifically within the LatticeKrig framework, these algorithms can be adapted to other basis function methods operating on regular grids.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13821&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Antony Sikorski, Daniel McKenzie, Douglas Nychka</name></author><category term="stat.CO," /><category term="stat.AP" /><summary type="html">In geostatistics, traditional spatial models often rely on the Gaussian Process (GP) to fit stationary covariances to data. It is well known that this approach becomes computationally infeasible when dealing with large data volumes, necessitating the use of approximate methods. A powerful class of methods approximate the GP as a sum of basis functions with random coefficients. Although this technique offers computational efficiency, it does not inherently guarantee a stationary covariance. To mitigate this issue, the basis functions can be “normalized” to maintain a constant marginal variance, avoiding unwanted artifacts and edge effects. This allows for the fitting of nearly stationary models to large, potentially non-stationary datasets, providing a rigorous base to extend to more complex problems. Unfortunately, the process of normalizing these basis functions is computationally demanding. To address this, we introduce two fast and accurate algorithms to the normalization step, allowing for efficient prediction on fine grids. The practical value of these algorithms is showcased in the context of a spatial analysis on a large dataset, where significant computational speedups are achieved. While implementation and testing are done specifically within the LatticeKrig framework, these algorithms can be adapted to other basis function methods operating on regular grids.</summary></entry><entry><title type="html">Omitted Labels in Causality: A Study of Paradoxes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/OmittedLabelsinCausalityAStudyofParadoxes.html" rel="alternate" type="text/html" title="Omitted Labels in Causality: A Study of Paradoxes" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/OmittedLabelsinCausalityAStudyofParadoxes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/OmittedLabelsinCausalityAStudyofParadoxes.html">&lt;p&gt;We explore what we call &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;omitted label contexts,&apos;&apos; in which training data is limited to a subset of the possible labels. This setting is common among specialized human experts or specific focused studies. We lean on well-studied paradoxes (Simpson&apos;s and Condorcet) to illustrate the more general difficulties of causal inference in omitted label contexts. Contrary to the fundamental principles on which much of causal inference is built, we show that&lt;/code&gt;correct’’ adjustments sometimes require non-exchangeable treatment and control groups. These pitfalls lead us to the study networks of conclusions drawn from different contexts and the structures the form, proving an interesting connection between these networks and social choice theory.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.06840&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bijan Mazaheri, Siddharth Jain, Matthew Cook, Jehoshua Bruck</name></author><category term="stat.ME" /><summary type="html">We explore what we call omitted label contexts,&apos;&apos; in which training data is limited to a subset of the possible labels. This setting is common among specialized human experts or specific focused studies. We lean on well-studied paradoxes (Simpson&apos;s and Condorcet) to illustrate the more general difficulties of causal inference in omitted label contexts. Contrary to the fundamental principles on which much of causal inference is built, we show thatcorrect’’ adjustments sometimes require non-exchangeable treatment and control groups. These pitfalls lead us to the study networks of conclusions drawn from different contexts and the structures the form, proving an interesting connection between these networks and social choice theory.</summary></entry><entry><title type="html">Online robust estimation and bootstrap inference for function-on-scalar regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Onlinerobustestimationandbootstrapinferenceforfunctiononscalarregression.html" rel="alternate" type="text/html" title="Online robust estimation and bootstrap inference for function-on-scalar regression" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Onlinerobustestimationandbootstrapinferenceforfunctiononscalarregression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Onlinerobustestimationandbootstrapinferenceforfunctiononscalarregression.html">&lt;p&gt;We propose a novel and robust online function-on-scalar regression technique via geometric median to learn associations between functional responses and scalar covariates based on massive or streaming datasets. The online estimation procedure, developed using the average stochastic gradient descent algorithm, offers an efficient and cost-effective method for analyzing sequentially augmented datasets, eliminating the need to store large volumes of data in memory. We establish the almost sure consistency, $L_p$ convergence, and asymptotic normality of the online estimator. To enable efficient and fast inference of the parameters of interest, including the derivation of confidence intervals, we also develop an innovative two-step online bootstrap procedure to approximate the limiting error distribution of the robust online estimator. Numerical studies under a variety of scenarios demonstrate the effectiveness and efficiency of the proposed online learning method. A real application analyzing PM$_{2.5}$ air-quality data is also included to exemplify the proposed online approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14628&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Guanghui Cheng, Wenjuan Hu, Ruitao Lin, Chen Wang</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">We propose a novel and robust online function-on-scalar regression technique via geometric median to learn associations between functional responses and scalar covariates based on massive or streaming datasets. The online estimation procedure, developed using the average stochastic gradient descent algorithm, offers an efficient and cost-effective method for analyzing sequentially augmented datasets, eliminating the need to store large volumes of data in memory. We establish the almost sure consistency, $L_p$ convergence, and asymptotic normality of the online estimator. To enable efficient and fast inference of the parameters of interest, including the derivation of confidence intervals, we also develop an innovative two-step online bootstrap procedure to approximate the limiting error distribution of the robust online estimator. Numerical studies under a variety of scenarios demonstrate the effectiveness and efficiency of the proposed online learning method. A real application analyzing PM$_{2.5}$ air-quality data is also included to exemplify the proposed online approach.</summary></entry><entry><title type="html">On the Identifying Power of Monotonicity for Average Treatment Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/OntheIdentifyingPowerofMonotonicityforAverageTreatmentEffects.html" rel="alternate" type="text/html" title="On the Identifying Power of Monotonicity for Average Treatment Effects" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/OntheIdentifyingPowerofMonotonicityforAverageTreatmentEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/OntheIdentifyingPowerofMonotonicityforAverageTreatmentEffects.html">&lt;p&gt;In the context of a binary outcome, treatment, and instrument, Balke and Pearl (1993, 1997) establish that adding monotonicity to the instrument exogeneity assumption does not decrease the identified sets for average potential outcomes and average treatment effect parameters when those assumptions are consistent with the distribution of the observable data. We show that the same results hold in the broader context of multi-valued outcome, treatment, and instrument. An important example of such a setting is a multi-arm randomized controlled trial with noncompliance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14104&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuehao Bai, Shunzhuang Huang, Sarah Moon, Azeem M. Shaikh, Edward J. Vytlacil</name></author><category term="stat.ME" /><summary type="html">In the context of a binary outcome, treatment, and instrument, Balke and Pearl (1993, 1997) establish that adding monotonicity to the instrument exogeneity assumption does not decrease the identified sets for average potential outcomes and average treatment effect parameters when those assumptions are consistent with the distribution of the observable data. We show that the same results hold in the broader context of multi-valued outcome, treatment, and instrument. An important example of such a setting is a multi-arm randomized controlled trial with noncompliance.</summary></entry><entry><title type="html">Optimal Bayesian predictive probability for delayed response in single-arm clinical trials with binary efficacy outcome</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/OptimalBayesianpredictiveprobabilityfordelayedresponseinsinglearmclinicaltrialswithbinaryefficacyoutcome.html" rel="alternate" type="text/html" title="Optimal Bayesian predictive probability for delayed response in single-arm clinical trials with binary efficacy outcome" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/OptimalBayesianpredictiveprobabilityfordelayedresponseinsinglearmclinicaltrialswithbinaryefficacyoutcome</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/OptimalBayesianpredictiveprobabilityfordelayedresponseinsinglearmclinicaltrialswithbinaryefficacyoutcome.html">&lt;p&gt;In oncology, phase II or multiple expansion cohort trials are crucial for clinical development plans. This is because they aid in identifying potent agents with sufficient activity to continue development and confirm the proof of concept. Typically, these clinical trials are single-arm trials, with the primary endpoint being short-term treatment efficacy. Despite the development of several well-designed methodologies, there may be a practical impediment in that the endpoints may be observed within a sufficient time such that adaptive go/no-go decisions can be made in a timely manner at each interim monitoring. Specifically, Response Evaluation Criteria in Solid Tumors guideline defines a confirmed response and necessitates it in non-randomized trials, where the response is the primary endpoint. However, obtaining the confirmed outcome from all participants entered at interim monitoring may be time-consuming as non-responders should be followed up until the disease progresses. Thus, this study proposed an approach to accelerate the decision-making process that incorporated the outcome without confirmation by discounting its contribution to the decision-making framework using the generalized Bayes’ theorem. Further, the behavior of the proposed approach was evaluated through a simple simulation study. The results demonstrated that the proposed approach made appropriate interim go/no-go decisions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14166&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Takuya Yoshimoto, Satoru Shinoda, Kouji Yamamoto, Kouji Tahata</name></author><category term="stat.ME" /><summary type="html">In oncology, phase II or multiple expansion cohort trials are crucial for clinical development plans. This is because they aid in identifying potent agents with sufficient activity to continue development and confirm the proof of concept. Typically, these clinical trials are single-arm trials, with the primary endpoint being short-term treatment efficacy. Despite the development of several well-designed methodologies, there may be a practical impediment in that the endpoints may be observed within a sufficient time such that adaptive go/no-go decisions can be made in a timely manner at each interim monitoring. Specifically, Response Evaluation Criteria in Solid Tumors guideline defines a confirmed response and necessitates it in non-randomized trials, where the response is the primary endpoint. However, obtaining the confirmed outcome from all participants entered at interim monitoring may be time-consuming as non-responders should be followed up until the disease progresses. Thus, this study proposed an approach to accelerate the decision-making process that incorporated the outcome without confirmation by discounting its contribution to the decision-making framework using the generalized Bayes’ theorem. Further, the behavior of the proposed approach was evaluated through a simple simulation study. The results demonstrated that the proposed approach made appropriate interim go/no-go decisions.</summary></entry><entry><title type="html">Prior-itizing Privacy: A Bayesian Approach to Setting the Privacy Budget in Differential Privacy</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/PrioritizingPrivacyABayesianApproachtoSettingthePrivacyBudgetinDifferentialPrivacy.html" rel="alternate" type="text/html" title="Prior-itizing Privacy: A Bayesian Approach to Setting the Privacy Budget in Differential Privacy" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/PrioritizingPrivacyABayesianApproachtoSettingthePrivacyBudgetinDifferentialPrivacy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/PrioritizingPrivacyABayesianApproachtoSettingthePrivacyBudgetinDifferentialPrivacy.html">&lt;p&gt;When releasing outputs from confidential data, agencies need to balance the analytical usefulness of the released data with the obligation to protect data subjects’ confidentiality. For releases satisfying differential privacy, this balance is reflected by the privacy budget, $\varepsilon$. We provide a framework for setting $\varepsilon$ based on its relationship with Bayesian posterior probabilities of disclosure. The agency responsible for the data release decides how much posterior risk it is willing to accept at various levels of prior risk, which implies a unique $\varepsilon$. Agencies can evaluate different risk profiles to determine one that leads to an acceptable trade-off in risk and utility.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.13214&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zeki Kazan, Jerome P. Reiter</name></author><category term="stat.ME" /><summary type="html">When releasing outputs from confidential data, agencies need to balance the analytical usefulness of the released data with the obligation to protect data subjects’ confidentiality. For releases satisfying differential privacy, this balance is reflected by the privacy budget, $\varepsilon$. We provide a framework for setting $\varepsilon$ based on its relationship with Bayesian posterior probabilities of disclosure. The agency responsible for the data release decides how much posterior risk it is willing to accept at various levels of prior risk, which implies a unique $\varepsilon$. Agencies can evaluate different risk profiles to determine one that leads to an acceptable trade-off in risk and utility.</summary></entry><entry><title type="html">Proximal Causal Inference With Text Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ProximalCausalInferenceWithTextData.html" rel="alternate" type="text/html" title="Proximal Causal Inference With Text Data" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ProximalCausalInferenceWithTextData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ProximalCausalInferenceWithTextData.html">&lt;p&gt;Recent text-based causal methods attempt to mitigate confounding bias by estimating proxies of confounding variables that are partially or imperfectly measured from unstructured text data. These approaches, however, assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is sometimes infeasible due to data privacy or annotation costs. In this work, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that uses multiple instances of pre-treatment text data, infers two proxies from two zero-shot models on the separate instances, and applies these proxies in the proximal g-formula. We prove that our text-based proxy method satisfies identification conditions required by the proximal g-formula while other seemingly reasonable proposals do not. We evaluate our method in synthetic and semi-synthetic settings and find that it produces estimates with low bias. To address untestable assumptions associated with the proximal g-formula, we further propose an odds ratio falsification heuristic. This new combination of proximal causal inference and zero-shot classifiers expands the set of text-specific causal methods available to practitioners.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.06687&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jacob M. Chen, Rohit Bhattacharya, Katherine A. Keith</name></author><category term="stat.ME" /><summary type="html">Recent text-based causal methods attempt to mitigate confounding bias by estimating proxies of confounding variables that are partially or imperfectly measured from unstructured text data. These approaches, however, assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is sometimes infeasible due to data privacy or annotation costs. In this work, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that uses multiple instances of pre-treatment text data, infers two proxies from two zero-shot models on the separate instances, and applies these proxies in the proximal g-formula. We prove that our text-based proxy method satisfies identification conditions required by the proximal g-formula while other seemingly reasonable proposals do not. We evaluate our method in synthetic and semi-synthetic settings and find that it produces estimates with low bias. To address untestable assumptions associated with the proximal g-formula, we further propose an odds ratio falsification heuristic. This new combination of proximal causal inference and zero-shot classifiers expands the set of text-specific causal methods available to practitioners.</summary></entry><entry><title type="html">Reinforcement Learning for Adaptive MCMC</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ReinforcementLearningforAdaptiveMCMC.html" rel="alternate" type="text/html" title="Reinforcement Learning for Adaptive MCMC" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ReinforcementLearningforAdaptiveMCMC</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ReinforcementLearningforAdaptiveMCMC.html">&lt;p&gt;An informal observation, made by several authors, is that the adaptive design of a Markov transition kernel has the flavour of a reinforcement learning task. Yet, to-date it has remained unclear how to actually exploit modern reinforcement learning technologies for adaptive MCMC. The aim of this paper is to set out a general framework, called Reinforcement Learning Metropolis–Hastings, that is theoretically supported and empirically validated. Our principal focus is on learning fast-mixing Metropolis–Hastings transition kernels, which we cast as deterministic policies and optimise via a policy gradient. Control of the learning rate provably ensures conditions for ergodicity are satisfied. The methodology is used to construct a gradient-free sampler that out-performs a popular gradient-free adaptive Metropolis–Hastings algorithm on $\approx 90 \%$ of tasks in the PosteriorDB benchmark.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13574&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Congye Wang, Wilson Chen, Heishiro Kanagawa, Chris. J. Oates</name></author><category term="stat.CO" /><summary type="html">An informal observation, made by several authors, is that the adaptive design of a Markov transition kernel has the flavour of a reinforcement learning task. Yet, to-date it has remained unclear how to actually exploit modern reinforcement learning technologies for adaptive MCMC. The aim of this paper is to set out a general framework, called Reinforcement Learning Metropolis–Hastings, that is theoretically supported and empirically validated. Our principal focus is on learning fast-mixing Metropolis–Hastings transition kernels, which we cast as deterministic policies and optimise via a policy gradient. Control of the learning rate provably ensures conditions for ergodicity are satisfied. The methodology is used to construct a gradient-free sampler that out-performs a popular gradient-free adaptive Metropolis–Hastings algorithm on $\approx 90 \%$ of tasks in the PosteriorDB benchmark.</summary></entry><entry><title type="html">Representative electricity price profiles for European day-ahead and intraday spot markets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/RepresentativeelectricitypriceprofilesforEuropeandayaheadandintradayspotmarkets.html" rel="alternate" type="text/html" title="Representative electricity price profiles for European day-ahead and intraday spot markets" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/RepresentativeelectricitypriceprofilesforEuropeandayaheadandintradayspotmarkets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/RepresentativeelectricitypriceprofilesforEuropeandayaheadandintradayspotmarkets.html">&lt;p&gt;We propose a method to construct representative price profiles of the day-ahead (DA) and the intraday (ID) electricity spot markets and use this method to provide examples of ready-to-use price data sets. In contrast to common scenario generation approaches, the method is deterministic and relies on a small number of degrees of freedom, with the aim to be well defined and easy to use. We thereby target an enhanced comparability of future research studies on demand-side management and energy cost optimization. We construct the price profiles based on historical time series from the spot markets of interest, e.g., European Power Exchange (EPEX) spot. To this end, we extract key price components from the data while also accounting for known dominant mechanisms in the price variation. Further, the method is able to preserve key statistical features of the historical data (e.g., mean and standard deviation) when constructing the benchmark profile. Finally, our approach ensures comparability of ID and DA price profiles by design, as their cumulative (integral) price can be made identical if needed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14403&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chrysanthi Papadimitriou, Jan C. Schulze, Alexander Mitsos</name></author><category term="stat.AP" /><summary type="html">We propose a method to construct representative price profiles of the day-ahead (DA) and the intraday (ID) electricity spot markets and use this method to provide examples of ready-to-use price data sets. In contrast to common scenario generation approaches, the method is deterministic and relies on a small number of degrees of freedom, with the aim to be well defined and easy to use. We thereby target an enhanced comparability of future research studies on demand-side management and energy cost optimization. We construct the price profiles based on historical time series from the spot markets of interest, e.g., European Power Exchange (EPEX) spot. To this end, we extract key price components from the data while also accounting for known dominant mechanisms in the price variation. Further, the method is able to preserve key statistical features of the historical data (e.g., mean and standard deviation) when constructing the benchmark profile. Finally, our approach ensures comparability of ID and DA price profiles by design, as their cumulative (integral) price can be made identical if needed.</summary></entry><entry><title type="html">Running in circles: is practical application feasible for data fission and data thinning in post-clustering differential analysis?</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Runningincirclesispracticalapplicationfeasiblefordatafissionanddatathinninginpostclusteringdifferentialanalysis.html" rel="alternate" type="text/html" title="Running in circles: is practical application feasible for data fission and data thinning in post-clustering differential analysis?" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Runningincirclesispracticalapplicationfeasiblefordatafissionanddatathinninginpostclusteringdifferentialanalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Runningincirclesispracticalapplicationfeasiblefordatafissionanddatathinninginpostclusteringdifferentialanalysis.html">&lt;p&gt;The standard pipeline to analyse single-cell RNA sequencing (scRNA-seq) often involves two steps : clustering and Differential Expression Analysis (DEA) to annotate cell populations based on gene expression. However, using clustering results for data-driven hypothesis formulation compromises statistical properties, especially Type I error control. Data fission was introduced to split the information contained in each observation into two independent parts that can be used for clustering and testing. However, data fission was originally designed for non-mixture distributions, and adapting it for mixtures requires knowledge of the unknown clustering structure to estimate component-specific scale parameters. As components are typically unavailable in practice, scale parameter estimators often exhibit bias. We explicitly quantify how this bias affects subsequent post-clustering differential analysis Type I error rate despite employing data fission. In response, we propose a novel approach that involves modeling each observation as a realization of its distribution, with scale parameters estimated non-parametrically. Simulations study showcase the efficacy of our method when component are clearly separated. However, the level of separability required to reach good performance presents complexities in its application to real scRNA-seq data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13591&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Benjamin Hivert, Denis Agniel, Rodolphe Thiébaut, Boris P. Hejblum</name></author><category term="stat.ME" /><summary type="html">The standard pipeline to analyse single-cell RNA sequencing (scRNA-seq) often involves two steps : clustering and Differential Expression Analysis (DEA) to annotate cell populations based on gene expression. However, using clustering results for data-driven hypothesis formulation compromises statistical properties, especially Type I error control. Data fission was introduced to split the information contained in each observation into two independent parts that can be used for clustering and testing. However, data fission was originally designed for non-mixture distributions, and adapting it for mixtures requires knowledge of the unknown clustering structure to estimate component-specific scale parameters. As components are typically unavailable in practice, scale parameter estimators often exhibit bias. We explicitly quantify how this bias affects subsequent post-clustering differential analysis Type I error rate despite employing data fission. In response, we propose a novel approach that involves modeling each observation as a realization of its distribution, with scale parameters estimated non-parametrically. Simulations study showcase the efficacy of our method when component are clearly separated. However, the level of separability required to reach good performance presents complexities in its application to real scRNA-seq data.</summary></entry><entry><title type="html">Sample size determination via learning-type curves</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Samplesizedeterminationvialearningtypecurves.html" rel="alternate" type="text/html" title="Sample size determination via learning-type curves" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Samplesizedeterminationvialearningtypecurves</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Samplesizedeterminationvialearningtypecurves.html">&lt;p&gt;This paper is concerned with sample size determination methodology for prediction models. We propose combining the individual calculations via a learning-type curve. We suggest two distinct ways of doing so, a deterministic skeleton of a learning curve and a Gaussian process centred upon its deterministic counterpart. We employ several learning algorithms for modelling the primary endpoint and distinct measures for trial efficacy. We find that the performance may vary with the sample size, but borrowing information across sample size universally improves the performance of such calculations. The Gaussian process-based learning curve appears more robust and statistically efficient, while computational efficiency is comparable. We suggest that anchoring against historical evidence when extrapolating sample sizes should be adopted when such data are available. The methods are illustrated on binary and survival endpoints.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.09575&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alimu Dayimu, Nikola Simidjievski, Nikolaos Demiris, Jean Abraham</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">This paper is concerned with sample size determination methodology for prediction models. We propose combining the individual calculations via a learning-type curve. We suggest two distinct ways of doing so, a deterministic skeleton of a learning curve and a Gaussian process centred upon its deterministic counterpart. We employ several learning algorithms for modelling the primary endpoint and distinct measures for trial efficacy. We find that the performance may vary with the sample size, but borrowing information across sample size universally improves the performance of such calculations. The Gaussian process-based learning curve appears more robust and statistically efficient, while computational efficiency is comparable. We suggest that anchoring against historical evidence when extrapolating sample sizes should be adopted when such data are available. The methods are illustrated on binary and survival endpoints.</summary></entry><entry><title type="html">Scalable Bayesian Inference for Bradley–Terry Models with Ties: An Application to Honour Based Abuse</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ScalableBayesianInferenceforBradleyTerryModelswithTiesAnApplicationtoHonourBasedAbuse.html" rel="alternate" type="text/html" title="Scalable Bayesian Inference for Bradley–Terry Models with Ties: An Application to Honour Based Abuse" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ScalableBayesianInferenceforBradleyTerryModelswithTiesAnApplicationtoHonourBasedAbuse</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ScalableBayesianInferenceforBradleyTerryModelswithTiesAnApplicationtoHonourBasedAbuse.html">&lt;p&gt;Honour based abuse covers a wide range of family abuse including female genital mutilation and forced marriage. Safeguarding professionals need to identify where abuses are happening in their local community to best support those at risk of these crimes and take preventative action. However, there is little local data about these kinds of crime. To tackle this problem, we ran comparative judgement surveys to map abuses at local level. In previous comparative judgement studies, participants reported fatigue associated with comparisons between areas with similar levels of abuse. Allowing for ties reduces fatigue, but increase the computational complexity when fitting the model. We designed an efficient Markov Chain Monte Carlo algorithm to fit the model, allowing for a wide range of prior distributions on the model parameters. Working with South Yorkshire Police and Oxford Against Cutting, we mapped the risk of honour based abuse at community level in two counties in the UK.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13399&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rowland G Seymour, Fabian Hernandez</name></author><category term="stat.AP" /><summary type="html">Honour based abuse covers a wide range of family abuse including female genital mutilation and forced marriage. Safeguarding professionals need to identify where abuses are happening in their local community to best support those at risk of these crimes and take preventative action. However, there is little local data about these kinds of crime. To tackle this problem, we ran comparative judgement surveys to map abuses at local level. In previous comparative judgement studies, participants reported fatigue associated with comparisons between areas with similar levels of abuse. Allowing for ties reduces fatigue, but increase the computational complexity when fitting the model. We designed an efficient Markov Chain Monte Carlo algorithm to fit the model, allowing for a wide range of prior distributions on the model parameters. Working with South Yorkshire Police and Oxford Against Cutting, we mapped the risk of honour based abuse at community level in two counties in the UK.</summary></entry><entry><title type="html">Scalable Bayesian inference for heat kernel Gaussian processes on manifolds</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ScalableBayesianinferenceforheatkernelGaussianprocessesonmanifolds.html" rel="alternate" type="text/html" title="Scalable Bayesian inference for heat kernel Gaussian processes on manifolds" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ScalableBayesianinferenceforheatkernelGaussianprocessesonmanifolds</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ScalableBayesianinferenceforheatkernelGaussianprocessesonmanifolds.html">&lt;p&gt;We develop scalable manifold learning methods and theory, motivated by the problem of estimating manifold of fMRI activation in the Human Connectome Project (HCP). We propose the Fast Graph Laplacian Estimation for Heat Kernel Gaussian Processes (FLGP) in the natural exponential family model. FLGP handles large sample sizes $ n $, preserves the intrinsic geometry of data, and significantly reduces computational complexity from $ \mathcal{O}(n^3) $ to $ \mathcal{O}(n) $ via a novel reduced-rank approximation of the graph Laplacian’s transition matrix and truncated Singular Value Decomposition for eigenpair computation. Our numerical experiments demonstrate FLGP’s scalability and improved accuracy for manifold learning from large-scale complex data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13342&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Junhui He, Guoxuan Ma, Jian Kang, Ying Yang</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We develop scalable manifold learning methods and theory, motivated by the problem of estimating manifold of fMRI activation in the Human Connectome Project (HCP). We propose the Fast Graph Laplacian Estimation for Heat Kernel Gaussian Processes (FLGP) in the natural exponential family model. FLGP handles large sample sizes $ n $, preserves the intrinsic geometry of data, and significantly reduces computational complexity from $ \mathcal{O}(n^3) $ to $ \mathcal{O}(n) $ via a novel reduced-rank approximation of the graph Laplacian’s transition matrix and truncated Singular Value Decomposition for eigenpair computation. Our numerical experiments demonstrate FLGP’s scalability and improved accuracy for manifold learning from large-scale complex data.</summary></entry><entry><title type="html">Sequential Bayesian inference for stochastic epidemic models of cumulative incidence</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/SequentialBayesianinferenceforstochasticepidemicmodelsofcumulativeincidence.html" rel="alternate" type="text/html" title="Sequential Bayesian inference for stochastic epidemic models of cumulative incidence" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/SequentialBayesianinferenceforstochasticepidemicmodelsofcumulativeincidence</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/SequentialBayesianinferenceforstochasticepidemicmodelsofcumulativeincidence.html">&lt;p&gt;Epidemics are inherently stochastic, and stochastic models provide an appropriate way to describe and analyse such phenomena. Given temporal incidence data consisting of, for example, the number of new infections or removals in a given time window, a continuous-time discrete-valued Markov process provides a natural description of the dynamics of each model component, typically taken to be the number of susceptible, exposed, infected or removed individuals. Fitting the SEIR model to time-course data is a challenging problem due incomplete observations and, consequently, the intractability of the observed data likelihood. Whilst sampling based inference schemes such as Markov chain Monte Carlo are routinely applied, their computational cost typically restricts analysis to data sets of no more than a few thousand infective cases. Instead, we develop a sequential inference scheme that makes use of a computationally cheap approximation of the most natural Markov process model. Crucially, the resulting model allows a tractable conditional parameter posterior which can be summarised in terms of a set of low dimensional statistics. This is used to rejuvenate parameter samples in conjunction with a novel bridge construct for propagating state trajectories conditional on the next observation of cumulative incidence. The resulting inference framework also allows for stochastic infection and reporting rates. We illustrate our approach using synthetic and real data applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13537&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sam A. Whitaker, Andrew Golightly, Colin S. Gillespie, Theodore Kypraios</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Epidemics are inherently stochastic, and stochastic models provide an appropriate way to describe and analyse such phenomena. Given temporal incidence data consisting of, for example, the number of new infections or removals in a given time window, a continuous-time discrete-valued Markov process provides a natural description of the dynamics of each model component, typically taken to be the number of susceptible, exposed, infected or removed individuals. Fitting the SEIR model to time-course data is a challenging problem due incomplete observations and, consequently, the intractability of the observed data likelihood. Whilst sampling based inference schemes such as Markov chain Monte Carlo are routinely applied, their computational cost typically restricts analysis to data sets of no more than a few thousand infective cases. Instead, we develop a sequential inference scheme that makes use of a computationally cheap approximation of the most natural Markov process model. Crucially, the resulting model allows a tractable conditional parameter posterior which can be summarised in terms of a set of low dimensional statistics. This is used to rejuvenate parameter samples in conjunction with a novel bridge construct for propagating state trajectories conditional on the next observation of cumulative incidence. The resulting inference framework also allows for stochastic infection and reporting rates. We illustrate our approach using synthetic and real data applications.</summary></entry><entry><title type="html">Shift-invariant homogeneous classes of random fields</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Shiftinvarianthomogeneousclassesofrandomfields.html" rel="alternate" type="text/html" title="Shift-invariant homogeneous classes of random fields" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Shiftinvarianthomogeneousclassesofrandomfields</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Shiftinvarianthomogeneousclassesofrandomfields.html">&lt;p&gt;Given an $R^d$-valued random field (rf) $Z(t),t\in T$ and an $\alpha$-homogeneous mapping $\kappa$ we define the corresponding equivalent class of rf’s (denoted by $K_\alpha$) which include representers of the same tail measure $\nu_Z$. When $T$ is an additive group, tractable equivalent classes of interest are the shift-invariant ones, which contain in particular all independent random shifts of $Z$. This contribution is mainly concerned with the investigation of the probabilistic properties of shift-invariant $K_\alpha$’s. Important objects introduced in our setting are tail and spectral tail rf’s. Further, the class of universal maps $U$ acting on elements of $K_\alpha$ turns out to be crucial for properties of functionals of $Z$. Applications of our findings concern max-stable and symmetric $\alpha$-stable rf’s, their maximal indices as well as their random shift-representations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.00792&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Enkelejd Hashorva</name></author><category term="stat.AP" /><summary type="html">Given an $R^d$-valued random field (rf) $Z(t),t\in T$ and an $\alpha$-homogeneous mapping $\kappa$ we define the corresponding equivalent class of rf’s (denoted by $K_\alpha$) which include representers of the same tail measure $\nu_Z$. When $T$ is an additive group, tractable equivalent classes of interest are the shift-invariant ones, which contain in particular all independent random shifts of $Z$. This contribution is mainly concerned with the investigation of the probabilistic properties of shift-invariant $K_\alpha$’s. Important objects introduced in our setting are tail and spectral tail rf’s. Further, the class of universal maps $U$ acting on elements of $K_\alpha$ turns out to be crucial for properties of functionals of $Z$. Applications of our findings concern max-stable and symmetric $\alpha$-stable rf’s, their maximal indices as well as their random shift-representations.</summary></entry><entry><title type="html">Skew-symmetric schemes for stochastic differential equations with non-Lipschitz drift: an unadjusted Barker algorithm</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/SkewsymmetricschemesforstochasticdifferentialequationswithnonLipschitzdriftanunadjustedBarkeralgorithm.html" rel="alternate" type="text/html" title="Skew-symmetric schemes for stochastic differential equations with non-Lipschitz drift: an unadjusted Barker algorithm" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/SkewsymmetricschemesforstochasticdifferentialequationswithnonLipschitzdriftanunadjustedBarkeralgorithm</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/SkewsymmetricschemesforstochasticdifferentialequationswithnonLipschitzdriftanunadjustedBarkeralgorithm.html">&lt;p&gt;We propose a new simple and explicit numerical scheme for time-homogeneous stochastic differential equations. The scheme is based on sampling increments at each time step from a skew-symmetric probability distribution, with the level of skewness determined by the drift and volatility of the underlying process. We show that as the step-size decreases the scheme converges weakly to the diffusion of interest. We then consider the problem of simulating from the limiting distribution of an ergodic diffusion process using the numerical scheme with a fixed step-size. We establish conditions under which the numerical scheme converges to equilibrium at a geometric rate, and quantify the bias between the equilibrium distributions of the scheme and of the true diffusion process. Notably, our results do not require a global Lipschitz assumption on the drift, in contrast to those required for the Euler–Maruyama scheme for long-time simulation at fixed step-sizes. Our weak convergence result relies on an extension of the theory of Milstein \&amp;amp; Tretyakov to stochastic differential equations with non-Lipschitz drift, which could also be of independent interest. We support our theoretical results with numerical simulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14373&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Samuel Livingstone, Nikolas Nüsken, Giorgos Vasdekis, Rui-Yang Zhang</name></author><category term="stat.CO" /><summary type="html">We propose a new simple and explicit numerical scheme for time-homogeneous stochastic differential equations. The scheme is based on sampling increments at each time step from a skew-symmetric probability distribution, with the level of skewness determined by the drift and volatility of the underlying process. We show that as the step-size decreases the scheme converges weakly to the diffusion of interest. We then consider the problem of simulating from the limiting distribution of an ergodic diffusion process using the numerical scheme with a fixed step-size. We establish conditions under which the numerical scheme converges to equilibrium at a geometric rate, and quantify the bias between the equilibrium distributions of the scheme and of the true diffusion process. Notably, our results do not require a global Lipschitz assumption on the drift, in contrast to those required for the Euler–Maruyama scheme for long-time simulation at fixed step-sizes. Our weak convergence result relies on an extension of the theory of Milstein \&amp;amp; Tretyakov to stochastic differential equations with non-Lipschitz drift, which could also be of independent interest. We support our theoretical results with numerical simulations.</summary></entry><entry><title type="html">Some models are useful, but for how long?: A decision theoretic approach to choosing when to refit large-scale prediction models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/SomemodelsareusefulbutforhowlongAdecisiontheoreticapproachtochoosingwhentorefitlargescalepredictionmodels.html" rel="alternate" type="text/html" title="Some models are useful, but for how long?: A decision theoretic approach to choosing when to refit large-scale prediction models" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/SomemodelsareusefulbutforhowlongAdecisiontheoreticapproachtochoosingwhentorefitlargescalepredictionmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/SomemodelsareusefulbutforhowlongAdecisiontheoreticapproachtochoosingwhentorefitlargescalepredictionmodels.html">&lt;p&gt;Large-scale prediction models (typically using tools from artificial intelligence, AI, or machine learning, ML) are increasingly ubiquitous across a variety of industries and scientific domains. Such methods are often paired with detailed data from sources such as electronic health records, wearable sensors, and omics data (high-throughput technology used to understand biology). Despite their utility, implementing AI and ML tools at the scale necessary to work with this data introduces two major challenges. First, it can cost tens of thousands of dollars to train a modern AI/ML model at scale. Second, once the model is trained, its predictions may become less relevant as patient and provider behavior change, and predictions made for one geographical area may be less accurate for another. These two challenges raise a fundamental question: how often should you refit the AI/ML model to optimally trade-off between cost and relevance? Our work provides a framework for making decisions about when to {\it refit} AI/ML models when the goal is to maintain valid statistical inference (e.g. estimating a treatment effect in a clinical trial). Drawing on portfolio optimization theory, we treat the decision of {\it recalibrating} versus {\it refitting} the model as a choice between ‘‘investing’’ in one of two ‘‘assets.’’ One asset, recalibrating the model based on another model, is quick and relatively inexpensive but bears uncertainty from sampling and the possibility that the other model is not relevant to current circumstances. The other asset, {\it refitting} the model, is costly but removes the irrelevance concern (though not the risk of sampling error). We explore the balancing act between these two potential investments in this paper.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13926&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kentaro Hoffman, Stephen Salerno, Jeff Leek, Tyler McCormick</name></author><category term="stat.ME" /><summary type="html">Large-scale prediction models (typically using tools from artificial intelligence, AI, or machine learning, ML) are increasingly ubiquitous across a variety of industries and scientific domains. Such methods are often paired with detailed data from sources such as electronic health records, wearable sensors, and omics data (high-throughput technology used to understand biology). Despite their utility, implementing AI and ML tools at the scale necessary to work with this data introduces two major challenges. First, it can cost tens of thousands of dollars to train a modern AI/ML model at scale. Second, once the model is trained, its predictions may become less relevant as patient and provider behavior change, and predictions made for one geographical area may be less accurate for another. These two challenges raise a fundamental question: how often should you refit the AI/ML model to optimally trade-off between cost and relevance? Our work provides a framework for making decisions about when to {\it refit} AI/ML models when the goal is to maintain valid statistical inference (e.g. estimating a treatment effect in a clinical trial). Drawing on portfolio optimization theory, we treat the decision of {\it recalibrating} versus {\it refitting} the model as a choice between ‘‘investing’’ in one of two ‘‘assets.’’ One asset, recalibrating the model based on another model, is quick and relatively inexpensive but bears uncertainty from sampling and the possibility that the other model is not relevant to current circumstances. The other asset, {\it refitting} the model, is costly but removes the irrelevance concern (though not the risk of sampling error). We explore the balancing act between these two potential investments in this paper.</summary></entry><entry><title type="html">Statistical inference for high-dimensional convoluted rank regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Statisticalinferenceforhighdimensionalconvolutedrankregression.html" rel="alternate" type="text/html" title="Statistical inference for high-dimensional convoluted rank regression" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Statisticalinferenceforhighdimensionalconvolutedrankregression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Statisticalinferenceforhighdimensionalconvolutedrankregression.html">&lt;p&gt;High-dimensional penalized rank regression is a powerful tool for modeling high-dimensional data due to its robustness and estimation efficiency. However, the non-smoothness of the rank loss brings great challenges to the computation. To solve this critical issue, high-dimensional convoluted rank regression is recently proposed, and penalized convoluted rank regression estimators are introduced. However, these developed estimators cannot be directly used to make inference. In this paper, we investigate the inference problem of high-dimensional convoluted rank regression. We first establish estimation error bounds of penalized convoluted rank regression estimators under weaker conditions on the predictors. Based on the penalized convoluted rank regression estimators, we further introduce a debiased estimator. We then provide Bahadur representation for our proposed estimator. We further develop simultaneous inference procedures. A novel bootstrap procedure is proposed and its theoretical validity is also established. Finally, simulation and real data analysis are conducted to illustrate the merits of our proposed methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14652&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Leheng Cai, Xu Guo, Heng Lian, Liping Zhu</name></author><category term="stat.ME" /><summary type="html">High-dimensional penalized rank regression is a powerful tool for modeling high-dimensional data due to its robustness and estimation efficiency. However, the non-smoothness of the rank loss brings great challenges to the computation. To solve this critical issue, high-dimensional convoluted rank regression is recently proposed, and penalized convoluted rank regression estimators are introduced. However, these developed estimators cannot be directly used to make inference. In this paper, we investigate the inference problem of high-dimensional convoluted rank regression. We first establish estimation error bounds of penalized convoluted rank regression estimators under weaker conditions on the predictors. Based on the penalized convoluted rank regression estimators, we further introduce a debiased estimator. We then provide Bahadur representation for our proposed estimator. We further develop simultaneous inference procedures. A novel bootstrap procedure is proposed and its theoretical validity is also established. Finally, simulation and real data analysis are conducted to illustrate the merits of our proposed methods.</summary></entry><entry><title type="html">Treatment Effects in Extreme Regimes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/TreatmentEffectsinExtremeRegimes.html" rel="alternate" type="text/html" title="Treatment Effects in Extreme Regimes" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/TreatmentEffectsinExtremeRegimes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/TreatmentEffectsinExtremeRegimes.html">&lt;p&gt;Understanding treatment effects in extreme regimes is important for characterizing risks associated with different interventions. This is hindered by the unavailability of counterfactual outcomes and the rarity and difficulty of collecting extreme data in practice. To address this issue, we propose a new framework based on extreme value theory for estimating treatment effects in extreme regimes. We quantify these effects using variations in tail decay rates of potential outcomes in the presence and absence of treatments. We establish algorithms for calculating these quantities and develop related theoretical results. We demonstrate the efficacy of our approach on various standard synthetic and semi-synthetic datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.11697&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ahmed Aloui, Ali Hasan, Yuting Ng, Miroslav Pajic, Vahid Tarokh</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Understanding treatment effects in extreme regimes is important for characterizing risks associated with different interventions. This is hindered by the unavailability of counterfactual outcomes and the rarity and difficulty of collecting extreme data in practice. To address this issue, we propose a new framework based on extreme value theory for estimating treatment effects in extreme regimes. We quantify these effects using variations in tail decay rates of potential outcomes in the presence and absence of treatments. We establish algorithms for calculating these quantities and develop related theoretical results. We demonstrate the efficacy of our approach on various standard synthetic and semi-synthetic datasets.</summary></entry><entry><title type="html">Unbiased Kinetic Langevin Monte Carlo with Inexact Gradients</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/UnbiasedKineticLangevinMonteCarlowithInexactGradients.html" rel="alternate" type="text/html" title="Unbiased Kinetic Langevin Monte Carlo with Inexact Gradients" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/UnbiasedKineticLangevinMonteCarlowithInexactGradients</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/UnbiasedKineticLangevinMonteCarlowithInexactGradients.html">&lt;p&gt;We present an unbiased method for Bayesian posterior means based on kinetic Langevin dynamics that combines advanced splitting methods with enhanced gradient approximations. Our approach avoids Metropolis correction by coupling Markov chains at different discretization levels in a multilevel Monte Carlo approach. Theoretical analysis demonstrates that our proposed estimator is unbiased, attains finite variance, and satisfies a central limit theorem. It can achieve accuracy $\epsilon&amp;gt;0$ for estimating expectations of Lipschitz functions in $d$ dimensions with $\mathcal{O}(d^{1/4}\epsilon^{-2})$ expected gradient evaluations, without assuming warm start. We exhibit similar bounds using both approximate and stochastic gradients, and our method’s computational cost is shown to scale independently of the size of the dataset. The proposed method is tested using a multinomial regression problem on the MNIST dataset and a Poisson regression model for soccer scores. Experiments indicate that the number of gradient evaluations per effective sample is independent of dimension, even when using inexact gradients. For product distributions, we give dimension-independent variance bounds. Our results demonstrate that the unbiased algorithm we present can be much more efficient than the ``gold-standard” randomized Hamiltonian Monte Carlo.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.05025&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Neil K. Chada, Benedict Leimkuhler, Daniel Paulin, Peter A. Whalley</name></author><category term="stat.CO," /><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We present an unbiased method for Bayesian posterior means based on kinetic Langevin dynamics that combines advanced splitting methods with enhanced gradient approximations. Our approach avoids Metropolis correction by coupling Markov chains at different discretization levels in a multilevel Monte Carlo approach. Theoretical analysis demonstrates that our proposed estimator is unbiased, attains finite variance, and satisfies a central limit theorem. It can achieve accuracy $\epsilon&amp;gt;0$ for estimating expectations of Lipschitz functions in $d$ dimensions with $\mathcal{O}(d^{1/4}\epsilon^{-2})$ expected gradient evaluations, without assuming warm start. We exhibit similar bounds using both approximate and stochastic gradients, and our method’s computational cost is shown to scale independently of the size of the dataset. The proposed method is tested using a multinomial regression problem on the MNIST dataset and a Poisson regression model for soccer scores. Experiments indicate that the number of gradient evaluations per effective sample is independent of dimension, even when using inexact gradients. For product distributions, we give dimension-independent variance bounds. Our results demonstrate that the unbiased algorithm we present can be much more efficient than the ``gold-standard” randomized Hamiltonian Monte Carlo.</summary></entry><entry><title type="html">Valores extremos de inflación en Costa Rica</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Valoresextremosdeinflaci%C3%B3nenCostaRica.html" rel="alternate" type="text/html" title="Valores extremos de inflación en Costa Rica" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Valoresextremosdeinflaci%C3%B3nenCostaRica</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/Valoresextremosdeinflaci%C3%B3nenCostaRica.html">&lt;p&gt;Maintaining low, non-negative and stable inflation levels is a necessary condition for the stability of the economy as a whole, because the monetary authorities of most industrialized countries, including the Central Bank of Costa Rica since 2005, they have oriented their monetary policy precisely to that task. Still Thus, both in Costa Rica and internationally, most of the statistical modeling of inflation has been limited to modeling their expectancy conditional on different covariates using linear models. This implies a lack of knowledge of the dynamics of the extreme values of the inflation rate and how these are related with other macroeconomic variables. In Costa Rica this is of particular importance since in several periods Negative quarter-on-quarter inflation rates have recently been experienced, which can be problematic if this becomes a recurring phenomenon. Therefore, in this work we propose to answer what is the relationship between the gap of GDP, inflation expectations, imported inflation rate, and the extreme values of the inflation rate in Costa Rica. That is, the main objective is to determine the relationship between the extreme values of the the inflation rate, GDP gap, inflation expectations and imported inflation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13251&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Daniel Aguilar, Breyner Chacón</name></author><category term="stat.AP" /><summary type="html">Maintaining low, non-negative and stable inflation levels is a necessary condition for the stability of the economy as a whole, because the monetary authorities of most industrialized countries, including the Central Bank of Costa Rica since 2005, they have oriented their monetary policy precisely to that task. Still Thus, both in Costa Rica and internationally, most of the statistical modeling of inflation has been limited to modeling their expectancy conditional on different covariates using linear models. This implies a lack of knowledge of the dynamics of the extreme values of the inflation rate and how these are related with other macroeconomic variables. In Costa Rica this is of particular importance since in several periods Negative quarter-on-quarter inflation rates have recently been experienced, which can be problematic if this becomes a recurring phenomenon. Therefore, in this work we propose to answer what is the relationship between the gap of GDP, inflation expectations, imported inflation rate, and the extreme values of the inflation rate in Costa Rica. That is, the main objective is to determine the relationship between the extreme values of the the inflation rate, GDP gap, inflation expectations and imported inflation.</summary></entry><entry><title type="html">Watermarking Generative Tabular Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/WatermarkingGenerativeTabularData.html" rel="alternate" type="text/html" title="Watermarking Generative Tabular Data" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/WatermarkingGenerativeTabularData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/WatermarkingGenerativeTabularData.html">&lt;p&gt;In this paper, we introduce a simple yet effective tabular data watermarking mechanism with statistical guarantees. We show theoretically that the proposed watermark can be effectively detected, while faithfully preserving the data fidelity, and also demonstrates appealing robustness against additive noise attack. The general idea is to achieve the watermarking through a strategic embedding based on simple data binning. Specifically, it divides the feature’s value range into finely segmented intervals and embeds watermarks into selected ``green list” intervals. To detect the watermarks, we develop a principled statistical hypothesis-testing framework with minimal assumptions: it remains valid as long as the underlying data distribution has a continuous density function. The watermarking efficacy is demonstrated through rigorous theoretical analysis and empirical validation, highlighting its utility in enhancing the security of synthetic and real-world datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14018&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hengzhi He, Peiyu Yu, Junpeng Ren, Ying Nian Wu, Guang Cheng</name></author><category term="stat.AP" /><summary type="html">In this paper, we introduce a simple yet effective tabular data watermarking mechanism with statistical guarantees. We show theoretically that the proposed watermark can be effectively detected, while faithfully preserving the data fidelity, and also demonstrates appealing robustness against additive noise attack. The general idea is to achieve the watermarking through a strategic embedding based on simple data binning. Specifically, it divides the feature’s value range into finely segmented intervals and embeds watermarks into selected ``green list” intervals. To detect the watermarks, we develop a principled statistical hypothesis-testing framework with minimal assumptions: it remains valid as long as the underlying data distribution has a continuous density function. The watermarking efficacy is demonstrated through rigorous theoretical analysis and empirical validation, highlighting its utility in enhancing the security of synthetic and real-world datasets.</summary></entry><entry><title type="html">Zero-inflation in the Multivariate Poisson Lognormal Family</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ZeroinflationintheMultivariatePoissonLognormalFamily.html" rel="alternate" type="text/html" title="Zero-inflation in the Multivariate Poisson Lognormal Family" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ZeroinflationintheMultivariatePoissonLognormalFamily</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/ZeroinflationintheMultivariatePoissonLognormalFamily.html">&lt;p&gt;Analyzing high-dimensional count data is a challenge and statistical model-based approaches provide an adequate and efficient framework that preserves explainability. The (multivariate) Poisson-Log-Normal (PLN) model is one such model: it assumes count data are driven by an underlying structured latent Gaussian variable, so that the dependencies between counts solely stems from the latent dependencies. However PLN doesn’t account for zero-inflation, a feature frequently observed in real-world datasets. Here we introduce the Zero-Inflated PLN (ZIPLN) model, adding a multivariate zero-inflated component to the model, as an additional Bernoulli latent variable. The Zero-Inflation can be fixed, site-specific, feature-specific or depends on covariates. We estimate model parameters using variational inference that scales up to datasets with a few thousands variables and compare two approximations: (i) independent Gaussian and Bernoulli variational distributions or (ii) Gaussian variational distribution conditioned on the Bernoulli one. The method is assessed on synthetic data and the efficiency of ZIPLN is established even when zero-inflation concerns up to $90\%$ of the observed counts. We then apply both ZIPLN and PLN to a cow microbiome dataset, containing $90.6\%$ of zeroes. Accounting for zero-inflation significantly increases log-likelihood and reduces dispersion in the latent space, thus leading to improved group discrimination.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14711&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bastien Batardière, Julien Chiquet, François Gindraud, Mahendra Mariadassou</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Analyzing high-dimensional count data is a challenge and statistical model-based approaches provide an adequate and efficient framework that preserves explainability. The (multivariate) Poisson-Log-Normal (PLN) model is one such model: it assumes count data are driven by an underlying structured latent Gaussian variable, so that the dependencies between counts solely stems from the latent dependencies. However PLN doesn’t account for zero-inflation, a feature frequently observed in real-world datasets. Here we introduce the Zero-Inflated PLN (ZIPLN) model, adding a multivariate zero-inflated component to the model, as an additional Bernoulli latent variable. The Zero-Inflation can be fixed, site-specific, feature-specific or depends on covariates. We estimate model parameters using variational inference that scales up to datasets with a few thousands variables and compare two approximations: (i) independent Gaussian and Bernoulli variational distributions or (ii) Gaussian variational distribution conditioned on the Bernoulli one. The method is assessed on synthetic data and the efficiency of ZIPLN is established even when zero-inflation concerns up to $90\%$ of the observed counts. We then apply both ZIPLN and PLN to a cow microbiome dataset, containing $90.6\%$ of zeroes. Accounting for zero-inflation significantly increases log-likelihood and reduces dispersion in the latent space, thus leading to improved group discrimination.</summary></entry><entry><title type="html">fsemipar: an R package for SoF semiparametric regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/fsemiparanRpackageforSoFsemiparametricregression.html" rel="alternate" type="text/html" title="fsemipar: an R package for SoF semiparametric regression" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/fsemiparanRpackageforSoFsemiparametricregression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/fsemiparanRpackageforSoFsemiparametricregression.html">&lt;p&gt;Functional data analysis has become a tool of interest in applied areas such as economics, medicine, and chemistry. Among the techniques developed in recent literature, functional semiparametric regression stands out for its balance between flexible modelling and output interpretation. Despite the large variety of research papers dealing with scalar-on-function (SoF) semiparametric models, there is a notable gap in software tools for their implementation. This article introduces the R package \texttt{fsemipar}, tailored for these models. \texttt{fsemipar} not only estimates functional single-index models using kernel smoothing techniques but also estimates and selects relevant scalar variables in semi-functional models with multivariate linear components. A standout feature is its ability to identify impact points of a curve on the response, even in models with multiple functional covariates, and to integrate both continuous and pointwise effects of functional predictors within a single model. In addition, it allows the use of location-adaptive estimators based on the $k$-nearest-neighbours approach for all the semiparametric models included. Its flexible interface empowers users to customise a wide range of input parameters and includes the standard S3 methods for prediction, statistical analysis, and estimate visualization (\texttt{predict}, \texttt{summary}, \texttt{print}, and \texttt{plot}), enhancing clear result interpretation. Throughout the article, we illustrate the functionalities and the practicality of \texttt{fsemipar} using two chemometric datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14048&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Silvia Novo, Germán Aneiros</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Functional data analysis has become a tool of interest in applied areas such as economics, medicine, and chemistry. Among the techniques developed in recent literature, functional semiparametric regression stands out for its balance between flexible modelling and output interpretation. Despite the large variety of research papers dealing with scalar-on-function (SoF) semiparametric models, there is a notable gap in software tools for their implementation. This article introduces the R package \texttt{fsemipar}, tailored for these models. \texttt{fsemipar} not only estimates functional single-index models using kernel smoothing techniques but also estimates and selects relevant scalar variables in semi-functional models with multivariate linear components. A standout feature is its ability to identify impact points of a curve on the response, even in models with multiple functional covariates, and to integrate both continuous and pointwise effects of functional predictors within a single model. In addition, it allows the use of location-adaptive estimators based on the $k$-nearest-neighbours approach for all the semiparametric models included. Its flexible interface empowers users to customise a wide range of input parameters and includes the standard S3 methods for prediction, statistical analysis, and estimate visualization (\texttt{predict}, \texttt{summary}, \texttt{print}, and \texttt{plot}), enhancing clear result interpretation. Throughout the article, we illustrate the functionalities and the practicality of \texttt{fsemipar} using two chemometric datasets.</summary></entry><entry><title type="html">pencal: an R Package for the Dynamic Prediction of Survival with Many Longitudinal Predictors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/pencalanRPackagefortheDynamicPredictionofSurvivalwithManyLongitudinalPredictors.html" rel="alternate" type="text/html" title="pencal: an R Package for the Dynamic Prediction of Survival with Many Longitudinal Predictors" /><published>2024-05-24T00:00:00+00:00</published><updated>2024-05-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/pencalanRPackagefortheDynamicPredictionofSurvivalwithManyLongitudinalPredictors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/24/pencalanRPackagefortheDynamicPredictionofSurvivalwithManyLongitudinalPredictors.html">&lt;p&gt;In survival analysis, longitudinal information on the health status of a patient can be used to dynamically update the predicted probability that a patient will experience an event of interest. Traditional approaches to dynamic prediction such as joint models become computationally unfeasible with more than a handful of longitudinal covariates, warranting the development of methods that can handle a larger number of longitudinal covariates. We introduce the R package pencal, which implements a Penalized Regression Calibration approach that makes it possible to handle many longitudinal covariates as predictors of survival. pencal uses mixed-effects models to summarize the trajectories of the longitudinal covariates up to a prespecified landmark time, and a penalized Cox model to predict survival based on both baseline covariates and summary measures of the longitudinal covariates. This article illustrates the structure of the R package, provides a step by step example showing how to estimate PRC, compute dynamic predictions of survival and validate performance, and shows how parallelization can be used to significantly reduce computing time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.15600&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mirko Signorelli</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">In survival analysis, longitudinal information on the health status of a patient can be used to dynamically update the predicted probability that a patient will experience an event of interest. Traditional approaches to dynamic prediction such as joint models become computationally unfeasible with more than a handful of longitudinal covariates, warranting the development of methods that can handle a larger number of longitudinal covariates. We introduce the R package pencal, which implements a Penalized Regression Calibration approach that makes it possible to handle many longitudinal covariates as predictors of survival. pencal uses mixed-effects models to summarize the trajectories of the longitudinal covariates up to a prespecified landmark time, and a penalized Cox model to predict survival based on both baseline covariates and summary measures of the longitudinal covariates. This article illustrates the structure of the R package, provides a step by step example showing how to estimate PRC, compute dynamic predictions of survival and validate performance, and shows how parallelization can be used to significantly reduce computing time.</summary></entry></feed>