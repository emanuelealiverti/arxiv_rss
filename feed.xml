<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-07-01T07:15:17+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Closed-Form Solution to the 2-Sample Problem for Quantifying Changes in Gene Expression using Bayes Factors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/AClosedFormSolutiontothe2SampleProblemforQuantifyingChangesinGeneExpressionusingBayesFactors.html" rel="alternate" type="text/html" title="A Closed-Form Solution to the 2-Sample Problem for Quantifying Changes in Gene Expression using Bayes Factors" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/AClosedFormSolutiontothe2SampleProblemforQuantifyingChangesinGeneExpressionusingBayesFactors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/AClosedFormSolutiontothe2SampleProblemforQuantifyingChangesinGeneExpressionusingBayesFactors.html">&lt;p&gt;Sequencing technologies have revolutionised the field of molecular biology. We now have the ability to routinely capture the complete RNA profile in tissue samples. This wealth of data allows for comparative analyses of RNA levels at different times, shedding light on the dynamics of developmental processes, and under different environmental responses, providing insights into gene expression regulation and stress responses. However, given the inherent variability of the data stemming from biological and technological sources, quantifying changes in gene expression proves to be a statistical challenge. Here, we present a closed-form Bayesian solution to this problem. Our approach is tailored to the differential gene expression analysis of processed RNA-Seq data. The framework unifies and streamlines an otherwise complex analysis, typically involving parameter estimations and multiple statistical tests, into a concise mathematical equation for the calculation of Bayes factors. Using conjugate priors we can solve the equations analytically. For each gene, we calculate a Bayes factor, which can be used for ranking genes according to the statistical evidence for the gene’s expression change given RNA-Seq data. The presented closed-form solution is derived under minimal assumptions and may be applied to a variety of other 2-sample problems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19989&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Franziska Hoerbst, Gurpinder Singh Sidhu, Melissa Tomkins, Richard J. Morris</name></author><category term="stat.ME" /><summary type="html">Sequencing technologies have revolutionised the field of molecular biology. We now have the ability to routinely capture the complete RNA profile in tissue samples. This wealth of data allows for comparative analyses of RNA levels at different times, shedding light on the dynamics of developmental processes, and under different environmental responses, providing insights into gene expression regulation and stress responses. However, given the inherent variability of the data stemming from biological and technological sources, quantifying changes in gene expression proves to be a statistical challenge. Here, we present a closed-form Bayesian solution to this problem. Our approach is tailored to the differential gene expression analysis of processed RNA-Seq data. The framework unifies and streamlines an otherwise complex analysis, typically involving parameter estimations and multiple statistical tests, into a concise mathematical equation for the calculation of Bayes factors. Using conjugate priors we can solve the equations analytically. For each gene, we calculate a Bayes factor, which can be used for ranking genes according to the statistical evidence for the gene’s expression change given RNA-Seq data. The presented closed-form solution is derived under minimal assumptions and may be applied to a variety of other 2-sample problems.</summary></entry><entry><title type="html">A Spatial-statistical model to analyse historical rutting data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ASpatialstatisticalmodeltoanalysehistoricalruttingdata.html" rel="alternate" type="text/html" title="A Spatial-statistical model to analyse historical rutting data" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ASpatialstatisticalmodeltoanalysehistoricalruttingdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ASpatialstatisticalmodeltoanalysehistoricalruttingdata.html">&lt;p&gt;Pavement rutting poses a significant challenge in flexible pavements, necessitating costly asphalt resurfacing. To address this issue comprehensively, we propose an advanced Bayesian hierarchical framework of latent Gaussian models with spatial components. Our model provides a thorough diagnostic analysis, pinpointing areas exhibiting unexpectedly high rutting rates. Incorporating spatial and random components, and important explanatory variables like annual average daily traffic (traffic intensity), asphalt type, rut depth and lane width, our proposed models account for and estimate the influence of these variables on rutting. This approach not only quantifies uncertainties and discerns locations at the highest risk of requiring maintenance, but also uncover spatial dependencies in rutting (millimetre/year). We apply our models to a data set spanning eleven years (2010-2020). Our findings emphasise the systematic unexplained spatial rutting effect, where some of the rutting variability is accounted for by spatial components, asphalt type, in conjunction with traffic intensity, is also found to be the primary driver of rutting. Furthermore, the spatial dependencies uncovered reveal road sections experiencing more than 1 millimeter of rutting beyond annual expectations. This leads to a halving of the expected pavement lifespan in these areas. Our study offers valuable insights, presenting maps indicating expected rutting, and identifying locations with accelerated rutting rates, resulting in a reduction in pavement life expectancy of at least 10 years.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.03633&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Natoya O. A. S. Jourdain, Ingelin Steinsland, Mamoona Birkhez-Shami, Emil Vedvik, William Olsen, Dagfin Gryteselv, Doreen Siebert, Alex Klein-Paste</name></author><category term="stat.AP," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Pavement rutting poses a significant challenge in flexible pavements, necessitating costly asphalt resurfacing. To address this issue comprehensively, we propose an advanced Bayesian hierarchical framework of latent Gaussian models with spatial components. Our model provides a thorough diagnostic analysis, pinpointing areas exhibiting unexpectedly high rutting rates. Incorporating spatial and random components, and important explanatory variables like annual average daily traffic (traffic intensity), asphalt type, rut depth and lane width, our proposed models account for and estimate the influence of these variables on rutting. This approach not only quantifies uncertainties and discerns locations at the highest risk of requiring maintenance, but also uncover spatial dependencies in rutting (millimetre/year). We apply our models to a data set spanning eleven years (2010-2020). Our findings emphasise the systematic unexplained spatial rutting effect, where some of the rutting variability is accounted for by spatial components, asphalt type, in conjunction with traffic intensity, is also found to be the primary driver of rutting. Furthermore, the spatial dependencies uncovered reveal road sections experiencing more than 1 millimeter of rutting beyond annual expectations. This leads to a halving of the expected pavement lifespan in these areas. Our study offers valuable insights, presenting maps indicating expected rutting, and identifying locations with accelerated rutting rates, resulting in a reduction in pavement life expectancy of at least 10 years.</summary></entry><entry><title type="html">A Statistical Model of Bipartite Networks: Application to Cosponsorship in the United States Senate</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/AStatisticalModelofBipartiteNetworksApplicationtoCosponsorshipintheUnitedStatesSenate.html" rel="alternate" type="text/html" title="A Statistical Model of Bipartite Networks: Application to Cosponsorship in the United States Senate" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/AStatisticalModelofBipartiteNetworksApplicationtoCosponsorshipintheUnitedStatesSenate</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/AStatisticalModelofBipartiteNetworksApplicationtoCosponsorshipintheUnitedStatesSenate.html">&lt;p&gt;Many networks in political and social research are bipartite, with edges connecting exclusively across two distinct types of nodes. A common example includes cosponsorship networks, in which legislators are connected indirectly through the bills they support. Yet most existing network models are designed for unipartite networks, where edges can arise between any pair of nodes. However, using a unipartite network model to analyze bipartite networks, as often done in practice, can result in aggregation bias and artificially high-clustering – a particularly insidious problem when studying the role groups play in network formation. To address these methodological problems, we develop a statistical model of bipartite networks theorized to be generated through group interactions by extending the popular mixed-membership stochastic blockmodel. Our model allows researchers to identify the groups of nodes, within each node type in the bipartite structure, that share common patterns of edge formation. The model also incorporates both node and dyad-level covariates as the predictors of group membership and of observed dyadic relations. We develop an efficient computational algorithm for fitting the model, and apply it to cosponsorship data from the United States Senate. We show that legislators in a Senate that was perfectly split along party lines were able to remain productive and pass major legislation by forming non-partisan, power-brokering coalitions that found common ground through their collaboration on low-stakes bills. We also find evidence for norms of reciprocity, and uncover the substantial role played by policy expertise in the formation of cosponsorships between senators and legislation. We make an open-source software package available that makes it possible for other researchers to uncover similar insights from bipartite networks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.05833&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Adeline Lo, Santiago Olivella, Kosuke Imai</name></author><category term="stat.AP" /><summary type="html">Many networks in political and social research are bipartite, with edges connecting exclusively across two distinct types of nodes. A common example includes cosponsorship networks, in which legislators are connected indirectly through the bills they support. Yet most existing network models are designed for unipartite networks, where edges can arise between any pair of nodes. However, using a unipartite network model to analyze bipartite networks, as often done in practice, can result in aggregation bias and artificially high-clustering – a particularly insidious problem when studying the role groups play in network formation. To address these methodological problems, we develop a statistical model of bipartite networks theorized to be generated through group interactions by extending the popular mixed-membership stochastic blockmodel. Our model allows researchers to identify the groups of nodes, within each node type in the bipartite structure, that share common patterns of edge formation. The model also incorporates both node and dyad-level covariates as the predictors of group membership and of observed dyadic relations. We develop an efficient computational algorithm for fitting the model, and apply it to cosponsorship data from the United States Senate. We show that legislators in a Senate that was perfectly split along party lines were able to remain productive and pass major legislation by forming non-partisan, power-brokering coalitions that found common ground through their collaboration on low-stakes bills. We also find evidence for norms of reciprocity, and uncover the substantial role played by policy expertise in the formation of cosponsorships between senators and legislation. We make an open-source software package available that makes it possible for other researchers to uncover similar insights from bipartite networks.</summary></entry><entry><title type="html">A comprehensive survey of the home advantage in American football</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/AcomprehensivesurveyofthehomeadvantageinAmericanfootball.html" rel="alternate" type="text/html" title="A comprehensive survey of the home advantage in American football" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/AcomprehensivesurveyofthehomeadvantageinAmericanfootball</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/AcomprehensivesurveyofthehomeadvantageinAmericanfootball.html">&lt;p&gt;The existence and justification to the home advantage – the benefit a sports team receives when playing at home – has been studied across sport. The majority of research on this topic is limited to individual leagues in short time frames, which hinders extrapolation and a deeper understanding of possible causes. Using nearly two decades of data from the National Football League (NFL), the National Collegiate Athletic Association (NCAA), and high schools from across the United States, we provide a uniform approach to understanding the home advantage in American football. Our findings suggest home advantage is declining in the NFL and the highest levels of collegiate football, but not in amateur football. This increases the possibility that characteristics of the NCAA and NFL, such as travel improvements and instant replay, have helped level the playing field.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.16392&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luke S. Benz, Thompson J. Bliss, Michael J. Lopez</name></author><category term="stat.AP" /><summary type="html">The existence and justification to the home advantage – the benefit a sports team receives when playing at home – has been studied across sport. The majority of research on this topic is limited to individual leagues in short time frames, which hinders extrapolation and a deeper understanding of possible causes. Using nearly two decades of data from the National Football League (NFL), the National Collegiate Athletic Association (NCAA), and high schools from across the United States, we provide a uniform approach to understanding the home advantage in American football. Our findings suggest home advantage is declining in the NFL and the highest levels of collegiate football, but not in amateur football. This increases the possibility that characteristics of the NCAA and NFL, such as travel improvements and instant replay, have helped level the playing field.</summary></entry><entry><title type="html">Active Sequential Two-Sample Testing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ActiveSequentialTwoSampleTesting.html" rel="alternate" type="text/html" title="Active Sequential Two-Sample Testing" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ActiveSequentialTwoSampleTesting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ActiveSequentialTwoSampleTesting.html">&lt;p&gt;A two-sample hypothesis test is a statistical procedure used to determine whether the distributions generating two samples are identical. We consider the two-sample testing problem in a new scenario where the sample measurements (or sample features) are inexpensive to access, but their group memberships (or labels) are costly. To address the problem, we devise the first \emph{active sequential two-sample testing framework} that not only sequentially but also \emph{actively queries}. Our test statistic is a likelihood ratio where one likelihood is found by maximization over all class priors, and the other is provided by a probabilistic classification model. The classification model is adaptively updated and used to predict where the (unlabelled) features have a high dependency on labels; labeling the ``high-dependency’’ features leads to the increased power of the proposed testing framework. In theory, we provide the proof that our framework produces an \emph{anytime-valid} $p$-value. In addition, we characterize the proposed framework’s gain in testing power by analyzing the mutual information between the feature and label variables in asymptotic and finite-sample scenarios. In practice, we introduce an instantiation of our framework and evaluate it using several experiments; the experiments on the synthetic, MNIST, and application-specific datasets demonstrate that the testing power of the instantiated active sequential test significantly increases while the Type I error is under control.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.12616&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Weizhi Li, Prad Kadambi, Pouria Saidi, Karthikeyan Natesan Ramamurthy, Gautam Dasarathy, Visar Berisha</name></author><category term="stat.ME" /><summary type="html">A two-sample hypothesis test is a statistical procedure used to determine whether the distributions generating two samples are identical. We consider the two-sample testing problem in a new scenario where the sample measurements (or sample features) are inexpensive to access, but their group memberships (or labels) are costly. To address the problem, we devise the first \emph{active sequential two-sample testing framework} that not only sequentially but also \emph{actively queries}. Our test statistic is a likelihood ratio where one likelihood is found by maximization over all class priors, and the other is provided by a probabilistic classification model. The classification model is adaptively updated and used to predict where the (unlabelled) features have a high dependency on labels; labeling the ``high-dependency’’ features leads to the increased power of the proposed testing framework. In theory, we provide the proof that our framework produces an \emph{anytime-valid} $p$-value. In addition, we characterize the proposed framework’s gain in testing power by analyzing the mutual information between the feature and label variables in asymptotic and finite-sample scenarios. In practice, we introduce an instantiation of our framework and evaluate it using several experiments; the experiments on the synthetic, MNIST, and application-specific datasets demonstrate that the testing power of the instantiated active sequential test significantly increases while the Type I error is under control.</summary></entry><entry><title type="html">A multiscale Bayesian nonparametric framework for partial hierarchical clustering</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/AmultiscaleBayesiannonparametricframeworkforpartialhierarchicalclustering.html" rel="alternate" type="text/html" title="A multiscale Bayesian nonparametric framework for partial hierarchical clustering" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/AmultiscaleBayesiannonparametricframeworkforpartialhierarchicalclustering</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/AmultiscaleBayesiannonparametricframeworkforpartialhierarchicalclustering.html">&lt;p&gt;In recent years, there has been a growing demand to discern clusters of subjects in datasets characterized by a large set of features. Often, these clusters may be highly variable in size and present partial hierarchical structures. In this context, model-based clustering approaches with nonparametric priors are gaining attention in the literature due to their flexibility and adaptability to new data. However, current approaches still face challenges in recognizing hierarchical cluster structures and in managing tiny clusters or singletons. To address these limitations, we propose a novel infinite mixture model with kernels organized within a multiscale structure. Leveraging a careful specification of the kernel parameters, our method allows the inclusion of additional information guiding possible hierarchies among clusters while maintaining flexibility. We provide theoretical support and an elegant, parsimonious formulation based on infinite factorization that allows efficient inference via Gibbs sampler.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19778&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lorenzo Schiavon, Mattia Stival</name></author><category term="stat.ME" /><summary type="html">In recent years, there has been a growing demand to discern clusters of subjects in datasets characterized by a large set of features. Often, these clusters may be highly variable in size and present partial hierarchical structures. In this context, model-based clustering approaches with nonparametric priors are gaining attention in the literature due to their flexibility and adaptability to new data. However, current approaches still face challenges in recognizing hierarchical cluster structures and in managing tiny clusters or singletons. To address these limitations, we propose a novel infinite mixture model with kernels organized within a multiscale structure. Leveraging a careful specification of the kernel parameters, our method allows the inclusion of additional information guiding possible hierarchies among clusters while maintaining flexibility. We provide theoretical support and an elegant, parsimonious formulation based on infinite factorization that allows efficient inference via Gibbs sampler.</summary></entry><entry><title type="html">Bayesian Rank-Clustering</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/BayesianRankClustering.html" rel="alternate" type="text/html" title="Bayesian Rank-Clustering" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/BayesianRankClustering</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/BayesianRankClustering.html">&lt;p&gt;In a traditional analysis of ordinal comparison data, the goal is to infer an overall ranking of objects from best to worst with each object having a unique rank. However, the ranks of some objects may not be statistically distinguishable. This could happen due to insufficient data or to the true underlying abilities or qualities being equal for some objects. In such cases, practitioners may prefer an overall ranking where groups of objects are allowed to have equal ranks or to be $\textit{rank-clustered}$. Existing models related to rank-clustering are limited by their inability to handle a variety of ordinal data types, to quantify uncertainty, or by the need to pre-specify the number and size of potential rank-clusters. We solve these limitations through the proposed Bayesian $\textit{Rank-Clustered Bradley-Terry-Luce}$ model. We allow for rank-clustering via parameter fusion by imposing a novel spike-and-slab prior on object-specific worth parameters in Bradley-Terry-Luce family of distributions for ordinal comparisons. We demonstrate the model on simulated and real datasets in survey analysis, elections, and sports.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19563&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael Pearce, Elena A. Erosheva</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In a traditional analysis of ordinal comparison data, the goal is to infer an overall ranking of objects from best to worst with each object having a unique rank. However, the ranks of some objects may not be statistically distinguishable. This could happen due to insufficient data or to the true underlying abilities or qualities being equal for some objects. In such cases, practitioners may prefer an overall ranking where groups of objects are allowed to have equal ranks or to be $\textit{rank-clustered}$. Existing models related to rank-clustering are limited by their inability to handle a variety of ordinal data types, to quantify uncertainty, or by the need to pre-specify the number and size of potential rank-clusters. We solve these limitations through the proposed Bayesian $\textit{Rank-Clustered Bradley-Terry-Luce}$ model. We allow for rank-clustering via parameter fusion by imposing a novel spike-and-slab prior on object-specific worth parameters in Bradley-Terry-Luce family of distributions for ordinal comparisons. We demonstrate the model on simulated and real datasets in survey analysis, elections, and sports.</summary></entry><entry><title type="html">Bayesian analysis of biomarker levels can predict time of recurrence of prostate cancer with strictly positive apparent Shannon information against an exponential attrition prior</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/BayesiananalysisofbiomarkerlevelscanpredicttimeofrecurrenceofprostatecancerwithstrictlypositiveapparentShannoninformationagainstanexponentialattritionprior.html" rel="alternate" type="text/html" title="Bayesian analysis of biomarker levels can predict time of recurrence of prostate cancer with strictly positive apparent Shannon information against an exponential attrition prior" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/BayesiananalysisofbiomarkerlevelscanpredicttimeofrecurrenceofprostatecancerwithstrictlypositiveapparentShannoninformationagainstanexponentialattritionprior</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/BayesiananalysisofbiomarkerlevelscanpredicttimeofrecurrenceofprostatecancerwithstrictlypositiveapparentShannoninformationagainstanexponentialattritionprior.html">&lt;p&gt;Shariat et al previously investigated the possibility of predicting from clinical data (including Gleason grade and stage) and preoperative biomarkers, which of any pair of patients would suffer recurrence of prostate cancer first. We wished to establish the extent to which predictions of time of relapse from such a model could be improved upon using Bayesian methods.
  The same dataset was reanalysed with a Bayesian skew-Student mixture model. Predictions were made of which of any pair of patients would relapse first and of the time of relapse. The benefit of using these biomarkers relative to predictions made without them was measured by the apparent Shannon information, using as prior an exponential attrition model of relapse time independent of input variables.
  Using half the dataset for training and the other half for testing, predictions of relapse time from the strict Cox model gave $-\infty$ nepers of apparent Shannon information (it predicts that relapse can only occur at times when patients in the training set relapsed). Deliberately smoothed predictions from the Cox model gave -0.001 (-0.131 to +0.120) nepers, while the Bayesian model gave +0.109 (+0.021 to +0.192) nepers (mean, 2.5 to 97.5 centiles), being positive with posterior probability 0.993 and beating the blurred Cox model with posterior probability 0.927.
  These predictions from the Bayesian model thus outperform those of the Cox model, but the overall yield of predictive information leaves scope for improvement of the range of biomarkers in use. The Bayesian model presented here is the first such model for prostate cancer to consider the variation of relapse hazard with biomarker concentrations to be smooth, as is intuitive. It is also the first to be shown to provide more apparent Shannon information than the Cox model or to be shown to provide positive apparent information relative to an exponential prior.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17857&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roger Sewell, Elisabeth Crowe, Sharokh F. Shariat</name></author><category term="stat.AP" /><summary type="html">Shariat et al previously investigated the possibility of predicting from clinical data (including Gleason grade and stage) and preoperative biomarkers, which of any pair of patients would suffer recurrence of prostate cancer first. We wished to establish the extent to which predictions of time of relapse from such a model could be improved upon using Bayesian methods. The same dataset was reanalysed with a Bayesian skew-Student mixture model. Predictions were made of which of any pair of patients would relapse first and of the time of relapse. The benefit of using these biomarkers relative to predictions made without them was measured by the apparent Shannon information, using as prior an exponential attrition model of relapse time independent of input variables. Using half the dataset for training and the other half for testing, predictions of relapse time from the strict Cox model gave $-\infty$ nepers of apparent Shannon information (it predicts that relapse can only occur at times when patients in the training set relapsed). Deliberately smoothed predictions from the Cox model gave -0.001 (-0.131 to +0.120) nepers, while the Bayesian model gave +0.109 (+0.021 to +0.192) nepers (mean, 2.5 to 97.5 centiles), being positive with posterior probability 0.993 and beating the blurred Cox model with posterior probability 0.927. These predictions from the Bayesian model thus outperform those of the Cox model, but the overall yield of predictive information leaves scope for improvement of the range of biomarkers in use. The Bayesian model presented here is the first such model for prostate cancer to consider the variation of relapse hazard with biomarker concentrations to be smooth, as is intuitive. It is also the first to be shown to provide more apparent Shannon information than the Cox model or to be shown to provide positive apparent information relative to an exponential prior.</summary></entry><entry><title type="html">Bayesian calibration of stochastic agent based model via random forest</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Bayesiancalibrationofstochasticagentbasedmodelviarandomforest.html" rel="alternate" type="text/html" title="Bayesian calibration of stochastic agent based model via random forest" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Bayesiancalibrationofstochasticagentbasedmodelviarandomforest</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Bayesiancalibrationofstochasticagentbasedmodelviarandomforest.html">&lt;p&gt;Agent-based models (ABM) provide an excellent framework for modeling outbreaks and interventions in epidemiology by explicitly accounting for diverse individual interactions and environments. However, these models are usually stochastic and highly parametrized, requiring precise calibration for predictive performance. When considering realistic numbers of agents and properly accounting for stochasticity, this high dimensional calibration can be computationally prohibitive. This paper presents a random forest based surrogate modeling technique to accelerate the evaluation of ABMs and demonstrates its use to calibrate an epidemiological ABM named CityCOVID via Markov chain Monte Carlo (MCMC). The technique is first outlined in the context of CityCOVID’s quantities of interest, namely hospitalizations and deaths, by exploring dimensionality reduction via temporal decomposition with principal component analysis (PCA) and via sensitivity analysis. The calibration problem is then presented and samples are generated to best match COVID-19 hospitalization and death numbers in Chicago from March to June in 2020. These results are compared with previous approximate Bayesian calibration (IMABC) results and their predictive performance is analyzed showing improved performance with a reduction in computation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19524&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Connor Robertson, Cosmin Safta, Nicholson Collier, Jonathan Ozik, Jaideep Ray</name></author><category term="stat.ML," /><category term="stat.AP" /><summary type="html">Agent-based models (ABM) provide an excellent framework for modeling outbreaks and interventions in epidemiology by explicitly accounting for diverse individual interactions and environments. However, these models are usually stochastic and highly parametrized, requiring precise calibration for predictive performance. When considering realistic numbers of agents and properly accounting for stochasticity, this high dimensional calibration can be computationally prohibitive. This paper presents a random forest based surrogate modeling technique to accelerate the evaluation of ABMs and demonstrates its use to calibrate an epidemiological ABM named CityCOVID via Markov chain Monte Carlo (MCMC). The technique is first outlined in the context of CityCOVID’s quantities of interest, namely hospitalizations and deaths, by exploring dimensionality reduction via temporal decomposition with principal component analysis (PCA) and via sensitivity analysis. The calibration problem is then presented and samples are generated to best match COVID-19 hospitalization and death numbers in Chicago from March to June in 2020. These results are compared with previous approximate Bayesian calibration (IMABC) results and their predictive performance is analyzed showing improved performance with a reduction in computation.</summary></entry><entry><title type="html">Causal Meta-Analysis by Integrating Multiple Observational Studies with Multivariate Outcomes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/CausalMetaAnalysisbyIntegratingMultipleObservationalStudieswithMultivariateOutcomes.html" rel="alternate" type="text/html" title="Causal Meta-Analysis by Integrating Multiple Observational Studies with Multivariate Outcomes" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/CausalMetaAnalysisbyIntegratingMultipleObservationalStudieswithMultivariateOutcomes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/CausalMetaAnalysisbyIntegratingMultipleObservationalStudieswithMultivariateOutcomes.html">&lt;p&gt;Integrating multiple observational studies to make unconfounded causal or descriptive comparisons of group potential outcomes in a large natural population is challenging. Moreover, retrospective cohorts, being convenience samples, are usually unrepresentative of the natural population of interest and have groups with unbalanced covariates. We propose a general covariate-balancing framework based on pseudo-populations that extends established weighting methods to the meta-analysis of multiple retrospective cohorts with multiple groups. Additionally, by maximizing the effective sample sizes of the cohorts, we propose a FLEXible, Optimized, and Realistic (FLEXOR) weighting method appropriate for integrative analyses. We develop new weighted estimators for unconfounded inferences on wide-ranging population-level features and estimands relevant to group comparisons of quantitative, categorical, or multivariate outcomes. Asymptotic properties of these estimators are examined. Through simulation studies and meta-analyses of TCGA datasets, we demonstrate the versatility and reliability of the proposed weighting strategy, especially for the FLEXOR pseudo-population.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.16715&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Subharup Guha, Yi Li</name></author><category term="stat.ME" /><summary type="html">Integrating multiple observational studies to make unconfounded causal or descriptive comparisons of group potential outcomes in a large natural population is challenging. Moreover, retrospective cohorts, being convenience samples, are usually unrepresentative of the natural population of interest and have groups with unbalanced covariates. We propose a general covariate-balancing framework based on pseudo-populations that extends established weighting methods to the meta-analysis of multiple retrospective cohorts with multiple groups. Additionally, by maximizing the effective sample sizes of the cohorts, we propose a FLEXible, Optimized, and Realistic (FLEXOR) weighting method appropriate for integrative analyses. We develop new weighted estimators for unconfounded inferences on wide-ranging population-level features and estimands relevant to group comparisons of quantitative, categorical, or multivariate outcomes. Asymptotic properties of these estimators are examined. Through simulation studies and meta-analyses of TCGA datasets, we demonstrate the versatility and reliability of the proposed weighting strategy, especially for the FLEXOR pseudo-population.</summary></entry><entry><title type="html">Censored extreme value estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Censoredextremevalueestimation.html" rel="alternate" type="text/html" title="Censored extreme value estimation" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Censoredextremevalueestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Censoredextremevalueestimation.html">&lt;p&gt;A novel and comprehensive methodology designed to tackle the challenges posed by extreme values in the context of random censorship is introduced. The main focus is on the analysis of integrals based on the product-limit estimator of normalized upper order statistics, called extreme Kaplan–Meier integrals. These integrals allow for the transparent derivation of various important asymptotic distributional properties, offering an alternative approach to conventional plug-in estimation methods. Notably, this methodology demonstrates robustness and wide applicability within the scope of max-domains of attraction. A noteworthy by-product is the extension of generalized Hill-type estimators of extremes to encompass all max-domains of attraction, which is of independent interest. The theoretical framework is applied to construct novel estimators for positive and real-valued extreme value indices for right-censored data. Simulation studies supporting the theory are provided.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.10499&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Martin Bladt, Igor Rodionov</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">A novel and comprehensive methodology designed to tackle the challenges posed by extreme values in the context of random censorship is introduced. The main focus is on the analysis of integrals based on the product-limit estimator of normalized upper order statistics, called extreme Kaplan–Meier integrals. These integrals allow for the transparent derivation of various important asymptotic distributional properties, offering an alternative approach to conventional plug-in estimation methods. Notably, this methodology demonstrates robustness and wide applicability within the scope of max-domains of attraction. A noteworthy by-product is the extension of generalized Hill-type estimators of extremes to encompass all max-domains of attraction, which is of independent interest. The theoretical framework is applied to construct novel estimators for positive and real-valued extreme value indices for right-censored data. Simulation studies supporting the theory are provided.</summary></entry><entry><title type="html">Closed-Form Power and Sample Size Calculations for Bayes Factors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ClosedFormPowerandSampleSizeCalculationsforBayesFactors.html" rel="alternate" type="text/html" title="Closed-Form Power and Sample Size Calculations for Bayes Factors" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ClosedFormPowerandSampleSizeCalculationsforBayesFactors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ClosedFormPowerandSampleSizeCalculationsforBayesFactors.html">&lt;p&gt;Determining an appropriate sample size is a critical element of study design, and the method used to determine it should be consistent with the planned analysis. When the planned analysis involves Bayes factor hypothesis testing, the sample size is usually desired to ensure a sufficiently high probability of obtaining a Bayes factor indicating compelling evidence for a hypothesis, given that the hypothesis is true. In practice, Bayes factor sample size determination is typically performed using computationally intensive Monte Carlo simulation. Here, we summarize alternative approaches that enable sample size determination without simulation. We show how, under approximate normality assumptions, sample sizes can be determined numerically, and provide the R package bfpwr for this purpose. Additionally, we identify conditions under which sample sizes can even be determined in closed-form, resulting in novel, easy-to-use formulas that also help foster intuition, enable asymptotic analysis, and can also be used for hybrid Bayesian/likelihoodist design. Furthermore, we show how in our framework power and sample size can be computed without simulation for more complex analysis priors, such as Jeffreys-Zellner-Siow priors or nonlocal normal moment priors. Case studies from medicine and psychology illustrate how researchers can use our methods to design informative yet cost-efficient studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19940&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Samuel Pawel, Leonhard Held</name></author><category term="stat.ME" /><summary type="html">Determining an appropriate sample size is a critical element of study design, and the method used to determine it should be consistent with the planned analysis. When the planned analysis involves Bayes factor hypothesis testing, the sample size is usually desired to ensure a sufficiently high probability of obtaining a Bayes factor indicating compelling evidence for a hypothesis, given that the hypothesis is true. In practice, Bayes factor sample size determination is typically performed using computationally intensive Monte Carlo simulation. Here, we summarize alternative approaches that enable sample size determination without simulation. We show how, under approximate normality assumptions, sample sizes can be determined numerically, and provide the R package bfpwr for this purpose. Additionally, we identify conditions under which sample sizes can even be determined in closed-form, resulting in novel, easy-to-use formulas that also help foster intuition, enable asymptotic analysis, and can also be used for hybrid Bayesian/likelihoodist design. Furthermore, we show how in our framework power and sample size can be computed without simulation for more complex analysis priors, such as Jeffreys-Zellner-Siow priors or nonlocal normal moment priors. Case studies from medicine and psychology illustrate how researchers can use our methods to design informative yet cost-efficient studies.</summary></entry><entry><title type="html">Confidence intervals for tree-structured varying coefficients</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Confidenceintervalsfortreestructuredvaryingcoefficients.html" rel="alternate" type="text/html" title="Confidence intervals for tree-structured varying coefficients" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Confidenceintervalsfortreestructuredvaryingcoefficients</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Confidenceintervalsfortreestructuredvaryingcoefficients.html">&lt;p&gt;The tree-structured varying coefficient model (TSVC) is a flexible regression approach that allows the effects of covariates to vary with the values of the effect modifiers. Relevant effect modifiers are identified inherently using recursive partitioning techniques. To quantify uncertainty in TSVC models, we propose a procedure to construct confidence intervals of the estimated partition-specific coefficients. This task constitutes a selective inference problem as the coefficients of a TSVC model result from data-driven model building. To account for this issue, we introduce a parametric bootstrap approach, which is tailored to the complex structure of TSVC. Finite sample properties, particularly coverage proportions, of the proposed confidence intervals are evaluated in a simulation study. For illustration, we consider applications to data from COVID-19 patients and from patients suffering from acute odontogenic infection. The proposed approach may also be adapted for constructing confidence intervals for other tree-based methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19887&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nikolai Spuck, Matthias Schmid, Malte Monin, Moritz Berger</name></author><category term="stat.ME" /><summary type="html">The tree-structured varying coefficient model (TSVC) is a flexible regression approach that allows the effects of covariates to vary with the values of the effect modifiers. Relevant effect modifiers are identified inherently using recursive partitioning techniques. To quantify uncertainty in TSVC models, we propose a procedure to construct confidence intervals of the estimated partition-specific coefficients. This task constitutes a selective inference problem as the coefficients of a TSVC model result from data-driven model building. To account for this issue, we introduce a parametric bootstrap approach, which is tailored to the complex structure of TSVC. Finite sample properties, particularly coverage proportions, of the proposed confidence intervals are evaluated in a simulation study. For illustration, we consider applications to data from COVID-19 patients and from patients suffering from acute odontogenic infection. The proposed approach may also be adapted for constructing confidence intervals for other tree-based methods.</summary></entry><entry><title type="html">Covariance Expressions for Multi-Fidelity Sampling with Multi-Output, Multi-Statistic Estimators: Application to Approximate Control Variates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/CovarianceExpressionsforMultiFidelitySamplingwithMultiOutputMultiStatisticEstimatorsApplicationtoApproximateControlVariates.html" rel="alternate" type="text/html" title="Covariance Expressions for Multi-Fidelity Sampling with Multi-Output, Multi-Statistic Estimators: Application to Approximate Control Variates" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/CovarianceExpressionsforMultiFidelitySamplingwithMultiOutputMultiStatisticEstimatorsApplicationtoApproximateControlVariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/CovarianceExpressionsforMultiFidelitySamplingwithMultiOutputMultiStatisticEstimatorsApplicationtoApproximateControlVariates.html">&lt;p&gt;We provide a collection of results on covariance expressions between Monte Carlo based multi-output mean, variance, and Sobol main effect variance estimators from an ensemble of models. These covariances can be used within multi-fidelity uncertainty quantification strategies that seek to reduce the estimator variance of high-fidelity Monte Carlo estimators with an ensemble of low-fidelity models. Such covariance expressions are required within approaches like the approximate control variate and multi-level best linear unbiased estimator. While the literature provides these expressions for some single-output cases such as mean and variance, our results are relevant to both multiple function outputs and multiple statistics across any sampling strategy. Following the description of these results, we use them within an approximate control variate scheme to show that leveraging multiple outputs can dramatically reduce estimator variance compared to single-output approaches. Synthetic examples are used to highlight the effects of optimal sample allocation and pilot sample estimation. A flight-trajectory simulation of entry, descent, and landing is used to demonstrate multi-output estimation in practical applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.00125&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thomas O. Dixon, James E. Warner, Geoffrey F. Bomarito, Alex A. Gorodetsky</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">We provide a collection of results on covariance expressions between Monte Carlo based multi-output mean, variance, and Sobol main effect variance estimators from an ensemble of models. These covariances can be used within multi-fidelity uncertainty quantification strategies that seek to reduce the estimator variance of high-fidelity Monte Carlo estimators with an ensemble of low-fidelity models. Such covariance expressions are required within approaches like the approximate control variate and multi-level best linear unbiased estimator. While the literature provides these expressions for some single-output cases such as mean and variance, our results are relevant to both multiple function outputs and multiple statistics across any sampling strategy. Following the description of these results, we use them within an approximate control variate scheme to show that leveraging multiple outputs can dramatically reduce estimator variance compared to single-output approaches. Synthetic examples are used to highlight the effects of optimal sample allocation and pilot sample estimation. A flight-trajectory simulation of entry, descent, and landing is used to demonstrate multi-output estimation in practical applications.</summary></entry><entry><title type="html">Deep Learning of Multivariate Extremes via a Geometric Representation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/DeepLearningofMultivariateExtremesviaaGeometricRepresentation.html" rel="alternate" type="text/html" title="Deep Learning of Multivariate Extremes via a Geometric Representation" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/DeepLearningofMultivariateExtremesviaaGeometricRepresentation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/DeepLearningofMultivariateExtremesviaaGeometricRepresentation.html">&lt;p&gt;The study of geometric extremes, where extremal dependence properties are inferred from the deterministic limiting shapes of scaled sample clouds, provides an exciting approach to modelling the extremes of multivariate data. These shapes, termed limit sets, link together several popular extremal dependence modelling frameworks. Although the geometric approach is becoming an increasingly popular modelling tool, current inference techniques are limited to a low dimensional setting (d &amp;lt; 4), and generally require rigid modelling assumptions. In this work, we propose a range of novel theoretical results to aid with the implementation of the geometric extremes framework and introduce the first approach to modelling limit sets using deep learning. By leveraging neural networks, we construct asymptotically-justified yet flexible semi-parametric models for extremal dependence of high-dimensional data. We showcase the efficacy of our deep approach by modelling the complex extremal dependencies between meteorological and oceanographic variables in the North Sea off the coast of the UK.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19936&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Callum J. R. Murphy-Barltrop, Reetam Majumder, Jordan Richards</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">The study of geometric extremes, where extremal dependence properties are inferred from the deterministic limiting shapes of scaled sample clouds, provides an exciting approach to modelling the extremes of multivariate data. These shapes, termed limit sets, link together several popular extremal dependence modelling frameworks. Although the geometric approach is becoming an increasingly popular modelling tool, current inference techniques are limited to a low dimensional setting (d &amp;lt; 4), and generally require rigid modelling assumptions. In this work, we propose a range of novel theoretical results to aid with the implementation of the geometric extremes framework and introduce the first approach to modelling limit sets using deep learning. By leveraging neural networks, we construct asymptotically-justified yet flexible semi-parametric models for extremal dependence of high-dimensional data. We showcase the efficacy of our deep approach by modelling the complex extremal dependencies between meteorological and oceanographic variables in the North Sea off the coast of the UK.</summary></entry><entry><title type="html">Design-based theory for Lasso adjustment in randomized block experiments and rerandomized experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/DesignbasedtheoryforLassoadjustmentinrandomizedblockexperimentsandrerandomizedexperiments.html" rel="alternate" type="text/html" title="Design-based theory for Lasso adjustment in randomized block experiments and rerandomized experiments" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/DesignbasedtheoryforLassoadjustmentinrandomizedblockexperimentsandrerandomizedexperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/DesignbasedtheoryforLassoadjustmentinrandomizedblockexperimentsandrerandomizedexperiments.html">&lt;p&gt;Blocking, a special case of rerandomization, is routinely implemented in the design stage of randomized experiments to balance the baseline covariates. This study proposes a regression adjustment method based on the least absolute shrinkage and selection operator (Lasso) to efficiently estimate the average treatment effect in randomized block experiments with high-dimensional covariates. We derive the asymptotic properties of the proposed estimator and outline the conditions under which this estimator is more efficient than the unadjusted one. We provide a conservative variance estimator to facilitate valid inferences. Our framework allows one treated or control unit in some blocks and heterogeneous propensity scores across blocks, thus including paired experiments and finely stratified experiments as special cases. We further accommodate rerandomized experiments and a combination of blocking and rerandomization. Moreover, our analysis allows both the number of blocks and block sizes to tend to infinity, as well as heterogeneous treatment effects across blocks without assuming a true outcome data-generating model. Simulation studies and two real-data analyses demonstrate the advantages of the proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2109.11271&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ke Zhu, Hanzhong Liu, Yuehan Yang</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Blocking, a special case of rerandomization, is routinely implemented in the design stage of randomized experiments to balance the baseline covariates. This study proposes a regression adjustment method based on the least absolute shrinkage and selection operator (Lasso) to efficiently estimate the average treatment effect in randomized block experiments with high-dimensional covariates. We derive the asymptotic properties of the proposed estimator and outline the conditions under which this estimator is more efficient than the unadjusted one. We provide a conservative variance estimator to facilitate valid inferences. Our framework allows one treated or control unit in some blocks and heterogeneous propensity scores across blocks, thus including paired experiments and finely stratified experiments as special cases. We further accommodate rerandomized experiments and a combination of blocking and rerandomization. Moreover, our analysis allows both the number of blocks and block sizes to tend to infinity, as well as heterogeneous treatment effects across blocks without assuming a true outcome data-generating model. Simulation studies and two real-data analyses demonstrate the advantages of the proposed method.</summary></entry><entry><title type="html">Discovery of Critical Thresholds in Mixed Exposures and Estimation of Policy Intervention Effects using Targeted Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/DiscoveryofCriticalThresholdsinMixedExposuresandEstimationofPolicyInterventionEffectsusingTargetedLearning.html" rel="alternate" type="text/html" title="Discovery of Critical Thresholds in Mixed Exposures and Estimation of Policy Intervention Effects using Targeted Learning" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/DiscoveryofCriticalThresholdsinMixedExposuresandEstimationofPolicyInterventionEffectsusingTargetedLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/DiscoveryofCriticalThresholdsinMixedExposuresandEstimationofPolicyInterventionEffectsusingTargetedLearning.html">&lt;p&gt;Traditional regulations of chemical exposure tend to focus on single exposures, overlooking the potential amplified toxicity due to multiple concurrent exposures. We are interested in understanding the average outcome if exposures were limited to fall under a multivariate threshold. Because threshold levels are often unknown a priori, we provide an algorithm that finds exposure threshold levels where the expected outcome is maximized or minimized. Because both identifying thresholds and estimating policy effects on the same data would lead to overfitting bias, we also provide a data-adaptive estimation framework, which allows for both threshold discovery and policy estimation. Simulation studies show asymptotic convergence to the optimal exposure region and to the true effect of an intervention. We demonstrate how our method identifies true interactions in a public synthetic mixture data set. Finally, we applied our method to NHANES data to discover metal exposures that have the most harmful effects on telomere length. We provide an implementation in the CVtreeMLE R package.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.07976&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David McCoy, Alan Hubbard, Alejandro Schuler, Mark van der Laan</name></author><category term="stat.ME" /><summary type="html">Traditional regulations of chemical exposure tend to focus on single exposures, overlooking the potential amplified toxicity due to multiple concurrent exposures. We are interested in understanding the average outcome if exposures were limited to fall under a multivariate threshold. Because threshold levels are often unknown a priori, we provide an algorithm that finds exposure threshold levels where the expected outcome is maximized or minimized. Because both identifying thresholds and estimating policy effects on the same data would lead to overfitting bias, we also provide a data-adaptive estimation framework, which allows for both threshold discovery and policy estimation. Simulation studies show asymptotic convergence to the optimal exposure region and to the true effect of an intervention. We demonstrate how our method identifies true interactions in a public synthetic mixture data set. Finally, we applied our method to NHANES data to discover metal exposures that have the most harmful effects on telomere length. We provide an implementation in the CVtreeMLE R package.</summary></entry><entry><title type="html">Electrostatics-based particle sampling and approximate inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Electrostaticsbasedparticlesamplingandapproximateinference.html" rel="alternate" type="text/html" title="Electrostatics-based particle sampling and approximate inference" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Electrostaticsbasedparticlesamplingandapproximateinference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Electrostaticsbasedparticlesamplingandapproximateinference.html">&lt;p&gt;A new particle-based sampling and approximate inference method, based on electrostatics and Newton mechanics principles, is introduced with theoretical ground, algorithm design and experimental validation. This method simulates an interacting particle system (IPS) where particles, i.e. the freely-moving negative charges and spatially-fixed positive charges with magnitudes proportional to the target distribution, interact with each other via attraction and repulsion induced by the resulting electric fields described by Poisson’s equation. The IPS evolves towards a steady-state where the distribution of negative charges conforms to the target distribution. This physics-inspired method offers deterministic, gradient-free sampling and inference, achieving comparable performance as other particle-based and MCMC methods in benchmark tasks of inferring complex densities, Bayesian logistic regression and dynamical system identification. A discrete-time, discrete-space algorithmic design, readily extendable to continuous time and space, is provided for usage in more general inference problems occurring in probabilistic machine learning scenarios such as Bayesian inference, generative modelling, and beyond.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.20044&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yongchao Huang</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">A new particle-based sampling and approximate inference method, based on electrostatics and Newton mechanics principles, is introduced with theoretical ground, algorithm design and experimental validation. This method simulates an interacting particle system (IPS) where particles, i.e. the freely-moving negative charges and spatially-fixed positive charges with magnitudes proportional to the target distribution, interact with each other via attraction and repulsion induced by the resulting electric fields described by Poisson’s equation. The IPS evolves towards a steady-state where the distribution of negative charges conforms to the target distribution. This physics-inspired method offers deterministic, gradient-free sampling and inference, achieving comparable performance as other particle-based and MCMC methods in benchmark tasks of inferring complex densities, Bayesian logistic regression and dynamical system identification. A discrete-time, discrete-space algorithmic design, readily extendable to continuous time and space, is provided for usage in more general inference problems occurring in probabilistic machine learning scenarios such as Bayesian inference, generative modelling, and beyond.</summary></entry><entry><title type="html">Estimation of Shannon differential entropy: An extensive comparative review</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/EstimationofShannondifferentialentropyAnextensivecomparativereview.html" rel="alternate" type="text/html" title="Estimation of Shannon differential entropy: An extensive comparative review" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/EstimationofShannondifferentialentropyAnextensivecomparativereview</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/EstimationofShannondifferentialentropyAnextensivecomparativereview.html">&lt;p&gt;In this research work, a total of 45 different estimators of the Shannon differential entropy were reviewed. The estimators were mainly based on three classes, namely: window size spacings, kernel density estimation (KDE) and k-nearest neighbour (kNN) estimation. A total of 16, 5 and 6 estimators were selected from each of the classes, respectively, for comparison. The performances of the 27 selected estimators, in terms of their bias values and root mean squared errors (RMSEs) as well as their asymptotic behaviours, were compared through extensive Monte Carlo simulations. The empirical comparisons were carried out at different sample sizes of 10, 50, and 100 and different variable dimensions of 1, 2, 3, and 5, for three groups of continuous distributions according to their symmetry and support. The results showed that the spacings based estimators generally performed better than the estimators from the other two classes at univariate level, but suffered from non existence at multivariate level. The kNN based estimators were generally inferior to the estimators from the other two classes considered but showed an advantage of existence for all dimensions. Also, a new class of optimal window size was obtained and sets of estimators were recommended for different groups of distributions at different variable dimensions. Finally, the asymptotic biases, variances and distributions of the ‘best estimators’ were considered.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19432&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mbanefo S. Madukaife, Ho Dang Phuc</name></author><category term="stat.ME" /><summary type="html">In this research work, a total of 45 different estimators of the Shannon differential entropy were reviewed. The estimators were mainly based on three classes, namely: window size spacings, kernel density estimation (KDE) and k-nearest neighbour (kNN) estimation. A total of 16, 5 and 6 estimators were selected from each of the classes, respectively, for comparison. The performances of the 27 selected estimators, in terms of their bias values and root mean squared errors (RMSEs) as well as their asymptotic behaviours, were compared through extensive Monte Carlo simulations. The empirical comparisons were carried out at different sample sizes of 10, 50, and 100 and different variable dimensions of 1, 2, 3, and 5, for three groups of continuous distributions according to their symmetry and support. The results showed that the spacings based estimators generally performed better than the estimators from the other two classes at univariate level, but suffered from non existence at multivariate level. The kNN based estimators were generally inferior to the estimators from the other two classes considered but showed an advantage of existence for all dimensions. Also, a new class of optimal window size was obtained and sets of estimators were recommended for different groups of distributions at different variable dimensions. Finally, the asymptotic biases, variances and distributions of the ‘best estimators’ were considered.</summary></entry><entry><title type="html">Exact Bayesian Gaussian Cox Processes Using Random Integral</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ExactBayesianGaussianCoxProcessesUsingRandomIntegral.html" rel="alternate" type="text/html" title="Exact Bayesian Gaussian Cox Processes Using Random Integral" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ExactBayesianGaussianCoxProcessesUsingRandomIntegral</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ExactBayesianGaussianCoxProcessesUsingRandomIntegral.html">&lt;p&gt;A Gaussian Cox process is a popular model for point process data, in which the intensity function is a transformation of a Gaussian process. Posterior inference of this intensity function involves an intractable integral (i.e., the cumulative intensity function) in the likelihood resulting in doubly intractable posterior distribution. Here, we propose a nonparametric Bayesian approach for estimating the intensity function of an inhomogeneous Poisson process without reliance on large data augmentation or approximations of the likelihood function. We propose to jointly model the intensity and the cumulative intensity function as a transformed Gaussian process, allowing us to directly bypass the need of approximating the cumulative intensity function in the likelihood. We propose an exact MCMC sampler for posterior inference and evaluate its performance on simulated data. We demonstrate the utility of our method in three real-world scenarios including temporal and spatial event data, as well as aggregated time count data collected at multiple resolutions. Finally, we discuss extensions of our proposed method to other point processes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19722&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bingjing Tang, Julia Palacios</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">A Gaussian Cox process is a popular model for point process data, in which the intensity function is a transformation of a Gaussian process. Posterior inference of this intensity function involves an intractable integral (i.e., the cumulative intensity function) in the likelihood resulting in doubly intractable posterior distribution. Here, we propose a nonparametric Bayesian approach for estimating the intensity function of an inhomogeneous Poisson process without reliance on large data augmentation or approximations of the likelihood function. We propose to jointly model the intensity and the cumulative intensity function as a transformed Gaussian process, allowing us to directly bypass the need of approximating the cumulative intensity function in the likelihood. We propose an exact MCMC sampler for posterior inference and evaluate its performance on simulated data. We demonstrate the utility of our method in three real-world scenarios including temporal and spatial event data, as well as aggregated time count data collected at multiple resolutions. Finally, we discuss extensions of our proposed method to other point processes.</summary></entry><entry><title type="html">Extended sample size calculations for evaluation of prediction models using a threshold for classification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Extendedsamplesizecalculationsforevaluationofpredictionmodelsusingathresholdforclassification.html" rel="alternate" type="text/html" title="Extended sample size calculations for evaluation of prediction models using a threshold for classification" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Extendedsamplesizecalculationsforevaluationofpredictionmodelsusingathresholdforclassification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Extendedsamplesizecalculationsforevaluationofpredictionmodelsusingathresholdforclassification.html">&lt;p&gt;When evaluating the performance of a model for individualised risk prediction, the sample size needs to be large enough to precisely estimate the performance measures of interest. Current sample size guidance is based on precisely estimating calibration, discrimination, and net benefit, which should be the first stage of calculating the minimum required sample size. However, when a clinically important threshold is used for classification, other performance measures can also be used. We extend the previously published guidance to precisely estimate threshold-based performance measures. We have developed closed-form solutions to estimate the sample size required to target sufficiently precise estimates of accuracy, specificity, sensitivity, PPV, NPV, and F1-score in an external evaluation study of a prediction model with a binary outcome. This approach requires the user to pre-specify the target standard error and the expected value for each performance measure. We describe how the sample size formulae were derived and demonstrate their use in an example. Extension to time-to-event outcomes is also considered. In our examples, the minimum sample size required was lower than that required to precisely estimate the calibration slope, and we expect this would most often be the case. Our formulae, along with corresponding Python code and updated R and Stata commands (pmvalsampsize), enable researchers to calculate the minimum sample size needed to precisely estimate threshold-based performance measures in an external evaluation study. These criteria should be used alongside previously published criteria to precisely estimate the calibration, discrimination, and net-benefit.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19673&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rebecca Whittle, Joie Ensor, Lucinda Archer, Gary S. Collins, Paula Dhiman, Alastair Denniston, Joseph Alderman, Amardeep Legha, Maarten van Smeden, Karel G. Moons, Jean-Baptiste Cazier, Richard D. Riley, Kym I. E. Snell</name></author><category term="stat.ME" /><summary type="html">When evaluating the performance of a model for individualised risk prediction, the sample size needs to be large enough to precisely estimate the performance measures of interest. Current sample size guidance is based on precisely estimating calibration, discrimination, and net benefit, which should be the first stage of calculating the minimum required sample size. However, when a clinically important threshold is used for classification, other performance measures can also be used. We extend the previously published guidance to precisely estimate threshold-based performance measures. We have developed closed-form solutions to estimate the sample size required to target sufficiently precise estimates of accuracy, specificity, sensitivity, PPV, NPV, and F1-score in an external evaluation study of a prediction model with a binary outcome. This approach requires the user to pre-specify the target standard error and the expected value for each performance measure. We describe how the sample size formulae were derived and demonstrate their use in an example. Extension to time-to-event outcomes is also considered. In our examples, the minimum sample size required was lower than that required to precisely estimate the calibration slope, and we expect this would most often be the case. Our formulae, along with corresponding Python code and updated R and Stata commands (pmvalsampsize), enable researchers to calculate the minimum sample size needed to precisely estimate threshold-based performance measures in an external evaluation study. These criteria should be used alongside previously published criteria to precisely estimate the calibration, discrimination, and net-benefit.</summary></entry><entry><title type="html">Flexible Conformal Highest Predictive Conditional Density Sets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/FlexibleConformalHighestPredictiveConditionalDensitySets.html" rel="alternate" type="text/html" title="Flexible Conformal Highest Predictive Conditional Density Sets" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/FlexibleConformalHighestPredictiveConditionalDensitySets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/FlexibleConformalHighestPredictiveConditionalDensitySets.html">&lt;p&gt;We introduce our method, conformal highest conditional density sets (CHCDS), that forms conformal prediction sets using existing estimated conditional highest density predictive regions. We prove the validity of the method and that conformal adjustment is negligible under some regularity conditions. In particular, if we correctly specify the underlying conditional density estimator, the conformal adjustment will be negligible. When the underlying model is incorrect, the conformal adjustment provides guaranteed nominal unconditional coverage. We compare the proposed method via simulation and a real data analysis to other existing methods. Our numerical results show that the flexibility of being able to use any existing conditional density estimation method is a large advantage for CHCDS compared to existing methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18052&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Max Sampson, Kung-Sik Chan</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We introduce our method, conformal highest conditional density sets (CHCDS), that forms conformal prediction sets using existing estimated conditional highest density predictive regions. We prove the validity of the method and that conformal adjustment is negligible under some regularity conditions. In particular, if we correctly specify the underlying conditional density estimator, the conformal adjustment will be negligible. When the underlying model is incorrect, the conformal adjustment provides guaranteed nominal unconditional coverage. We compare the proposed method via simulation and a real data analysis to other existing methods. Our numerical results show that the flexibility of being able to use any existing conditional density estimation method is a large advantage for CHCDS compared to existing methods.</summary></entry><entry><title type="html">Functional Time Transformation Model with Applications to Digital Health</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/FunctionalTimeTransformationModelwithApplicationstoDigitalHealth.html" rel="alternate" type="text/html" title="Functional Time Transformation Model with Applications to Digital Health" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/FunctionalTimeTransformationModelwithApplicationstoDigitalHealth</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/FunctionalTimeTransformationModelwithApplicationstoDigitalHealth.html">&lt;p&gt;The advent of wearable and sensor technologies now leads to functional predictors which are intrinsically infinite dimensional. While the existing approaches for functional data and survival outcomes lean on the well-established Cox model, the proportional hazard (PH) assumption might not always be suitable in real-world applications. Motivated by physiological signals encountered in digital medicine, we develop a more general and flexible functional time-transformation model for estimating the conditional survival function with both functional and scalar covariates. A partially functional regression model is used to directly model the survival time on the covariates through an unknown monotone transformation and a known error distribution. We use Bernstein polynomials to model the monotone transformation function and the smooth functional coefficients. A sieve method of maximum likelihood is employed for estimation. Numerical simulations illustrate a satisfactory performance of the proposed method in estimation and inference. We demonstrate the application of the proposed model through two case studies involving wearable data i) Understanding the association between diurnal physical activity pattern and all-cause mortality based on accelerometer data from the National Health and Nutrition Examination Survey (NHANES) 2011-2014 and ii) Modelling Time-to-Hypoglycemia events in a cohort of diabetic patients based on distributional representation of continuous glucose monitoring (CGM) data. The results provide important epidemiological insights into the direct association between survival times and the physiological signals and also exhibit superior predictive performance compared to traditional summary based biomarkers in the CGM study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19716&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rahul Ghosal, Marcos Matabuena, Sujit K. Ghosh</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The advent of wearable and sensor technologies now leads to functional predictors which are intrinsically infinite dimensional. While the existing approaches for functional data and survival outcomes lean on the well-established Cox model, the proportional hazard (PH) assumption might not always be suitable in real-world applications. Motivated by physiological signals encountered in digital medicine, we develop a more general and flexible functional time-transformation model for estimating the conditional survival function with both functional and scalar covariates. A partially functional regression model is used to directly model the survival time on the covariates through an unknown monotone transformation and a known error distribution. We use Bernstein polynomials to model the monotone transformation function and the smooth functional coefficients. A sieve method of maximum likelihood is employed for estimation. Numerical simulations illustrate a satisfactory performance of the proposed method in estimation and inference. We demonstrate the application of the proposed model through two case studies involving wearable data i) Understanding the association between diurnal physical activity pattern and all-cause mortality based on accelerometer data from the National Health and Nutrition Examination Survey (NHANES) 2011-2014 and ii) Modelling Time-to-Hypoglycemia events in a cohort of diabetic patients based on distributional representation of continuous glucose monitoring (CGM) data. The results provide important epidemiological insights into the direct association between survival times and the physiological signals and also exhibit superior predictive performance compared to traditional summary based biomarkers in the CGM study.</summary></entry><entry><title type="html">Futility analyses for the MCP-Mod methodology based on longitudinal models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/FutilityanalysesfortheMCPModmethodologybasedonlongitudinalmodels.html" rel="alternate" type="text/html" title="Futility analyses for the MCP-Mod methodology based on longitudinal models" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/FutilityanalysesfortheMCPModmethodologybasedonlongitudinalmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/FutilityanalysesfortheMCPModmethodologybasedonlongitudinalmodels.html">&lt;p&gt;This article discusses futility analyses for the MCP-Mod methodology. Formulas are derived for calculating predictive and conditional power for MCP-Mod, which also cover the case when longitudinal models are used allowing to utilize incomplete data from patients at interim. A simulation study is conducted to evaluate the repeated sampling properties of the proposed decision rules and to assess the benefit of using a longitudinal versus a completer only model for decision making at interim. The results suggest that the proposed methods perform adequately and a longitudinal analysis outperforms a completer only analysis, particularly when the recruitment speed is higher and the correlation over time is larger. The proposed methodology is illustrated using real data from a dose-finding study for severe uncontrolled asthma.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19965&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Björn Bornkamp, Jie Zhou, Dong Xi, Weihua Cao</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">This article discusses futility analyses for the MCP-Mod methodology. Formulas are derived for calculating predictive and conditional power for MCP-Mod, which also cover the case when longitudinal models are used allowing to utilize incomplete data from patients at interim. A simulation study is conducted to evaluate the repeated sampling properties of the proposed decision rules and to assess the benefit of using a longitudinal versus a completer only model for decision making at interim. The results suggest that the proposed methods perform adequately and a longitudinal analysis outperforms a completer only analysis, particularly when the recruitment speed is higher and the correlation over time is larger. The proposed methodology is illustrated using real data from a dose-finding study for severe uncontrolled asthma.</summary></entry><entry><title type="html">Generalizing self-normalized importance sampling with couplings</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Generalizingselfnormalizedimportancesamplingwithcouplings.html" rel="alternate" type="text/html" title="Generalizing self-normalized importance sampling with couplings" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Generalizingselfnormalizedimportancesamplingwithcouplings</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Generalizingselfnormalizedimportancesamplingwithcouplings.html">&lt;p&gt;An essential problem in statistics and machine learning is the estimation of expectations involving PDFs with intractable normalizing constants. The self-normalized importance sampling (SNIS) estimator, which normalizes the IS weights, has become the standard approach due to its simplicity. However, the SNIS has been shown to exhibit high variance in challenging estimation problems, e.g, involving rare events or posterior predictive distributions in Bayesian statistics. Further, most of the state-of-the-art adaptive importance sampling (AIS) methods adapt the proposal as if the weights had not been normalized. In this paper, we propose a framework that considers the original task as estimation of a ratio of two integrals. In our new formulation, we obtain samples from a joint proposal distribution in an extended space, with two of its marginals playing the role of proposals used to estimate each integral. Importantly, the framework allows us to induce and control a dependency between both estimators. We propose a construction of the joint proposal that decomposes in two (multivariate) marginals and a coupling. This leads to a two-stage framework suitable to be integrated with existing or new AIS and/or variational inference (VI) algorithms. The marginals are adapted in the first stage, while the coupling can be chosen and adapted in the second stage. We show in several examples the benefits of the proposed methodology, including an application to Bayesian prediction with misspecified models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19974&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nicola Branchini, Víctor Elvira</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">An essential problem in statistics and machine learning is the estimation of expectations involving PDFs with intractable normalizing constants. The self-normalized importance sampling (SNIS) estimator, which normalizes the IS weights, has become the standard approach due to its simplicity. However, the SNIS has been shown to exhibit high variance in challenging estimation problems, e.g, involving rare events or posterior predictive distributions in Bayesian statistics. Further, most of the state-of-the-art adaptive importance sampling (AIS) methods adapt the proposal as if the weights had not been normalized. In this paper, we propose a framework that considers the original task as estimation of a ratio of two integrals. In our new formulation, we obtain samples from a joint proposal distribution in an extended space, with two of its marginals playing the role of proposals used to estimate each integral. Importantly, the framework allows us to induce and control a dependency between both estimators. We propose a construction of the joint proposal that decomposes in two (multivariate) marginals and a coupling. This leads to a two-stage framework suitable to be integrated with existing or new AIS and/or variational inference (VI) algorithms. The marginals are adapted in the first stage, while the coupling can be chosen and adapted in the second stage. We show in several examples the benefits of the proposed methodology, including an application to Bayesian prediction with misspecified models.</summary></entry><entry><title type="html">Geodesic Causal Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/GeodesicCausalInference.html" rel="alternate" type="text/html" title="Geodesic Causal Inference" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/GeodesicCausalInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/GeodesicCausalInference.html">&lt;p&gt;Adjusting for confounding and imbalance when establishing statistical relationships is an increasingly important task, and causal inference methods have emerged as the most popular tool to achieve this. Causal inference has been developed mainly for scalar outcomes and recently for distributional outcomes. We introduce here a general framework for causal inference when outcomes reside in general geodesic metric spaces, where we draw on a novel geodesic calculus that facilitates scalar multiplication for geodesics and the characterization of treatment effects through the concept of the geodesic average treatment effect. Using ideas from Fr&apos;echet regression, we develop estimation methods of the geodesic average treatment effect and derive consistency and rates of convergence for the proposed estimators. We also study uncertainty quantification and inference for the treatment effect. Our methodology is illustrated by a simulation study and real data examples for compositional outcomes of U.S. statewise energy source data to study the effect of coal mining, network data of New York taxi trips, where the effect of the COVID-19 pandemic is of interest, and brain functional connectivity network data to study the effect of Alzheimer’s disease.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19604&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Daisuke Kurisu, Yidong Zhou, Taisuke Otsu, Hans-Georg Müller</name></author><category term="stat.ME" /><summary type="html">Adjusting for confounding and imbalance when establishing statistical relationships is an increasingly important task, and causal inference methods have emerged as the most popular tool to achieve this. Causal inference has been developed mainly for scalar outcomes and recently for distributional outcomes. We introduce here a general framework for causal inference when outcomes reside in general geodesic metric spaces, where we draw on a novel geodesic calculus that facilitates scalar multiplication for geodesics and the characterization of treatment effects through the concept of the geodesic average treatment effect. Using ideas from Fr&apos;echet regression, we develop estimation methods of the geodesic average treatment effect and derive consistency and rates of convergence for the proposed estimators. We also study uncertainty quantification and inference for the treatment effect. Our methodology is illustrated by a simulation study and real data examples for compositional outcomes of U.S. statewise energy source data to study the effect of coal mining, network data of New York taxi trips, where the effect of the COVID-19 pandemic is of interest, and brain functional connectivity network data to study the effect of Alzheimer’s disease.</summary></entry><entry><title type="html">Hierarchical Mixture of Finite Mixtures</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/HierarchicalMixtureofFiniteMixtures.html" rel="alternate" type="text/html" title="Hierarchical Mixture of Finite Mixtures" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/HierarchicalMixtureofFiniteMixtures</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/HierarchicalMixtureofFiniteMixtures.html">&lt;p&gt;Statistical modelling in the presence of data organized in groups is a crucial task in Bayesian statistics. The present paper conceives a mixture model based on a novel family of Bayesian priors designed for multilevel data and obtained by normalizing a finite point process. In particular, the work extends the popular Mixture of Finite Mixture model to the hierarchical framework to capture heterogeneity within and between groups. A full distribution theory for this new family and the induced clustering is developed, including the marginal, posterior, and predictive distributions. Efficient marginal and conditional Gibbs samplers are designed to provide posterior inference. The proposed mixture model overcomes the Hierarchical Dirichlet Process, the utmost tool for handling multilevel data, in terms of analytical feasibility, clustering discovery, and computational time. The motivating application comes from the analysis of shot put data, which contains performance measurements of athletes across different seasons. In this setting, the proposed model is exploited to induce clustering of the observations across seasons and athletes. By linking clusters across seasons, similarities and differences in athletes’ performances are identified.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.20376&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alessandro Colombi, Raffaele Argiento, Federico Camerlenghi, Lucia Paci</name></author><category term="stat.ME" /><summary type="html">Statistical modelling in the presence of data organized in groups is a crucial task in Bayesian statistics. The present paper conceives a mixture model based on a novel family of Bayesian priors designed for multilevel data and obtained by normalizing a finite point process. In particular, the work extends the popular Mixture of Finite Mixture model to the hierarchical framework to capture heterogeneity within and between groups. A full distribution theory for this new family and the induced clustering is developed, including the marginal, posterior, and predictive distributions. Efficient marginal and conditional Gibbs samplers are designed to provide posterior inference. The proposed mixture model overcomes the Hierarchical Dirichlet Process, the utmost tool for handling multilevel data, in terms of analytical feasibility, clustering discovery, and computational time. The motivating application comes from the analysis of shot put data, which contains performance measurements of athletes across different seasons. In this setting, the proposed model is exploited to induce clustering of the observations across seasons and athletes. By linking clusters across seasons, similarities and differences in athletes’ performances are identified.</summary></entry><entry><title type="html">Hypothesis-driven mediation analysis for compositional data: an application to gut microbiome</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Hypothesisdrivenmediationanalysisforcompositionaldataanapplicationtogutmicrobiome.html" rel="alternate" type="text/html" title="Hypothesis-driven mediation analysis for compositional data: an application to gut microbiome" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Hypothesisdrivenmediationanalysisforcompositionaldataanapplicationtogutmicrobiome</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Hypothesisdrivenmediationanalysisforcompositionaldataanapplicationtogutmicrobiome.html">&lt;p&gt;Biological sequencing data consist of read counts, e.g. of specified taxa and often exhibit sparsity (zero-count inflation) and overdispersion (extra-Poisson variability). As most sequencing techniques provide an arbitrary total count, taxon-specific counts should ideally be treated as proportions under the compositional data-analytic framework. There is increasing interest in the role of the gut microbiome composition in mediating the effects of different exposures on health outcomes. Most previous approaches to compositional mediation have addressed the problem of identifying potentially mediating taxa among a large number of candidates. We here consider causal inference in compositional mediation when a priori knowledge is available about the hierarchy for a restricted number of taxa, building on a single hypothesis structured in terms of contrasts between appropriate sub-compositions. Based on the theory on multiple contemporaneous mediators and the assumed causal graph, we define non-parametric estimands for overall and coordinate-wise mediation effects, and show how these indirect effects can be estimated from empirical data based on simple parametric linear models. The mediators have straightforward and coherent interpretations, related to specific causal questions about the interrelationships between the sub-compositions. We perform a simulation study focusing on the impact of sparsity and overdispersion on estimation of mediation. While unbiased, the precision of the estimators depends, for any given magnitude of indirect effect, on sparsity and the relative magnitudes of exposure-to-mediator and mediator-to-outcome effects in a complex manner. We demonstrate the approach on empirical data, finding an inverse association of fibre intake on insulin level, mainly attributable to direct rather than indirect effects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.16000&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Noora Kartiosuo, Jaakko Nevalainen, Olli Raitakari, Katja Pahkala, Kari Auranen</name></author><category term="stat.AP" /><summary type="html">Biological sequencing data consist of read counts, e.g. of specified taxa and often exhibit sparsity (zero-count inflation) and overdispersion (extra-Poisson variability). As most sequencing techniques provide an arbitrary total count, taxon-specific counts should ideally be treated as proportions under the compositional data-analytic framework. There is increasing interest in the role of the gut microbiome composition in mediating the effects of different exposures on health outcomes. Most previous approaches to compositional mediation have addressed the problem of identifying potentially mediating taxa among a large number of candidates. We here consider causal inference in compositional mediation when a priori knowledge is available about the hierarchy for a restricted number of taxa, building on a single hypothesis structured in terms of contrasts between appropriate sub-compositions. Based on the theory on multiple contemporaneous mediators and the assumed causal graph, we define non-parametric estimands for overall and coordinate-wise mediation effects, and show how these indirect effects can be estimated from empirical data based on simple parametric linear models. The mediators have straightforward and coherent interpretations, related to specific causal questions about the interrelationships between the sub-compositions. We perform a simulation study focusing on the impact of sparsity and overdispersion on estimation of mediation. While unbiased, the precision of the estimators depends, for any given magnitude of indirect effect, on sparsity and the relative magnitudes of exposure-to-mediator and mediator-to-outcome effects in a complex manner. We demonstrate the approach on empirical data, finding an inverse association of fibre intake on insulin level, mainly attributable to direct rather than indirect effects.</summary></entry><entry><title type="html">Improving Finite Sample Performance of Causal Discovery by Exploiting Temporal Structure</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ImprovingFiniteSamplePerformanceofCausalDiscoverybyExploitingTemporalStructure.html" rel="alternate" type="text/html" title="Improving Finite Sample Performance of Causal Discovery by Exploiting Temporal Structure" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ImprovingFiniteSamplePerformanceofCausalDiscoverybyExploitingTemporalStructure</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ImprovingFiniteSamplePerformanceofCausalDiscoverybyExploitingTemporalStructure.html">&lt;p&gt;Methods of causal discovery aim to identify causal structures in a data driven way. Existing algorithms are known to be unstable and sensitive to statistical errors, and are therefore rarely used with biomedical or epidemiological data. We present an algorithm that efficiently exploits temporal structure, so-called tiered background knowledge, for estimating causal structures. Tiered background knowledge is readily available from, e.g., cohort or registry data. When used efficiently it renders the algorithm more robust to statistical errors and ultimately increases accuracy in finite samples. We describe the algorithm and illustrate how it proceeds. Moreover, we offer formal proofs as well as examples of desirable properties of the algorithm, which we demonstrate empirically in an extensive simulation study. To illustrate its usefulness in practice, we apply the algorithm to data from a children’s cohort study investigating the interplay of diet, physical activity and other lifestyle factors for health outcomes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19503&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Christine W Bang, Janine Witte, Ronja Foraita, Vanessa Didelez</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Methods of causal discovery aim to identify causal structures in a data driven way. Existing algorithms are known to be unstable and sensitive to statistical errors, and are therefore rarely used with biomedical or epidemiological data. We present an algorithm that efficiently exploits temporal structure, so-called tiered background knowledge, for estimating causal structures. Tiered background knowledge is readily available from, e.g., cohort or registry data. When used efficiently it renders the algorithm more robust to statistical errors and ultimately increases accuracy in finite samples. We describe the algorithm and illustrate how it proceeds. Moreover, we offer formal proofs as well as examples of desirable properties of the algorithm, which we demonstrate empirically in an extensive simulation study. To illustrate its usefulness in practice, we apply the algorithm to data from a children’s cohort study investigating the interplay of diet, physical activity and other lifestyle factors for health outcomes.</summary></entry><entry><title type="html">Instrumental Variable Estimation of Distributional Causal Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/InstrumentalVariableEstimationofDistributionalCausalEffects.html" rel="alternate" type="text/html" title="Instrumental Variable Estimation of Distributional Causal Effects" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/InstrumentalVariableEstimationofDistributionalCausalEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/InstrumentalVariableEstimationofDistributionalCausalEffects.html">&lt;p&gt;Estimating the causal effect of a treatment on the entire response distribution is an important yet challenging task. For instance, one might be interested in how a pension plan affects not only the average savings among all individuals but also how it affects the entire savings distribution. While sufficiently large randomized studies can be used to estimate such distributional causal effects, they are often either not feasible in practice or involve non-compliance. A well-established class of methods for estimating average causal effects from either observational studies with unmeasured confounding or randomized studies with non-compliance are instrumental variable (IV) methods. In this work, we develop an IV-based approach for identifying and estimating distributional causal effects. We introduce a distributional IV model with corresponding assumptions, which leads to a novel identification result for the interventional cumulative distribution function (CDF) under a binary treatment. We then use this identification to construct a nonparametric estimator, called DIVE, for estimating the interventional CDFs under both treatments. We empirically assess the performance of DIVE in a simulation experiment and illustrate the usefulness of distributional causal effects on two real-data applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19986&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lucas Kook, Niklas Pfister</name></author><category term="stat.ME" /><summary type="html">Estimating the causal effect of a treatment on the entire response distribution is an important yet challenging task. For instance, one might be interested in how a pension plan affects not only the average savings among all individuals but also how it affects the entire savings distribution. While sufficiently large randomized studies can be used to estimate such distributional causal effects, they are often either not feasible in practice or involve non-compliance. A well-established class of methods for estimating average causal effects from either observational studies with unmeasured confounding or randomized studies with non-compliance are instrumental variable (IV) methods. In this work, we develop an IV-based approach for identifying and estimating distributional causal effects. We introduce a distributional IV model with corresponding assumptions, which leads to a novel identification result for the interventional cumulative distribution function (CDF) under a binary treatment. We then use this identification to construct a nonparametric estimator, called DIVE, for estimating the interventional CDFs under both treatments. We empirically assess the performance of DIVE in a simulation experiment and illustrate the usefulness of distributional causal effects on two real-data applications.</summary></entry><entry><title type="html">Joint estimation of insurance loss development factors using Bayesian hidden Markov models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/JointestimationofinsurancelossdevelopmentfactorsusingBayesianhiddenMarkovmodels.html" rel="alternate" type="text/html" title="Joint estimation of insurance loss development factors using Bayesian hidden Markov models" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/JointestimationofinsurancelossdevelopmentfactorsusingBayesianhiddenMarkovmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/JointestimationofinsurancelossdevelopmentfactorsusingBayesianhiddenMarkovmodels.html">&lt;p&gt;Loss development modelling is the actuarial practice of predicting the total ‘ultimate’ losses incurred on a set of policies once all claims are reported and settled. This poses a challenging prediction task as losses frequently take years to fully emerge from reported claims, and not all claims might yet be reported. Loss development models frequently estimate a set of ‘link ratios’ from insurance loss triangles, which are multiplicative factors transforming losses at one time point to ultimate. However, link ratios estimated using classical methods typically underestimate ultimate losses and cannot be extrapolated outside the domains of the triangle, requiring extension by ‘tail factors’ from another model. Although flexible, this two-step process relies on subjective decision points that might bias inference. Methods that jointly estimate ‘body’ link ratios and smooth tail factors offer an attractive alternative. This paper proposes a novel application of Bayesian hidden Markov models to loss development modelling, where discrete, latent states representing body and tail processes are automatically learned from the data. The hidden Markov development model is found to perform comparably to, and frequently better than, the two-step approach on numerical examples and industry datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19903&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Conor Goold</name></author><category term="stat.ME" /><summary type="html">Loss development modelling is the actuarial practice of predicting the total ‘ultimate’ losses incurred on a set of policies once all claims are reported and settled. This poses a challenging prediction task as losses frequently take years to fully emerge from reported claims, and not all claims might yet be reported. Loss development models frequently estimate a set of ‘link ratios’ from insurance loss triangles, which are multiplicative factors transforming losses at one time point to ultimate. However, link ratios estimated using classical methods typically underestimate ultimate losses and cannot be extrapolated outside the domains of the triangle, requiring extension by ‘tail factors’ from another model. Although flexible, this two-step process relies on subjective decision points that might bias inference. Methods that jointly estimate ‘body’ link ratios and smooth tail factors offer an attractive alternative. This paper proposes a novel application of Bayesian hidden Markov models to loss development modelling, where discrete, latent states representing body and tail processes are automatically learned from the data. The hidden Markov development model is found to perform comparably to, and frequently better than, the two-step approach on numerical examples and industry datasets.</summary></entry><entry><title type="html">Markov chain Monte Carlo without evaluating the target: an auxiliary variable approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/MarkovchainMonteCarlowithoutevaluatingthetargetanauxiliaryvariableapproach.html" rel="alternate" type="text/html" title="Markov chain Monte Carlo without evaluating the target: an auxiliary variable approach" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/MarkovchainMonteCarlowithoutevaluatingthetargetanauxiliaryvariableapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/MarkovchainMonteCarlowithoutevaluatingthetargetanauxiliaryvariableapproach.html">&lt;p&gt;In sampling tasks, it is common for target distributions to be known up to a normalising constant. However, in many situations, evaluating even the unnormalised distribution can be costly or infeasible. This issue arises in scenarios such as sampling from the Bayesian posterior for tall datasets and the ‘doubly-intractable’ distributions. In this paper, we begin by observing that seemingly different Markov chain Monte Carlo (MCMC) algorithms, such as the exchange algorithm, PoissonMH, and TunaMH, can be unified under a simple common procedure. We then extend this procedure into a novel framework that allows the use of auxiliary variables in both the proposal and acceptance-rejection steps. We develop the theory of the new framework, applying it to existing algorithms to simplify and extend their results. Several new algorithms emerge from this framework, with improved performance demonstrated on both synthetic and real datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05242&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wei Yuan, Guanyang Wang</name></author><category term="stat.CO," /><category term="stat.ME," /><category term="stat.ML" /><summary type="html">In sampling tasks, it is common for target distributions to be known up to a normalising constant. However, in many situations, evaluating even the unnormalised distribution can be costly or infeasible. This issue arises in scenarios such as sampling from the Bayesian posterior for tall datasets and the ‘doubly-intractable’ distributions. In this paper, we begin by observing that seemingly different Markov chain Monte Carlo (MCMC) algorithms, such as the exchange algorithm, PoissonMH, and TunaMH, can be unified under a simple common procedure. We then extend this procedure into a novel framework that allows the use of auxiliary variables in both the proposal and acceptance-rejection steps. We develop the theory of the new framework, applying it to existing algorithms to simplify and extend their results. Several new algorithms emerge from this framework, with improved performance demonstrated on both synthetic and real datasets.</summary></entry><entry><title type="html">Mathematical modelling and uncertainty quantification for analysis of biphasic coral reef recovery patterns</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Mathematicalmodellinganduncertaintyquantificationforanalysisofbiphasiccoralreefrecoverypatterns.html" rel="alternate" type="text/html" title="Mathematical modelling and uncertainty quantification for analysis of biphasic coral reef recovery patterns" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Mathematicalmodellinganduncertaintyquantificationforanalysisofbiphasiccoralreefrecoverypatterns</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Mathematicalmodellinganduncertaintyquantificationforanalysisofbiphasiccoralreefrecoverypatterns.html">&lt;p&gt;Coral reefs are increasingly subjected to major disturbances threatening the health of marine ecosystems. Substantial research underway to develop intervention strategies that assist reefs in recovery from, and resistance to, inevitable future climate and weather extremes. To assess potential benefits of interventions, mechanistic understanding of coral reef recovery and resistance patterns is essential. Recent evidence suggests that more than half of the reefs surveyed across the Great Barrier Reef (GBR) exhibit deviations from standard recovery modelling assumptions when the initial coral cover is low ($\leq 10$\%). New modelling is necessary to account for these observed patterns to better inform management strategies. We consider a new model for reef recovery at the coral cover scale that accounts for biphasic recovery patterns. The model is based on a multispecies Richards’ growth model that includes a change point in the recovery patterns. Bayesian inference is applied for uncertainty quantification of key parameters for assessing reef health and recovery patterns. This analysis is applied to benthic survey data from the Australian Institute of Marine Sciences (AIMS). We demonstrate agreement between model predictions and data across every recorded recovery trajectory with at least two years of observations following disturbance events occurring between 1992–2020. This new approach will enable new insights into the biological, ecological and environmental factors that contribute to the duration and severity of biphasic coral recovery patterns across the GBR. These new insights will help to inform managements and monitoring practice to mitigate the impacts of climate change on coral reefs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19591&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David J. Warne, Kerryn Crossman, Grace E. M. Heron, Jesse A. Sharp, Wang Jin, Paul Pao-Yen Wu, Matthew J. Simpson, Kerrie Mengersen, Juan-Carlos Ortiz</name></author><category term="stat.AP" /><summary type="html">Coral reefs are increasingly subjected to major disturbances threatening the health of marine ecosystems. Substantial research underway to develop intervention strategies that assist reefs in recovery from, and resistance to, inevitable future climate and weather extremes. To assess potential benefits of interventions, mechanistic understanding of coral reef recovery and resistance patterns is essential. Recent evidence suggests that more than half of the reefs surveyed across the Great Barrier Reef (GBR) exhibit deviations from standard recovery modelling assumptions when the initial coral cover is low ($\leq 10$\%). New modelling is necessary to account for these observed patterns to better inform management strategies. We consider a new model for reef recovery at the coral cover scale that accounts for biphasic recovery patterns. The model is based on a multispecies Richards’ growth model that includes a change point in the recovery patterns. Bayesian inference is applied for uncertainty quantification of key parameters for assessing reef health and recovery patterns. This analysis is applied to benthic survey data from the Australian Institute of Marine Sciences (AIMS). We demonstrate agreement between model predictions and data across every recorded recovery trajectory with at least two years of observations following disturbance events occurring between 1992–2020. This new approach will enable new insights into the biological, ecological and environmental factors that contribute to the duration and severity of biphasic coral recovery patterns across the GBR. These new insights will help to inform managements and monitoring practice to mitigate the impacts of climate change on coral reefs.</summary></entry><entry><title type="html">Minimax And Adaptive Transfer Learning for Nonparametric Classification under Distributed Differential Privacy Constraints</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/MinimaxAndAdaptiveTransferLearningforNonparametricClassificationunderDistributedDifferentialPrivacyConstraints.html" rel="alternate" type="text/html" title="Minimax And Adaptive Transfer Learning for Nonparametric Classification under Distributed Differential Privacy Constraints" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/MinimaxAndAdaptiveTransferLearningforNonparametricClassificationunderDistributedDifferentialPrivacyConstraints</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/MinimaxAndAdaptiveTransferLearningforNonparametricClassificationunderDistributedDifferentialPrivacyConstraints.html">&lt;p&gt;This paper considers minimax and adaptive transfer learning for nonparametric classification under the posterior drift model with distributed differential privacy constraints. Our study is conducted within a heterogeneous framework, encompassing diverse sample sizes, varying privacy parameters, and data heterogeneity across different servers. We first establish the minimax misclassification rate, precisely characterizing the effects of privacy constraints, source samples, and target samples on classification accuracy. The results reveal interesting phase transition phenomena and highlight the intricate trade-offs between preserving privacy and achieving classification accuracy. We then develop a data-driven adaptive classifier that achieves the optimal rate within a logarithmic factor across a large collection of parameter spaces while satisfying the same set of differential privacy constraints. Simulation studies and real-world data applications further elucidate the theoretical analysis with numerical results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.20088&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arnab Auddy, T. Tony Cai, Abhinav Chakraborty</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">This paper considers minimax and adaptive transfer learning for nonparametric classification under the posterior drift model with distributed differential privacy constraints. Our study is conducted within a heterogeneous framework, encompassing diverse sample sizes, varying privacy parameters, and data heterogeneity across different servers. We first establish the minimax misclassification rate, precisely characterizing the effects of privacy constraints, source samples, and target samples on classification accuracy. The results reveal interesting phase transition phenomena and highlight the intricate trade-offs between preserving privacy and achieving classification accuracy. We then develop a data-driven adaptive classifier that achieves the optimal rate within a logarithmic factor across a large collection of parameter spaces while satisfying the same set of differential privacy constraints. Simulation studies and real-world data applications further elucidate the theoretical analysis with numerical results.</summary></entry><entry><title type="html">Mixture of Directed Graphical Models for Discrete Spatial Random Fields</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/MixtureofDirectedGraphicalModelsforDiscreteSpatialRandomFields.html" rel="alternate" type="text/html" title="Mixture of Directed Graphical Models for Discrete Spatial Random Fields" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/MixtureofDirectedGraphicalModelsforDiscreteSpatialRandomFields</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/MixtureofDirectedGraphicalModelsforDiscreteSpatialRandomFields.html">&lt;p&gt;Current approaches for modeling discrete-valued outcomes associated with spatially-dependent areal units incur computational and theoretical challenges, especially in the Bayesian setting when full posterior inference is desired. As an alternative, we propose a novel statistical modeling framework for this data setting, namely a mixture of directed graphical models (MDGMs). The components of the mixture, directed graphical models, can be represented by directed acyclic graphs (DAGs) and are computationally quick to evaluate. The DAGs representing the mixture components are selected to correspond to an undirected graphical representation of an assumed spatial contiguity/dependence structure of the areal units, which underlies the specification of traditional modeling approaches for discrete spatial processes such as Markov random fields (MRFs). We introduce the concept of compatibility to show how an undirected graph can be used as a template for the structural dependencies between areal units to create sets of DAGs which, as a collection, preserve the structural dependencies represented in the template undirected graph. We then introduce three classes of compatible DAGs and corresponding algorithms for fitting MDGMs based on these classes. In addition, we compare MDGMs to MRFs and a popular Bayesian MRF model approximation used in high-dimensional settings in a series of simulations and an analysis of ecometrics data collected as part of the Adolescent Health and Development in Context Study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15700&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>J. Brandon Carter, Catherine A. Calder</name></author><category term="stat.ME" /><summary type="html">Current approaches for modeling discrete-valued outcomes associated with spatially-dependent areal units incur computational and theoretical challenges, especially in the Bayesian setting when full posterior inference is desired. As an alternative, we propose a novel statistical modeling framework for this data setting, namely a mixture of directed graphical models (MDGMs). The components of the mixture, directed graphical models, can be represented by directed acyclic graphs (DAGs) and are computationally quick to evaluate. The DAGs representing the mixture components are selected to correspond to an undirected graphical representation of an assumed spatial contiguity/dependence structure of the areal units, which underlies the specification of traditional modeling approaches for discrete spatial processes such as Markov random fields (MRFs). We introduce the concept of compatibility to show how an undirected graph can be used as a template for the structural dependencies between areal units to create sets of DAGs which, as a collection, preserve the structural dependencies represented in the template undirected graph. We then introduce three classes of compatible DAGs and corresponding algorithms for fitting MDGMs based on these classes. In addition, we compare MDGMs to MRFs and a popular Bayesian MRF model approximation used in high-dimensional settings in a series of simulations and an analysis of ecometrics data collected as part of the Adolescent Health and Development in Context Study.</summary></entry><entry><title type="html">Modeling trajectories using functional linear differential equations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Modelingtrajectoriesusingfunctionallineardifferentialequations.html" rel="alternate" type="text/html" title="Modeling trajectories using functional linear differential equations" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Modelingtrajectoriesusingfunctionallineardifferentialequations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Modelingtrajectoriesusingfunctionallineardifferentialequations.html">&lt;p&gt;We are motivated by a study that seeks to better understand the dynamic relationship between muscle activation and paw position during locomotion. For each gait cycle in this experiment, activation in the biceps and triceps is measured continuously and in parallel with paw position as a mouse trotted on a treadmill. We propose an innovative general regression method that draws from both ordinary differential equations and functional data analysis to model the relationship between these functional inputs and responses as a dynamical system that evolves over time. Specifically, our model addresses gaps in both literatures and borrows strength across curves estimating ODE parameters across all curves simultaneously rather than separately modeling each functional observation. Our approach compares favorably to related functional data methods in simulations and in cross-validated predictive accuracy of paw position in the gait data. In the analysis of the gait cycles, we find that paw speed and position are dynamically influenced by inputs from the biceps and triceps muscles, and that the effect of muscle activation persists beyond the activation itself.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19535&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Julia Wrobel, Britton Sauerbrei, Erik A. Kirk, Jian-Zhong Guo, Adam Hantman, Jeff Goldsmith</name></author><category term="stat.ME" /><summary type="html">We are motivated by a study that seeks to better understand the dynamic relationship between muscle activation and paw position during locomotion. For each gait cycle in this experiment, activation in the biceps and triceps is measured continuously and in parallel with paw position as a mouse trotted on a treadmill. We propose an innovative general regression method that draws from both ordinary differential equations and functional data analysis to model the relationship between these functional inputs and responses as a dynamical system that evolves over time. Specifically, our model addresses gaps in both literatures and borrows strength across curves estimating ODE parameters across all curves simultaneously rather than separately modeling each functional observation. Our approach compares favorably to related functional data methods in simulations and in cross-validated predictive accuracy of paw position in the gait data. In the analysis of the gait cycles, we find that paw speed and position are dynamically influenced by inputs from the biceps and triceps muscles, and that the effect of muscle activation persists beyond the activation itself.</summary></entry><entry><title type="html">On Counterfactual Interventions in Vector Autoregressive Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/OnCounterfactualInterventionsinVectorAutoregressiveModels.html" rel="alternate" type="text/html" title="On Counterfactual Interventions in Vector Autoregressive Models" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/OnCounterfactualInterventionsinVectorAutoregressiveModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/OnCounterfactualInterventionsinVectorAutoregressiveModels.html">&lt;p&gt;Counterfactual reasoning allows us to explore hypothetical scenarios in order to explain the impacts of our decisions. However, addressing such inquires is impossible without establishing the appropriate mathematical framework. In this work, we introduce the problem of counterfactual reasoning in the context of vector autoregressive (VAR) processes. We also formulate the inference of a causal model as a joint regression task where for inference we use both data with and without interventions. After learning the model, we exploit linearity of the VAR model to make exact predictions about the effects of counterfactual interventions. Furthermore, we quantify the total causal effects of past counterfactual interventions. The source code for this project is freely available at https://github.com/KurtButler/counterfactual_interventions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19573&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kurt Butler, Marija Iloska, Petar M. Djuric</name></author><category term="stat.ME" /><summary type="html">Counterfactual reasoning allows us to explore hypothetical scenarios in order to explain the impacts of our decisions. However, addressing such inquires is impossible without establishing the appropriate mathematical framework. In this work, we introduce the problem of counterfactual reasoning in the context of vector autoregressive (VAR) processes. We also formulate the inference of a causal model as a joint regression task where for inference we use both data with and without interventions. After learning the model, we exploit linearity of the VAR model to make exact predictions about the effects of counterfactual interventions. Furthermore, we quantify the total causal effects of past counterfactual interventions. The source code for this project is freely available at https://github.com/KurtButler/counterfactual_interventions.</summary></entry><entry><title type="html">Optimal subsampling for functional composite quantile regression in massive data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Optimalsubsamplingforfunctionalcompositequantileregressioninmassivedata.html" rel="alternate" type="text/html" title="Optimal subsampling for functional composite quantile regression in massive data" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Optimalsubsamplingforfunctionalcompositequantileregressioninmassivedata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Optimalsubsamplingforfunctionalcompositequantileregressioninmassivedata.html">&lt;p&gt;As computer resources become increasingly limited, traditional statistical methods face challenges in analyzing massive data, especially in functional data analysis. To address this issue, subsampling offers a viable solution by significantly reducing computational requirements. This paper introduces a subsampling technique for composite quantile regression, designed for efficient application within the functional linear model on large datasets. We establish the asymptotic distribution of the subsampling estimator and introduce an optimal subsampling method based on the functional L-optimality criterion. Results from simulation studies and the real data analysis consistently demonstrate the superiority of the L-optimality criterion-based optimal subsampling method over the uniform subsampling approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19691&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jingxiang Pan, Xiaohui Yuan, Xiaohui Yuan</name></author><category term="stat.ME" /><summary type="html">As computer resources become increasingly limited, traditional statistical methods face challenges in analyzing massive data, especially in functional data analysis. To address this issue, subsampling offers a viable solution by significantly reducing computational requirements. This paper introduces a subsampling technique for composite quantile regression, designed for efficient application within the functional linear model on large datasets. We establish the asymptotic distribution of the subsampling estimator and introduce an optimal subsampling method based on the functional L-optimality criterion. Results from simulation studies and the real data analysis consistently demonstrate the superiority of the L-optimality criterion-based optimal subsampling method over the uniform subsampling approach.</summary></entry><entry><title type="html">Predictability of climate tipping focusing on the internal variability of the Earth system</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/PredictabilityofclimatetippingfocusingontheinternalvariabilityoftheEarthsystem.html" rel="alternate" type="text/html" title="Predictability of climate tipping focusing on the internal variability of the Earth system" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/PredictabilityofclimatetippingfocusingontheinternalvariabilityoftheEarthsystem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/PredictabilityofclimatetippingfocusingontheinternalvariabilityoftheEarthsystem.html">&lt;p&gt;Prediction of climate tipping is challenging due to the lack of recent observation of actual climate tipping. Despite many previous efforts to accurately predict the existence and timing of climate tippings under specific climate scenarios, the predictability of climate tipping, the necessary conditions under which climate tipping can be predicted, has yet to be explored. In this study, the predictability of climate tipping is analyzed by Observation System Simulation Experiment (OSSE), in which the value of observation for prediction is assessed through the idealized experiment of data assimilation, using a simplified dynamic vegetation model and an Atlantic Meridional Overturning Circulation (AMOC) two box model. We find that the ratio of internal variability to observation error, or signal-to-noise ratio, should be large enough to accurately predict climate tippings. When observation can accurately resolve the internal variability of the system, assimilating these observations into process-based models can effectively improve the skill of predicting climate tippings. Our quantitative estimation of required observation accuracy to predict climate tipping implies that the existing observation network is not always sufficient to accurately project climate tipping.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19639&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Amane Kubo, Yohei Sawada</name></author><category term="stat.AP" /><summary type="html">Prediction of climate tipping is challenging due to the lack of recent observation of actual climate tipping. Despite many previous efforts to accurately predict the existence and timing of climate tippings under specific climate scenarios, the predictability of climate tipping, the necessary conditions under which climate tipping can be predicted, has yet to be explored. In this study, the predictability of climate tipping is analyzed by Observation System Simulation Experiment (OSSE), in which the value of observation for prediction is assessed through the idealized experiment of data assimilation, using a simplified dynamic vegetation model and an Atlantic Meridional Overturning Circulation (AMOC) two box model. We find that the ratio of internal variability to observation error, or signal-to-noise ratio, should be large enough to accurately predict climate tippings. When observation can accurately resolve the internal variability of the system, assimilating these observations into process-based models can effectively improve the skill of predicting climate tippings. Our quantitative estimation of required observation accuracy to predict climate tipping implies that the existing observation network is not always sufficient to accurately project climate tipping.</summary></entry><entry><title type="html">Provably Efficient Posterior Sampling for Sparse Linear Regression via Measure Decomposition</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ProvablyEfficientPosteriorSamplingforSparseLinearRegressionviaMeasureDecomposition.html" rel="alternate" type="text/html" title="Provably Efficient Posterior Sampling for Sparse Linear Regression via Measure Decomposition" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ProvablyEfficientPosteriorSamplingforSparseLinearRegressionviaMeasureDecomposition</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/ProvablyEfficientPosteriorSamplingforSparseLinearRegressionviaMeasureDecomposition.html">&lt;p&gt;We consider the problem of sampling from the posterior distribution of a $d$-dimensional coefficient vector $\boldsymbol{\theta}$, given linear observations $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\theta}+\boldsymbol{\varepsilon}$. In general, such posteriors are multimodal, and therefore challenging to sample from. This observation has prompted the exploration of various heuristics that aim at approximating the posterior distribution.
  In this paper, we study a different approach based on decomposing the posterior distribution into a log-concave mixture of simple product measures. This decomposition allows us to reduce sampling from a multimodal distribution of interest to sampling from a log-concave one, which is tractable and has been investigated in detail. We prove that, under mild conditions on the prior, for random designs, such measure decomposition is generally feasible when the number of samples per parameter $n/d$ exceeds a constant threshold. We thus obtain a provably efficient (polynomial time) sampling algorithm in a regime where this was previously not known. Numerical simulations confirm that the algorithm is practical, and reveal that it has attractive statistical properties compared to state-of-the-art methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19550&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrea Montanari, Yuchen Wu</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We consider the problem of sampling from the posterior distribution of a $d$-dimensional coefficient vector $\boldsymbol{\theta}$, given linear observations $\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\theta}+\boldsymbol{\varepsilon}$. In general, such posteriors are multimodal, and therefore challenging to sample from. This observation has prompted the exploration of various heuristics that aim at approximating the posterior distribution. In this paper, we study a different approach based on decomposing the posterior distribution into a log-concave mixture of simple product measures. This decomposition allows us to reduce sampling from a multimodal distribution of interest to sampling from a log-concave one, which is tractable and has been investigated in detail. We prove that, under mild conditions on the prior, for random designs, such measure decomposition is generally feasible when the number of samples per parameter $n/d$ exceeds a constant threshold. We thus obtain a provably efficient (polynomial time) sampling algorithm in a regime where this was previously not known. Numerical simulations confirm that the algorithm is practical, and reveal that it has attractive statistical properties compared to state-of-the-art methods.</summary></entry><entry><title type="html">Semiparametric Discovery and Estimation of Interaction in Mixed Exposures using Stochastic Interventions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/SemiparametricDiscoveryandEstimationofInteractioninMixedExposuresusingStochasticInterventions.html" rel="alternate" type="text/html" title="Semiparametric Discovery and Estimation of Interaction in Mixed Exposures using Stochastic Interventions" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/SemiparametricDiscoveryandEstimationofInteractioninMixedExposuresusingStochasticInterventions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/SemiparametricDiscoveryandEstimationofInteractioninMixedExposuresusingStochasticInterventions.html">&lt;p&gt;This study introduces a nonparametric definition of interaction and provides an approach to both interaction discovery and efficient estimation of this parameter. Using stochastic shift interventions and ensemble machine learning, our approach identifies and quantifies interaction effects through a model-independent target parameter, estimated via targeted maximum likelihood and cross-validation. This method contrasts the expected outcomes of joint interventions with those of individual interventions. Validation through simulation and application to the National Institute of Environmental Health Sciences Mixtures Workshop data demonstrate the efficacy of our method in detecting true interaction directions and its consistency in identifying significant impacts of furan exposure on leukocyte telomere length. Our method, called InterXshift, advances the ability to analyze multi-exposure interactions within high-dimensional data, offering significant methodological improvements to understand complex exposure dynamics in health research. We provide peer-reviewed open-source software that employs or proposed methodology in the InterXshift R package.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.01849&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David B. McCoy, Alan E. Hubbard, Alejandro Schuler, Mark J. van der Laan</name></author><category term="stat.ME" /><summary type="html">This study introduces a nonparametric definition of interaction and provides an approach to both interaction discovery and efficient estimation of this parameter. Using stochastic shift interventions and ensemble machine learning, our approach identifies and quantifies interaction effects through a model-independent target parameter, estimated via targeted maximum likelihood and cross-validation. This method contrasts the expected outcomes of joint interventions with those of individual interventions. Validation through simulation and application to the National Institute of Environmental Health Sciences Mixtures Workshop data demonstrate the efficacy of our method in detecting true interaction directions and its consistency in identifying significant impacts of furan exposure on leukocyte telomere length. Our method, called InterXshift, advances the ability to analyze multi-exposure interactions within high-dimensional data, offering significant methodological improvements to understand complex exposure dynamics in health research. We provide peer-reviewed open-source software that employs or proposed methodology in the InterXshift R package.</summary></entry><entry><title type="html">Stock Volume Forecasting with Advanced Information by Conditional Variational Auto-Encoder</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/StockVolumeForecastingwithAdvancedInformationbyConditionalVariationalAutoEncoder.html" rel="alternate" type="text/html" title="Stock Volume Forecasting with Advanced Information by Conditional Variational Auto-Encoder" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/StockVolumeForecastingwithAdvancedInformationbyConditionalVariationalAutoEncoder</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/StockVolumeForecastingwithAdvancedInformationbyConditionalVariationalAutoEncoder.html">&lt;p&gt;We demonstrate the use of Conditional Variational Encoder (CVAE) to improve the forecasts of daily stock volume time series in both short and long term forecasting tasks, with the use of advanced information of input variables such as rebalancing dates. CVAE generates non-linear time series as out-of-sample forecasts, which have better accuracy and closer fit of correlation to the actual data, compared to traditional linear models. These generative forecasts can also be used for scenario generation, which aids interpretation. We further discuss correlations in non-stationary time series and other potential extensions from the CVAE forecasts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19414&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Parley R Yang, Alexander Y Shestopaloff</name></author><category term="stat.AP," /><category term="stat.ML," /><category term="stat.OT" /><summary type="html">We demonstrate the use of Conditional Variational Encoder (CVAE) to improve the forecasts of daily stock volume time series in both short and long term forecasting tasks, with the use of advanced information of input variables such as rebalancing dates. CVAE generates non-linear time series as out-of-sample forecasts, which have better accuracy and closer fit of correlation to the actual data, compared to traditional linear models. These generative forecasts can also be used for scenario generation, which aids interpretation. We further discuss correlations in non-stationary time series and other potential extensions from the CVAE forecasts.</summary></entry><entry><title type="html">Structured prior distributions for the covariance matrix in latent factor models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Structuredpriordistributionsforthecovariancematrixinlatentfactormodels.html" rel="alternate" type="text/html" title="Structured prior distributions for the covariance matrix in latent factor models" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Structuredpriordistributionsforthecovariancematrixinlatentfactormodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/Structuredpriordistributionsforthecovariancematrixinlatentfactormodels.html">&lt;p&gt;Factor models are widely used for dimension reduction in the analysis of multivariate data. This is achieved through decomposition of a p x p covariance matrix into the sum of two components. Through a latent factor representation, they can be interpreted as a diagonal matrix of idiosyncratic variances and a shared variation matrix, that is, the product of a p x k factor loadings matrix and its transpose. If k « p, this defines a parsimonious factorisation of the covariance matrix. Historically, little attention has been paid to incorporating prior information in Bayesian analyses using factor models where, at best, the prior for the factor loadings is order invariant. In this work, a class of structured priors is developed that can encode ideas of dependence structure about the shared variation matrix. The construction allows data-informed shrinkage towards sensible parametric structures while also facilitating inference over the number of factors. Using an unconstrained reparameterisation of stationary vector autoregressions, the methodology is extended to stationary dynamic factor models. For computational inference, parameter-expanded Markov chain Monte Carlo samplers are proposed, including an efficient adaptive Gibbs sampler. Two substantive applications showcase the scope of the methodology and its inferential benefits.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.07831&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sarah Elizabeth Heaps, Ian Hyla Jermyn</name></author><category term="stat.ME" /><summary type="html">Factor models are widely used for dimension reduction in the analysis of multivariate data. This is achieved through decomposition of a p x p covariance matrix into the sum of two components. Through a latent factor representation, they can be interpreted as a diagonal matrix of idiosyncratic variances and a shared variation matrix, that is, the product of a p x k factor loadings matrix and its transpose. If k « p, this defines a parsimonious factorisation of the covariance matrix. Historically, little attention has been paid to incorporating prior information in Bayesian analyses using factor models where, at best, the prior for the factor loadings is order invariant. In this work, a class of structured priors is developed that can encode ideas of dependence structure about the shared variation matrix. The construction allows data-informed shrinkage towards sensible parametric structures while also facilitating inference over the number of factors. Using an unconstrained reparameterisation of stationary vector autoregressions, the methodology is extended to stationary dynamic factor models. For computational inference, parameter-expanded Markov chain Monte Carlo samplers are proposed, including an efficient adaptive Gibbs sampler. Two substantive applications showcase the scope of the methodology and its inferential benefits.</summary></entry><entry><title type="html">Surrogate model for Bayesian optimal experimental design in chromatography</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/SurrogatemodelforBayesianoptimalexperimentaldesigninchromatography.html" rel="alternate" type="text/html" title="Surrogate model for Bayesian optimal experimental design in chromatography" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/SurrogatemodelforBayesianoptimalexperimentaldesigninchromatography</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/SurrogatemodelforBayesianoptimalexperimentaldesigninchromatography.html">&lt;p&gt;We applied Bayesian Optimal Experimental Design (OED) in the estimation of parameters involved in the Equilibrium Dispersive Model for chromatography with two components with the Langmuir adsorption isotherm. The coefficients estimated were Henry’s coefficients, the total absorption capacity and the number of theoretical plates, while the design variables were the injection time and the initial concentration. The Bayesian OED algorithm is based on nested Monte Carlo estimation, which becomes computationally challenging due to the simulation time of the PDE involved in the dispersive model. This complication was relaxed by introducing a surrogate model based on Piecewise Sparse Linear Interpolation. Using the surrogate model instead the original reduces significantly the simulation time and it approximates the solution of the PDE with high degree of accuracy. The estimation of the parameters over strategical design points provided by OED reduces the uncertainty in the estimation of parameters. Additionally, the Bayesian OED methodology indicates no improvements when increasing the number of measurements in temporal nodes above a threshold value.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19835&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jose Rodrigo Rojo-Garcia, Heikki Haario, Tapio Helin, Tuomo Sainio</name></author><category term="stat.AP" /><summary type="html">We applied Bayesian Optimal Experimental Design (OED) in the estimation of parameters involved in the Equilibrium Dispersive Model for chromatography with two components with the Langmuir adsorption isotherm. The coefficients estimated were Henry’s coefficients, the total absorption capacity and the number of theoretical plates, while the design variables were the injection time and the initial concentration. The Bayesian OED algorithm is based on nested Monte Carlo estimation, which becomes computationally challenging due to the simulation time of the PDE involved in the dispersive model. This complication was relaxed by introducing a surrogate model based on Piecewise Sparse Linear Interpolation. Using the surrogate model instead the original reduces significantly the simulation time and it approximates the solution of the PDE with high degree of accuracy. The estimation of the parameters over strategical design points provided by OED reduces the uncertainty in the estimation of parameters. Additionally, the Bayesian OED methodology indicates no improvements when increasing the number of measurements in temporal nodes above a threshold value.</summary></entry><entry><title type="html">Trade-off between predictive performance and FDR control for high-dimensional Gaussian model selection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/TradeoffbetweenpredictiveperformanceandFDRcontrolforhighdimensionalGaussianmodelselection.html" rel="alternate" type="text/html" title="Trade-off between predictive performance and FDR control for high-dimensional Gaussian model selection" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/TradeoffbetweenpredictiveperformanceandFDRcontrolforhighdimensionalGaussianmodelselection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/TradeoffbetweenpredictiveperformanceandFDRcontrolforhighdimensionalGaussianmodelselection.html">&lt;p&gt;In the context of high-dimensional Gaussian linear regression for ordered variables, we study the variable selection procedure via the minimization of the penalized least-squares criterion. We focus on model selection where the penalty function depends on an unknown multiplicative constant commonly calibrated for prediction. We propose a new proper calibration of this hyperparameter to simultaneously control predictive risk and false discovery rate. We obtain non-asymptotic bounds on the False Discovery Rate with respect to the hyperparameter and we provide an algorithm to calibrate it. This algorithm is based on quantities that can typically be observed in real data applications. The algorithm is validated in an extensive simulation study and is compared with several existing variable selection procedures. Finally, we study an extension of our approach to the case in which an ordering of the variables is not available.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.01831&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Perrine Lacroix, Marie-Laure Martin</name></author><category term="stat.AP," /><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In the context of high-dimensional Gaussian linear regression for ordered variables, we study the variable selection procedure via the minimization of the penalized least-squares criterion. We focus on model selection where the penalty function depends on an unknown multiplicative constant commonly calibrated for prediction. We propose a new proper calibration of this hyperparameter to simultaneously control predictive risk and false discovery rate. We obtain non-asymptotic bounds on the False Discovery Rate with respect to the hyperparameter and we provide an algorithm to calibrate it. This algorithm is based on quantities that can typically be observed in real data applications. The algorithm is validated in an extensive simulation study and is compared with several existing variable selection procedures. Finally, we study an extension of our approach to the case in which an ordering of the variables is not available.</summary></entry><entry><title type="html">Vector AutoRegressive Moving Average Models: A Review</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/VectorAutoRegressiveMovingAverageModelsAReview.html" rel="alternate" type="text/html" title="Vector AutoRegressive Moving Average Models: A Review" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/VectorAutoRegressiveMovingAverageModelsAReview</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/VectorAutoRegressiveMovingAverageModelsAReview.html">&lt;p&gt;Vector AutoRegressive Moving Average (VARMA) models form a powerful and general model class for analyzing dynamics among multiple time series. While VARMA models encompass the Vector AutoRegressive (VAR) models, their popularity in empirical applications is dominated by the latter. Can this phenomenon be explained fully by the simplicity of VAR models? Perhaps many users of VAR models have not fully appreciated what VARMA models can provide. The goal of this review is to provide a comprehensive resource for researchers and practitioners seeking insights into the advantages and capabilities of VARMA models. We start by reviewing the identification challenges inherent to VARMA models thereby encompassing classical and modern identification schemes and we continue along the same lines regarding estimation, specification and diagnosis of VARMA models. We then highlight the practical utility of VARMA models in terms of Granger Causality analysis, forecasting and structural analysis as well as recent advances and extensions of VARMA models to further facilitate their adoption in practice. Finally, we discuss some interesting future research directions where VARMA models can fulfill their potentials in applications as compared to their subclass of VAR models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19702&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marie-Christine Düker, David S. Matteson, Ruey S. Tsay, Ines Wilms</name></author><category term="stat.ME" /><summary type="html">Vector AutoRegressive Moving Average (VARMA) models form a powerful and general model class for analyzing dynamics among multiple time series. While VARMA models encompass the Vector AutoRegressive (VAR) models, their popularity in empirical applications is dominated by the latter. Can this phenomenon be explained fully by the simplicity of VAR models? Perhaps many users of VAR models have not fully appreciated what VARMA models can provide. The goal of this review is to provide a comprehensive resource for researchers and practitioners seeking insights into the advantages and capabilities of VARMA models. We start by reviewing the identification challenges inherent to VARMA models thereby encompassing classical and modern identification schemes and we continue along the same lines regarding estimation, specification and diagnosis of VARMA models. We then highlight the practical utility of VARMA models in terms of Granger Causality analysis, forecasting and structural analysis as well as recent advances and extensions of VARMA models to further facilitate their adoption in practice. Finally, we discuss some interesting future research directions where VARMA models can fulfill their potentials in applications as compared to their subclass of VAR models.</summary></entry><entry><title type="html">Weighted Particle-Based Optimization for Efficient Generalized Posterior Calibration</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/WeightedParticleBasedOptimizationforEfficientGeneralizedPosteriorCalibration.html" rel="alternate" type="text/html" title="Weighted Particle-Based Optimization for Efficient Generalized Posterior Calibration" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/WeightedParticleBasedOptimizationforEfficientGeneralizedPosteriorCalibration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/WeightedParticleBasedOptimizationforEfficientGeneralizedPosteriorCalibration.html">&lt;p&gt;In the realm of statistical learning, the increasing volume of accessible data and increasing model complexity necessitate robust methodologies. This paper explores two branches of robust Bayesian methods in response to this trend. The first is generalized Bayesian inference, which introduces a learning rate parameter to enhance robustness against model misspecifications. The second is Gibbs posterior inference, which formulates inferential problems using generic loss functions rather than probabilistic models. In such approaches, it is necessary to calibrate the spread of the posterior distribution by selecting a learning rate parameter. The study aims to enhance the generalized posterior calibration (GPC) algorithm proposed by [1]. Their algorithm chooses the learning rate to achieve the nominal frequentist coverage probability, but it is computationally intensive because it requires repeated posterior simulations for bootstrap samples. We propose a more efficient version of the GPC inspired by sequential Monte Carlo (SMC) samplers. A target distribution with a different learning rate is evaluated without posterior simulation as in the reweighting step in SMC sampling. Thus, the proposed algorithm can reach the desirable value within a few iterations. This improvement substantially reduces the computational cost of the GPC. Its efficacy is demonstrated through synthetic and real data applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04845&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Masahiro Tanaka</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">In the realm of statistical learning, the increasing volume of accessible data and increasing model complexity necessitate robust methodologies. This paper explores two branches of robust Bayesian methods in response to this trend. The first is generalized Bayesian inference, which introduces a learning rate parameter to enhance robustness against model misspecifications. The second is Gibbs posterior inference, which formulates inferential problems using generic loss functions rather than probabilistic models. In such approaches, it is necessary to calibrate the spread of the posterior distribution by selecting a learning rate parameter. The study aims to enhance the generalized posterior calibration (GPC) algorithm proposed by [1]. Their algorithm chooses the learning rate to achieve the nominal frequentist coverage probability, but it is computationally intensive because it requires repeated posterior simulations for bootstrap samples. We propose a more efficient version of the GPC inspired by sequential Monte Carlo (SMC) samplers. A target distribution with a different learning rate is evaluated without posterior simulation as in the reweighting step in SMC sampling. Thus, the proposed algorithm can reach the desirable value within a few iterations. This improvement substantially reduces the computational cost of the GPC. Its efficacy is demonstrated through synthetic and real data applications.</summary></entry><entry><title type="html">What’s the Weight? Estimating Controlled Outcome Differences in Complex Surveys for Health Disparities Research</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/WhatstheWeightEstimatingControlledOutcomeDifferencesinComplexSurveysforHealthDisparitiesResearch.html" rel="alternate" type="text/html" title="What’s the Weight? Estimating Controlled Outcome Differences in Complex Surveys for Health Disparities Research" /><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/WhatstheWeightEstimatingControlledOutcomeDifferencesinComplexSurveysforHealthDisparitiesResearch</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/07/01/WhatstheWeightEstimatingControlledOutcomeDifferencesinComplexSurveysforHealthDisparitiesResearch.html">&lt;p&gt;A basic descriptive question in statistics often asks whether there are differences in mean outcomes between groups based on levels of a discrete covariate (e.g., racial disparities in health outcomes). However, when this categorical covariate of interest is correlated with other factors related to the outcome, direct comparisons may lead to biased estimates and invalid inferential conclusions without appropriate adjustment. Propensity score methods are broadly employed with observational data as a tool to achieve covariate balance, but how to implement them in complex surveys is less studied - in particular, when the survey weights depend on the group variable under comparison. In this work, we focus on a specific example when sample selection depends on race. We propose identification formulas to properly estimate the average controlled difference (ACD) in outcomes between Black and White individuals, with appropriate weighting for covariate imbalance across the two racial groups and generalizability. Via extensive simulation, we show that our proposed methods outperform traditional analytic approaches in terms of bias, mean squared error, and coverage. We are motivated by the interplay between race and social determinants of health when estimating racial differences in telomere length using data from the National Health and Nutrition Examination Survey. We build a propensity for race to properly adjust for other social determinants while characterizing the controlled effect of race on telomere length. We find that evidence of racial differences in telomere length between Black and White individuals attenuates after accounting for confounding by socioeconomic factors and after utilizing appropriate propensity score and survey weighting techniques. Software to implement these methods can be found in the R package svycdiff at https://github.com/salernos/svycdiff.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19597&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Stephen Salerno, Emily K. Roberts, Belinda L. Needham, Tyler H. McCormick, Bhramar Mukherjee, Xu Shi</name></author><category term="stat.ME" /><summary type="html">A basic descriptive question in statistics often asks whether there are differences in mean outcomes between groups based on levels of a discrete covariate (e.g., racial disparities in health outcomes). However, when this categorical covariate of interest is correlated with other factors related to the outcome, direct comparisons may lead to biased estimates and invalid inferential conclusions without appropriate adjustment. Propensity score methods are broadly employed with observational data as a tool to achieve covariate balance, but how to implement them in complex surveys is less studied - in particular, when the survey weights depend on the group variable under comparison. In this work, we focus on a specific example when sample selection depends on race. We propose identification formulas to properly estimate the average controlled difference (ACD) in outcomes between Black and White individuals, with appropriate weighting for covariate imbalance across the two racial groups and generalizability. Via extensive simulation, we show that our proposed methods outperform traditional analytic approaches in terms of bias, mean squared error, and coverage. We are motivated by the interplay between race and social determinants of health when estimating racial differences in telomere length using data from the National Health and Nutrition Examination Survey. We build a propensity for race to properly adjust for other social determinants while characterizing the controlled effect of race on telomere length. We find that evidence of racial differences in telomere length between Black and White individuals attenuates after accounting for confounding by socioeconomic factors and after utilizing appropriate propensity score and survey weighting techniques. Software to implement these methods can be found in the R package svycdiff at https://github.com/salernos/svycdiff.</summary></entry></feed>