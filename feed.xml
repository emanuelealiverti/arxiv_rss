<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-13T07:14:07+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Diagnostic Tool for Functional Causal Discovery</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ADiagnosticToolforFunctionalCausalDiscovery.html" rel="alternate" type="text/html" title="A Diagnostic Tool for Functional Causal Discovery" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ADiagnosticToolforFunctionalCausalDiscovery</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ADiagnosticToolforFunctionalCausalDiscovery.html">&lt;p&gt;Causal discovery methods aim to determine the causal direction between variables using observational data. Functional causal discovery methods, such as those based on the Linear Non-Gaussian Acyclic Model (LiNGAM), rely on structural and distributional assumptions to infer the causal direction. However, approaches for assessing causal discovery methods’ performance as a function of sample size or the impact of assumption violations, inevitable in real-world scenarios, are lacking. To address this need, we propose Causal Direction Detection Rate (CDDR) diagnostic that evaluates whether and to what extent the interaction between assumption violations and sample size affects the ability to identify the hypothesized causal direction. Given a bivariate dataset of size N on a pair of variables, X and Y, CDDR diagnostic is the plotted comparison of the probability of each causal discovery outcome (e.g. X causes Y, Y causes X, or inconclusive) as a function of sample size less than N. We fully develop CDDR diagnostic in a bivariate case and demonstrate its use for two methods, LiNGAM and our new test-based causal discovery approach. We find CDDR diagnostic for the test-based approach to be more informative since it uses a richer set of causal discovery outcomes. Under certain assumptions, we prove that the probability estimates of detecting each possible causal discovery outcome are consistent and asymptotically normal. Through simulations, we study CDDR diagnostic’s behavior when linearity and non-Gaussianity assumptions are violated. Additionally, we illustrate CDDR diagnostic on four real datasets, including three for which the causal direction is known.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07787&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shreya Prakash, Fan Xia, Elena Erosheva</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Causal discovery methods aim to determine the causal direction between variables using observational data. Functional causal discovery methods, such as those based on the Linear Non-Gaussian Acyclic Model (LiNGAM), rely on structural and distributional assumptions to infer the causal direction. However, approaches for assessing causal discovery methods’ performance as a function of sample size or the impact of assumption violations, inevitable in real-world scenarios, are lacking. To address this need, we propose Causal Direction Detection Rate (CDDR) diagnostic that evaluates whether and to what extent the interaction between assumption violations and sample size affects the ability to identify the hypothesized causal direction. Given a bivariate dataset of size N on a pair of variables, X and Y, CDDR diagnostic is the plotted comparison of the probability of each causal discovery outcome (e.g. X causes Y, Y causes X, or inconclusive) as a function of sample size less than N. We fully develop CDDR diagnostic in a bivariate case and demonstrate its use for two methods, LiNGAM and our new test-based causal discovery approach. We find CDDR diagnostic for the test-based approach to be more informative since it uses a richer set of causal discovery outcomes. Under certain assumptions, we prove that the probability estimates of detecting each possible causal discovery outcome are consistent and asymptotically normal. Through simulations, we study CDDR diagnostic’s behavior when linearity and non-Gaussianity assumptions are violated. Additionally, we illustrate CDDR diagnostic on four real datasets, including three for which the causal direction is known.</summary></entry><entry><title type="html">A Survey of Pipeline Tools for Data Engineering</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ASurveyofPipelineToolsforDataEngineering.html" rel="alternate" type="text/html" title="A Survey of Pipeline Tools for Data Engineering" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ASurveyofPipelineToolsforDataEngineering</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ASurveyofPipelineToolsforDataEngineering.html">&lt;p&gt;Currently, a variety of pipeline tools are available for use in data engineering. Data scientists can use these tools to resolve data wrangling issues associated with data and accomplish some data engineering tasks from data ingestion through data preparation to utilization as input for machine learning (ML). Some of these tools have essential built-in components or can be combined with other tools to perform desired data engineering operations. While some tools are wholly or partly commercial, several open-source tools are available to perform expert-level data engineering tasks. This survey examines the broad categories and examples of pipeline tools based on their design and data engineering intentions. These categories are Extract Transform Load/Extract Load Transform (ETL/ELT), pipelines for Data Integration, Ingestion, and Transformation, Data Pipeline Orchestration and Workflow Management, and Machine Learning Pipelines. The survey also provides a broad outline of the utilization with examples within these broad groups and finally, a discussion is presented with case studies indicating the usage of pipeline tools for data engineering. The studies present some first-user application experiences with sample data, some complexities of the applied pipeline, and a summary note of approaches to using these tools to prepare data for machine learning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08335&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anthony Mbata, Yaji Sripada, Mingjun Zhong</name></author><category term="stat.CO" /><summary type="html">Currently, a variety of pipeline tools are available for use in data engineering. Data scientists can use these tools to resolve data wrangling issues associated with data and accomplish some data engineering tasks from data ingestion through data preparation to utilization as input for machine learning (ML). Some of these tools have essential built-in components or can be combined with other tools to perform desired data engineering operations. While some tools are wholly or partly commercial, several open-source tools are available to perform expert-level data engineering tasks. This survey examines the broad categories and examples of pipeline tools based on their design and data engineering intentions. These categories are Extract Transform Load/Extract Load Transform (ETL/ELT), pipelines for Data Integration, Ingestion, and Transformation, Data Pipeline Orchestration and Workflow Management, and Machine Learning Pipelines. The survey also provides a broad outline of the utilization with examples within these broad groups and finally, a discussion is presented with case studies indicating the usage of pipeline tools for data engineering. The studies present some first-user application experiences with sample data, some complexities of the applied pipeline, and a summary note of approaches to using these tools to prepare data for machine learning.</summary></entry><entry><title type="html">A computationally efficient procedure for combining ecological datasets by means of sequential consensus inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Acomputationallyefficientprocedureforcombiningecologicaldatasetsbymeansofsequentialconsensusinference.html" rel="alternate" type="text/html" title="A computationally efficient procedure for combining ecological datasets by means of sequential consensus inference" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Acomputationallyefficientprocedureforcombiningecologicaldatasetsbymeansofsequentialconsensusinference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Acomputationallyefficientprocedureforcombiningecologicaldatasetsbymeansofsequentialconsensusinference.html">&lt;p&gt;Combining data has become an indispensable tool for managing the current diversity and abundance of data. But, as data complexity and data volume swell, the computational demands of previously proposed models for combining data escalate proportionally, posing a significant challenge to practical implementation. This study presents a sequential consensus Bayesian inference procedure that allows for a flexible definition of models, aiming to emulate the versatility of integrated models while significantly reducing their computational cost. The method is based on updating the distribution of the fixed effects and hyperparameters from their marginal posterior distribution throughout a sequential inference procedure, and performing a consensus on the random effects after the sequential inference is completed. The applicability, together with its strengths and limitations, is outlined in the methodological description of the procedure. The sequential consensus method is presented in two distinct algorithms. The first algorithm performs a sequential updating and consensus from the stored values of the marginal or joint posterior distribution of the random effects. The second algorithm performs an extra step, addressing the deficiencies that may arise when the model partition does not share the whole latent field. The performance of the procedure is shown by three different examples – one simulated and two with real data – intending to expose its strengths and limitations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08174&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mario Figueira, David Conesa, Antonio López-Quílez, Iosu Paradinas</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Combining data has become an indispensable tool for managing the current diversity and abundance of data. But, as data complexity and data volume swell, the computational demands of previously proposed models for combining data escalate proportionally, posing a significant challenge to practical implementation. This study presents a sequential consensus Bayesian inference procedure that allows for a flexible definition of models, aiming to emulate the versatility of integrated models while significantly reducing their computational cost. The method is based on updating the distribution of the fixed effects and hyperparameters from their marginal posterior distribution throughout a sequential inference procedure, and performing a consensus on the random effects after the sequential inference is completed. The applicability, together with its strengths and limitations, is outlined in the methodological description of the procedure. The sequential consensus method is presented in two distinct algorithms. The first algorithm performs a sequential updating and consensus from the stored values of the marginal or joint posterior distribution of the random effects. The second algorithm performs an extra step, addressing the deficiencies that may arise when the model partition does not share the whole latent field. The performance of the procedure is shown by three different examples – one simulated and two with real data – intending to expose its strengths and limitations.</summary></entry><entry><title type="html">A flexible empirical Bayes approach to multiple linear regression and connections with penalized regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/AflexibleempiricalBayesapproachtomultiplelinearregressionandconnectionswithpenalizedregression.html" rel="alternate" type="text/html" title="A flexible empirical Bayes approach to multiple linear regression and connections with penalized regression" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/AflexibleempiricalBayesapproachtomultiplelinearregressionandconnectionswithpenalizedregression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/AflexibleempiricalBayesapproachtomultiplelinearregressionandconnectionswithpenalizedregression.html">&lt;p&gt;We introduce a new empirical Bayes approach for large-scale multiple linear regression. Our approach combines two key ideas: (i) the use of flexible “adaptive shrinkage” priors, which approximate the nonparametric family of scale mixture of normal distributions by a finite mixture of normal distributions; and (ii) the use of variational approximations to efficiently estimate prior hyperparameters and compute approximate posteriors. Combining these two ideas results in fast and flexible methods, with computational speed comparable to fast penalized regression methods such as the Lasso, and with competitive prediction accuracy across a wide range of scenarios. Further, we provide new results that establish conceptual connections between our empirical Bayes methods and penalized methods. Specifically, we show that the posterior mean from our method solves a penalized regression problem, with the form of the penalty function being learned from the data by directly solving an optimization problem (rather than being tuned by cross-validation). Our methods are implemented in an R package, mr.ash.alpha, available from https://github.com/stephenslab/mr.ash.alpha.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.10910&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Youngseok Kim, Wei Wang, Peter Carbonetto, Matthew Stephens</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We introduce a new empirical Bayes approach for large-scale multiple linear regression. Our approach combines two key ideas: (i) the use of flexible “adaptive shrinkage” priors, which approximate the nonparametric family of scale mixture of normal distributions by a finite mixture of normal distributions; and (ii) the use of variational approximations to efficiently estimate prior hyperparameters and compute approximate posteriors. Combining these two ideas results in fast and flexible methods, with computational speed comparable to fast penalized regression methods such as the Lasso, and with competitive prediction accuracy across a wide range of scenarios. Further, we provide new results that establish conceptual connections between our empirical Bayes methods and penalized methods. Specifically, we show that the posterior mean from our method solves a penalized regression problem, with the form of the penalty function being learned from the data by directly solving an optimization problem (rather than being tuned by cross-validation). Our methods are implemented in an R package, mr.ash.alpha, available from https://github.com/stephenslab/mr.ash.alpha.</summary></entry><entry><title type="html">Arbitrary-Length Generalization for Addition in a Tiny Transformer</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ArbitraryLengthGeneralizationforAdditioninaTinyTransformer.html" rel="alternate" type="text/html" title="Arbitrary-Length Generalization for Addition in a Tiny Transformer" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ArbitraryLengthGeneralizationforAdditioninaTinyTransformer</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ArbitraryLengthGeneralizationforAdditioninaTinyTransformer.html">&lt;p&gt;This paper introduces a novel training methodology that enables a Transformer model to generalize the addition of two-digit numbers to numbers with unseen lengths of digits. The proposed approach employs an autoregressive generation technique, processing from right to left, which mimics a common manual method for adding large numbers. To the best of my knowledge, this methodology has not been previously explored in the literature. All results are reproducible, and the corresponding R code is available at github.com/AGPatriota/ALGA-R/.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00075&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexandre Galvao Patriota</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">This paper introduces a novel training methodology that enables a Transformer model to generalize the addition of two-digit numbers to numbers with unseen lengths of digits. The proposed approach employs an autoregressive generation technique, processing from right to left, which mimics a common manual method for adding large numbers. To the best of my knowledge, this methodology has not been previously explored in the literature. All results are reproducible, and the corresponding R code is available at github.com/AGPatriota/ALGA-R/.</summary></entry><entry><title type="html">A smoothed-Bayesian approach to frequency recovery from sketched data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/AsmoothedBayesianapproachtofrequencyrecoveryfromsketcheddata.html" rel="alternate" type="text/html" title="A smoothed-Bayesian approach to frequency recovery from sketched data" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/AsmoothedBayesianapproachtofrequencyrecoveryfromsketcheddata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/AsmoothedBayesianapproachtofrequencyrecoveryfromsketcheddata.html">&lt;p&gt;We provide a novel statistical perspective on a classical problem at the intersection of computer science and information theory: recovering the empirical frequency of a symbol in a large discrete dataset using only a compressed representation, or sketch, obtained via random hashing. Departing from traditional algorithmic approaches, recent works have proposed Bayesian nonparametric (BNP) methods that can provide more informative frequency estimates by leveraging modeling assumptions about the distribution of the sketched data. In this paper, we propose a {\em smoothed-Bayesian} method, inspired by existing BNP approaches but designed in a frequentist framework to overcome the computational limitations of the BNP approaches when dealing with large-scale data from realistic distributions, including those with power-law tail behaviors. For sketches obtained with a single hash function, our approach is supported by rigorous frequentist properties, including unbiasedness and optimality under a squared error loss function within an intuitive class of linear estimators. For sketches with multiple hash functions, we introduce an approach based on \emph{multi-view} learning to construct computationally efficient frequency estimators. We validate our method on synthetic and real data, comparing its performance to that of existing alternatives.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.15408&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mario Beraha, Stefano Favaro, Matteo Sesia</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We provide a novel statistical perspective on a classical problem at the intersection of computer science and information theory: recovering the empirical frequency of a symbol in a large discrete dataset using only a compressed representation, or sketch, obtained via random hashing. Departing from traditional algorithmic approaches, recent works have proposed Bayesian nonparametric (BNP) methods that can provide more informative frequency estimates by leveraging modeling assumptions about the distribution of the sketched data. In this paper, we propose a {\em smoothed-Bayesian} method, inspired by existing BNP approaches but designed in a frequentist framework to overcome the computational limitations of the BNP approaches when dealing with large-scale data from realistic distributions, including those with power-law tail behaviors. For sketches obtained with a single hash function, our approach is supported by rigorous frequentist properties, including unbiasedness and optimality under a squared error loss function within an intuitive class of linear estimators. For sketches with multiple hash functions, we introduce an approach based on \emph{multi-view} learning to construct computationally efficient frequency estimators. We validate our method on synthetic and real data, comparing its performance to that of existing alternatives.</summary></entry><entry><title type="html">Assessing Extreme Risk using Stochastic Simulation of Extremes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/AssessingExtremeRiskusingStochasticSimulationofExtremes.html" rel="alternate" type="text/html" title="Assessing Extreme Risk using Stochastic Simulation of Extremes" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/AssessingExtremeRiskusingStochasticSimulationofExtremes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/AssessingExtremeRiskusingStochasticSimulationofExtremes.html">&lt;p&gt;Risk management is particularly concerned with extreme events, but analysing these events is often hindered by the scarcity of data, especially in a multivariate context. This data scarcity complicates risk management efforts. Various tools can assess the risk posed by extreme events, even under extraordinary circumstances. This paper studies the evaluation of univariate risk for a given risk factor using metrics that account for its asymptotic dependence on other risk factors. Data availability is crucial, particularly for extreme events where it is often limited by the nature of the phenomenon itself, making estimation challenging. To address this issue, two non-parametric simulation algorithms based on multivariate extreme theory are developed. These algorithms aim to extend a sample of extremes jointly and conditionally for asymptotically dependent variables using stochastic simulation and multivariate Generalised Pareto Distributions. The approach is illustrated with numerical analyses of both simulated and real data to assess the accuracy of extreme risk metric estimations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08019&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nisrine Madhar, Juliette Legrand, Maud Thomas</name></author><category term="stat.ME" /><summary type="html">Risk management is particularly concerned with extreme events, but analysing these events is often hindered by the scarcity of data, especially in a multivariate context. This data scarcity complicates risk management efforts. Various tools can assess the risk posed by extreme events, even under extraordinary circumstances. This paper studies the evaluation of univariate risk for a given risk factor using metrics that account for its asymptotic dependence on other risk factors. Data availability is crucial, particularly for extreme events where it is often limited by the nature of the phenomenon itself, making estimation challenging. To address this issue, two non-parametric simulation algorithms based on multivariate extreme theory are developed. These algorithms aim to extend a sample of extremes jointly and conditionally for asymptotically dependent variables using stochastic simulation and multivariate Generalised Pareto Distributions. The approach is illustrated with numerical analyses of both simulated and real data to assess the accuracy of extreme risk metric estimations.</summary></entry><entry><title type="html">Batch and match: black-box variational inference with a score-based divergence</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Batchandmatchblackboxvariationalinferencewithascorebaseddivergence.html" rel="alternate" type="text/html" title="Batch and match: black-box variational inference with a score-based divergence" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Batchandmatchblackboxvariationalinferencewithascorebaseddivergence</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Batchandmatchblackboxvariationalinferencewithascorebaseddivergence.html">&lt;p&gt;Most leading implementations of black-box variational inference (BBVI) are based on optimizing a stochastic evidence lower bound (ELBO). But such approaches to BBVI often converge slowly due to the high variance of their gradient estimates and their sensitivity to hyperparameters. In this work, we propose batch and match (BaM), an alternative approach to BBVI based on a score-based divergence. Notably, this score-based divergence can be optimized by a closed-form proximal update for Gaussian variational families with full covariance matrices. We analyze the convergence of BaM when the target distribution is Gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance. We also evaluate the performance of BaM on Gaussian and non-Gaussian target distributions that arise from posterior inference in hierarchical and deep generative models. In these experiments, we find that BaM typically converges in fewer (and sometimes significantly fewer) gradient evaluations than leading implementations of BBVI based on ELBO maximization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.14758&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Diana Cai, Chirag Modi, Loucas Pillaud-Vivien, Charles C. Margossian, Robert M. Gower, David M. Blei, Lawrence K. Saul</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Most leading implementations of black-box variational inference (BBVI) are based on optimizing a stochastic evidence lower bound (ELBO). But such approaches to BBVI often converge slowly due to the high variance of their gradient estimates and their sensitivity to hyperparameters. In this work, we propose batch and match (BaM), an alternative approach to BBVI based on a score-based divergence. Notably, this score-based divergence can be optimized by a closed-form proximal update for Gaussian variational families with full covariance matrices. We analyze the convergence of BaM when the target distribution is Gaussian, and we prove that in the limit of infinite batch size the variational parameter updates converge exponentially quickly to the target mean and covariance. We also evaluate the performance of BaM on Gaussian and non-Gaussian target distributions that arise from posterior inference in hierarchical and deep generative models. In these experiments, we find that BaM typically converges in fewer (and sometimes significantly fewer) gradient evaluations than leading implementations of BBVI based on ELBO maximization.</summary></entry><entry><title type="html">Bridging multiple worlds: multi-marginal optimal transport for causal partial-identification problem</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Bridgingmultipleworldsmultimarginaloptimaltransportforcausalpartialidentificationproblem.html" rel="alternate" type="text/html" title="Bridging multiple worlds: multi-marginal optimal transport for causal partial-identification problem" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Bridgingmultipleworldsmultimarginaloptimaltransportforcausalpartialidentificationproblem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Bridgingmultipleworldsmultimarginaloptimaltransportforcausalpartialidentificationproblem.html">&lt;p&gt;Under the prevalent potential outcome model in causal inference, each unit is associated with multiple potential outcomes but at most one of which is observed, leading to many causal quantities being only partially identified. The inherent missing data issue echoes the multi-marginal optimal transport (MOT) problem, where marginal distributions are known, but how the marginals couple to form the joint distribution is unavailable. In this paper, we cast the causal partial identification problem in the framework of MOT with $K$ margins and $d$-dimensional outcomes and obtain the exact partial identified set. In order to estimate the partial identified set via MOT, statistically, we establish a convergence rate of the plug-in MOT estimator for general quadratic objective functions and prove it is minimax optimal for a quadratic objective function stemming from the variance minimization problem with arbitrary $K$ and $d \le 4$. Numerically, we demonstrate the efficacy of our method over several real-world datasets where our proposal consistently outperforms the baseline by a significant margin (over 70%). In addition, we provide efficient off-the-shelf implementations of MOT with general objective functions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07868&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zijun Gao, Shu Ge, Jian Qian</name></author><category term="stat.ME" /><summary type="html">Under the prevalent potential outcome model in causal inference, each unit is associated with multiple potential outcomes but at most one of which is observed, leading to many causal quantities being only partially identified. The inherent missing data issue echoes the multi-marginal optimal transport (MOT) problem, where marginal distributions are known, but how the marginals couple to form the joint distribution is unavailable. In this paper, we cast the causal partial identification problem in the framework of MOT with $K$ margins and $d$-dimensional outcomes and obtain the exact partial identified set. In order to estimate the partial identified set via MOT, statistically, we establish a convergence rate of the plug-in MOT estimator for general quadratic objective functions and prove it is minimax optimal for a quadratic objective function stemming from the variance minimization problem with arbitrary $K$ and $d \le 4$. Numerically, we demonstrate the efficacy of our method over several real-world datasets where our proposal consistently outperforms the baseline by a significant margin (over 70%). In addition, we provide efficient off-the-shelf implementations of MOT with general objective functions.</summary></entry><entry><title type="html">Claim Reserving via Inverse Probability Weighting: A Micro-Level Chain-Ladder Method</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ClaimReservingviaInverseProbabilityWeightingAMicroLevelChainLadderMethod.html" rel="alternate" type="text/html" title="Claim Reserving via Inverse Probability Weighting: A Micro-Level Chain-Ladder Method" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ClaimReservingviaInverseProbabilityWeightingAMicroLevelChainLadderMethod</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ClaimReservingviaInverseProbabilityWeightingAMicroLevelChainLadderMethod.html">&lt;p&gt;Claim reserving primarily relies on macro-level models, with the Chain-Ladder method being the most widely adopted. These methods were heuristically developed without minimal statistical foundations, relying on oversimplified data assumptions and neglecting policyholder heterogeneity, often resulting in conservative reserve predictions. Micro-level reserving, utilizing stochastic modeling with granular information, can improve predictions but tends to involve less attractive and complex models for practitioners. This paper aims to strike a practical balance between aggregate and individual models by introducing a methodology that enables the Chain-Ladder method to incorporate individual information. We achieve this by proposing a novel framework, formulating the claim reserving problem within a population sampling context. We introduce a reserve estimator in a frequency and severity distribution-free manner that utilizes inverse probability weights (IPW) driven by individual information, akin to propensity scores. We demonstrate that the Chain-Ladder method emerges as a particular case of such an IPW estimator, thereby inheriting a statistically sound foundation based on population sampling theory that enables the use of granular information, and other extensions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.10808&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sebastian Calcetero-Vanegas, Andrei L. Badescu, X. Sheldon Lin</name></author><category term="stat.AP" /><summary type="html">Claim reserving primarily relies on macro-level models, with the Chain-Ladder method being the most widely adopted. These methods were heuristically developed without minimal statistical foundations, relying on oversimplified data assumptions and neglecting policyholder heterogeneity, often resulting in conservative reserve predictions. Micro-level reserving, utilizing stochastic modeling with granular information, can improve predictions but tends to involve less attractive and complex models for practitioners. This paper aims to strike a practical balance between aggregate and individual models by introducing a methodology that enables the Chain-Ladder method to incorporate individual information. We achieve this by proposing a novel framework, formulating the claim reserving problem within a population sampling context. We introduce a reserve estimator in a frequency and severity distribution-free manner that utilizes inverse probability weights (IPW) driven by individual information, akin to propensity scores. We demonstrate that the Chain-Ladder method emerges as a particular case of such an IPW estimator, thereby inheriting a statistically sound foundation based on population sampling theory that enables the use of granular information, and other extensions.</summary></entry><entry><title type="html">Classification Modeling with RNN-Based, Random Forest, and XGBoost for Imbalanced Data: A Case of Early Crash Detection in ASEAN-5 Stock Markets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ClassificationModelingwithRNNBasedRandomForestandXGBoostforImbalancedDataACaseofEarlyCrashDetectioninASEAN5StockMarkets.html" rel="alternate" type="text/html" title="Classification Modeling with RNN-Based, Random Forest, and XGBoost for Imbalanced Data: A Case of Early Crash Detection in ASEAN-5 Stock Markets" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ClassificationModelingwithRNNBasedRandomForestandXGBoostforImbalancedDataACaseofEarlyCrashDetectioninASEAN5StockMarkets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ClassificationModelingwithRNNBasedRandomForestandXGBoostforImbalancedDataACaseofEarlyCrashDetectioninASEAN5StockMarkets.html">&lt;p&gt;This research aims to evaluate the performance of several Recurrent Neural Network (RNN) architectures including Simple RNN, Gated Recurrent Units (GRU), and Long Short-Term Memory (LSTM), compared to classic algorithms such as Random Forest and XGBoost in building classification models for early crash detection in ASEAN-5 stock markets. The study is examined using imbalanced data, which is common due to the rarity of market crashes. The study analyzes daily data from 2010 to 2023 across the major stock markets of the ASEAN-5 countries, including Indonesia, Malaysia, Singapore, Thailand, and Philippines. Market crash is identified as the target variable when the major stock price indices fall below the Value at Risk (VaR) thresholds of 5%, 2.5% and 1%. predictors involving technical indicators of major local and global markets as well as commodity markets. This study includes 213 predictors with their respective lags (5, 10, 15, 22, 50, 200) and uses a time step of 7, expanding the total number of predictors to 1491. The challenge of data imbalance is addressed with SMOTE-ENN. The results show that all RNN-Based architectures outperform Random Forest and XGBoost. Among the various RNN architectures, Simple RNN stands out as the most superior, mainly due to the data characteristics that are not overly complex and focus more on short-term information. This study enhances and extends the range of phenomena observed in previous studies by incorporating variables like different geographical zones and time periods, as well as methodological adjustments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07888&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Deri Siswara, Agus M. Soleh, Aji Hamim Wigena</name></author><category term="stat.AP" /><summary type="html">This research aims to evaluate the performance of several Recurrent Neural Network (RNN) architectures including Simple RNN, Gated Recurrent Units (GRU), and Long Short-Term Memory (LSTM), compared to classic algorithms such as Random Forest and XGBoost in building classification models for early crash detection in ASEAN-5 stock markets. The study is examined using imbalanced data, which is common due to the rarity of market crashes. The study analyzes daily data from 2010 to 2023 across the major stock markets of the ASEAN-5 countries, including Indonesia, Malaysia, Singapore, Thailand, and Philippines. Market crash is identified as the target variable when the major stock price indices fall below the Value at Risk (VaR) thresholds of 5%, 2.5% and 1%. predictors involving technical indicators of major local and global markets as well as commodity markets. This study includes 213 predictors with their respective lags (5, 10, 15, 22, 50, 200) and uses a time step of 7, expanding the total number of predictors to 1491. The challenge of data imbalance is addressed with SMOTE-ENN. The results show that all RNN-Based architectures outperform Random Forest and XGBoost. Among the various RNN architectures, Simple RNN stands out as the most superior, mainly due to the data characteristics that are not overly complex and focus more on short-term information. This study enhances and extends the range of phenomena observed in previous studies by incorporating variables like different geographical zones and time periods, as well as methodological adjustments.</summary></entry><entry><title type="html">Continuous-time multivariate analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Continuoustimemultivariateanalysis.html" rel="alternate" type="text/html" title="Continuous-time multivariate analysis" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Continuoustimemultivariateanalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Continuoustimemultivariateanalysis.html">&lt;p&gt;The starting point for much of multivariate analysis (MVA) is an $n\times p$ data matrix whose $n$ rows represent observations and whose $p$ columns represent variables. Some multivariate data sets, however, may be best conceptualized not as $n$ discrete $p$-variate observations, but as $p$ curves or functions defined on a common time interval. Here we introduce a framework for extending techniques of multivariate analysis to such settings. The proposed continuous-time multivariate analysis (CTMVA) framework rests on the assumption that the curves can be represented as linear combinations of basis functions such as $B$-splines, as in the Ramsay-Silverman representation of functional data; but whereas functional data analysis extends MVA to the case of observations that are curves rather than vectors – heuristically, $n\times p$ data with $p$ infinite – we are instead concerned with what happens when $n$ is infinite. We present continuous-time extensions of the classical MVA methods of covariance and correlation estimation, principal component analysis, Fisher’s linear discriminant analysis, and $k$-means clustering. We show that CTMVA can improve on the performance of classical MVA, in particular for correlation estimation and clustering, and can be applied in some settings where classical MVA cannot, including variables observed at disparate time points. CTMVA is illustrated with a novel perspective on a well-known Canadian weather data set, and with applications to data sets involving international development, brain signals, and air quality. The proposed methods are implemented in the publicly available R package \texttt{ctmva}.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.09404&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Biplab Paul, Philip T. Reiss, Erjia Cui, Noemi Foà</name></author><category term="stat.ME" /><summary type="html">The starting point for much of multivariate analysis (MVA) is an $n\times p$ data matrix whose $n$ rows represent observations and whose $p$ columns represent variables. Some multivariate data sets, however, may be best conceptualized not as $n$ discrete $p$-variate observations, but as $p$ curves or functions defined on a common time interval. Here we introduce a framework for extending techniques of multivariate analysis to such settings. The proposed continuous-time multivariate analysis (CTMVA) framework rests on the assumption that the curves can be represented as linear combinations of basis functions such as $B$-splines, as in the Ramsay-Silverman representation of functional data; but whereas functional data analysis extends MVA to the case of observations that are curves rather than vectors – heuristically, $n\times p$ data with $p$ infinite – we are instead concerned with what happens when $n$ is infinite. We present continuous-time extensions of the classical MVA methods of covariance and correlation estimation, principal component analysis, Fisher’s linear discriminant analysis, and $k$-means clustering. We show that CTMVA can improve on the performance of classical MVA, in particular for correlation estimation and clustering, and can be applied in some settings where classical MVA cannot, including variables observed at disparate time points. CTMVA is illustrated with a novel perspective on a well-known Canadian weather data set, and with applications to data sets involving international development, brain signals, and air quality. The proposed methods are implemented in the publicly available R package \texttt{ctmva}.</summary></entry><entry><title type="html">Coordinated Trading Strategies for Battery Storage in Reserve and Spot Markets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/CoordinatedTradingStrategiesforBatteryStorageinReserveandSpotMarkets.html" rel="alternate" type="text/html" title="Coordinated Trading Strategies for Battery Storage in Reserve and Spot Markets" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/CoordinatedTradingStrategiesforBatteryStorageinReserveandSpotMarkets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/CoordinatedTradingStrategiesforBatteryStorageinReserveandSpotMarkets.html">&lt;p&gt;Quantity and price risks are key uncertainties market participants face in electricity markets with increased volatility, for instance, due to high shares of renewables. From day ahead until real-time, there is a large variation in the best available information, leading to price changes that flexible assets, such as battery storage, can exploit economically. This study contributes to understanding how coordinated bidding strategies can enhance multi-market trading and large-scale energy storage integration. Our findings shed light on the complexities arising from interdependencies and the high-dimensional nature of the problem. We show how stochastic dual dynamic programming is a suitable solution technique for such an environment. We include the three markets of the frequency containment reserve, day-ahead, and intraday in stochastic modelling and develop a multi-stage stochastic program. Prices are represented in a multidimensional Markov Chain, following the scheduling of the markets and allowing for time-dependent randomness. Using the example of a battery storage in the German energy sector, we provide valuable insights into the technical aspects of our method and the economic feasibility of battery storage operation. We find that capacity reservation in the frequency containment reserve dominates over the battery’s cycling in spot markets at the given resolution on prices in 2022. In an adjusted price environment, we find that coordination can yield an additional value of up to 12.5%.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08390&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Paul E. Seifert, Emil Kraft, Steffen Bakker, Stein-Erik Fleten</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Quantity and price risks are key uncertainties market participants face in electricity markets with increased volatility, for instance, due to high shares of renewables. From day ahead until real-time, there is a large variation in the best available information, leading to price changes that flexible assets, such as battery storage, can exploit economically. This study contributes to understanding how coordinated bidding strategies can enhance multi-market trading and large-scale energy storage integration. Our findings shed light on the complexities arising from interdependencies and the high-dimensional nature of the problem. We show how stochastic dual dynamic programming is a suitable solution technique for such an environment. We include the three markets of the frequency containment reserve, day-ahead, and intraday in stochastic modelling and develop a multi-stage stochastic program. Prices are represented in a multidimensional Markov Chain, following the scheduling of the markets and allowing for time-dependent randomness. Using the example of a battery storage in the German energy sector, we provide valuable insights into the technical aspects of our method and the economic feasibility of battery storage operation. We find that capacity reservation in the frequency containment reserve dominates over the battery’s cycling in spot markets at the given resolution on prices in 2022. In an adjusted price environment, we find that coordination can yield an additional value of up to 12.5%.</summary></entry><entry><title type="html">Deep Bayes Factors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/DeepBayesFactors.html" rel="alternate" type="text/html" title="Deep Bayes Factors" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/DeepBayesFactors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/DeepBayesFactors.html">&lt;p&gt;The is no other model or hypothesis verification tool in Bayesian statistics that is as widely used as the Bayes factor. We focus on generative models that are likelihood-free and, therefore, render the computation of Bayes factors (marginal likelihood ratios) far from obvious. We propose a deep learning estimator of the Bayes factor based on simulated data from two competing models using the likelihood ratio trick. This estimator is devoid of summary statistics and obviates some of the difficulties with ABC model choice. We establish sufficient conditions for consistency of our Deep Bayes Factor estimator as well as its consistency as a model selection tool. We investigate the performance of our estimator on various examples using a wide range of quality metrics related to estimation and model decision accuracy. After training, our deep learning approach enables rapid evaluations of the Bayes factor estimator at any fictional data arriving from either hypothesized model, not just the observed data $Y_0$. This allows us to inspect entire Bayes factor distributions under the two models and to quantify the relative location of the Bayes factor evaluated at $Y_0$ in light of these distributions. Such tail area evaluations are not possible for Bayes factor estimators tailored to $Y_0$. We find the performance of our Deep Bayes Factors competitive with existing MCMC techniques that require the knowledge of the likelihood function. We also consider variants for posterior or intrinsic Bayes factors estimation. We demonstrate the usefulness of our approach on a relatively high-dimensional real data example about determining cognitive biases.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.05411&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jungeum Kim, Veronika Rockova</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">The is no other model or hypothesis verification tool in Bayesian statistics that is as widely used as the Bayes factor. We focus on generative models that are likelihood-free and, therefore, render the computation of Bayes factors (marginal likelihood ratios) far from obvious. We propose a deep learning estimator of the Bayes factor based on simulated data from two competing models using the likelihood ratio trick. This estimator is devoid of summary statistics and obviates some of the difficulties with ABC model choice. We establish sufficient conditions for consistency of our Deep Bayes Factor estimator as well as its consistency as a model selection tool. We investigate the performance of our estimator on various examples using a wide range of quality metrics related to estimation and model decision accuracy. After training, our deep learning approach enables rapid evaluations of the Bayes factor estimator at any fictional data arriving from either hypothesized model, not just the observed data $Y_0$. This allows us to inspect entire Bayes factor distributions under the two models and to quantify the relative location of the Bayes factor evaluated at $Y_0$ in light of these distributions. Such tail area evaluations are not possible for Bayes factor estimators tailored to $Y_0$. We find the performance of our Deep Bayes Factors competitive with existing MCMC techniques that require the knowledge of the likelihood function. We also consider variants for posterior or intrinsic Bayes factors estimation. We demonstrate the usefulness of our approach on a relatively high-dimensional real data example about determining cognitive biases.</summary></entry><entry><title type="html">Effective experience rating for large insurance portfolios via surrogate modeling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Effectiveexperienceratingforlargeinsuranceportfoliosviasurrogatemodeling.html" rel="alternate" type="text/html" title="Effective experience rating for large insurance portfolios via surrogate modeling" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Effectiveexperienceratingforlargeinsuranceportfoliosviasurrogatemodeling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Effectiveexperienceratingforlargeinsuranceportfoliosviasurrogatemodeling.html">&lt;p&gt;Experience rating in insurance uses a Bayesian credibility model to upgrade the current premiums of a contract by taking into account policyholders’ attributes and their claim history. Most data-driven models used for this task are mathematically intractable, and premiums must be obtained through numerical methods such as simulation via MCMC. However, these methods can be computationally expensive and even prohibitive for large portfolios when applied at the policyholder level. Additionally, these computations become ``black-box” procedures as there is no analytical expression showing how the claim history of policyholders is used to upgrade their premiums. To address these challenges, this paper proposes a surrogate modeling approach to inexpensively derive an analytical expression for computing the Bayesian premiums for any given model, approximately. As a part of the methodology, the paper introduces a \emph{likelihood-based summary statistic} of the policyholder’s claim history that serves as the main input of the surrogate model and that is sufficient for certain families of distribution, including the exponential dispersion family. As a result, the computational burden of experience rating for large portfolios is reduced through the direct evaluation of such analytical expression, which can provide a transparent and interpretable way of computing Bayesian premiums.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.06568&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sebastian Calcetero-Vanegas, Andrei L. Badescu, X. Sheldon Lin</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Experience rating in insurance uses a Bayesian credibility model to upgrade the current premiums of a contract by taking into account policyholders’ attributes and their claim history. Most data-driven models used for this task are mathematically intractable, and premiums must be obtained through numerical methods such as simulation via MCMC. However, these methods can be computationally expensive and even prohibitive for large portfolios when applied at the policyholder level. Additionally, these computations become ``black-box” procedures as there is no analytical expression showing how the claim history of policyholders is used to upgrade their premiums. To address these challenges, this paper proposes a surrogate modeling approach to inexpensively derive an analytical expression for computing the Bayesian premiums for any given model, approximately. As a part of the methodology, the paper introduces a \emph{likelihood-based summary statistic} of the policyholder’s claim history that serves as the main input of the surrogate model and that is sufficient for certain families of distribution, including the exponential dispersion family. As a result, the computational burden of experience rating for large portfolios is reduced through the direct evaluation of such analytical expression, which can provide a transparent and interpretable way of computing Bayesian premiums.</summary></entry><entry><title type="html">Efficiency and Robustness of Rosenbaum’s Regression (Un)-Adjusted Rank-based Estimator in Randomized Experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/EfficiencyandRobustnessofRosenbaumsRegressionUnAdjustedRankbasedEstimatorinRandomizedExperiments.html" rel="alternate" type="text/html" title="Efficiency and Robustness of Rosenbaum’s Regression (Un)-Adjusted Rank-based Estimator in Randomized Experiments" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/EfficiencyandRobustnessofRosenbaumsRegressionUnAdjustedRankbasedEstimatorinRandomizedExperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/EfficiencyandRobustnessofRosenbaumsRegressionUnAdjustedRankbasedEstimatorinRandomizedExperiments.html">&lt;p&gt;Mean-based estimators of the causal effect in a completely randomized experiment may behave poorly if the potential outcomes have a heavy-tail, or contain outliers. We study an alternative estimator by Rosenbaum that estimates the constant additive treatment effect by inverting a randomization test using ranks. By investigating the breakdown point and asymptotic relative efficiency of this rank-based estimator, we show that it is provably robust against outliers and heavy-tailed potential outcomes, and has asymptotic variance at most 1.16 times that of the difference-in-means estimator (and much smaller when the potential outcomes are not light-tailed). We further derive a consistent estimator of the asymptotic standard error for Rosenbaum’s estimator which yields a readily computable confidence interval for the treatment effect. We also study a regression adjusted version of Rosenbaum’s estimator to incorporate additional covariate information in randomization inference. We prove gain in efficiency by this regression adjustment method under a linear regression model. We illustrate through synthetic and real data that, unlike the mean-based estimators, these rank-based estimators (both unadjusted or regression adjusted) are efficient and robust against heavy-tailed distributions, contamination, and model misspecification. Finally, we initiate the study of Rosenbaum’s estimator when the constant treatment effect assumption may be violated.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.15524&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aditya Ghosh, Nabarun Deb, Bikram Karmakar, Bodhisattva Sen</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Mean-based estimators of the causal effect in a completely randomized experiment may behave poorly if the potential outcomes have a heavy-tail, or contain outliers. We study an alternative estimator by Rosenbaum that estimates the constant additive treatment effect by inverting a randomization test using ranks. By investigating the breakdown point and asymptotic relative efficiency of this rank-based estimator, we show that it is provably robust against outliers and heavy-tailed potential outcomes, and has asymptotic variance at most 1.16 times that of the difference-in-means estimator (and much smaller when the potential outcomes are not light-tailed). We further derive a consistent estimator of the asymptotic standard error for Rosenbaum’s estimator which yields a readily computable confidence interval for the treatment effect. We also study a regression adjusted version of Rosenbaum’s estimator to incorporate additional covariate information in randomization inference. We prove gain in efficiency by this regression adjustment method under a linear regression model. We illustrate through synthetic and real data that, unlike the mean-based estimators, these rank-based estimators (both unadjusted or regression adjusted) are efficient and robust against heavy-tailed distributions, contamination, and model misspecification. Finally, we initiate the study of Rosenbaum’s estimator when the constant treatment effect assumption may be violated.</summary></entry><entry><title type="html">Extrapolation-Aware Nonparametric Statistical Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ExtrapolationAwareNonparametricStatisticalInference.html" rel="alternate" type="text/html" title="Extrapolation-Aware Nonparametric Statistical Inference" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ExtrapolationAwareNonparametricStatisticalInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ExtrapolationAwareNonparametricStatisticalInference.html">&lt;p&gt;We define extrapolation as any type of statistical inference on a conditional function (e.g., a conditional expectation or conditional quantile) evaluated outside of the support of the conditioning variable. This type of extrapolation occurs in many data analysis applications and can invalidate the resulting conclusions if not taken into account. While extrapolating is straightforward in parametric models, it becomes challenging in nonparametric models. In this work, we extend the nonparametric statistical model to explicitly allow for extrapolation and introduce a class of extrapolation assumptions that can be combined with existing inference techniques to draw extrapolation-aware conclusions. The proposed class of extrapolation assumptions stipulate that the conditional function attains its minimal and maximal directional derivative, in each direction, within the observed support. We illustrate how the framework applies to several statistical applications including prediction and uncertainty quantification. We furthermore propose a consistent estimation procedure that can be used to adjust existing nonparametric estimates to account for extrapolation by providing lower and upper extrapolation bounds. The procedure is empirically evaluated on both simulated and real-world data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.09758&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Niklas Pfister, Peter Bühlmann</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">We define extrapolation as any type of statistical inference on a conditional function (e.g., a conditional expectation or conditional quantile) evaluated outside of the support of the conditioning variable. This type of extrapolation occurs in many data analysis applications and can invalidate the resulting conclusions if not taken into account. While extrapolating is straightforward in parametric models, it becomes challenging in nonparametric models. In this work, we extend the nonparametric statistical model to explicitly allow for extrapolation and introduce a class of extrapolation assumptions that can be combined with existing inference techniques to draw extrapolation-aware conclusions. The proposed class of extrapolation assumptions stipulate that the conditional function attains its minimal and maximal directional derivative, in each direction, within the observed support. We illustrate how the framework applies to several statistical applications including prediction and uncertainty quantification. We furthermore propose a consistent estimation procedure that can be used to adjust existing nonparametric estimates to account for extrapolation by providing lower and upper extrapolation bounds. The procedure is empirically evaluated on both simulated and real-world data.</summary></entry><entry><title type="html">Fault detection in propulsion motors in the presence of concept drift</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Faultdetectioninpropulsionmotorsinthepresenceofconceptdrift.html" rel="alternate" type="text/html" title="Fault detection in propulsion motors in the presence of concept drift" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Faultdetectioninpropulsionmotorsinthepresenceofconceptdrift</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Faultdetectioninpropulsionmotorsinthepresenceofconceptdrift.html">&lt;p&gt;Machine learning and statistical methods can be used to enhance monitoring and fault prediction in marine systems. These methods rely on a dataset with records of historical system behaviour, potentially containing periods of both fault-free and faulty operation. An unexpected change in the underlying system, called a concept drift, may impact the performance of these methods, triggering the need for model retraining or other adaptations. In this article, we present an approach for detecting overheating in stator windings of marine propulsion motors that is able to successfully operate during concept drift without the need for full model retraining. Two distinct approaches are presented and tested. All models are trained and verified using a dataset from operational propulsion motors, with known, sudden concept drifts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08030&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Martin Tveten, Morten Stakkeland</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Machine learning and statistical methods can be used to enhance monitoring and fault prediction in marine systems. These methods rely on a dataset with records of historical system behaviour, potentially containing periods of both fault-free and faulty operation. An unexpected change in the underlying system, called a concept drift, may impact the performance of these methods, triggering the need for model retraining or other adaptations. In this article, we present an approach for detecting overheating in stator windings of marine propulsion motors that is able to successfully operate during concept drift without the need for full model retraining. Two distinct approaches are presented and tested. All models are trained and verified using a dataset from operational propulsion motors, with known, sudden concept drifts.</summary></entry><entry><title type="html">Global Tests for Smoothed Functions in Mean Field Variational Additive Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/GlobalTestsforSmoothedFunctionsinMeanFieldVariationalAdditiveModels.html" rel="alternate" type="text/html" title="Global Tests for Smoothed Functions in Mean Field Variational Additive Models" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/GlobalTestsforSmoothedFunctionsinMeanFieldVariationalAdditiveModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/GlobalTestsforSmoothedFunctionsinMeanFieldVariationalAdditiveModels.html">&lt;p&gt;Variational regression methods are an increasingly popular tool for their efficient estimation of complex. Given the mixed model representation of penalized effects, additive regression models with smoothed effects and scalar-on-function regression models can be fit relatively efficiently in a variational framework. However, inferential procedures for smoothed and functional effects in such a context is limited. We demonstrate that by using the Mean Field Variational Bayesian (MFVB) approximation to the additive model and the subsequent Coordinate Ascent Variational Inference (CAVI) algorithm, we can obtain a form of the estimated effects required of a Frequentist test for semiparametric curves. We establish MFVB approximations and CAVI algorithms for both Gaussian and binary additive models with an arbitrary number of smoothed and functional effects. We then derive a global testing framework for smoothed and functional effects. Our empirical study demonstrates that the test maintains good Frequentist properties in the variational framework and can be used to directly test results from a converged, MFVB approximation and CAVI algorithm. We illustrate the applicability of this approach in a wide range of data illustrations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08168&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mark J. Meyer, Junyi Wei</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Variational regression methods are an increasingly popular tool for their efficient estimation of complex. Given the mixed model representation of penalized effects, additive regression models with smoothed effects and scalar-on-function regression models can be fit relatively efficiently in a variational framework. However, inferential procedures for smoothed and functional effects in such a context is limited. We demonstrate that by using the Mean Field Variational Bayesian (MFVB) approximation to the additive model and the subsequent Coordinate Ascent Variational Inference (CAVI) algorithm, we can obtain a form of the estimated effects required of a Frequentist test for semiparametric curves. We establish MFVB approximations and CAVI algorithms for both Gaussian and binary additive models with an arbitrary number of smoothed and functional effects. We then derive a global testing framework for smoothed and functional effects. Our empirical study demonstrates that the test maintains good Frequentist properties in the variational framework and can be used to directly test results from a converged, MFVB approximation and CAVI algorithm. We illustrate the applicability of this approach in a wide range of data illustrations.</summary></entry><entry><title type="html">Hierarchical Bayesian Emulation of the Expected Net Present Value Utility Function via a Multi-Model Ensemble Member Decomposition</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/HierarchicalBayesianEmulationoftheExpectedNetPresentValueUtilityFunctionviaaMultiModelEnsembleMemberDecomposition.html" rel="alternate" type="text/html" title="Hierarchical Bayesian Emulation of the Expected Net Present Value Utility Function via a Multi-Model Ensemble Member Decomposition" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/HierarchicalBayesianEmulationoftheExpectedNetPresentValueUtilityFunctionviaaMultiModelEnsembleMemberDecomposition</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/HierarchicalBayesianEmulationoftheExpectedNetPresentValueUtilityFunctionviaaMultiModelEnsembleMemberDecomposition.html">&lt;p&gt;Computer models are widely used to study complex real world physical systems. However, there are major limitations to their direct use including: their complex structure; large numbers of inputs and outputs; and long evaluation times. Bayesian emulators are an effective means of addressing these challenges providing fast and efficient statistical approximation for computer model outputs. It is commonly assumed that computer models behave like a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;black-box&apos;&apos; function with no knowledge of the output prior to its evaluation. This ensures that emulators are generalisable but potentially limits their accuracy compared with exploiting such knowledge of constrained or structured output behaviour. We assume a&lt;/code&gt;grey-box’’ computer model and establish a hierarchical emulation framework encompassing structured emulators which exploit known constrained and structured behaviour of constituent computer model outputs. This achieves greater physical interpretability and more accurate emulator predictions. This research is motivated by and applied to the commercially important TNO OLYMPUS Well Control Optimisation Challenge from the petroleum industry. We re-express this as a decision support under uncertainty problem. First, we reduce the computational expense of the analysis by identifying a representative subset of models using an efficient multi-model ensemble subsampling technique. Next we apply our hierarchical emulation methodology to the expected Net Present Value utility function with well control decision parameters as inputs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08367&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jonathan Owen, Ian Vernon</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Computer models are widely used to study complex real world physical systems. However, there are major limitations to their direct use including: their complex structure; large numbers of inputs and outputs; and long evaluation times. Bayesian emulators are an effective means of addressing these challenges providing fast and efficient statistical approximation for computer model outputs. It is commonly assumed that computer models behave like a black-box&apos;&apos; function with no knowledge of the output prior to its evaluation. This ensures that emulators are generalisable but potentially limits their accuracy compared with exploiting such knowledge of constrained or structured output behaviour. We assume agrey-box’’ computer model and establish a hierarchical emulation framework encompassing structured emulators which exploit known constrained and structured behaviour of constituent computer model outputs. This achieves greater physical interpretability and more accurate emulator predictions. This research is motivated by and applied to the commercially important TNO OLYMPUS Well Control Optimisation Challenge from the petroleum industry. We re-express this as a decision support under uncertainty problem. First, we reduce the computational expense of the analysis by identifying a representative subset of models using an efficient multi-model ensemble subsampling technique. Next we apply our hierarchical emulation methodology to the expected Net Present Value utility function with well control decision parameters as inputs.</summary></entry><entry><title type="html">Highest Probability Density Conformal Regions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/HighestProbabilityDensityConformalRegions.html" rel="alternate" type="text/html" title="Highest Probability Density Conformal Regions" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/HighestProbabilityDensityConformalRegions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/HighestProbabilityDensityConformalRegions.html">&lt;p&gt;We propose a new method for finding the highest predictive density set or region using signed conformal inference. The proposed method is computationally efficient, while also carrying conformal coverage guarantees. We prove that under, mild regularity conditions, the conformal prediction set is asymptotically close to its oracle counterpart. The efficacy of the method is illustrated through simulations and real applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08366&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Max Sampson, Kung-Sik Chan</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">We propose a new method for finding the highest predictive density set or region using signed conformal inference. The proposed method is computationally efficient, while also carrying conformal coverage guarantees. We prove that under, mild regularity conditions, the conformal prediction set is asymptotically close to its oracle counterpart. The efficacy of the method is illustrated through simulations and real applications.</summary></entry><entry><title type="html">Improving subgroup analysis using methods to extend inferences to specific target populations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Improvingsubgroupanalysisusingmethodstoextendinferencestospecifictargetpopulations.html" rel="alternate" type="text/html" title="Improving subgroup analysis using methods to extend inferences to specific target populations" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Improvingsubgroupanalysisusingmethodstoextendinferencestospecifictargetpopulations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Improvingsubgroupanalysisusingmethodstoextendinferencestospecifictargetpopulations.html">&lt;p&gt;Subgroup analyses are common in epidemiologic and clinical research. Unfortunately, restriction to subgroup members to test for heterogeneity can yield imprecise effect estimates. If the true effect differs between members and non-members due to different distributions of other measured effect measure modifiers (EMMs), leveraging data from non-members can improve the precision of subgroup effect estimates. We obtained data from the PRIME RCT of panitumumab in patients with metastatic colon and rectal cancer from Project Datasphere(TM) to demonstrate this method. We weighted non-Hispanic White patients to resemble Hispanic patients in measured potential EMMs (e.g., age, KRAS distribution, sex), combined Hispanic and weighted non-Hispanic White patients in one data set, and estimated 1-year differences in progression-free survival (PFS). We obtained percentile-based 95% confidence limits for this 1-year difference in PFS from 2,000 bootstraps. To show when the method is less helpful, we also reweighted male patients to resemble female patients and mutant-type KRAS (no treatment benefit) patients to resemble wild-type KRAS (treatment benefit) patients. The PRIME RCT included 795 non-Hispanic White and 42 Hispanic patients with complete data on EMMs. While the Hispanic-only analysis estimated a one-year PFS change of -17% (95% C.I. -45%, 8.8%) with panitumumab, the combined weighted estimate was more precise (-8.7%, 95% CI -22%, 5.3%) while differing from the full population estimate (1.0%, 95% CI: -5.9%, 7.5%). When targeting wild-type KRAS patients the combined weighted estimate incorrectly suggested no benefit (one-year PFS change: 0.9%, 95% CI: -6.0%, 7.2%). Methods to extend inferences from study populations to specific targets can improve the precision of estimates of subgroup effect estimates when their assumptions are met. Violations of those assumptions can lead to bias, however.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08297&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael Webster-Clark, Anthony A. Matthews, Alan R. Ellis, Alan C. Kinlaw, Robert W. Platt</name></author><category term="stat.AP" /><summary type="html">Subgroup analyses are common in epidemiologic and clinical research. Unfortunately, restriction to subgroup members to test for heterogeneity can yield imprecise effect estimates. If the true effect differs between members and non-members due to different distributions of other measured effect measure modifiers (EMMs), leveraging data from non-members can improve the precision of subgroup effect estimates. We obtained data from the PRIME RCT of panitumumab in patients with metastatic colon and rectal cancer from Project Datasphere(TM) to demonstrate this method. We weighted non-Hispanic White patients to resemble Hispanic patients in measured potential EMMs (e.g., age, KRAS distribution, sex), combined Hispanic and weighted non-Hispanic White patients in one data set, and estimated 1-year differences in progression-free survival (PFS). We obtained percentile-based 95% confidence limits for this 1-year difference in PFS from 2,000 bootstraps. To show when the method is less helpful, we also reweighted male patients to resemble female patients and mutant-type KRAS (no treatment benefit) patients to resemble wild-type KRAS (treatment benefit) patients. The PRIME RCT included 795 non-Hispanic White and 42 Hispanic patients with complete data on EMMs. While the Hispanic-only analysis estimated a one-year PFS change of -17% (95% C.I. -45%, 8.8%) with panitumumab, the combined weighted estimate was more precise (-8.7%, 95% CI -22%, 5.3%) while differing from the full population estimate (1.0%, 95% CI: -5.9%, 7.5%). When targeting wild-type KRAS patients the combined weighted estimate incorrectly suggested no benefit (one-year PFS change: 0.9%, 95% CI: -6.0%, 7.2%). Methods to extend inferences from study populations to specific targets can improve the precision of estimates of subgroup effect estimates when their assumptions are met. Violations of those assumptions can lead to bias, however.</summary></entry><entry><title type="html">Inductive Global and Local Manifold Approximation and Projection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/InductiveGlobalandLocalManifoldApproximationandProjection.html" rel="alternate" type="text/html" title="Inductive Global and Local Manifold Approximation and Projection" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/InductiveGlobalandLocalManifoldApproximationandProjection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/InductiveGlobalandLocalManifoldApproximationandProjection.html">&lt;p&gt;Nonlinear dimensional reduction with the manifold assumption, often called manifold learning, has proven its usefulness in a wide range of high-dimensional data analysis. The significant impact of t-SNE and UMAP has catalyzed intense research interest, seeking further innovations toward visualizing not only the local but also the global structure information of the data. Moreover, there have been consistent efforts toward generalizable dimensional reduction that handles unseen data. In this paper, we first propose GLoMAP, a novel manifold learning method for dimensional reduction and high-dimensional data visualization. GLoMAP preserves locally and globally meaningful distance estimates and displays a progression from global to local formation during the course of optimization. Furthermore, we extend GLoMAP to its inductive version, iGLoMAP, which utilizes a deep neural network to map data to its lower-dimensional representation. This allows iGLoMAP to provide lower-dimensional embeddings for unseen points without needing to re-train the algorithm. iGLoMAP is also well-suited for mini-batch learning, enabling large-scale, accelerated gradient calculations. We have successfully applied both GLoMAP and iGLoMAP to the simulated and real-data settings, with competitive experiments against the state-of-the-art methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08097&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jungeum Kim, Xiao Wang</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Nonlinear dimensional reduction with the manifold assumption, often called manifold learning, has proven its usefulness in a wide range of high-dimensional data analysis. The significant impact of t-SNE and UMAP has catalyzed intense research interest, seeking further innovations toward visualizing not only the local but also the global structure information of the data. Moreover, there have been consistent efforts toward generalizable dimensional reduction that handles unseen data. In this paper, we first propose GLoMAP, a novel manifold learning method for dimensional reduction and high-dimensional data visualization. GLoMAP preserves locally and globally meaningful distance estimates and displays a progression from global to local formation during the course of optimization. Furthermore, we extend GLoMAP to its inductive version, iGLoMAP, which utilizes a deep neural network to map data to its lower-dimensional representation. This allows iGLoMAP to provide lower-dimensional embeddings for unseen points without needing to re-train the algorithm. iGLoMAP is also well-suited for mini-batch learning, enabling large-scale, accelerated gradient calculations. We have successfully applied both GLoMAP and iGLoMAP to the simulated and real-data settings, with competitive experiments against the state-of-the-art methods.</summary></entry><entry><title type="html">MMIL: A novel algorithm for disease associated cell type discovery</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/MMILAnovelalgorithmfordiseaseassociatedcelltypediscovery.html" rel="alternate" type="text/html" title="MMIL: A novel algorithm for disease associated cell type discovery" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/MMILAnovelalgorithmfordiseaseassociatedcelltypediscovery</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/MMILAnovelalgorithmfordiseaseassociatedcelltypediscovery.html">&lt;p&gt;Single-cell datasets often lack individual cell labels, making it challenging to identify cells associated with disease. To address this, we introduce Mixture Modeling for Multiple Instance Learning (MMIL), an expectation maximization method that enables the training and calibration of cell-level classifiers using patient-level labels. Our approach can be used to train e.g. lasso logistic regression models, gradient boosted trees, and neural networks. When applied to clinically-annotated, primary patient samples in Acute Myeloid Leukemia (AML) and Acute Lymphoblastic Leukemia (ALL), our method accurately identifies cancer cells, generalizes across tissues and treatment timepoints, and selects biologically relevant features. In addition, MMIL is capable of incorporating cell labels into model training when they are known, providing a powerful framework for leveraging both labeled and unlabeled data simultaneously. Mixture Modeling for MIL offers a novel approach for cell classification, with significant potential to advance disease understanding and management, especially in scenarios with unknown gold-standard labels and high dimensionality.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08322&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Erin Craig, Timothy Keyes, Jolanda Sarno, Maxim Zaslavsky, Garry Nolan, Kara Davis, Trevor Hastie, Robert Tibshirani</name></author><category term="stat.ME" /><summary type="html">Single-cell datasets often lack individual cell labels, making it challenging to identify cells associated with disease. To address this, we introduce Mixture Modeling for Multiple Instance Learning (MMIL), an expectation maximization method that enables the training and calibration of cell-level classifiers using patient-level labels. Our approach can be used to train e.g. lasso logistic regression models, gradient boosted trees, and neural networks. When applied to clinically-annotated, primary patient samples in Acute Myeloid Leukemia (AML) and Acute Lymphoblastic Leukemia (ALL), our method accurately identifies cancer cells, generalizes across tissues and treatment timepoints, and selects biologically relevant features. In addition, MMIL is capable of incorporating cell labels into model training when they are known, providing a powerful framework for leveraging both labeled and unlabeled data simultaneously. Mixture Modeling for MIL offers a novel approach for cell classification, with significant potential to advance disease understanding and management, especially in scenarios with unknown gold-standard labels and high dimensionality.</summary></entry><entry><title type="html">Mode-based estimation of the center of symmetry</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Modebasedestimationofthecenterofsymmetry.html" rel="alternate" type="text/html" title="Mode-based estimation of the center of symmetry" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Modebasedestimationofthecenterofsymmetry</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Modebasedestimationofthecenterofsymmetry.html">&lt;p&gt;In the mean-median-mode triad of univariate centrality measures, the mode has been overlooked for estimating the center of symmetry in continuous and unimodal settings. This paper expands on the connection between kernel mode estimators and M-estimators for location, bridging the gap between the nonparametrics and robust statistics communities. The variance of modal estimators is studied in terms of a bandwidth parameter, establishing conditions for an optimal solution that outperforms the household sample mean. A purely nonparametric approach is adopted, modeling heavy-tailedness through regular variation. The results lead to an estimator proposal that includes a novel one-parameter family of kernels with compact support, offering extra robustness and efficiency. The effectiveness and versatility of the new method are demonstrated in a real-world case study and a thorough simulation study, comparing favorably to traditional and more competitive alternatives. Several myths about the mode are clarified along the way, reopening the quest for flexible and efficient nonparametric estimators.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08241&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>José E. Chacón, Javier Fernández Serrano</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In the mean-median-mode triad of univariate centrality measures, the mode has been overlooked for estimating the center of symmetry in continuous and unimodal settings. This paper expands on the connection between kernel mode estimators and M-estimators for location, bridging the gap between the nonparametrics and robust statistics communities. The variance of modal estimators is studied in terms of a bandwidth parameter, establishing conditions for an optimal solution that outperforms the household sample mean. A purely nonparametric approach is adopted, modeling heavy-tailedness through regular variation. The results lead to an estimator proposal that includes a novel one-parameter family of kernels with compact support, offering extra robustness and efficiency. The effectiveness and versatility of the new method are demonstrated in a real-world case study and a thorough simulation study, comparing favorably to traditional and more competitive alternatives. Several myths about the mode are clarified along the way, reopening the quest for flexible and efficient nonparametric estimators.</summary></entry><entry><title type="html">Non-asymptotic convergence bounds for modified tamed unadjusted Langevin algorithm in non-convex setting</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/NonasymptoticconvergenceboundsformodifiedtamedunadjustedLangevinalgorithminnonconvexsetting.html" rel="alternate" type="text/html" title="Non-asymptotic convergence bounds for modified tamed unadjusted Langevin algorithm in non-convex setting" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/NonasymptoticconvergenceboundsformodifiedtamedunadjustedLangevinalgorithminnonconvexsetting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/NonasymptoticconvergenceboundsformodifiedtamedunadjustedLangevinalgorithminnonconvexsetting.html">&lt;p&gt;We consider the problem of sampling from a high-dimensional target distribution $\pi_\beta$ on $\mathbb{R}^d$ with density proportional to $\theta\mapsto e^{-\beta U(\theta)}$ using explicit numerical schemes based on discretising the Langevin stochastic differential equation (SDE). In recent literature, taming has been proposed and studied as a method for ensuring stability of Langevin-based numerical schemes in the case of super-linearly growing drift coefficients for the Langevin SDE. In particular, the Tamed Unadjusted Langevin Algorithm (TULA) was proposed in [Bro+19] to sample from such target distributions with the gradient of the potential $U$ being super-linearly growing. However, theoretical guarantees in Wasserstein distances for Langevin-based algorithms have traditionally been derived assuming strong convexity of the potential $U$. In this paper, we propose a novel taming factor and derive, under a setting with possibly non-convex potential $U$ and super-linearly growing gradient of $U$, non-asymptotic theoretical bounds in Wasserstein-1 and Wasserstein-2 distances between the law of our algorithm, which we name the modified Tamed Unadjusted Langevin Algorithm (mTULA), and the target distribution $\pi_\beta$. We obtain respective rates of convergence $\mathcal{O}(\lambda)$ and $\mathcal{O}(\lambda^{1/2})$ in Wasserstein-1 and Wasserstein-2 distances for the discretisation error of mTULA in step size $\lambda$. High-dimensional numerical simulations which support our theoretical findings are presented to showcase the applicability of our algorithm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2207.02600&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ariel Neufeld, Matthew Ng Cheng En, Ying Zhang</name></author><category term="stat.CO," /><category term="stat.TH" /><summary type="html">We consider the problem of sampling from a high-dimensional target distribution $\pi_\beta$ on $\mathbb{R}^d$ with density proportional to $\theta\mapsto e^{-\beta U(\theta)}$ using explicit numerical schemes based on discretising the Langevin stochastic differential equation (SDE). In recent literature, taming has been proposed and studied as a method for ensuring stability of Langevin-based numerical schemes in the case of super-linearly growing drift coefficients for the Langevin SDE. In particular, the Tamed Unadjusted Langevin Algorithm (TULA) was proposed in [Bro+19] to sample from such target distributions with the gradient of the potential $U$ being super-linearly growing. However, theoretical guarantees in Wasserstein distances for Langevin-based algorithms have traditionally been derived assuming strong convexity of the potential $U$. In this paper, we propose a novel taming factor and derive, under a setting with possibly non-convex potential $U$ and super-linearly growing gradient of $U$, non-asymptotic theoretical bounds in Wasserstein-1 and Wasserstein-2 distances between the law of our algorithm, which we name the modified Tamed Unadjusted Langevin Algorithm (mTULA), and the target distribution $\pi_\beta$. We obtain respective rates of convergence $\mathcal{O}(\lambda)$ and $\mathcal{O}(\lambda^{1/2})$ in Wasserstein-1 and Wasserstein-2 distances for the discretisation error of mTULA in step size $\lambda$. High-dimensional numerical simulations which support our theoretical findings are presented to showcase the applicability of our algorithm.</summary></entry><entry><title type="html">Non-robustness of diffusion estimates on networks with measurement error</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Nonrobustnessofdiffusionestimatesonnetworkswithmeasurementerror.html" rel="alternate" type="text/html" title="Non-robustness of diffusion estimates on networks with measurement error" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Nonrobustnessofdiffusionestimatesonnetworkswithmeasurementerror</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Nonrobustnessofdiffusionestimatesonnetworkswithmeasurementerror.html">&lt;p&gt;Network diffusion models are used to study things like disease transmission, information spread, and technology adoption. However, small amounts of mismeasurement are extremely likely in the networks constructed to operationalize these models. We show that estimates of diffusions are highly non-robust to this measurement error. First, we show that even when measurement error is vanishingly small, such that the share of missed links is close to zero, forecasts about the extent of diffusion will greatly underestimate the truth. Second, a small mismeasurement in the identity of the initial seed generates a large shift in the locations of expected diffusion path. We show that both of these results still hold when the vanishing measurement error is only local in nature. Such non-robustness in forecasting exists even under conditions where the basic reproductive number is consistently estimable. Possible solutions, such as estimating the measurement error or implementing widespread detection efforts, still face difficulties because the number of missed links are so small. Finally, we conduct Monte Carlo simulations on simulated networks, and real networks from three settings: travel data from the COVID-19 pandemic in the western US, a mobile phone marketing campaign in rural India, and in an insurance experiment in China.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.05704&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arun G. Chandrasekhar, Paul Goldsmith-Pinkham, Tyler H. McCormick, Samuel Thau, Jerry Wei</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Network diffusion models are used to study things like disease transmission, information spread, and technology adoption. However, small amounts of mismeasurement are extremely likely in the networks constructed to operationalize these models. We show that estimates of diffusions are highly non-robust to this measurement error. First, we show that even when measurement error is vanishingly small, such that the share of missed links is close to zero, forecasts about the extent of diffusion will greatly underestimate the truth. Second, a small mismeasurement in the identity of the initial seed generates a large shift in the locations of expected diffusion path. We show that both of these results still hold when the vanishing measurement error is only local in nature. Such non-robustness in forecasting exists even under conditions where the basic reproductive number is consistently estimable. Possible solutions, such as estimating the measurement error or implementing widespread detection efforts, still face difficulties because the number of missed links are so small. Finally, we conduct Monte Carlo simulations on simulated networks, and real networks from three settings: travel data from the COVID-19 pandemic in the western US, a mobile phone marketing campaign in rural India, and in an insurance experiment in China.</summary></entry><entry><title type="html">Null hypothesis Bayes factor estimates can be biased in (some) common factorial designs: A simulation study</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/NullhypothesisBayesfactorestimatescanbebiasedinsomecommonfactorialdesignsAsimulationstudy.html" rel="alternate" type="text/html" title="Null hypothesis Bayes factor estimates can be biased in (some) common factorial designs: A simulation study" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/NullhypothesisBayesfactorestimatescanbebiasedinsomecommonfactorialdesignsAsimulationstudy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/NullhypothesisBayesfactorestimatescanbebiasedinsomecommonfactorialdesignsAsimulationstudy.html">&lt;p&gt;Bayes factor null hypothesis tests provide a viable alternative to frequentist measures of evidence quantification. Bayes factors for realistic interesting models cannot be calculated exactly, but have to be estimated, which involves approximations to complex integrals. Crucially, the accuracy of these estimates, i.e., whether an estimated Bayes factor corresponds to the true Bayes factor, is unknown, and may depend on data, prior, and likelihood. We have recently developed a novel statistical procedure, namely simulation-based calibration (SBC) for Bayes factors, to test for a given analysis, whether the computed Bayes factors are accurate. Here, we use SBC for Bayes factors to test for some common cognitive designs, whether Bayes factors are estimated accurately. We use the bridgesampling/brms packages as well as the BayesFactor package in R. We find that Bayes factor estimates are accurate and exhibit only little bias in Latin square designs with (a) random effects for subjects only and (b) for crossed random effects for subjects and items, but a single fixed-factor. However, Bayes factor estimates turn out biased and liberal in a 2x2 design with crossed random effects for subjects and items. These results suggest that researchers should test for their individual analysis, whether Bayes factor estimates are accurate. Moreover, future research is needed to determine the boundary conditions under which Bayes factor estimates are accurate or biased, as well as software development to improve estimation accuracy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08022&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Daniel J. Schad, Shravan Vasishth</name></author><category term="stat.ME" /><summary type="html">Bayes factor null hypothesis tests provide a viable alternative to frequentist measures of evidence quantification. Bayes factors for realistic interesting models cannot be calculated exactly, but have to be estimated, which involves approximations to complex integrals. Crucially, the accuracy of these estimates, i.e., whether an estimated Bayes factor corresponds to the true Bayes factor, is unknown, and may depend on data, prior, and likelihood. We have recently developed a novel statistical procedure, namely simulation-based calibration (SBC) for Bayes factors, to test for a given analysis, whether the computed Bayes factors are accurate. Here, we use SBC for Bayes factors to test for some common cognitive designs, whether Bayes factors are estimated accurately. We use the bridgesampling/brms packages as well as the BayesFactor package in R. We find that Bayes factor estimates are accurate and exhibit only little bias in Latin square designs with (a) random effects for subjects only and (b) for crossed random effects for subjects and items, but a single fixed-factor. However, Bayes factor estimates turn out biased and liberal in a 2x2 design with crossed random effects for subjects and items. These results suggest that researchers should test for their individual analysis, whether Bayes factor estimates are accurate. Moreover, future research is needed to determine the boundary conditions under which Bayes factor estimates are accurate or biased, as well as software development to improve estimation accuracy.</summary></entry><entry><title type="html">On Conditional least squares estimation for the AD(1,n) model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/OnConditionalleastsquaresestimationfortheAD1nmodel.html" rel="alternate" type="text/html" title="On Conditional least squares estimation for the AD(1,n) model" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/OnConditionalleastsquaresestimationfortheAD1nmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/OnConditionalleastsquaresestimationfortheAD1nmodel.html">&lt;p&gt;This paper deals with the problem of global parameter estimation of AD(1, n) where n is a positive integer which is a subclass of affine diffusions introduced by Duffie, Filipovic, and Schachermayer. In general affine models are applied to the pricing of bond and stock options, which is illustrated for the Vasicek, Cox-Ingersoll-Ross and Heston models. Our main results are about the conditional least squares estimation of AD(1, n) drift parameters based on two types of observations : continuous time observations and discrete time observations with high frequency and infinite horizon. Then, for each case, we study the asymptotic properties according to ergodic and non-ergodic cases. This paper introduces as well some moment results relative to the AD(1, n) model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07653&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mohamed Ben Alaya, Houssem Dahbi, Hamdi Fathallah</name></author><category term="stat.AP," /><category term="stat.TH" /><summary type="html">This paper deals with the problem of global parameter estimation of AD(1, n) where n is a positive integer which is a subclass of affine diffusions introduced by Duffie, Filipovic, and Schachermayer. In general affine models are applied to the pricing of bond and stock options, which is illustrated for the Vasicek, Cox-Ingersoll-Ross and Heston models. Our main results are about the conditional least squares estimation of AD(1, n) drift parameters based on two types of observations : continuous time observations and discrete time observations with high frequency and infinite horizon. Then, for each case, we study the asymptotic properties according to ergodic and non-ergodic cases. This paper introduces as well some moment results relative to the AD(1, n) model.</summary></entry><entry><title type="html">On clustering levels of a hierarchical categorical risk factor</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Onclusteringlevelsofahierarchicalcategoricalriskfactor.html" rel="alternate" type="text/html" title="On clustering levels of a hierarchical categorical risk factor" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Onclusteringlevelsofahierarchicalcategoricalriskfactor</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/Onclusteringlevelsofahierarchicalcategoricalriskfactor.html">&lt;p&gt;Handling nominal covariates with a large number of categories is challenging for both statistical and machine learning techniques. This problem is further exacerbated when the nominal variable has a hierarchical structure. We commonly rely on methods such as the random effects approach (Campo and Antonio, 2023) to incorporate these covariates in a predictive model. Nonetheless, in certain situations, even the random effects approach may encounter estimation problems. We propose the data-driven Partitioning Hierarchical Risk-factors Adaptive Top-down (PHiRAT) algorithm to reduce the hierarchically structured risk factor to its essence, by grouping similar categories at each level of the hierarchy. We work top-down and engineer several features to characterize the profile of the categories at a specific level in the hierarchy. In our workers’ compensation case study, we characterize the risk profile of an industry via its observed damage rates and claim frequencies. In addition, we use embeddings (Mikolov et al., 2013; Cer et al., 2018) to encode the textual description of the economic activity of the insured company. These features are then used as input in a clustering algorithm to group similar categories. Our method substantially reduces the number of categories and results in a grouping that is generalizable to out-of-sample data. Moreover, we obtain a better differentiation between high-risk and low-risk companies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.09046&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bavo D. C. Campo, Katrien Antonio</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Handling nominal covariates with a large number of categories is challenging for both statistical and machine learning techniques. This problem is further exacerbated when the nominal variable has a hierarchical structure. We commonly rely on methods such as the random effects approach (Campo and Antonio, 2023) to incorporate these covariates in a predictive model. Nonetheless, in certain situations, even the random effects approach may encounter estimation problems. We propose the data-driven Partitioning Hierarchical Risk-factors Adaptive Top-down (PHiRAT) algorithm to reduce the hierarchically structured risk factor to its essence, by grouping similar categories at each level of the hierarchy. We work top-down and engineer several features to characterize the profile of the categories at a specific level in the hierarchy. In our workers’ compensation case study, we characterize the risk profile of an industry via its observed damage rates and claim frequencies. In addition, we use embeddings (Mikolov et al., 2013; Cer et al., 2018) to encode the textual description of the economic activity of the insured company. These features are then used as input in a clustering algorithm to group similar categories. Our method substantially reduces the number of categories and results in a grouping that is generalizable to out-of-sample data. Moreover, we obtain a better differentiation between high-risk and low-risk companies.</summary></entry><entry><title type="html">Predictive Performance Comparison of Decision Policies Under Confounding</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/PredictivePerformanceComparisonofDecisionPoliciesUnderConfounding.html" rel="alternate" type="text/html" title="Predictive Performance Comparison of Decision Policies Under Confounding" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/PredictivePerformanceComparisonofDecisionPoliciesUnderConfounding</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/PredictivePerformanceComparisonofDecisionPoliciesUnderConfounding.html">&lt;p&gt;Predictive models are often introduced to decision-making tasks under the rationale that they improve performance over an existing decision-making policy. However, it is challenging to compare predictive performance against an existing decision-making policy that is generally under-specified and dependent on unobservable factors. These sources of uncertainty are often addressed in practice by making strong assumptions about the data-generating mechanism. In this work, we propose a method to compare the predictive performance of decision policies under a variety of modern identification approaches from the causal inference and off-policy evaluation literatures (e.g., instrumental variable, marginal sensitivity model, proximal variable). Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison. We develop a practical approach for finite-sample estimation of regret intervals under no assumptions on the parametric form of the status quo policy. We verify our framework theoretically and via synthetic data experiments. We conclude with a real-world application using our framework to support a pre-deployment evaluation of a proposed modification to a healthcare enrollment policy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.00848&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luke Guerdan, Amanda Coston, Kenneth Holstein, Zhiwei Steven Wu</name></author><category term="stat.ME" /><summary type="html">Predictive models are often introduced to decision-making tasks under the rationale that they improve performance over an existing decision-making policy. However, it is challenging to compare predictive performance against an existing decision-making policy that is generally under-specified and dependent on unobservable factors. These sources of uncertainty are often addressed in practice by making strong assumptions about the data-generating mechanism. In this work, we propose a method to compare the predictive performance of decision policies under a variety of modern identification approaches from the causal inference and off-policy evaluation literatures (e.g., instrumental variable, marginal sensitivity model, proximal variable). Key to our method is the insight that there are regions of uncertainty that we can safely ignore in the policy comparison. We develop a practical approach for finite-sample estimation of regret intervals under no assumptions on the parametric form of the status quo policy. We verify our framework theoretically and via synthetic data experiments. We conclude with a real-world application using our framework to support a pre-deployment evaluation of a proposed modification to a healthcare enrollment policy.</summary></entry><entry><title type="html">Probabilistic Reconstruction of Paleodemographic Signals</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ProbabilisticReconstructionofPaleodemographicSignals.html" rel="alternate" type="text/html" title="Probabilistic Reconstruction of Paleodemographic Signals" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ProbabilisticReconstructionofPaleodemographicSignals</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ProbabilisticReconstructionofPaleodemographicSignals.html">&lt;p&gt;We present a comprehensive Bayesian approach to paleodemography, emphasizing the proper handling of uncertainties. We then apply that framework to survey data from Cyprus, and quantify the uncertainties in the paleodemographic estimates to demonstrate the applicability of the Bayesian approach and to show the large uncertainties present in current paleodemographic models and data. We also discuss methods to reduce the uncertainties and improve the efficacy of paleodemographic models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.05152&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>L. M. Arthur, F. Chelazzi, D. Lawrence, M. D. Price</name></author><category term="stat.AP" /><summary type="html">We present a comprehensive Bayesian approach to paleodemography, emphasizing the proper handling of uncertainties. We then apply that framework to survey data from Cyprus, and quantify the uncertainties in the paleodemographic estimates to demonstrate the applicability of the Bayesian approach and to show the large uncertainties present in current paleodemographic models and data. We also discuss methods to reduce the uncertainties and improve the efficacy of paleodemographic models.</summary></entry><entry><title type="html">Sequential Monte Carlo for Cut-Bayesian Posterior Computation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/SequentialMonteCarloforCutBayesianPosteriorComputation.html" rel="alternate" type="text/html" title="Sequential Monte Carlo for Cut-Bayesian Posterior Computation" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/SequentialMonteCarloforCutBayesianPosteriorComputation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/SequentialMonteCarloforCutBayesianPosteriorComputation.html">&lt;p&gt;We propose a sequential Monte Carlo (SMC) method to efficiently and accurately compute cut-Bayesian posterior quantities of interest, variations of standard Bayesian approaches constructed primarily to account for model misspecification. We prove finite sample concentration bounds for estimators derived from the proposed method along with a linear tempering extension and apply these results to a realistic setting where a computer model is misspecified. We then illustrate the SMC method for inference in a modular chemical reactor example that includes submodels for reaction kinetics, turbulence, mass transfer, and diffusion. The samples obtained are commensurate with a direct-sampling approach that consists of running multiple Markov chains, with computational efficiency gains using the SMC method. Overall, the SMC method presented yields a novel, rigorous approach to computing with cut-Bayesian posterior distributions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07555&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joseph Mathews, Giri Gopalan, James Gattiker, Sean Smith, Devin Francom</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">We propose a sequential Monte Carlo (SMC) method to efficiently and accurately compute cut-Bayesian posterior quantities of interest, variations of standard Bayesian approaches constructed primarily to account for model misspecification. We prove finite sample concentration bounds for estimators derived from the proposed method along with a linear tempering extension and apply these results to a realistic setting where a computer model is misspecified. We then illustrate the SMC method for inference in a modular chemical reactor example that includes submodels for reaction kinetics, turbulence, mass transfer, and diffusion. The samples obtained are commensurate with a direct-sampling approach that consists of running multiple Markov chains, with computational efficiency gains using the SMC method. Overall, the SMC method presented yields a novel, rigorous approach to computing with cut-Bayesian posterior distributions.</summary></entry><entry><title type="html">Shape-Constrained Distributional Optimization via Importance-Weighted Sample Average Approximation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ShapeConstrainedDistributionalOptimizationviaImportanceWeightedSampleAverageApproximation.html" rel="alternate" type="text/html" title="Shape-Constrained Distributional Optimization via Importance-Weighted Sample Average Approximation" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ShapeConstrainedDistributionalOptimizationviaImportanceWeightedSampleAverageApproximation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/ShapeConstrainedDistributionalOptimizationviaImportanceWeightedSampleAverageApproximation.html">&lt;p&gt;Shape-constrained optimization arises in a wide range of problems including distributionally robust optimization (DRO) that has surging popularity in recent years. In the DRO literature, these problems are usually solved via reduction into moment-constrained problems using the Choquet representation. While powerful, such an approach could face tractability challenges arising from the geometries and the compatibility between the shape and the objective function and moment constraints. In this paper, we propose an alternative methodology to solve shape-constrained optimization problems by integrating sample average approximation with importance sampling, the latter used to convert the distributional optimization into an optimization problem over the likelihood ratio with respect to a sampling distribution. We demonstrate how our approach, which relies on finite-dimensional linear programs, can handle a range of shape-constrained problems beyond the reach of previous Choquet-based reformulations, and entails vanishing and quantifiable optimality gaps. Moreover, our theoretical analyses based on strong duality and empirical processes reveal the critical role of shape constraints in guaranteeing desirable consistency and convergence rates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07825&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Henry Lam, Zhenyuan Liu, Dashi I. Singham</name></author><category term="stat.ME" /><summary type="html">Shape-constrained optimization arises in a wide range of problems including distributionally robust optimization (DRO) that has surging popularity in recent years. In the DRO literature, these problems are usually solved via reduction into moment-constrained problems using the Choquet representation. While powerful, such an approach could face tractability challenges arising from the geometries and the compatibility between the shape and the objective function and moment constraints. In this paper, we propose an alternative methodology to solve shape-constrained optimization problems by integrating sample average approximation with importance sampling, the latter used to convert the distributional optimization into an optimization problem over the likelihood ratio with respect to a sampling distribution. We demonstrate how our approach, which relies on finite-dimensional linear programs, can handle a range of shape-constrained problems beyond the reach of previous Choquet-based reformulations, and entails vanishing and quantifiable optimality gaps. Moreover, our theoretical analyses based on strong duality and empirical processes reveal the critical role of shape constraints in guaranteeing desirable consistency and convergence rates.</summary></entry><entry><title type="html">Simple yet Sharp Sensitivity Analysis for Any Contrast Under Unmeasured Confounding</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/SimpleyetSharpSensitivityAnalysisforAnyContrastUnderUnmeasuredConfounding.html" rel="alternate" type="text/html" title="Simple yet Sharp Sensitivity Analysis for Any Contrast Under Unmeasured Confounding" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/SimpleyetSharpSensitivityAnalysisforAnyContrastUnderUnmeasuredConfounding</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/SimpleyetSharpSensitivityAnalysisforAnyContrastUnderUnmeasuredConfounding.html">&lt;p&gt;We extend our previous work on sensitivity analysis for the risk ratio and difference contrasts under unmeasured confounding to any contrast. We prove that the bounds produced are still arbitrarily sharp, i.e. practically attainable. We illustrate the usability of the bounds with real data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07940&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jose M. Peña</name></author><category term="stat.ME" /><summary type="html">We extend our previous work on sensitivity analysis for the risk ratio and difference contrasts under unmeasured confounding to any contrast. We prove that the bounds produced are still arbitrarily sharp, i.e. practically attainable. We illustrate the usability of the bounds with real data.</summary></entry><entry><title type="html">Sparse Bayesian Multidimensional Item Response Theory</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/SparseBayesianMultidimensionalItemResponseTheory.html" rel="alternate" type="text/html" title="Sparse Bayesian Multidimensional Item Response Theory" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/SparseBayesianMultidimensionalItemResponseTheory</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/SparseBayesianMultidimensionalItemResponseTheory.html">&lt;p&gt;Multivariate Item Response Theory (MIRT) is sought-after widely by applied researchers looking for interpretable (sparse) explanations underlying response patterns in questionnaire data. There is, however, an unmet demand for such sparsity discovery tools in practice. Our paper develops a Bayesian platform for binary and ordinal item MIRT which requires minimal tuning and scales well on large datasets due to its parallelizable features. Bayesian methodology for MIRT models has traditionally relied on MCMC simulation, which cannot only be slow in practice, but also often renders exact sparsity recovery impossible without additional thresholding. In this work, we develop a scalable Bayesian EM algorithm to estimate sparse factor loadings from mixed continuous, binary, and ordinal item responses. We address the seemingly insurmountable problem of unknown latent factor dimensionality with tools from Bayesian nonparametrics which enable estimating the number of factors. Rotations to sparsity through parameter expansion further enhance convergence and interpretability without identifiability constraints. In our simulation study, we show that our method reliably recovers both the factor dimensionality as well as the latent structure on high-dimensional synthetic data even for small samples. We demonstrate the practical usefulness of our approach on three datasets: an educational assessment dataset, a quality-of-life measurement dataset, and a bio-behavioral dataset. All demonstrations show that our tool yields interpretable estimates, facilitating interesting discoveries that might otherwise go unnoticed under a pure confirmatory factor analysis setting.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.17820&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiguang Li, Robert Gibbons, Veronika Rockova</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Multivariate Item Response Theory (MIRT) is sought-after widely by applied researchers looking for interpretable (sparse) explanations underlying response patterns in questionnaire data. There is, however, an unmet demand for such sparsity discovery tools in practice. Our paper develops a Bayesian platform for binary and ordinal item MIRT which requires minimal tuning and scales well on large datasets due to its parallelizable features. Bayesian methodology for MIRT models has traditionally relied on MCMC simulation, which cannot only be slow in practice, but also often renders exact sparsity recovery impossible without additional thresholding. In this work, we develop a scalable Bayesian EM algorithm to estimate sparse factor loadings from mixed continuous, binary, and ordinal item responses. We address the seemingly insurmountable problem of unknown latent factor dimensionality with tools from Bayesian nonparametrics which enable estimating the number of factors. Rotations to sparsity through parameter expansion further enhance convergence and interpretability without identifiability constraints. In our simulation study, we show that our method reliably recovers both the factor dimensionality as well as the latent structure on high-dimensional synthetic data even for small samples. We demonstrate the practical usefulness of our approach on three datasets: an educational assessment dataset, a quality-of-life measurement dataset, and a bio-behavioral dataset. All demonstrations show that our tool yields interpretable estimates, facilitating interesting discoveries that might otherwise go unnoticed under a pure confirmatory factor analysis setting.</summary></entry><entry><title type="html">Statistical Principles for Platform Trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/StatisticalPrinciplesforPlatformTrials.html" rel="alternate" type="text/html" title="Statistical Principles for Platform Trials" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/StatisticalPrinciplesforPlatformTrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/StatisticalPrinciplesforPlatformTrials.html">&lt;p&gt;While within a clinical study there may be multiple doses and endpoints, across different studies each study will result in either an approval or a lack of approval of the drug compound studied. The term False Approval Rate (FAR) is the term this paper utilizes to represent the proportion of drug compounds that lack efficacy incorrectly approved by regulators. (In the U.S., compounds that have efficacy and are approved are not involved in the FAR consideration, according to our reading of the relevant U.S. Congressional statute).
  While Tukey’s (1953) Error Rate Familywise (ERFw) is meant to be applied within a clinical study, Tukey’s (1953) Error Rate per Family (ERpF), defined along-side ERFw, is meant to be applied across studies. We show that controlling Error Rate Familywise (ERFw) within a clinical study at 5% in turn controls Error Rate per Family (ERpF) across studies at 5-per-100, regardless of whether the studies are correlated or not. Further, we show that ongoing regulatory practice, the additive multiplicity adjustment method of controlling ERpF, is controlling False Approval Rate (FAR) exactly (not conservatively) at 5-per-100 (even for Platform trials).
  In contrast, if a regulatory agency chooses to control the False Discovery Rate (FDR) across studies at 5% instead, then this change in policy from ERpF control to FDR control will result in incorrectly approving drug compounds that lack efficacy at a rate higher than 5-per-100, because in essence it gives the industry additional rewards for successfully developing compounds that have efficacy and are approved. Seems to us the discussion of such a change in policy would be at a level higher than merely statistical, needing harmonizsation/harmonization (In the U.S., policy is set by the Congress).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.12728&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinping Cui, Emily Ouyang, Yi Liu, Jingjing Schneider, Hong Tian, Bushi Wang, Jason C. Hsu</name></author><category term="stat.ME" /><summary type="html">While within a clinical study there may be multiple doses and endpoints, across different studies each study will result in either an approval or a lack of approval of the drug compound studied. The term False Approval Rate (FAR) is the term this paper utilizes to represent the proportion of drug compounds that lack efficacy incorrectly approved by regulators. (In the U.S., compounds that have efficacy and are approved are not involved in the FAR consideration, according to our reading of the relevant U.S. Congressional statute). While Tukey’s (1953) Error Rate Familywise (ERFw) is meant to be applied within a clinical study, Tukey’s (1953) Error Rate per Family (ERpF), defined along-side ERFw, is meant to be applied across studies. We show that controlling Error Rate Familywise (ERFw) within a clinical study at 5% in turn controls Error Rate per Family (ERpF) across studies at 5-per-100, regardless of whether the studies are correlated or not. Further, we show that ongoing regulatory practice, the additive multiplicity adjustment method of controlling ERpF, is controlling False Approval Rate (FAR) exactly (not conservatively) at 5-per-100 (even for Platform trials). In contrast, if a regulatory agency chooses to control the False Discovery Rate (FDR) across studies at 5% instead, then this change in policy from ERpF control to FDR control will result in incorrectly approving drug compounds that lack efficacy at a rate higher than 5-per-100, because in essence it gives the industry additional rewards for successfully developing compounds that have efficacy and are approved. Seems to us the discussion of such a change in policy would be at a level higher than merely statistical, needing harmonizsation/harmonization (In the U.S., policy is set by the Congress).</summary></entry><entry><title type="html">Statistics in Survey Sampling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/StatisticsinSurveySampling.html" rel="alternate" type="text/html" title="Statistics in Survey Sampling" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/StatisticsinSurveySampling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/StatisticsinSurveySampling.html">&lt;p&gt;Survey sampling theory and methods are introduced. Sampling designs and estimation methods are carefully discussed as a textbook for survey sampling. Topics includes Horvitz-Thompson estimation, simple random sampling, stratified sampling, cluster sampling, ratio estimation, regression estimation, variance estimation, two-phase sampling, and nonresponse adjustment methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.07625&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jae Kwang Kim</name></author><category term="stat.ME" /><summary type="html">Survey sampling theory and methods are introduced. Sampling designs and estimation methods are carefully discussed as a textbook for survey sampling. Topics includes Horvitz-Thompson estimation, simple random sampling, stratified sampling, cluster sampling, ratio estimation, regression estimation, variance estimation, two-phase sampling, and nonresponse adjustment methods.</summary></entry><entry><title type="html">Stochastic Process-based Method for Degree-Degree Correlation of Evolving Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/StochasticProcessbasedMethodforDegreeDegreeCorrelationofEvolvingNetworks.html" rel="alternate" type="text/html" title="Stochastic Process-based Method for Degree-Degree Correlation of Evolving Networks" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/StochasticProcessbasedMethodforDegreeDegreeCorrelationofEvolvingNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/StochasticProcessbasedMethodforDegreeDegreeCorrelationofEvolvingNetworks.html">&lt;p&gt;Existing studies on the degree correlation of evolving networks typically rely on differential equations and statistical analysis, resulting in only approximate solutions due to inherent randomness. To address this limitation, we propose an improved Markov chain method for modeling degree correlation in evolving networks. By redesigning the network evolution rules to reflect actual network dynamics more accurately, we achieve a topological structure that closely matches real-world network evolution. Our method models the degree correlation evolution process for both directed and undirected networks and provides theoretical results that are verified through simulations. This work offers the first theoretical solution for the steady-state degree correlation in evolving network models and is applicable to more complex evolution mechanisms and networks with directional attributes. Additionally, it supports the study of dynamic characteristic control based on network structure at any given time, offering a new tool for researchers in the field.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08180&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yue Xiao, Xiaojun Zhang</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Existing studies on the degree correlation of evolving networks typically rely on differential equations and statistical analysis, resulting in only approximate solutions due to inherent randomness. To address this limitation, we propose an improved Markov chain method for modeling degree correlation in evolving networks. By redesigning the network evolution rules to reflect actual network dynamics more accurately, we achieve a topological structure that closely matches real-world network evolution. Our method models the degree correlation evolution process for both directed and undirected networks and provides theoretical results that are verified through simulations. This work offers the first theoretical solution for the steady-state degree correlation in evolving network models and is applicable to more complex evolution mechanisms and networks with directional attributes. Additionally, it supports the study of dynamic characteristic control based on network structure at any given time, offering a new tool for researchers in the field.</summary></entry><entry><title type="html">The Exchangeability Assumption for Permutation Tests of Multiple Regression Models: Implications for Statistics and Data Science</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/TheExchangeabilityAssumptionforPermutationTestsofMultipleRegressionModelsImplicationsforStatisticsandDataScience.html" rel="alternate" type="text/html" title="The Exchangeability Assumption for Permutation Tests of Multiple Regression Models: Implications for Statistics and Data Science" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/TheExchangeabilityAssumptionforPermutationTestsofMultipleRegressionModelsImplicationsforStatisticsandDataScience</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/TheExchangeabilityAssumptionforPermutationTestsofMultipleRegressionModelsImplicationsforStatisticsandDataScience.html">&lt;p&gt;Permutation tests are a powerful and flexible approach to inference via resampling. As computational methods become more ubiquitous in the statistics curriculum, use of permutation tests has become more tractable. At the heart of the permutation approach is the exchangeability assumption, which determines the appropriate null sampling distribution. We explore the exchangeability assumption in the context of permutation tests for multiple linear regression models. Various permutation schemes for the multiple linear regression setting have been previously proposed and assessed in the literature. As has been demonstrated previously, in most settings, the choice of how to permute a multiple linear regression model does not materially change inferential conclusions. Regardless, we believe that (1) understanding exchangeability in the multiple linear regression setting and also (2) how it relates to the null hypothesis of interest is valuable. We also briefly explore model settings beyond multiple linear regression (e.g., settings where clustering or hierarchical relationships exist) as a motivation for the benefit and flexibility of permutation tests. We close with pedagogical recommendations for instructors who want to bring multiple linear regression permutation inference into their classroom as a way to deepen student understanding of resampling-based inference.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07756&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Johanna Hardin, Lauren Quesada, Julie Ye, Nicholas J. Horton</name></author><category term="stat.ME" /><summary type="html">Permutation tests are a powerful and flexible approach to inference via resampling. As computational methods become more ubiquitous in the statistics curriculum, use of permutation tests has become more tractable. At the heart of the permutation approach is the exchangeability assumption, which determines the appropriate null sampling distribution. We explore the exchangeability assumption in the context of permutation tests for multiple linear regression models. Various permutation schemes for the multiple linear regression setting have been previously proposed and assessed in the literature. As has been demonstrated previously, in most settings, the choice of how to permute a multiple linear regression model does not materially change inferential conclusions. Regardless, we believe that (1) understanding exchangeability in the multiple linear regression setting and also (2) how it relates to the null hypothesis of interest is valuable. We also briefly explore model settings beyond multiple linear regression (e.g., settings where clustering or hierarchical relationships exist) as a motivation for the benefit and flexibility of permutation tests. We close with pedagogical recommendations for instructors who want to bring multiple linear regression permutation inference into their classroom as a way to deepen student understanding of resampling-based inference.</summary></entry><entry><title type="html">Who is driving the conversation? Analysing the nodality of British MPs and journalists on Twitter</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/WhoisdrivingtheconversationAnalysingthenodalityofBritishMPsandjournalistsonTwitter.html" rel="alternate" type="text/html" title="Who is driving the conversation? Analysing the nodality of British MPs and journalists on Twitter" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/WhoisdrivingtheconversationAnalysingthenodalityofBritishMPsandjournalistsonTwitter</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/WhoisdrivingtheconversationAnalysingthenodalityofBritishMPsandjournalistsonTwitter.html">&lt;p&gt;Who sets the policy agenda? In this paper, we explore the roles of policy actors in agenda setting by studying their relative influence in policy-related discussions. Our approach builds on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nodality&apos;&apos; \textemdash a concept in political science that determines the capacity of an actor to share information and to be at the centre of information networks. We propose a novel methodology that quantifies the nodality of all individual actors in any conversation by analysing a comprehensive set of their centrality measures in the related information network. We combine this with the analysis of the activity time-series, of the related conversation (or topic), to demonstrate how nodality scores relate to the capacity to drive topic-related activity. Here we analyse policy-related discussions on X (previously Twitter) and quantify the nodality of two sets of actors in the UK political system \textemdash Members of Parliament (MPs) and accredited journalists - on four policy topics: The Russia-Ukraine War, the Cost-of-Living Crisis, Brexit and COVID-19. Our results show that the capacity to influence the activity related to a topic is significantly and positively associated with nodality. In particular, we identify two dimensions of nodality that drive the capacity to influence topic-related activity. The first is&lt;/code&gt;active nodality”, which reflects the level of topic-related engagement an individual actor has on the platform. The second dimension is ``inherent nodality” which is entirely independent of the platform and reflects the actor’s institutional position (such as an MP in a front-bench role, or a journalist’s position at a prominent media outlet).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.08765&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Leonardo Castro-Gonzalez, Sukankana Chakraborty, Helen Margetts, Hardik Rajpal, Daniele Guariso, Jonathan Bright</name></author><category term="stat.AP" /><summary type="html">Who sets the policy agenda? In this paper, we explore the roles of policy actors in agenda setting by studying their relative influence in policy-related discussions. Our approach builds on nodality&apos;&apos; \textemdash a concept in political science that determines the capacity of an actor to share information and to be at the centre of information networks. We propose a novel methodology that quantifies the nodality of all individual actors in any conversation by analysing a comprehensive set of their centrality measures in the related information network. We combine this with the analysis of the activity time-series, of the related conversation (or topic), to demonstrate how nodality scores relate to the capacity to drive topic-related activity. Here we analyse policy-related discussions on X (previously Twitter) and quantify the nodality of two sets of actors in the UK political system \textemdash Members of Parliament (MPs) and accredited journalists - on four policy topics: The Russia-Ukraine War, the Cost-of-Living Crisis, Brexit and COVID-19. Our results show that the capacity to influence the activity related to a topic is significantly and positively associated with nodality. In particular, we identify two dimensions of nodality that drive the capacity to influence topic-related activity. The first isactive nodality”, which reflects the level of topic-related engagement an individual actor has on the platform. The second dimension is ``inherent nodality” which is entirely independent of the platform and reflects the actor’s institutional position (such as an MP in a front-bench role, or a journalist’s position at a prominent media outlet).</summary></entry><entry><title type="html">inlamemi: An R package for missing data imputation and measurement error modelling using INLA</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/inlamemiAnRpackageformissingdataimputationandmeasurementerrormodellingusingINLA.html" rel="alternate" type="text/html" title="inlamemi: An R package for missing data imputation and measurement error modelling using INLA" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/inlamemiAnRpackageformissingdataimputationandmeasurementerrormodellingusingINLA</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/inlamemiAnRpackageformissingdataimputationandmeasurementerrormodellingusingINLA.html">&lt;p&gt;Measurement error and missing data in variables used in statistical models are common, and can at worst lead to serious biases in analyses if they are ignored. Yet, these problems are often not dealt with adequately, presumably in part because analysts lack simple enough tools to account for error and missingness. In this R package, we provide functions to aid fitting hierarchical Bayesian models that account for cases where either measurement error (classical or Berkson), missing data, or both are present in continuous covariates. Model fitting is done in a Bayesian framework using integrated nested Laplace approximations (INLA), an approach that is growing in popularity due to its combination of computational speed and accuracy. The {inlamemi} R package is suitable for data analysts who have little prior experience using the R package {R-INLA}, and aids in formulating suitable hierarchical models for a variety of scenarios in order to appropriately capture the processes that generate the measurement error and/or missingness. Numerous examples are given to help analysts identify scenarios similar to their own, and make the process of specifying a suitable model easier.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.08172&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emma Skarstein, Stefanie Muff</name></author><category term="stat.ME" /><summary type="html">Measurement error and missing data in variables used in statistical models are common, and can at worst lead to serious biases in analyses if they are ignored. Yet, these problems are often not dealt with adequately, presumably in part because analysts lack simple enough tools to account for error and missingness. In this R package, we provide functions to aid fitting hierarchical Bayesian models that account for cases where either measurement error (classical or Berkson), missing data, or both are present in continuous covariates. Model fitting is done in a Bayesian framework using integrated nested Laplace approximations (INLA), an approach that is growing in popularity due to its combination of computational speed and accuracy. The {inlamemi} R package is suitable for data analysts who have little prior experience using the R package {R-INLA}, and aids in formulating suitable hierarchical models for a variety of scenarios in order to appropriately capture the processes that generate the measurement error and/or missingness. Numerous examples are given to help analysts identify scenarios similar to their own, and make the process of specifying a suitable model easier.</summary></entry><entry><title type="html">scores: A Python package for verifying and evaluating models and predictions with xarray and pandas</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/scoresAPythonpackageforverifyingandevaluatingmodelsandpredictionswithxarrayandpandas.html" rel="alternate" type="text/html" title="scores: A Python package for verifying and evaluating models and predictions with xarray and pandas" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/scoresAPythonpackageforverifyingandevaluatingmodelsandpredictionswithxarrayandpandas</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/scoresAPythonpackageforverifyingandevaluatingmodelsandpredictionswithxarrayandpandas.html">&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scores&lt;/code&gt; is a Python package containing mathematical functions for the verification, evaluation and optimisation of forecasts, predictions or models. It primarily supports the geoscience communities; in particular, the meteorological, climatological and oceanographic communities. In addition to supporting the Earth system science communities, it also has wide potential application in machine learning and other domains such as economics. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scores&lt;/code&gt; not only includes common scores (e.g. Mean Absolute Error), it also includes novel scores not commonly found elsewhere (e.g. FIxed Risk Multicategorical (FIRM) score, Flip-Flop Index), complex scores (e.g. threshold-weighted continuous ranked probability score), and statistical tests (such as the Diebold Mariano test). It also contains isotonic regression which is becoming an increasingly important tool in forecast verification and can be used to generate stable reliability diagrams. Additionally, it provides pre-processing tools for preparing data for scores in a variety of formats including cumulative distribution functions (CDF). At the time of writing, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scores&lt;/code&gt; includes over 50 metrics, statistical techniques and data processing tools. All of the scores and statistical techniques in this package have undergone a thorough scientific and software review. Every score has a companion Jupyter Notebook tutorial that demonstrates its use in practice. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scores&lt;/code&gt; primarily supports &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xarray&lt;/code&gt; datatypes for Earth system data, allowing it to work with NetCDF4, HDF5, Zarr and GRIB data sources among others. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scores&lt;/code&gt; uses Dask for scaling and performance. It has expanding support for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pandas&lt;/code&gt;. The software repository can be found at https://github.com/nci/scores/&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07817&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tennessee Leeuwenburg, Nicholas Loveday, Elizabeth E. Ebert, Harrison Cook, Mohammadreza Khanarmuei, Robert J. Taggart, Nikeeth Ramanathan, Maree Carroll, Stephanie Chong, Aidan Griffiths, John Sharples</name></author><category term="stat.AP" /><summary type="html">scores is a Python package containing mathematical functions for the verification, evaluation and optimisation of forecasts, predictions or models. It primarily supports the geoscience communities; in particular, the meteorological, climatological and oceanographic communities. In addition to supporting the Earth system science communities, it also has wide potential application in machine learning and other domains such as economics. scores not only includes common scores (e.g. Mean Absolute Error), it also includes novel scores not commonly found elsewhere (e.g. FIxed Risk Multicategorical (FIRM) score, Flip-Flop Index), complex scores (e.g. threshold-weighted continuous ranked probability score), and statistical tests (such as the Diebold Mariano test). It also contains isotonic regression which is becoming an increasingly important tool in forecast verification and can be used to generate stable reliability diagrams. Additionally, it provides pre-processing tools for preparing data for scores in a variety of formats including cumulative distribution functions (CDF). At the time of writing, scores includes over 50 metrics, statistical techniques and data processing tools. All of the scores and statistical techniques in this package have undergone a thorough scientific and software review. Every score has a companion Jupyter Notebook tutorial that demonstrates its use in practice. scores primarily supports xarray datatypes for Earth system data, allowing it to work with NetCDF4, HDF5, Zarr and GRIB data sources among others. scores uses Dask for scaling and performance. It has expanding support for pandas. The software repository can be found at https://github.com/nci/scores/</summary></entry><entry><title type="html">surveygenmod2: A SAS macro for estimating complex survey adjusted generalized linear models and Wald-type tests</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/surveygenmod2ASASmacroforestimatingcomplexsurveyadjustedgeneralizedlinearmodelsandWaldtypetests.html" rel="alternate" type="text/html" title="surveygenmod2: A SAS macro for estimating complex survey adjusted generalized linear models and Wald-type tests" /><published>2024-06-13T00:00:00+00:00</published><updated>2024-06-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/surveygenmod2ASASmacroforestimatingcomplexsurveyadjustedgeneralizedlinearmodelsandWaldtypetests</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/13/surveygenmod2ASASmacroforestimatingcomplexsurveyadjustedgeneralizedlinearmodelsandWaldtypetests.html">&lt;p&gt;surveygenmod2 builds on the macro written by da Silva (2017) for generalized linear models under complex survey designs. The updated macro fixed several minor bugs we encountered while updating the macro for use in SAS\textregistered. We added additional features for conducting basic Wald-type tests on groups of parameters based on the estimated regression coefficients and parameter variance-covariance matrix.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07651&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>R. Noah Padgett, Ying Chen</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">surveygenmod2 builds on the macro written by da Silva (2017) for generalized linear models under complex survey designs. The updated macro fixed several minor bugs we encountered while updating the macro for use in SAS\textregistered. We added additional features for conducting basic Wald-type tests on groups of parameters based on the estimated regression coefficients and parameter variance-covariance matrix.</summary></entry></feed>