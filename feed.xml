<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-24T07:15:53+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A General Control-Theoretic Approach for Reinforcement Learning: Theory and Algorithms</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/AGeneralControlTheoreticApproachforReinforcementLearningTheoryandAlgorithms.html" rel="alternate" type="text/html" title="A General Control-Theoretic Approach for Reinforcement Learning: Theory and Algorithms" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/AGeneralControlTheoreticApproachforReinforcementLearningTheoryandAlgorithms</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/AGeneralControlTheoreticApproachforReinforcementLearningTheoryandAlgorithms.html">&lt;p&gt;We devise a control-theoretic reinforcement learning approach to support direct learning of the optimal policy. We establish theoretical properties of our approach and derive an algorithm based on a specific instance of this approach. Our empirical results demonstrate the significant benefits of our approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14753&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Weiqin Chen, Mark S. Squillante, Chai Wah Wu, Santiago Paternain</name></author><category term="stat.ME" /><summary type="html">We devise a control-theoretic reinforcement learning approach to support direct learning of the optimal policy. We establish theoretical properties of our approach and derive an algorithm based on a specific instance of this approach. Our empirical results demonstrate the significant benefits of our approach.</summary></entry><entry><title type="html">A clustering approach for pairwise comparison matrices</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Aclusteringapproachforpairwisecomparisonmatrices.html" rel="alternate" type="text/html" title="A clustering approach for pairwise comparison matrices" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Aclusteringapproachforpairwisecomparisonmatrices</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Aclusteringapproachforpairwisecomparisonmatrices.html">&lt;p&gt;We consider clustering in group decision making where the opinions are given by pairwise comparison matrices. In particular, the k-medoids model is suggested to classify the matrices since it has a linear programming problem formulation that may contain any condition on the properties of the cluster centres. Its objective function depends on the measure of dissimilarity between the matrices but not on the weights derived from them. Our methodology provides a convenient tool for decision support, for instance, it can be used to quantify the reliability of the aggregation. The proposed theoretical framework is applied to a large-scale experimental dataset, on which it is able to automatically detect some mistakes made by the decision-makers, as well as to identify a common source of inconsistency.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.06061&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kolos Csaba Ágoston, Sándor Bozóki, László Csató</name></author><category term="stat.AP" /><summary type="html">We consider clustering in group decision making where the opinions are given by pairwise comparison matrices. In particular, the k-medoids model is suggested to classify the matrices since it has a linear programming problem formulation that may contain any condition on the properties of the cluster centres. Its objective function depends on the measure of dissimilarity between the matrices but not on the weights derived from them. Our methodology provides a convenient tool for decision support, for instance, it can be used to quantify the reliability of the aggregation. The proposed theoretical framework is applied to a large-scale experimental dataset, on which it is able to automatically detect some mistakes made by the decision-makers, as well as to identify a common source of inconsistency.</summary></entry><entry><title type="html">Analysis of Linked Files: A Missing Data Perspective</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/AnalysisofLinkedFilesAMissingDataPerspective.html" rel="alternate" type="text/html" title="Analysis of Linked Files: A Missing Data Perspective" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/AnalysisofLinkedFilesAMissingDataPerspective</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/AnalysisofLinkedFilesAMissingDataPerspective.html">&lt;p&gt;In many applications, researchers seek to identify overlapping entities across multiple data files. Record linkage algorithms facilitate this task, in the absence of unique identifiers. As these algorithms rely on semi-identifying information, they may miss records that represent the same entity, or incorrectly link records that do not represent the same entity. Analysis of linked files commonly ignores such linkage errors, resulting in biased, or overly precise estimates of the associations of interest. We view record linkage as a missing data problem, and delineate the linkage mechanisms that underpin analysis methods with linked files. Following the missing data literature, we group these methods under three categories: likelihood and Bayesian methods, imputation methods, and weighting methods. We summarize the assumptions and limitations of the methods, and evaluate their performance in a wide range of simulation scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14717&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Gauri Kamat, Roee Gutman</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In many applications, researchers seek to identify overlapping entities across multiple data files. Record linkage algorithms facilitate this task, in the absence of unique identifiers. As these algorithms rely on semi-identifying information, they may miss records that represent the same entity, or incorrectly link records that do not represent the same entity. Analysis of linked files commonly ignores such linkage errors, resulting in biased, or overly precise estimates of the associations of interest. We view record linkage as a missing data problem, and delineate the linkage mechanisms that underpin analysis methods with linked files. Following the missing data literature, we group these methods under three categories: likelihood and Bayesian methods, imputation methods, and weighting methods. We summarize the assumptions and limitations of the methods, and evaluate their performance in a wide range of simulation scenarios.</summary></entry><entry><title type="html">A review of feature selection strategies utilizing graph data structures and knowledge graphs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Areviewoffeatureselectionstrategiesutilizinggraphdatastructuresandknowledgegraphs.html" rel="alternate" type="text/html" title="A review of feature selection strategies utilizing graph data structures and knowledge graphs" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Areviewoffeatureselectionstrategiesutilizinggraphdatastructuresandknowledgegraphs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Areviewoffeatureselectionstrategiesutilizinggraphdatastructuresandknowledgegraphs.html">&lt;p&gt;Feature selection in Knowledge Graphs (KGs) are increasingly utilized in diverse domains, including biomedical research, Natural Language Processing (NLP), and personalized recommendation systems. This paper delves into the methodologies for feature selection within KGs, emphasizing their roles in enhancing machine learning (ML) model efficacy, hypothesis generation, and interpretability. Through this comprehensive review, we aim to catalyze further innovation in feature selection for KGs, paving the way for more insightful, efficient, and interpretable analytical models across various domains. Our exploration reveals the critical importance of scalability, accuracy, and interpretability in feature selection techniques, advocating for the integration of domain knowledge to refine the selection process. We highlight the burgeoning potential of multi-objective optimization and interdisciplinary collaboration in advancing KG feature selection, underscoring the transformative impact of such methodologies on precision medicine, among other fields. The paper concludes by charting future directions, including the development of scalable, dynamic feature selection algorithms and the integration of explainable AI principles to foster transparency and trust in KG-driven models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14864&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sisi Shao, Pedro Henrique Ribeiro, Christina Ramirez, Jason H. Moore</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Feature selection in Knowledge Graphs (KGs) are increasingly utilized in diverse domains, including biomedical research, Natural Language Processing (NLP), and personalized recommendation systems. This paper delves into the methodologies for feature selection within KGs, emphasizing their roles in enhancing machine learning (ML) model efficacy, hypothesis generation, and interpretability. Through this comprehensive review, we aim to catalyze further innovation in feature selection for KGs, paving the way for more insightful, efficient, and interpretable analytical models across various domains. Our exploration reveals the critical importance of scalability, accuracy, and interpretability in feature selection techniques, advocating for the integration of domain knowledge to refine the selection process. We highlight the burgeoning potential of multi-objective optimization and interdisciplinary collaboration in advancing KG feature selection, underscoring the transformative impact of such methodologies on precision medicine, among other fields. The paper concludes by charting future directions, including the development of scalable, dynamic feature selection algorithms and the integration of explainable AI principles to foster transparency and trust in KG-driven models.</summary></entry><entry><title type="html">Bayesian neural networks for predicting uncertainty in full-field material response</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Bayesianneuralnetworksforpredictinguncertaintyinfullfieldmaterialresponse.html" rel="alternate" type="text/html" title="Bayesian neural networks for predicting uncertainty in full-field material response" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Bayesianneuralnetworksforpredictinguncertaintyinfullfieldmaterialresponse</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Bayesianneuralnetworksforpredictinguncertaintyinfullfieldmaterialresponse.html">&lt;p&gt;Stress and material deformation field predictions are among the most important tasks in computational mechanics. These predictions are typically made by solving the governing equations of continuum mechanics using finite element analysis, which can become computationally prohibitive considering complex microstructures and material behaviors. Machine learning (ML) methods offer potentially cost effective surrogates for these applications. However, existing ML surrogates are either limited to low-dimensional problems and/or do not provide uncertainty estimates in the predictions. This work proposes an ML surrogate framework for stress field prediction and uncertainty quantification for diverse materials microstructures. A modified Bayesian U-net architecture is employed to provide a data-driven image-to-image mapping from initial microstructure to stress field with prediction (epistemic) uncertainty estimates. The Bayesian posterior distributions for the U-net parameters are estimated using three state-of-the-art inference algorithms: the posterior sampling-based Hamiltonian Monte Carlo method and two variational approaches, the Monte-Carlo Dropout method and the Bayes by Backprop algorithm. A systematic comparison of the predictive accuracy and uncertainty estimates for these methods is performed for a fiber reinforced composite material and polycrystalline microstructure application. It is shown that the proposed methods yield predictions of high accuracy compared to the FEA solution, while uncertainty estimates depend on the inference approach. Generally, the Hamiltonian Monte Carlo and Bayes by Backprop methods provide consistent uncertainty estimates. Uncertainty estimates from Monte Carlo Dropout, on the other hand, are more difficult to interpret and depend strongly on the method’s design.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14838&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>George D. Pasparakis, Lori Graham-Brady, Michael D. Shields</name></author><category term="stat.ML," /><category term="stat.AP" /><summary type="html">Stress and material deformation field predictions are among the most important tasks in computational mechanics. These predictions are typically made by solving the governing equations of continuum mechanics using finite element analysis, which can become computationally prohibitive considering complex microstructures and material behaviors. Machine learning (ML) methods offer potentially cost effective surrogates for these applications. However, existing ML surrogates are either limited to low-dimensional problems and/or do not provide uncertainty estimates in the predictions. This work proposes an ML surrogate framework for stress field prediction and uncertainty quantification for diverse materials microstructures. A modified Bayesian U-net architecture is employed to provide a data-driven image-to-image mapping from initial microstructure to stress field with prediction (epistemic) uncertainty estimates. The Bayesian posterior distributions for the U-net parameters are estimated using three state-of-the-art inference algorithms: the posterior sampling-based Hamiltonian Monte Carlo method and two variational approaches, the Monte-Carlo Dropout method and the Bayes by Backprop algorithm. A systematic comparison of the predictive accuracy and uncertainty estimates for these methods is performed for a fiber reinforced composite material and polycrystalline microstructure application. It is shown that the proposed methods yield predictions of high accuracy compared to the FEA solution, while uncertainty estimates depend on the inference approach. Generally, the Hamiltonian Monte Carlo and Bayes by Backprop methods provide consistent uncertainty estimates. Uncertainty estimates from Monte Carlo Dropout, on the other hand, are more difficult to interpret and depend strongly on the method’s design.</summary></entry><entry><title type="html">Change Acceleration and Detection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/ChangeAccelerationandDetection.html" rel="alternate" type="text/html" title="Change Acceleration and Detection" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/ChangeAccelerationandDetection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/ChangeAccelerationandDetection.html">&lt;p&gt;A novel sequential change detection problem is proposed, in which the goal is to not only detect but also accelerate the change. Specifically, it is assumed that the sequentially collected observations are responses to treatments selected in real time. The assigned treatments determine the pre-change and post-change distributions of the responses and also influence when the change happens. The goal is to find a treatment assignment rule and a stopping rule that minimize the expected total number of observations subject to a user-specified bound on the false alarm probability. The optimal solution is obtained under a general Markovian change-point model. Moreover, an alternative procedure is proposed, whose applicability is not restricted to Markovian change-point models and whose design requires minimal computation. For a large class of change-point models, the proposed procedure is shown to achieve the optimal performance in an asymptotic sense. Finally, its performance is found in simulation studies to be comparable to the optimal, uniformly with respect to the error probability.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.00915&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yanglei Song, Georgios Fellouris</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">A novel sequential change detection problem is proposed, in which the goal is to not only detect but also accelerate the change. Specifically, it is assumed that the sequentially collected observations are responses to treatments selected in real time. The assigned treatments determine the pre-change and post-change distributions of the responses and also influence when the change happens. The goal is to find a treatment assignment rule and a stopping rule that minimize the expected total number of observations subject to a user-specified bound on the false alarm probability. The optimal solution is obtained under a general Markovian change-point model. Moreover, an alternative procedure is proposed, whose applicability is not restricted to Markovian change-point models and whose design requires minimal computation. For a large class of change-point models, the proposed procedure is shown to achieve the optimal performance in an asymptotic sense. Finally, its performance is found in simulation studies to be comparable to the optimal, uniformly with respect to the error probability.</summary></entry><entry><title type="html">Conditional correlation estimation and serial dependence identification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Conditionalcorrelationestimationandserialdependenceidentification.html" rel="alternate" type="text/html" title="Conditional correlation estimation and serial dependence identification" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Conditionalcorrelationestimationandserialdependenceidentification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Conditionalcorrelationestimationandserialdependenceidentification.html">&lt;p&gt;It has been recently shown in Jaworski, P., Jelito, D. and Pitera, M. (2024), ‘A note on the equivalence between the conditional uncorrelation and the independence of random variables’, Electronic Journal of Statistics 18(1), that one can characterise the independence of random variables via the family of conditional correlations on quantile-induced sets. This effectively shows that the localized linear measure of dependence is able to detect any form of nonlinear dependence for appropriately chosen conditioning sets. In this paper, we expand this concept, focusing on the statistical properties of conditional correlation estimators and their potential usage in serial dependence identification. In particular, we show how to estimate conditional correlations in generic and serial dependence setups, discuss key properties of the related estimators, define the conditional equivalent of the autocorrelation function, and provide a series of examples which prove that the proposed framework could be efficiently used in many practical econometric applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14650&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kewin Pączek, Damian Jelito, Marcin Pitera, Agnieszka Wy{\l}omańska</name></author><category term="stat.ME" /><summary type="html">It has been recently shown in Jaworski, P., Jelito, D. and Pitera, M. (2024), ‘A note on the equivalence between the conditional uncorrelation and the independence of random variables’, Electronic Journal of Statistics 18(1), that one can characterise the independence of random variables via the family of conditional correlations on quantile-induced sets. This effectively shows that the localized linear measure of dependence is able to detect any form of nonlinear dependence for appropriately chosen conditioning sets. In this paper, we expand this concept, focusing on the statistical properties of conditional correlation estimators and their potential usage in serial dependence identification. In particular, we show how to estimate conditional correlations in generic and serial dependence setups, discuss key properties of the related estimators, define the conditional equivalent of the autocorrelation function, and provide a series of examples which prove that the proposed framework could be efficiently used in many practical econometric applications.</summary></entry><entry><title type="html">Consistent community detection in multi-layer networks with heterogeneous differential privacy</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Consistentcommunitydetectioninmultilayernetworkswithheterogeneousdifferentialprivacy.html" rel="alternate" type="text/html" title="Consistent community detection in multi-layer networks with heterogeneous differential privacy" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Consistentcommunitydetectioninmultilayernetworkswithheterogeneousdifferentialprivacy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Consistentcommunitydetectioninmultilayernetworkswithheterogeneousdifferentialprivacy.html">&lt;p&gt;As network data has become increasingly prevalent, a substantial amount of attention has been paid to the privacy issue in publishing network data. One of the critical challenges for data publishers is to preserve the topological structures of the original network while protecting sensitive information. In this paper, we propose a personalized edge flipping mechanism that allows data publishers to protect edge information based on each node’s privacy preference. It can achieve differential privacy while preserving the community structure under the multi-layer degree-corrected stochastic block model after appropriately debiasing, and thus consistent community detection in the privatized multi-layer networks is achievable. Theoretically, we establish the consistency of community detection in the privatized multi-layer network and show that better privacy protection of edges can be obtained for a proportion of nodes while allowing other nodes to give up their privacy. Furthermore, the advantage of the proposed personalized edge-flipping mechanism is also supported by its numerical performance on various synthetic networks and a real-life multi-layer network.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14772&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yaoming Zhen, Shirong Xu, Junhui Wang</name></author><category term="stat.ME" /><summary type="html">As network data has become increasingly prevalent, a substantial amount of attention has been paid to the privacy issue in publishing network data. One of the critical challenges for data publishers is to preserve the topological structures of the original network while protecting sensitive information. In this paper, we propose a personalized edge flipping mechanism that allows data publishers to protect edge information based on each node’s privacy preference. It can achieve differential privacy while preserving the community structure under the multi-layer degree-corrected stochastic block model after appropriately debiasing, and thus consistent community detection in the privatized multi-layer networks is achievable. Theoretically, we establish the consistency of community detection in the privatized multi-layer network and show that better privacy protection of edges can be obtained for a proportion of nodes while allowing other nodes to give up their privacy. Furthermore, the advantage of the proposed personalized edge-flipping mechanism is also supported by its numerical performance on various synthetic networks and a real-life multi-layer network.</summary></entry><entry><title type="html">Contamination Bias in Linear Regressions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/ContaminationBiasinLinearRegressions.html" rel="alternate" type="text/html" title="Contamination Bias in Linear Regressions" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/ContaminationBiasinLinearRegressions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/ContaminationBiasinLinearRegressions.html">&lt;p&gt;We study regressions with multiple treatments and a set of controls that is flexible enough to purge omitted variable bias. We show that these regressions generally fail to estimate convex averages of heterogeneous treatment effects – instead, estimates of each treatment’s effect are contaminated by non-convex averages of the effects of other treatments. We discuss three estimation approaches that avoid such contamination bias, including the targeting of easiest-to-estimate weighted average effects. A re-analysis of nine empirical applications finds economically and statistically meaningful contamination bias in observational studies; contamination bias in experimental studies is more limited due to smaller variability in propensity scores.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.05024&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Paul Goldsmith-Pinkham, Peter Hull, Michal Kolesár</name></author><category term="stat.ME" /><summary type="html">We study regressions with multiple treatments and a set of controls that is flexible enough to purge omitted variable bias. We show that these regressions generally fail to estimate convex averages of heterogeneous treatment effects – instead, estimates of each treatment’s effect are contaminated by non-convex averages of the effects of other treatments. We discuss three estimation approaches that avoid such contamination bias, including the targeting of easiest-to-estimate weighted average effects. A re-analysis of nine empirical applications finds economically and statistically meaningful contamination bias in observational studies; contamination bias in experimental studies is more limited due to smaller variability in propensity scores.</summary></entry><entry><title type="html">Discovering the Signal Subgraph: An Iterative Screening Approach on Graphs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/DiscoveringtheSignalSubgraphAnIterativeScreeningApproachonGraphs.html" rel="alternate" type="text/html" title="Discovering the Signal Subgraph: An Iterative Screening Approach on Graphs" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/DiscoveringtheSignalSubgraphAnIterativeScreeningApproachonGraphs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/DiscoveringtheSignalSubgraphAnIterativeScreeningApproachonGraphs.html">&lt;p&gt;Supervised learning on graphs is a challenging task due to the high dimensionality and inherent structural dependencies in the data, where each edge depends on a pair of vertices. Existing conventional methods are designed for standard Euclidean data and do not account for the structural information inherent in graphs. In this paper, we propose an iterative vertex screening method to achieve dimension reduction across multiple graph datasets with matched vertex sets and associated graph attributes. Our method aims to identify a signal subgraph to provide a more concise representation of the full graphs, potentially benefiting subsequent vertex classification tasks. The method screens the rows and columns of the adjacency matrix concurrently and stops when the resulting distance correlation is maximized. We establish the theoretical foundation of our method by proving that it estimates the true signal subgraph with high probability. Additionally, we establish the convergence rate of classification error under the Erdos-Renyi random graph model and prove that the subsequent classification can be asymptotically optimal, outperforming the entire graph under high-dimensional conditions. Our method is evaluated on various simulated datasets and real-world human and murine graphs derived from functional and structural magnetic resonance images. The results demonstrate its excellent performance in estimating the ground-truth signal subgraph and achieving superior classification accuracy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1801.07683&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Cencheng Shen, Shangsi Wang, Alexandra Badea, Carey E. Priebe, Joshua T. Vogelstein</name></author><category term="stat.ME" /><summary type="html">Supervised learning on graphs is a challenging task due to the high dimensionality and inherent structural dependencies in the data, where each edge depends on a pair of vertices. Existing conventional methods are designed for standard Euclidean data and do not account for the structural information inherent in graphs. In this paper, we propose an iterative vertex screening method to achieve dimension reduction across multiple graph datasets with matched vertex sets and associated graph attributes. Our method aims to identify a signal subgraph to provide a more concise representation of the full graphs, potentially benefiting subsequent vertex classification tasks. The method screens the rows and columns of the adjacency matrix concurrently and stops when the resulting distance correlation is maximized. We establish the theoretical foundation of our method by proving that it estimates the true signal subgraph with high probability. Additionally, we establish the convergence rate of classification error under the Erdos-Renyi random graph model and prove that the subsequent classification can be asymptotically optimal, outperforming the entire graph under high-dimensional conditions. Our method is evaluated on various simulated datasets and real-world human and murine graphs derived from functional and structural magnetic resonance images. The results demonstrate its excellent performance in estimating the ground-truth signal subgraph and achieving superior classification accuracy.</summary></entry><entry><title type="html">Dynamic Modeling of Sparse Longitudinal Data and Functional Snippets With Stochastic Differential Equations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/DynamicModelingofSparseLongitudinalDataandFunctionalSnippetsWithStochasticDifferentialEquations.html" rel="alternate" type="text/html" title="Dynamic Modeling of Sparse Longitudinal Data and Functional Snippets With Stochastic Differential Equations" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/DynamicModelingofSparseLongitudinalDataandFunctionalSnippetsWithStochasticDifferentialEquations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/DynamicModelingofSparseLongitudinalDataandFunctionalSnippetsWithStochasticDifferentialEquations.html">&lt;p&gt;Sparse functional/longitudinal data have attracted widespread interest due to the prevalence of such data in social and life sciences. A prominent scenario where such data are routinely encountered are accelerated longitudinal studies, where subjects are enrolled in the study at a random time and are only tracked for a short amount of time relative to the domain of interest. The statistical analysis of such functional snippets is challenging since information for the far-off-diagonal regions of the covariance structure is missing. Our main methodological contribution is to address this challenge by bypassing covariance estimation and instead modeling the underlying process as the solution of a data-adaptive stochastic differential equation. Taking advantage of the interface between Gaussian functional data and stochastic differential equations makes it possible to efficiently reconstruct the target process by estimating its dynamic distribution. The proposed approach allows one to consistently recover forward sample paths from functional snippets at the subject level. We establish the existence and uniqueness of the solution to the proposed data-driven stochastic differential equation and derive rates of convergence for the corresponding estimators. The finite-sample performance is demonstrated with simulation studies and functional snippets arising from a growth study and spinal bone mineral density data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.10221&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yidong Zhou, Hans-Georg Müller</name></author><category term="stat.ME" /><summary type="html">Sparse functional/longitudinal data have attracted widespread interest due to the prevalence of such data in social and life sciences. A prominent scenario where such data are routinely encountered are accelerated longitudinal studies, where subjects are enrolled in the study at a random time and are only tracked for a short amount of time relative to the domain of interest. The statistical analysis of such functional snippets is challenging since information for the far-off-diagonal regions of the covariance structure is missing. Our main methodological contribution is to address this challenge by bypassing covariance estimation and instead modeling the underlying process as the solution of a data-adaptive stochastic differential equation. Taking advantage of the interface between Gaussian functional data and stochastic differential equations makes it possible to efficiently reconstruct the target process by estimating its dynamic distribution. The proposed approach allows one to consistently recover forward sample paths from functional snippets at the subject level. We establish the existence and uniqueness of the solution to the proposed data-driven stochastic differential equation and derive rates of convergence for the corresponding estimators. The finite-sample performance is demonstrated with simulation studies and functional snippets arising from a growth study and spinal bone mineral density data.</summary></entry><entry><title type="html">Enhancing Actuarial Non-Life Pricing Models via Transformers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/EnhancingActuarialNonLifePricingModelsviaTransformers.html" rel="alternate" type="text/html" title="Enhancing Actuarial Non-Life Pricing Models via Transformers" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/EnhancingActuarialNonLifePricingModelsviaTransformers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/EnhancingActuarialNonLifePricingModelsviaTransformers.html">&lt;p&gt;Currently, there is a lot of research in the field of neural networks for non-life insurance pricing. The usual goal is to improve the predictive power via neural networks while building upon the generalized linear model, which is the current industry standard. Our paper contributes to this current journey via novel methods to enhance actuarial non-life models with transformer models for tabular data. We build here upon the foundation laid out by the combined actuarial neural network as well as the localGLMnet and enhance those models via the feature tokenizer transformer. The manuscript demonstrates the performance of the proposed methods on a real-world claim frequency dataset and compares them with several benchmark models such as generalized linear models, feed-forward neural networks, combined actuarial neural networks, LocalGLMnet, and pure feature tokenizer transformer. The paper shows that the new methods can achieve better results than the benchmark models while preserving certain generalized linear model advantages. The paper also discusses the practical implications and challenges of applying transformer models in actuarial settings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.07597&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexej Brauer</name></author><category term="stat.AP" /><summary type="html">Currently, there is a lot of research in the field of neural networks for non-life insurance pricing. The usual goal is to improve the predictive power via neural networks while building upon the generalized linear model, which is the current industry standard. Our paper contributes to this current journey via novel methods to enhance actuarial non-life models with transformer models for tabular data. We build here upon the foundation laid out by the combined actuarial neural network as well as the localGLMnet and enhance those models via the feature tokenizer transformer. The manuscript demonstrates the performance of the proposed methods on a real-world claim frequency dataset and compares them with several benchmark models such as generalized linear models, feed-forward neural networks, combined actuarial neural networks, LocalGLMnet, and pure feature tokenizer transformer. The paper shows that the new methods can achieve better results than the benchmark models while preserving certain generalized linear model advantages. The paper also discusses the practical implications and challenges of applying transformer models in actuarial settings.</summary></entry><entry><title type="html">Enhancing reliability in prediction intervals using point forecasters: Heteroscedastic Quantile Regression and Width-Adaptive Conformal Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/EnhancingreliabilityinpredictionintervalsusingpointforecastersHeteroscedasticQuantileRegressionandWidthAdaptiveConformalInference.html" rel="alternate" type="text/html" title="Enhancing reliability in prediction intervals using point forecasters: Heteroscedastic Quantile Regression and Width-Adaptive Conformal Inference" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/EnhancingreliabilityinpredictionintervalsusingpointforecastersHeteroscedasticQuantileRegressionandWidthAdaptiveConformalInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/EnhancingreliabilityinpredictionintervalsusingpointforecastersHeteroscedasticQuantileRegressionandWidthAdaptiveConformalInference.html">&lt;p&gt;Building prediction intervals for time series forecasting problems presents a complex challenge, particularly when relying solely on point predictors, a common scenario for practitioners in the industry. While research has primarily focused on achieving increasingly efficient valid intervals, we argue that, when evaluating a set of intervals, traditional measures alone are insufficient. There are additional crucial characteristics: the intervals must vary in length, with this variation directly linked to the difficulty of the prediction, and the coverage of the interval must remain independent of the difficulty of the prediction for practical utility. We propose the Heteroscedastic Quantile Regression (HQR) model and the Width-Adaptive Conformal Inference (WACI) method, providing theoretical coverage guarantees, to overcome those issues, respectively. The methodologies are evaluated in the context of Electricity Price Forecasting and Wind Power Forecasting, representing complex scenarios in time series forecasting. The results demonstrate that HQR and WACI not only improve or achieve typical measures of validity and efficiency but also successfully fulfil the commonly ignored mentioned characteristics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14904&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carlos Sebastián, Carlos E. González-Guillén, Jesús Juan</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Building prediction intervals for time series forecasting problems presents a complex challenge, particularly when relying solely on point predictors, a common scenario for practitioners in the industry. While research has primarily focused on achieving increasingly efficient valid intervals, we argue that, when evaluating a set of intervals, traditional measures alone are insufficient. There are additional crucial characteristics: the intervals must vary in length, with this variation directly linked to the difficulty of the prediction, and the coverage of the interval must remain independent of the difficulty of the prediction for practical utility. We propose the Heteroscedastic Quantile Regression (HQR) model and the Width-Adaptive Conformal Inference (WACI) method, providing theoretical coverage guarantees, to overcome those issues, respectively. The methodologies are evaluated in the context of Electricity Price Forecasting and Wind Power Forecasting, representing complex scenarios in time series forecasting. The results demonstrate that HQR and WACI not only improve or achieve typical measures of validity and efficiency but also successfully fulfil the commonly ignored mentioned characteristics.</summary></entry><entry><title type="html">Estimation of Over-parameterized Models from an Auto-Modeling Perspective</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/EstimationofOverparameterizedModelsfromanAutoModelingPerspective.html" rel="alternate" type="text/html" title="Estimation of Over-parameterized Models from an Auto-Modeling Perspective" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/EstimationofOverparameterizedModelsfromanAutoModelingPerspective</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/EstimationofOverparameterizedModelsfromanAutoModelingPerspective.html">&lt;p&gt;From a model-building perspective, we propose a paradigm shift for fitting over-parameterized models. Philosophically, the mindset is to fit models to future observations rather than to the observed sample. Technically, given an imputation method to generate future observations, we fit over-parameterized models to these future observations by optimizing an approximation of the desired expected loss function based on its sample counterpart and an adaptive $\textit{duality function}$. The required imputation method is also developed using the same estimation technique with an adaptive $m$-out-of-$n$ bootstrap approach. We illustrate its applications with the many-normal-means problem, $n &amp;lt; p$ linear regression, and neural network-based image classification of MNIST digits. The numerical results demonstrate its superior performance across these diverse applications. While primarily expository, the paper conducts an in-depth investigation into the theoretical aspects of the topic. It concludes with remarks on some open problems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2206.01824&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yiran Jiang, Chuanhai Liu</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.TH" /><summary type="html">From a model-building perspective, we propose a paradigm shift for fitting over-parameterized models. Philosophically, the mindset is to fit models to future observations rather than to the observed sample. Technically, given an imputation method to generate future observations, we fit over-parameterized models to these future observations by optimizing an approximation of the desired expected loss function based on its sample counterpart and an adaptive $\textit{duality function}$. The required imputation method is also developed using the same estimation technique with an adaptive $m$-out-of-$n$ bootstrap approach. We illustrate its applications with the many-normal-means problem, $n &amp;lt; p$ linear regression, and neural network-based image classification of MNIST digits. The numerical results demonstrate its superior performance across these diverse applications. While primarily expository, the paper conducts an in-depth investigation into the theoretical aspects of the topic. It concludes with remarks on some open problems.</summary></entry><entry><title type="html">Exact discovery is polynomial for sparse causal Bayesian networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/ExactdiscoveryispolynomialforsparsecausalBayesiannetworks.html" rel="alternate" type="text/html" title="Exact discovery is polynomial for sparse causal Bayesian networks" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/ExactdiscoveryispolynomialforsparsecausalBayesiannetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/ExactdiscoveryispolynomialforsparsecausalBayesiannetworks.html">&lt;p&gt;Causal Bayesian networks are widely used tools for summarising the dependencies between variables and elucidating their putative causal relationships. Learning networks from data is computationally hard in general. The current state-of-the-art approaches for exact causal discovery are integer linear programming over the underlying space of directed acyclic graphs, dynamic programming and shortest-path searches over the space of topological orders, and constraint programming combining both. For dynamic programming over orders, the computational complexity is known to be exponential base 2 in the number of variables in the network. We demonstrate how to use properties of Bayesian networks to prune the search space and lower the computational cost, while still guaranteeing exact discovery. When including new path-search and divide-and-conquer criteria, we prove optimality in quadratic time for matchings, and polynomial time for any network class with logarithmically-bound largest connected components. In simulation studies we observe the polynomial dependence for sparse networks and that, beyond some critical value, the logarithm of the base grows with the network density. Our approach then out-competes the state-of-the-art at lower densities. These results therefore pave the way for faster exact causal discovery in larger and sparser networks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15012&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Felix L. Rios, Giusi Moffa, Jack Kuipers</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">Causal Bayesian networks are widely used tools for summarising the dependencies between variables and elucidating their putative causal relationships. Learning networks from data is computationally hard in general. The current state-of-the-art approaches for exact causal discovery are integer linear programming over the underlying space of directed acyclic graphs, dynamic programming and shortest-path searches over the space of topological orders, and constraint programming combining both. For dynamic programming over orders, the computational complexity is known to be exponential base 2 in the number of variables in the network. We demonstrate how to use properties of Bayesian networks to prune the search space and lower the computational cost, while still guaranteeing exact discovery. When including new path-search and divide-and-conquer criteria, we prove optimality in quadratic time for matchings, and polynomial time for any network class with logarithmically-bound largest connected components. In simulation studies we observe the polynomial dependence for sparse networks and that, beyond some critical value, the logarithm of the base grows with the network density. Our approach then out-competes the state-of-the-art at lower densities. These results therefore pave the way for faster exact causal discovery in larger and sparser networks.</summary></entry><entry><title type="html">Fast sampling from constrained spaces using the Metropolis-adjusted Mirror Langevin algorithm</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/FastsamplingfromconstrainedspacesusingtheMetropolisadjustedMirrorLangevinalgorithm.html" rel="alternate" type="text/html" title="Fast sampling from constrained spaces using the Metropolis-adjusted Mirror Langevin algorithm" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/FastsamplingfromconstrainedspacesusingtheMetropolisadjustedMirrorLangevinalgorithm</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/FastsamplingfromconstrainedspacesusingtheMetropolisadjustedMirrorLangevinalgorithm.html">&lt;p&gt;We propose a new method called the Metropolis-adjusted Mirror Langevin algorithm for approximate sampling from distributions whose support is a compact and convex set. This algorithm adds an accept-reject filter to the Markov chain induced by a single step of the Mirror Langevin algorithm (Zhang et al., 2020), which is a basic discretisation of the Mirror Langevin dynamics. Due to the inclusion of this filter, our method is unbiased relative to the target, while known discretisations of the Mirror Langevin dynamics including the Mirror Langevin algorithm have an asymptotic bias. For this algorithm, we also give upper bounds for the number of iterations taken to mix to a constrained distribution whose potential is relatively smooth, convex, and Lipschitz continuous with respect to a self-concordant mirror function. As a consequence of the reversibility of the Markov chain induced by the inclusion of the Metropolis-Hastings filter, we obtain an exponentially better dependence on the error tolerance for approximate constrained sampling. We also present numerical experiments that corroborate our theoretical findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.08823&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vishwak Srinivasan, Andre Wibisono, Ashia Wilson</name></author><category term="stat.CO," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">We propose a new method called the Metropolis-adjusted Mirror Langevin algorithm for approximate sampling from distributions whose support is a compact and convex set. This algorithm adds an accept-reject filter to the Markov chain induced by a single step of the Mirror Langevin algorithm (Zhang et al., 2020), which is a basic discretisation of the Mirror Langevin dynamics. Due to the inclusion of this filter, our method is unbiased relative to the target, while known discretisations of the Mirror Langevin dynamics including the Mirror Langevin algorithm have an asymptotic bias. For this algorithm, we also give upper bounds for the number of iterations taken to mix to a constrained distribution whose potential is relatively smooth, convex, and Lipschitz continuous with respect to a self-concordant mirror function. As a consequence of the reversibility of the Markov chain induced by the inclusion of the Metropolis-Hastings filter, we obtain an exponentially better dependence on the error tolerance for approximate constrained sampling. We also present numerical experiments that corroborate our theoretical findings.</summary></entry><entry><title type="html">Frank copula is minimum information copula under fixed Kendall’s $\tau$</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/FrankcopulaisminimuminformationcopulaunderfixedKendallstau.html" rel="alternate" type="text/html" title="Frank copula is minimum information copula under fixed Kendall’s $\tau$" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/FrankcopulaisminimuminformationcopulaunderfixedKendallstau</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/FrankcopulaisminimuminformationcopulaunderfixedKendallstau.html">&lt;p&gt;In dependence modeling, various copulas have been utilized. Among them, the Frank copula has been one of the most typical choices due to its simplicity. In this work, we demonstrate that the Frank copula is the minimum information copula under fixed Kendall’s $\tau$ (MICK), both theoretically and numerically. First, we explain that both MICK and the Frank density follow the hyperbolic Liouville equation. Moreover, we show that the copula density satisfying the Liouville equation is uniquely the Frank copula. Our result asserts that selecting the Frank copula as an appropriate copula model is equivalent to using Kendall’s $\tau$ as the sole available information about the true distribution, based on the entropy maximization principle.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14814&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Issey Sukeda, Tomonari Sei</name></author><category term="stat.ME" /><summary type="html">In dependence modeling, various copulas have been utilized. Among them, the Frank copula has been one of the most typical choices due to its simplicity. In this work, we demonstrate that the Frank copula is the minimum information copula under fixed Kendall’s $\tau$ (MICK), both theoretically and numerically. First, we explain that both MICK and the Frank density follow the hyperbolic Liouville equation. Moreover, we show that the copula density satisfying the Liouville equation is uniquely the Frank copula. Our result asserts that selecting the Frank copula as an appropriate copula model is equivalent to using Kendall’s $\tau$ as the sole available information about the true distribution, based on the entropy maximization principle.</summary></entry><entry><title type="html">Functional Clustering for Longitudinal Associations between Social Determinants of Health and Stroke Mortality in the US</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/FunctionalClusteringforLongitudinalAssociationsbetweenSocialDeterminantsofHealthandStrokeMortalityintheUS.html" rel="alternate" type="text/html" title="Functional Clustering for Longitudinal Associations between Social Determinants of Health and Stroke Mortality in the US" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/FunctionalClusteringforLongitudinalAssociationsbetweenSocialDeterminantsofHealthandStrokeMortalityintheUS</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/FunctionalClusteringforLongitudinalAssociationsbetweenSocialDeterminantsofHealthandStrokeMortalityintheUS.html">&lt;p&gt;Understanding longitudinally changing associations between Social determinants of health (SDOH) and stroke mortality is crucial for timely stroke management. Previous studies have revealed a significant regional disparity in the SDOH – stroke mortality associations. However, they do not develop data-driven methods based on these longitudinal associations for regional division in stroke control. To fill this gap, we propose a novel clustering method for SDOH – stroke mortality associations in the US counties. To enhance interpretability and statistical efficiency of the clustering outcomes, we introduce a new class of smoothness-sparsity pursued penalties for simultaneous clustering and variable selection in the longitudinal associations. As a result, we can identify important SDOH that contribute to longitudinal changes in the stroke mortality, facilitating clustering of US counties into several regions based on how these SDOH relate to stroke mortality. The effectiveness of our proposed method is demonstrated through extensive numerical studies. By applying our method to a county-level SDOH and stroke mortality longitudinal data, we identify 18 important SDOH for stroke mortality and divide the US counties into two clusters based on these selected SDOH. Our findings unveil complex regional heterogeneity in the longitudinal associations between SDOH and stroke mortality, providing valuable insights in region-specific SDOH adjustments for mitigating stroke mortality.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10499&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fangzhi Luo, Jianbin Tan, Donglan Zhang, Hui Huang, Ye Shen</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Understanding longitudinally changing associations between Social determinants of health (SDOH) and stroke mortality is crucial for timely stroke management. Previous studies have revealed a significant regional disparity in the SDOH – stroke mortality associations. However, they do not develop data-driven methods based on these longitudinal associations for regional division in stroke control. To fill this gap, we propose a novel clustering method for SDOH – stroke mortality associations in the US counties. To enhance interpretability and statistical efficiency of the clustering outcomes, we introduce a new class of smoothness-sparsity pursued penalties for simultaneous clustering and variable selection in the longitudinal associations. As a result, we can identify important SDOH that contribute to longitudinal changes in the stroke mortality, facilitating clustering of US counties into several regions based on how these SDOH relate to stroke mortality. The effectiveness of our proposed method is demonstrated through extensive numerical studies. By applying our method to a county-level SDOH and stroke mortality longitudinal data, we identify 18 important SDOH for stroke mortality and divide the US counties into two clusters based on these selected SDOH. Our findings unveil complex regional heterogeneity in the longitudinal associations between SDOH and stroke mortality, providing valuable insights in region-specific SDOH adjustments for mitigating stroke mortality.</summary></entry><entry><title type="html">Inference for Delay Differential Equations Using Manifold-Constrained Gaussian Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/InferenceforDelayDifferentialEquationsUsingManifoldConstrainedGaussianProcesses.html" rel="alternate" type="text/html" title="Inference for Delay Differential Equations Using Manifold-Constrained Gaussian Processes" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/InferenceforDelayDifferentialEquationsUsingManifoldConstrainedGaussianProcesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/InferenceforDelayDifferentialEquationsUsingManifoldConstrainedGaussianProcesses.html">&lt;p&gt;Dynamic systems described by differential equations often involve feedback among system components. When there are time delays for components to sense and respond to feedback, delay differential equation (DDE) models are commonly used. This paper considers the problem of inferring unknown system parameters, including the time delays, from noisy and sparse experimental data observed from the system. We propose an extension of manifold-constrained Gaussian processes to conduct parameter inference for DDEs, whereas the time delay parameters have posed a challenge for existing methods that bypass numerical solvers. Our method uses a Bayesian framework to impose a Gaussian process model over the system trajectory, conditioned on the manifold constraint that satisfies the DDEs. For efficient computation, a linear interpolation scheme is developed to approximate the values of the time-delayed system outputs, along with corresponding theoretical error bounds on the approximated derivatives. Two simulation examples, based on Hutchinson’s equation and the lac operon system, together with a real-world application using Ontario COVID-19 data, are used to illustrate the efficacy of our method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15170&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuxuan Zhao, Samuel W. K. Wong</name></author><category term="stat.ME" /><summary type="html">Dynamic systems described by differential equations often involve feedback among system components. When there are time delays for components to sense and respond to feedback, delay differential equation (DDE) models are commonly used. This paper considers the problem of inferring unknown system parameters, including the time delays, from noisy and sparse experimental data observed from the system. We propose an extension of manifold-constrained Gaussian processes to conduct parameter inference for DDEs, whereas the time delay parameters have posed a challenge for existing methods that bypass numerical solvers. Our method uses a Bayesian framework to impose a Gaussian process model over the system trajectory, conditioned on the manifold constraint that satisfies the DDEs. For efficient computation, a linear interpolation scheme is developed to approximate the values of the time-delayed system outputs, along with corresponding theoretical error bounds on the approximated derivatives. Two simulation examples, based on Hutchinson’s equation and the lac operon system, together with a real-world application using Ontario COVID-19 data, are used to illustrate the efficacy of our method.</summary></entry><entry><title type="html">MSmix: An R Package for clustering partial rankings via mixtures of Mallows Models with Spearman distance</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/MSmixAnRPackageforclusteringpartialrankingsviamixturesofMallowsModelswithSpearmandistance.html" rel="alternate" type="text/html" title="MSmix: An R Package for clustering partial rankings via mixtures of Mallows Models with Spearman distance" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/MSmixAnRPackageforclusteringpartialrankingsviamixturesofMallowsModelswithSpearmandistance</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/MSmixAnRPackageforclusteringpartialrankingsviamixturesofMallowsModelswithSpearmandistance.html">&lt;p&gt;MSmix is a recently developed R package implementing maximum likelihood estimation of finite mixtures of Mallows models with Spearman distance for full and partial rankings. The package is designed to implement computationally tractable estimation routines of the model parameters, with the ability to handle arbitrary forms of partial rankings and sequences of a large number of items. The frequentist estimation task is accomplished via EM algorithms, integrating data augmentation strategies to recover the unobserved heterogeneity and the missing ranks. The package also provides functionalities for uncertainty quantification of the estimated parameters, via diverse bootstrap methods and asymptotic confidence intervals. Generic methods for S3 class objects are constructed for more effectively managing the output of the main routines. The usefulness of the package and its computational performance compared with competing software is illustrated via applications to both simulated and original real ranking datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14636&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marta Crispino, Cristina Mollica, Lucia Modugno</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">MSmix is a recently developed R package implementing maximum likelihood estimation of finite mixtures of Mallows models with Spearman distance for full and partial rankings. The package is designed to implement computationally tractable estimation routines of the model parameters, with the ability to handle arbitrary forms of partial rankings and sequences of a large number of items. The frequentist estimation task is accomplished via EM algorithms, integrating data augmentation strategies to recover the unobserved heterogeneity and the missing ranks. The package also provides functionalities for uncertainty quantification of the estimated parameters, via diverse bootstrap methods and asymptotic confidence intervals. Generic methods for S3 class objects are constructed for more effectively managing the output of the main routines. The usefulness of the package and its computational performance compared with competing software is illustrated via applications to both simulated and original real ranking datasets.</summary></entry><entry><title type="html">Monte Carlo Integration in Simple and Complex Simulation Designs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/MonteCarloIntegrationinSimpleandComplexSimulationDesigns.html" rel="alternate" type="text/html" title="Monte Carlo Integration in Simple and Complex Simulation Designs" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/MonteCarloIntegrationinSimpleandComplexSimulationDesigns</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/MonteCarloIntegrationinSimpleandComplexSimulationDesigns.html">&lt;p&gt;Simulation studies are used to evaluate and compare the properties of statistical methods in controlled experimental settings. In most cases, performing a simulation study requires knowledge of the true value of the parameter, or estimand, of interest. However, in many simulation designs, the true value of the estimand is difficult to compute analytically. Here, we illustrate the use of Monte Carlo integration to compute true estimand values in simple and complex simulation designs. We provide general pseudocode that can be replicated in any software program of choice to demonstrate key principles in using Monte Carlo integration in two scenarios: a simple three variable simulation where interest lies in the marginally adjusted odds ratio; and a more complex causal mediation analysis where interest lies in the controlled direct effect in the presence of mediator-outcome confounders affected by the exposure. We discuss general strategies that can be used to minimize Monte Carlo error, and to serve as checks on the simulation program to avoid coding errors. R programming code is provided illustrating the application of our pseudocode in these settings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15285&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ashley I. Naimi, David Benkeser, Jacqueline E. Rudolph</name></author><category term="stat.ME" /><summary type="html">Simulation studies are used to evaluate and compare the properties of statistical methods in controlled experimental settings. In most cases, performing a simulation study requires knowledge of the true value of the parameter, or estimand, of interest. However, in many simulation designs, the true value of the estimand is difficult to compute analytically. Here, we illustrate the use of Monte Carlo integration to compute true estimand values in simple and complex simulation designs. We provide general pseudocode that can be replicated in any software program of choice to demonstrate key principles in using Monte Carlo integration in two scenarios: a simple three variable simulation where interest lies in the marginally adjusted odds ratio; and a more complex causal mediation analysis where interest lies in the controlled direct effect in the presence of mediator-outcome confounders affected by the exposure. We discuss general strategies that can be used to minimize Monte Carlo error, and to serve as checks on the simulation program to avoid coding errors. R programming code is provided illustrating the application of our pseudocode in these settings.</summary></entry><entry><title type="html">Multiscale modelling of animal movement with persistent dynamics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Multiscalemodellingofanimalmovementwithpersistentdynamics.html" rel="alternate" type="text/html" title="Multiscale modelling of animal movement with persistent dynamics" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Multiscalemodellingofanimalmovementwithpersistentdynamics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Multiscalemodellingofanimalmovementwithpersistentdynamics.html">&lt;p&gt;Wild animals are commonly fitted with trackers that record their position through time, to learn about their behaviour. Broadly, statistical models for tracking data often fall into two categories: local models focus on describing small-scale movement decisions, and global models capture large-scale spatial distributions. Due to this dichotomy, it is challenging to describe mathematically how animals’ distributions arise from their short-term movement patterns, and to combine data sets collected at different scales. We propose a multiscale model of animal movement and space use based on the underdamped Langevin process, widely used in statistical physics. The model is convenient to describe animal movement for three reasons: it is specified in continuous time (such that its parameters are not dependent on an arbitrary time scale), its speed and direction are autocorrelated (similarly to real animal trajectories), and it has a closed form stationary distribution that we can view as a model of long-term space use. We use the common form of a resource selection function for the stationary distribution, to model the environmental drivers behind the animal’s movement decisions. We further increase flexibility by allowing movement parameters to be time-varying, e.g., to account for daily cycles in an animal’s activity. We formulate the model as a state-space model and present a method of inference based on the Kalman filter. The approach requires discretising the continuous-time process, and we use simulations to investigate performance for various time resolutions of observation. The approach works well at fine resolutions, though the estimated stationary distribution tends to be too flat when time intervals between observations are very long.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15195&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Théo Michelot</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Wild animals are commonly fitted with trackers that record their position through time, to learn about their behaviour. Broadly, statistical models for tracking data often fall into two categories: local models focus on describing small-scale movement decisions, and global models capture large-scale spatial distributions. Due to this dichotomy, it is challenging to describe mathematically how animals’ distributions arise from their short-term movement patterns, and to combine data sets collected at different scales. We propose a multiscale model of animal movement and space use based on the underdamped Langevin process, widely used in statistical physics. The model is convenient to describe animal movement for three reasons: it is specified in continuous time (such that its parameters are not dependent on an arbitrary time scale), its speed and direction are autocorrelated (similarly to real animal trajectories), and it has a closed form stationary distribution that we can view as a model of long-term space use. We use the common form of a resource selection function for the stationary distribution, to model the environmental drivers behind the animal’s movement decisions. We further increase flexibility by allowing movement parameters to be time-varying, e.g., to account for daily cycles in an animal’s activity. We formulate the model as a state-space model and present a method of inference based on the Kalman filter. The approach requires discretising the continuous-time process, and we use simulations to investigate performance for various time resolutions of observation. The approach works well at fine resolutions, though the estimated stationary distribution tends to be too flat when time intervals between observations are very long.</summary></entry><entry><title type="html">New iterative algorithms for estimation of item functioning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Newiterativealgorithmsforestimationofitemfunctioning.html" rel="alternate" type="text/html" title="New iterative algorithms for estimation of item functioning" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Newiterativealgorithmsforestimationofitemfunctioning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Newiterativealgorithmsforestimationofitemfunctioning.html">&lt;p&gt;When the item functioning of multi-item measurement is modeled with three or four-parameter models, parameter estimation may become challenging. Effective algorithms are crucial in such scenarios. This paper explores innovations to parameter estimation in generalized logistic regression models, which may be used in item response modeling to account for guessing/pretending or slipping/dissimulation and for the effect of covariates. We introduce a new implementation of the EM algorithm and propose a new algorithm based on the parametrized link function. The two novel iterative algorithms are compared to existing methods in a simulation study. Additionally, the study examines software implementation, including the specification of initial values for numerical algorithms and asymptotic properties with an estimation of standard errors. Overall, the newly proposed algorithm based on the parametrized link function outperforms other procedures, especially for small sample sizes. Moreover, the newly implemented EM algorithm provides additional information regarding respondents’ inclination to guess or pretend and slip or dissimulate when answering the item. The study also discusses applications of the methods in the context of the detection of differential item functioning. Methods are demonstrated using real data from psychological and educational assessments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.12648&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Adéla Hladká, Patrícia Martinková, Marek Brabec</name></author><category term="stat.ME" /><summary type="html">When the item functioning of multi-item measurement is modeled with three or four-parameter models, parameter estimation may become challenging. Effective algorithms are crucial in such scenarios. This paper explores innovations to parameter estimation in generalized logistic regression models, which may be used in item response modeling to account for guessing/pretending or slipping/dissimulation and for the effect of covariates. We introduce a new implementation of the EM algorithm and propose a new algorithm based on the parametrized link function. The two novel iterative algorithms are compared to existing methods in a simulation study. Additionally, the study examines software implementation, including the specification of initial values for numerical algorithms and asymptotic properties with an estimation of standard errors. Overall, the newly proposed algorithm based on the parametrized link function outperforms other procedures, especially for small sample sizes. Moreover, the newly implemented EM algorithm provides additional information regarding respondents’ inclination to guess or pretend and slip or dissimulate when answering the item. The study also discusses applications of the methods in the context of the detection of differential item functioning. Methods are demonstrated using real data from psychological and educational assessments.</summary></entry><entry><title type="html">On the estimation rate of Bayesian PINN for inverse problems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/OntheestimationrateofBayesianPINNforinverseproblems.html" rel="alternate" type="text/html" title="On the estimation rate of Bayesian PINN for inverse problems" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/OntheestimationrateofBayesianPINNforinverseproblems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/OntheestimationrateofBayesianPINNforinverseproblems.html">&lt;p&gt;Solving partial differential equations (PDEs) and their inverse problems using Physics-informed neural networks (PINNs) is a rapidly growing approach in the physics and machine learning community. Although several architectures exist for PINNs that work remarkably in practice, our theoretical understanding of their performances is somewhat limited. In this work, we study the behavior of a Bayesian PINN estimator of the solution of a PDE from $n$ independent noisy measurement of the solution. We focus on a class of equations that are linear in their parameters (with unknown coefficients $\theta_\star$). We show that when the partial differential equation admits a classical solution (say $u_\star$), differentiable to order $\beta$, the mean square error of the Bayesian posterior mean is at least of order $n^{-2\beta/(2\beta + d)}$. Furthermore, we establish a convergence rate of the linear coefficients of $\theta_\star$ depending on the order of the underlying differential operator. Last but not least, our theoretical results are validated through extensive simulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14808&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yi Sun, Debarghya Mukherjee, Yves Atchade</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Solving partial differential equations (PDEs) and their inverse problems using Physics-informed neural networks (PINNs) is a rapidly growing approach in the physics and machine learning community. Although several architectures exist for PINNs that work remarkably in practice, our theoretical understanding of their performances is somewhat limited. In this work, we study the behavior of a Bayesian PINN estimator of the solution of a PDE from $n$ independent noisy measurement of the solution. We focus on a class of equations that are linear in their parameters (with unknown coefficients $\theta_\star$). We show that when the partial differential equation admits a classical solution (say $u_\star$), differentiable to order $\beta$, the mean square error of the Bayesian posterior mean is at least of order $n^{-2\beta/(2\beta + d)}$. Furthermore, we establish a convergence rate of the linear coefficients of $\theta_\star$ depending on the order of the underlying differential operator. Last but not least, our theoretical results are validated through extensive simulations.</summary></entry><entry><title type="html">Phylogenetic least squares estimation without genetic distances</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Phylogeneticleastsquaresestimationwithoutgeneticdistances.html" rel="alternate" type="text/html" title="Phylogenetic least squares estimation without genetic distances" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Phylogeneticleastsquaresestimationwithoutgeneticdistances</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Phylogeneticleastsquaresestimationwithoutgeneticdistances.html">&lt;p&gt;Least squares estimation of phylogenies is an established family of methods with good statistical properties. State-of-the-art least squares phylogenetic estimation proceeds by first estimating a distance matrix, which is then used to determine the phylogeny by minimizing a squared-error loss function. Here, we develop a method for least squares phylogenetic inference that does not rely on a pre-estimated distance matrix. Our approach allows us to circumvent the typical need to first estimate a distance matrix by forming a new loss function inspired by the phylogenetic likelihood score function; in this manner, inference is not based on a summary statistic of the sequence data, but directly on the sequence data itself. We use a Jukes-Cantor substitution model to show that our method leads to improvements over ordinary least squares phylogenetic inference, and is even observed to rival maximum likelihood estimation in terms of topology estimation efficiency. Using a Kimura 2-parameter model, we show that our method also allows for estimation of the global transition/transversion ratio simultaneously with the phylogeny and its branch lengths. This is impossible to accomplish with any other distance-based method as far as we know. Our developments pave the way for more optimal phylogenetic inference under the least squares framework, particularly in settings under which likelihood-based inference is infeasible, including when one desires to build a phylogeny based on information provided by only a subset of all possible nucleotide substitutions such as synonymous or non-synonymous substitutions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.12717&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Peter B. Chi, Volodymyr M. Minin</name></author><category term="stat.ME" /><summary type="html">Least squares estimation of phylogenies is an established family of methods with good statistical properties. State-of-the-art least squares phylogenetic estimation proceeds by first estimating a distance matrix, which is then used to determine the phylogeny by minimizing a squared-error loss function. Here, we develop a method for least squares phylogenetic inference that does not rely on a pre-estimated distance matrix. Our approach allows us to circumvent the typical need to first estimate a distance matrix by forming a new loss function inspired by the phylogenetic likelihood score function; in this manner, inference is not based on a summary statistic of the sequence data, but directly on the sequence data itself. We use a Jukes-Cantor substitution model to show that our method leads to improvements over ordinary least squares phylogenetic inference, and is even observed to rival maximum likelihood estimation in terms of topology estimation efficiency. Using a Kimura 2-parameter model, we show that our method also allows for estimation of the global transition/transversion ratio simultaneously with the phylogeny and its branch lengths. This is impossible to accomplish with any other distance-based method as far as we know. Our developments pave the way for more optimal phylogenetic inference under the least squares framework, particularly in settings under which likelihood-based inference is infeasible, including when one desires to build a phylogeny based on information provided by only a subset of all possible nucleotide substitutions such as synonymous or non-synonymous substitutions.</summary></entry><entry><title type="html">Population Activity Recovery: Milestones Unfolding, Temporal Interdependencies, and Relationship with Physical and Social Vulnerability</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/PopulationActivityRecoveryMilestonesUnfoldingTemporalInterdependenciesandRelationshipwithPhysicalandSocialVulnerability.html" rel="alternate" type="text/html" title="Population Activity Recovery: Milestones Unfolding, Temporal Interdependencies, and Relationship with Physical and Social Vulnerability" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/PopulationActivityRecoveryMilestonesUnfoldingTemporalInterdependenciesandRelationshipwithPhysicalandSocialVulnerability</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/PopulationActivityRecoveryMilestonesUnfoldingTemporalInterdependenciesandRelationshipwithPhysicalandSocialVulnerability.html">&lt;p&gt;Understanding sequential community recovery milestones is crucial for proactive recovery planning and monitoring. This study investigates these milestones related to population activities to examine their temporal interdependencies and evaluate the relationship between recovery milestones and physical (residential property damage) and social vulnerability (household income). This study leverages post-2017 Hurricane Harvey mobility data from Harris County to specify and analyze temporal recovery milestones and their interdependencies. The analysis examined four key milestones: return to evacuated areas, recovery of essential and nonessential services, and the rate of home-switch (moving out of residences). Robust linear regression validates interdependencies between across milestone lags and sequences: achieving earlier milestones accelerates subsequent recovery milestones. The study thus identifies six primary recovery milestone sequences. We found that social vulnerability accounted through the median household income level, rather than physical vulnerability to flooding accounted through the property damage extent, correlates with recovery delays between milestones. We studied variations in recovery sequences across lower and upper quantiles of property damage extent and median household income: lower property damage extent and lower household income show greater representation in the (slowest to recover) sequence, while households with greater damage and higher income are predominant in the group with the (fastest recovery sequences). Milestone sequence variability aligns closely with social vulnerability, independent of physical vulnerability. Understanding the variation in recovery sequences, milestone interdependencies, and social vulnerability disparities provides crucial evidence for targeted interventions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14720&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Flavia Ioana Patrascu, Ali Mostafavi</name></author><category term="stat.AP" /><summary type="html">Understanding sequential community recovery milestones is crucial for proactive recovery planning and monitoring. This study investigates these milestones related to population activities to examine their temporal interdependencies and evaluate the relationship between recovery milestones and physical (residential property damage) and social vulnerability (household income). This study leverages post-2017 Hurricane Harvey mobility data from Harris County to specify and analyze temporal recovery milestones and their interdependencies. The analysis examined four key milestones: return to evacuated areas, recovery of essential and nonessential services, and the rate of home-switch (moving out of residences). Robust linear regression validates interdependencies between across milestone lags and sequences: achieving earlier milestones accelerates subsequent recovery milestones. The study thus identifies six primary recovery milestone sequences. We found that social vulnerability accounted through the median household income level, rather than physical vulnerability to flooding accounted through the property damage extent, correlates with recovery delays between milestones. We studied variations in recovery sequences across lower and upper quantiles of property damage extent and median household income: lower property damage extent and lower household income show greater representation in the (slowest to recover) sequence, while households with greater damage and higher income are predominant in the group with the (fastest recovery sequences). Milestone sequence variability aligns closely with social vulnerability, independent of physical vulnerability. Understanding the variation in recovery sequences, milestone interdependencies, and social vulnerability disparities provides crucial evidence for targeted interventions.</summary></entry><entry><title type="html">Predicting Progression Events in Multiple Myeloma from Routine Blood Work</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/PredictingProgressionEventsinMultipleMyelomafromRoutineBloodWork.html" rel="alternate" type="text/html" title="Predicting Progression Events in Multiple Myeloma from Routine Blood Work" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/PredictingProgressionEventsinMultipleMyelomafromRoutineBloodWork</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/PredictingProgressionEventsinMultipleMyelomafromRoutineBloodWork.html">&lt;p&gt;The ability to accurately predict disease progression is paramount for optimizing multiple myeloma patient care. This study introduces a hybrid neural network architecture, combining Long Short-Term Memory networks with a Conditional Restricted Boltzmann Machine, to predict future blood work of affected patients from a series of historical laboratory results. We demonstrate that our model can replicate the statistical moments of the time series ($0.95~\pm~0.01~\geq~R^2~\geq~0.83~\pm~0.03$) and forecast future blood work features with high correlation to actual patient data ($0.92\pm0.02~\geq~r~\geq~0.52~\pm~0.09$). Subsequently, a second Long Short-Term Memory network is employed to detect and annotate disease progression events within the forecasted blood work time series. We show that these annotations enable the prediction of progression events with significant reliability (AUROC$~=~0.88~\pm~0.01$), up to 12 months in advance (AUROC($t+12~$mos)$~=0.65~\pm~0.01$). Our system is designed in a modular fashion, featuring separate entities for forecasting and progression event annotation. This structure not only enhances interpretability but also facilitates the integration of additional modules to perform subsequent operations on the generated outputs. Our approach utilizes a minimal set of routine blood work measurements, which avoids the need for expensive or resource-intensive tests and ensures accessibility of the system in clinical routine. This capability allows for individualized risk assessment and making informed treatment decisions tailored to a patient’s unique disease kinetics. The represented approach contributes to the development of a scalable and cost-effective virtual human twin system for optimized healthcare resource utilization and improved patient outcomes in multiple myeloma care.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18051&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maximilian Ferle, Nora Grieb, Markus Kreuz, Uwe Platzbecker, Thomas Neumuth, Kristin Reiche, Alexander Oeser, Maximilian Merz</name></author><category term="stat.AP" /><summary type="html">The ability to accurately predict disease progression is paramount for optimizing multiple myeloma patient care. This study introduces a hybrid neural network architecture, combining Long Short-Term Memory networks with a Conditional Restricted Boltzmann Machine, to predict future blood work of affected patients from a series of historical laboratory results. We demonstrate that our model can replicate the statistical moments of the time series ($0.95~\pm~0.01~\geq~R^2~\geq~0.83~\pm~0.03$) and forecast future blood work features with high correlation to actual patient data ($0.92\pm0.02~\geq~r~\geq~0.52~\pm~0.09$). Subsequently, a second Long Short-Term Memory network is employed to detect and annotate disease progression events within the forecasted blood work time series. We show that these annotations enable the prediction of progression events with significant reliability (AUROC$~=~0.88~\pm~0.01$), up to 12 months in advance (AUROC($t+12~$mos)$~=0.65~\pm~0.01$). Our system is designed in a modular fashion, featuring separate entities for forecasting and progression event annotation. This structure not only enhances interpretability but also facilitates the integration of additional modules to perform subsequent operations on the generated outputs. Our approach utilizes a minimal set of routine blood work measurements, which avoids the need for expensive or resource-intensive tests and ensures accessibility of the system in clinical routine. This capability allows for individualized risk assessment and making informed treatment decisions tailored to a patient’s unique disease kinetics. The represented approach contributes to the development of a scalable and cost-effective virtual human twin system for optimized healthcare resource utilization and improved patient outcomes in multiple myeloma care.</summary></entry><entry><title type="html">Radial Neighbors for Provably Accurate Scalable Approximations of Gaussian Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/RadialNeighborsforProvablyAccurateScalableApproximationsofGaussianProcesses.html" rel="alternate" type="text/html" title="Radial Neighbors for Provably Accurate Scalable Approximations of Gaussian Processes" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/RadialNeighborsforProvablyAccurateScalableApproximationsofGaussianProcesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/RadialNeighborsforProvablyAccurateScalableApproximationsofGaussianProcesses.html">&lt;p&gt;In geostatistical problems with massive sample size, Gaussian processes can be approximated using sparse directed acyclic graphs to achieve scalable $O(n)$ computational complexity. In these models, data at each location are typically assumed conditionally dependent on a small set of parents which usually include a subset of the nearest neighbors. These methodologies often exhibit excellent empirical performance, but the lack of theoretical validation leads to unclear guidance in specifying the underlying graphical model and sensitivity to graph choice. We address these issues by introducing radial neighbors Gaussian processes (RadGP), a class of Gaussian processes based on directed acyclic graphs in which directed edges connect every location to all of its neighbors within a predetermined radius. We prove that any radial neighbors Gaussian process can accurately approximate the corresponding unrestricted Gaussian process in Wasserstein-2 distance, with an error rate determined by the approximation radius, the spatial covariance function, and the spatial dispersion of samples. We offer further empirical validation of our approach via applications on simulated and real world data showing excellent performance in both prior and posterior approximations to the original Gaussian process.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.14692&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yichen Zhu, Michele Peruzzi, Cheng Li, David B. Dunson</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In geostatistical problems with massive sample size, Gaussian processes can be approximated using sparse directed acyclic graphs to achieve scalable $O(n)$ computational complexity. In these models, data at each location are typically assumed conditionally dependent on a small set of parents which usually include a subset of the nearest neighbors. These methodologies often exhibit excellent empirical performance, but the lack of theoretical validation leads to unclear guidance in specifying the underlying graphical model and sensitivity to graph choice. We address these issues by introducing radial neighbors Gaussian processes (RadGP), a class of Gaussian processes based on directed acyclic graphs in which directed edges connect every location to all of its neighbors within a predetermined radius. We prove that any radial neighbors Gaussian process can accurately approximate the corresponding unrestricted Gaussian process in Wasserstein-2 distance, with an error rate determined by the approximation radius, the spatial covariance function, and the spatial dispersion of samples. We offer further empirical validation of our approach via applications on simulated and real world data showing excellent performance in both prior and posterior approximations to the original Gaussian process.</summary></entry><entry><title type="html">Random Pareto front surfaces</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/RandomParetofrontsurfaces.html" rel="alternate" type="text/html" title="Random Pareto front surfaces" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/RandomParetofrontsurfaces</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/RandomParetofrontsurfaces.html">&lt;p&gt;The goal of multi-objective optimisation is to identify the Pareto front surface which is the set obtained by connecting the best trade-off points. Typically this surface is computed by evaluating the objectives at different points and then interpolating between the subset of the best evaluated trade-off points. In this work, we propose to parameterise the Pareto front surface using polar coordinates. More precisely, we show that any Pareto front surface can be equivalently represented using a scalar-valued length function which returns the projected length along any positive radial direction. We then use this representation in order to rigorously develop the theory and applications of stochastic Pareto front surfaces. In particular, we derive many Pareto front surface statistics of interest such as the expectation, covariance and quantiles. We then discuss how these can be used in practice within a design of experiments setting, where the goal is to both infer and use the Pareto front surface distribution in order to make effective decisions. Our framework allows for clear uncertainty quantification and we also develop advanced visualisation techniques for this purpose. Finally we discuss the applicability of our ideas within multivariate extreme value theory and illustrate our methodology in a variety of numerical examples, including a case study with a real-world air pollution data set.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01404&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ben Tu, Nikolas Kantas, Robert M. Lee, Behrang Shafei</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">The goal of multi-objective optimisation is to identify the Pareto front surface which is the set obtained by connecting the best trade-off points. Typically this surface is computed by evaluating the objectives at different points and then interpolating between the subset of the best evaluated trade-off points. In this work, we propose to parameterise the Pareto front surface using polar coordinates. More precisely, we show that any Pareto front surface can be equivalently represented using a scalar-valued length function which returns the projected length along any positive radial direction. We then use this representation in order to rigorously develop the theory and applications of stochastic Pareto front surfaces. In particular, we derive many Pareto front surface statistics of interest such as the expectation, covariance and quantiles. We then discuss how these can be used in practice within a design of experiments setting, where the goal is to both infer and use the Pareto front surface distribution in order to make effective decisions. Our framework allows for clear uncertainty quantification and we also develop advanced visualisation techniques for this purpose. Finally we discuss the applicability of our ideas within multivariate extreme value theory and illustrate our methodology in a variety of numerical examples, including a case study with a real-world air pollution data set.</summary></entry><entry><title type="html">Robust parameter estimation for partially observed second-order diffusion processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Robustparameterestimationforpartiallyobservedsecondorderdiffusionprocesses.html" rel="alternate" type="text/html" title="Robust parameter estimation for partially observed second-order diffusion processes" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Robustparameterestimationforpartiallyobservedsecondorderdiffusionprocesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Robustparameterestimationforpartiallyobservedsecondorderdiffusionprocesses.html">&lt;p&gt;Estimating parameters of a diffusion process given continuous-time observations of the process via maximum likelihood approaches or, online, via stochastic gradient descent or Kalman filter formulations constitutes a well-established research area. It has also been established previously that these techniques are, in general, not robust to perturbations in the data in the form of temporal correlations. While the subject is relatively well understood and appropriate modifications have been suggested in the context of multi-scale diffusion processes and their reduced model equations, we consider here an alternative setting where a second-order diffusion process in positions and velocities is only observed via its positions. In this note, we propose a simple modification to standard stochastic gradient descent and Kalman filter formulations, which eliminates the arising systematic estimation biases. The modification can be extended to standard maximum likelihood approaches and avoids computation of previously proposed correction terms.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.14738&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jan Albrecht, Sebastian Reich</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Estimating parameters of a diffusion process given continuous-time observations of the process via maximum likelihood approaches or, online, via stochastic gradient descent or Kalman filter formulations constitutes a well-established research area. It has also been established previously that these techniques are, in general, not robust to perturbations in the data in the form of temporal correlations. While the subject is relatively well understood and appropriate modifications have been suggested in the context of multi-scale diffusion processes and their reduced model equations, we consider here an alternative setting where a second-order diffusion process in positions and velocities is only observed via its positions. In this note, we propose a simple modification to standard stochastic gradient descent and Kalman filter formulations, which eliminates the arising systematic estimation biases. The modification can be extended to standard maximum likelihood approaches and avoids computation of previously proposed correction terms.</summary></entry><entry><title type="html">Semiparametric inference of effective reproduction number dynamics from wastewater pathogen surveillance data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Semiparametricinferenceofeffectivereproductionnumberdynamicsfromwastewaterpathogensurveillancedata.html" rel="alternate" type="text/html" title="Semiparametric inference of effective reproduction number dynamics from wastewater pathogen surveillance data" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Semiparametricinferenceofeffectivereproductionnumberdynamicsfromwastewaterpathogensurveillancedata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Semiparametricinferenceofeffectivereproductionnumberdynamicsfromwastewaterpathogensurveillancedata.html">&lt;p&gt;Concentrations of pathogen genomes measured in wastewater have recently become available as a new data source to use when modeling the spread of infectious diseases. One promising use for this data source is inference of the effective reproduction number, the average number of individuals a newly infected person will infect. We propose a model where new infections arrive according to a time-varying immigration rate which can be interpreted as an average number of secondary infections produced by one infectious individual per unit time. This model allows us to estimate the effective reproduction number from concentrations of pathogen genomes while avoiding difficult to verify assumptions about the dynamics of the susceptible population. As a byproduct of our primary goal, we also produce a new model for estimating the effective reproduction number from case data using the same framework. We test this modeling framework in an agent-based simulation study with a realistic data generating mechanism which accounts for the time-varying dynamics of pathogen shedding. Finally, we apply our new model to estimating the effective reproduction number of SARS-CoV-2 in Los Angeles, California, using pathogen RNA concentrations collected from a large wastewater treatment facility.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.15770&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Isaac H. Goldstein, Daniel M. Parker, Sunny Jiang, Volodymyr M. Minin</name></author><category term="stat.ME" /><summary type="html">Concentrations of pathogen genomes measured in wastewater have recently become available as a new data source to use when modeling the spread of infectious diseases. One promising use for this data source is inference of the effective reproduction number, the average number of individuals a newly infected person will infect. We propose a model where new infections arrive according to a time-varying immigration rate which can be interpreted as an average number of secondary infections produced by one infectious individual per unit time. This model allows us to estimate the effective reproduction number from concentrations of pathogen genomes while avoiding difficult to verify assumptions about the dynamics of the susceptible population. As a byproduct of our primary goal, we also produce a new model for estimating the effective reproduction number from case data using the same framework. We test this modeling framework in an agent-based simulation study with a realistic data generating mechanism which accounts for the time-varying dynamics of pathogen shedding. Finally, we apply our new model to estimating the effective reproduction number of SARS-CoV-2 in Los Angeles, California, using pathogen RNA concentrations collected from a large wastewater treatment facility.</summary></entry><entry><title type="html">Sharp detection of low-dimensional structure in probability measures via dimensional logarithmic Sobolev inequalities</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/SharpdetectionoflowdimensionalstructureinprobabilitymeasuresviadimensionallogarithmicSobolevinequalities.html" rel="alternate" type="text/html" title="Sharp detection of low-dimensional structure in probability measures via dimensional logarithmic Sobolev inequalities" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/SharpdetectionoflowdimensionalstructureinprobabilitymeasuresviadimensionallogarithmicSobolevinequalities</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/SharpdetectionoflowdimensionalstructureinprobabilitymeasuresviadimensionallogarithmicSobolevinequalities.html">&lt;p&gt;Identifying low-dimensional structure in high-dimensional probability measures is an essential pre-processing step for efficient sampling. We introduce a method for identifying and approximating a target measure $\pi$ as a perturbation of a given reference measure $\mu$ along a few significant directions of $\mathbb{R}^{d}$. The reference measure can be a Gaussian or a nonlinear transformation of a Gaussian, as commonly arising in generative modeling. Our method extends prior work on minimizing majorizations of the Kullback–Leibler divergence to identify optimal approximations within this class of measures. Our main contribution unveils a connection between the \emph{dimensional} logarithmic Sobolev inequality (LSI) and approximations with this ansatz. Specifically, when the target and reference are both Gaussian, we show that minimizing the dimensional LSI is equivalent to minimizing the KL divergence restricted to this ansatz. For general non-Gaussian measures, the dimensional LSI produces majorants that uniformly improve on previous majorants for gradient-based dimension reduction. We further demonstrate the applicability of this analysis to the squared Hellinger distance, where analogous reasoning shows that the dimensional Poincar&apos;e inequality offers improved bounds.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.13036&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matthew T. C. Li, Tiangang Cui, Fengyi Li, Youssef Marzouk, Olivier Zahm</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Identifying low-dimensional structure in high-dimensional probability measures is an essential pre-processing step for efficient sampling. We introduce a method for identifying and approximating a target measure $\pi$ as a perturbation of a given reference measure $\mu$ along a few significant directions of $\mathbb{R}^{d}$. The reference measure can be a Gaussian or a nonlinear transformation of a Gaussian, as commonly arising in generative modeling. Our method extends prior work on minimizing majorizations of the Kullback–Leibler divergence to identify optimal approximations within this class of measures. Our main contribution unveils a connection between the \emph{dimensional} logarithmic Sobolev inequality (LSI) and approximations with this ansatz. Specifically, when the target and reference are both Gaussian, we show that minimizing the dimensional LSI is equivalent to minimizing the KL divergence restricted to this ansatz. For general non-Gaussian measures, the dimensional LSI produces majorants that uniformly improve on previous majorants for gradient-based dimension reduction. We further demonstrate the applicability of this analysis to the squared Hellinger distance, where analogous reasoning shows that the dimensional Poincar&apos;e inequality offers improved bounds.</summary></entry><entry><title type="html">Simplifying debiased inference via automatic differentiation and probabilistic programming</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Simplifyingdebiasedinferenceviaautomaticdifferentiationandprobabilisticprogramming.html" rel="alternate" type="text/html" title="Simplifying debiased inference via automatic differentiation and probabilistic programming" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Simplifyingdebiasedinferenceviaautomaticdifferentiationandprobabilisticprogramming</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/Simplifyingdebiasedinferenceviaautomaticdifferentiationandprobabilisticprogramming.html">&lt;p&gt;We introduce an algorithm that simplifies the construction of efficient estimators, making them accessible to a broader audience. ‘Dimple’ takes as input computer code representing a parameter of interest and outputs an efficient estimator. Unlike standard approaches, it does not require users to derive a functional derivative known as the efficient influence function. Dimple avoids this task by applying automatic differentiation to the statistical functional of interest. Doing so requires expressing this functional as a composition of primitives satisfying a novel differentiability condition. Dimple also uses this composition to determine the nuisances it must estimate. In software, primitives can be implemented independently of one another and reused across different estimation problems. We provide a proof-of-concept Python implementation and showcase through examples how it allows users to go from parameter specification to efficient estimation with just a few lines of code.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08675&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alex Luedtke</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">We introduce an algorithm that simplifies the construction of efficient estimators, making them accessible to a broader audience. ‘Dimple’ takes as input computer code representing a parameter of interest and outputs an efficient estimator. Unlike standard approaches, it does not require users to derive a functional derivative known as the efficient influence function. Dimple avoids this task by applying automatic differentiation to the statistical functional of interest. Doing so requires expressing this functional as a composition of primitives satisfying a novel differentiability condition. Dimple also uses this composition to determine the nuisances it must estimate. In software, primitives can be implemented independently of one another and reused across different estimation problems. We provide a proof-of-concept Python implementation and showcase through examples how it allows users to go from parameter specification to efficient estimation with just a few lines of code.</summary></entry><entry><title type="html">Small-time approximation of the transition density for diffusions with singularities. Application to the Wright-Fisher model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/SmalltimeapproximationofthetransitiondensityfordiffusionswithsingularitiesApplicationtotheWrightFishermodel.html" rel="alternate" type="text/html" title="Small-time approximation of the transition density for diffusions with singularities. Application to the Wright-Fisher model" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/SmalltimeapproximationofthetransitiondensityfordiffusionswithsingularitiesApplicationtotheWrightFishermodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/SmalltimeapproximationofthetransitiondensityfordiffusionswithsingularitiesApplicationtotheWrightFishermodel.html">&lt;p&gt;The Wright-Fisher (W-F) diffusion model serves as a foundational framework for interpreting population evolution through allele frequency dynamics over time. Despite the known transition probability between consecutive generations, an exact analytical expression for the transition density at arbitrary time intervals remains elusive. Commonly utilized distributions such as Gaussian or Beta inadequately address the fixation issue at extreme allele frequencies (0 or 1), particularly for short periods. In this study, we introduce two alternative parametric functions, namely the Asymptotic Expansion (AE) and the Gaussian approximation (GaussA), derived through probabilistic methodologies, aiming to better approximate this density. The AE function provides a suitable density for allele frequency distributions, encompassing extreme values within the interval [0,1]. Additionally, we outline the range of validity for the GaussA approximation. While our primary focus is on W-F diffusion, we demonstrate how our findings extend to other diffusion models featuring singularities. Through simulations of allele frequencies under a W-F process and employing a recently developed adaptive density estimation method, we conduct a comparative analysis to assess the fit of the proposed densities against the Beta and Gaussian distributions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.11442&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tania Roa, María Inés Fariello, Gerardo Martínez, José León</name></author><category term="stat.ME" /><summary type="html">The Wright-Fisher (W-F) diffusion model serves as a foundational framework for interpreting population evolution through allele frequency dynamics over time. Despite the known transition probability between consecutive generations, an exact analytical expression for the transition density at arbitrary time intervals remains elusive. Commonly utilized distributions such as Gaussian or Beta inadequately address the fixation issue at extreme allele frequencies (0 or 1), particularly for short periods. In this study, we introduce two alternative parametric functions, namely the Asymptotic Expansion (AE) and the Gaussian approximation (GaussA), derived through probabilistic methodologies, aiming to better approximate this density. The AE function provides a suitable density for allele frequency distributions, encompassing extreme values within the interval [0,1]. Additionally, we outline the range of validity for the GaussA approximation. While our primary focus is on W-F diffusion, we demonstrate how our findings extend to other diffusion models featuring singularities. Through simulations of allele frequencies under a W-F process and employing a recently developed adaptive density estimation method, we conduct a comparative analysis to assess the fit of the proposed densities against the Beta and Gaussian distributions.</summary></entry><entry><title type="html">Tackling GenAI Copyright Issues: Originality Estimation and Genericization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/TacklingGenAICopyrightIssuesOriginalityEstimationandGenericization.html" rel="alternate" type="text/html" title="Tackling GenAI Copyright Issues: Originality Estimation and Genericization" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/TacklingGenAICopyrightIssuesOriginalityEstimationandGenericization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/TacklingGenAICopyrightIssuesOriginalityEstimationandGenericization.html">&lt;p&gt;The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. While some studies explore methods to mitigate copyright risks by steering the outputs of generative models away from those resembling copyrighted data, little attention has been paid to the question of how much of a resemblance is undesirable; more original or unique data are afforded stronger protection, and the threshold level of resemblance for constituting infringement correspondingly lower. Here, leveraging this principle, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright. To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework. This metric can be practically estimated by drawing samples from a generative model, which is then used for the genericization process. Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03341&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hiroaki Chiba-Okabe, Weijie J. Su</name></author><category term="stat.AP," /><category term="stat.ME," /><category term="stat.ML" /><summary type="html">The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. While some studies explore methods to mitigate copyright risks by steering the outputs of generative models away from those resembling copyrighted data, little attention has been paid to the question of how much of a resemblance is undesirable; more original or unique data are afforded stronger protection, and the threshold level of resemblance for constituting infringement correspondingly lower. Here, leveraging this principle, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright. To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework. This metric can be practically estimated by drawing samples from a generative model, which is then used for the genericization process. Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images.</summary></entry><entry><title type="html">Testing Calibration in Nearly-Linear Time</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/TestingCalibrationinNearlyLinearTime.html" rel="alternate" type="text/html" title="Testing Calibration in Nearly-Linear Time" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/TestingCalibrationinNearlyLinearTime</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/TestingCalibrationinNearlyLinearTime.html">&lt;p&gt;In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. Motivated by [BGHN23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. We define the problem of calibration testing from samples where given $n$ draws from a distribution $\mathcal{D}$ on $(predictions, binary outcomes)$, our goal is to distinguish between the case where $\mathcal{D}$ is perfectly calibrated, and the case where $\mathcal{D}$ is $\varepsilon$-far from calibration.
  We make the simple observation that the empirical smooth calibration linear program can be reformulated as an instance of minimum-cost flow on a highly-structured graph, and design an exact dynamic programming-based solver for it which runs in time $O(n\log^2(n))$, and solves the calibration testing problem information-theoretically optimally in the same time. This improves upon state-of-the-art black-box linear program solvers requiring $\Omega(n^\omega)$ time, where $\omega &amp;gt; 2$ is the exponent of matrix multiplication. We also develop algorithms for tolerant variants of our testing problem improving upon black-box linear program solvers, and give sample complexity lower bounds for alternative calibration measures to the one considered in this work. Finally, we present experiments showing the testing problem we define faithfully captures standard notions of calibration, and that our algorithms scale efficiently to accommodate large sample sizes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.13187&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lunjia Hu, Arun Jambulapati, Kevin Tian, Chutong Yang</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">In the recent literature on machine learning and decision making, calibration has emerged as a desirable and widely-studied statistical property of the outputs of binary prediction models. However, the algorithmic aspects of measuring model calibration have remained relatively less well-explored. Motivated by [BGHN23], which proposed a rigorous framework for measuring distances to calibration, we initiate the algorithmic study of calibration through the lens of property testing. We define the problem of calibration testing from samples where given $n$ draws from a distribution $\mathcal{D}$ on $(predictions, binary outcomes)$, our goal is to distinguish between the case where $\mathcal{D}$ is perfectly calibrated, and the case where $\mathcal{D}$ is $\varepsilon$-far from calibration. We make the simple observation that the empirical smooth calibration linear program can be reformulated as an instance of minimum-cost flow on a highly-structured graph, and design an exact dynamic programming-based solver for it which runs in time $O(n\log^2(n))$, and solves the calibration testing problem information-theoretically optimally in the same time. This improves upon state-of-the-art black-box linear program solvers requiring $\Omega(n^\omega)$ time, where $\omega &amp;gt; 2$ is the exponent of matrix multiplication. We also develop algorithms for tolerant variants of our testing problem improving upon black-box linear program solvers, and give sample complexity lower bounds for alternative calibration measures to the one considered in this work. Finally, we present experiments showing the testing problem we define faithfully captures standard notions of calibration, and that our algorithms scale efficiently to accommodate large sample sizes.</summary></entry><entry><title type="html">The Influence of Nuisance Parameter Uncertainty on Statistical Inference in Practical Data Science Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/TheInfluenceofNuisanceParameterUncertaintyonStatisticalInferenceinPracticalDataScienceModels.html" rel="alternate" type="text/html" title="The Influence of Nuisance Parameter Uncertainty on Statistical Inference in Practical Data Science Models" /><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/TheInfluenceofNuisanceParameterUncertaintyonStatisticalInferenceinPracticalDataScienceModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/24/TheInfluenceofNuisanceParameterUncertaintyonStatisticalInferenceinPracticalDataScienceModels.html">&lt;p&gt;For multiple reasons – such as avoiding overtraining from one data set or because of having received numerical estimates for some parameters in a model from an alternative source – it is sometimes useful to divide a model’s parameters into one group of primary parameters and one group of nuisance parameters. However, uncertainty in the values of nuisance parameters is an inevitable factor that impacts the model’s reliability. This paper examines the issue of uncertainty calculation for primary parameters of interest in the presence of nuisance parameters. We illustrate a general procedure on two distinct model forms: 1) the GARCH time series model with univariate nuisance parameter and 2) multiple hidden layer feed-forward neural network models with multivariate nuisance parameters. Leveraging an existing theoretical framework for nuisance parameter uncertainty, we show how to modify the confidence regions for the primary parameters while considering the inherent uncertainty introduced by nuisance parameters. Furthermore, our study validates the practical effectiveness of adjusted confidence regions that properly account for uncertainty in nuisance parameters. Such an adjustment helps data scientists produce results that more honestly reflect the overall uncertainty.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15078&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yunrong Wan, James Spall</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">For multiple reasons – such as avoiding overtraining from one data set or because of having received numerical estimates for some parameters in a model from an alternative source – it is sometimes useful to divide a model’s parameters into one group of primary parameters and one group of nuisance parameters. However, uncertainty in the values of nuisance parameters is an inevitable factor that impacts the model’s reliability. This paper examines the issue of uncertainty calculation for primary parameters of interest in the presence of nuisance parameters. We illustrate a general procedure on two distinct model forms: 1) the GARCH time series model with univariate nuisance parameter and 2) multiple hidden layer feed-forward neural network models with multivariate nuisance parameters. Leveraging an existing theoretical framework for nuisance parameter uncertainty, we show how to modify the confidence regions for the primary parameters while considering the inherent uncertainty introduced by nuisance parameters. Furthermore, our study validates the practical effectiveness of adjusted confidence regions that properly account for uncertainty in nuisance parameters. Such an adjustment helps data scientists produce results that more honestly reflect the overall uncertainty.</summary></entry></feed>