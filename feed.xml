<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-17T07:14:33+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">Another look at forecast trimming for combinations: robustness, accuracy and diversity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Anotherlookatforecasttrimmingforcombinationsrobustnessaccuracyanddiversity.html" rel="alternate" type="text/html" title="Another look at forecast trimming for combinations: robustness, accuracy and diversity" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Anotherlookatforecasttrimmingforcombinationsrobustnessaccuracyanddiversity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Anotherlookatforecasttrimmingforcombinationsrobustnessaccuracyanddiversity.html">&lt;p&gt;Forecast combination is widely recognized as a preferred strategy over forecast selection due to its ability to mitigate the uncertainty associated with identifying a single “best” forecast. Nonetheless, sophisticated combinations are often empirically dominated by simple averaging, which is commonly attributed to the weight estimation error. The issue becomes more problematic when dealing with a forecast pool containing a large number of individual forecasts. In this paper, we propose a new forecast trimming algorithm to identify an optimal subset from the original forecast pool for forecast combination tasks. In contrast to existing approaches, our proposed algorithm simultaneously takes into account the robustness, accuracy and diversity issues of the forecast pool, rather than isolating each one of these issues. We also develop five forecast trimming algorithms as benchmarks, including one trimming-free algorithm and several trimming algorithms that isolate each one of the three key issues. Experimental results show that our algorithm achieves superior forecasting performance in general in terms of both point forecasts and prediction intervals. Nevertheless, we argue that diversity does not always have to be addressed in forecast trimming. Based on the results, we offer some practical guidelines on the selection of forecast trimming algorithms for a target series.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.00139&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xiaoqian Wang, Yanfei Kang, Feng Li</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Forecast combination is widely recognized as a preferred strategy over forecast selection due to its ability to mitigate the uncertainty associated with identifying a single “best” forecast. Nonetheless, sophisticated combinations are often empirically dominated by simple averaging, which is commonly attributed to the weight estimation error. The issue becomes more problematic when dealing with a forecast pool containing a large number of individual forecasts. In this paper, we propose a new forecast trimming algorithm to identify an optimal subset from the original forecast pool for forecast combination tasks. In contrast to existing approaches, our proposed algorithm simultaneously takes into account the robustness, accuracy and diversity issues of the forecast pool, rather than isolating each one of these issues. We also develop five forecast trimming algorithms as benchmarks, including one trimming-free algorithm and several trimming algorithms that isolate each one of the three key issues. Experimental results show that our algorithm achieves superior forecasting performance in general in terms of both point forecasts and prediction intervals. Nevertheless, we argue that diversity does not always have to be addressed in forecast trimming. Based on the results, we offer some practical guidelines on the selection of forecast trimming algorithms for a target series.</summary></entry><entry><title type="html">A robust statistical framework for cyber-vulnerability prioritisation under partial information in threat intelligence</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Arobuststatisticalframeworkforcybervulnerabilityprioritisationunderpartialinformationinthreatintelligence.html" rel="alternate" type="text/html" title="A robust statistical framework for cyber-vulnerability prioritisation under partial information in threat intelligence" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Arobuststatisticalframeworkforcybervulnerabilityprioritisationunderpartialinformationinthreatintelligence</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Arobuststatisticalframeworkforcybervulnerabilityprioritisationunderpartialinformationinthreatintelligence.html">&lt;p&gt;Proactive cyber-risk assessment is gaining momentum due to the wide range of sectors that can benefit from the prevention of cyber-incidents by preserving integrity, confidentiality, and the availability of data. The rising attention to cybersecurity also results from the increasing connectivity of cyber-physical systems, which generates multiple sources of uncertainty about emerging cyber-vulnerabilities. This work introduces a robust statistical framework for quantitative and qualitative reasoning under uncertainty about cyber-vulnerabilities and their prioritisation. Specifically, we take advantage of mid-quantile regression to deal with ordinal risk assessments, and we compare it to current alternatives for cyber-risk ranking and graded responses. For this purpose, we identify a novel accuracy measure suited for rank invariance under partial knowledge of the whole set of existing vulnerabilities. The model is tested on both simulated and real data from selected databases that support the evaluation, exploitation, or response to cyber-vulnerabilities in realistic contexts. Such datasets allow us to compare multiple models and accuracy measures, discussing the implications of partial knowledge about cyber-vulnerabilities on threat intelligence and decision-making in operational scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.08348&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mario Angelelli, Serena Arima, Christian Catalano, Enrico Ciavolino</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Proactive cyber-risk assessment is gaining momentum due to the wide range of sectors that can benefit from the prevention of cyber-incidents by preserving integrity, confidentiality, and the availability of data. The rising attention to cybersecurity also results from the increasing connectivity of cyber-physical systems, which generates multiple sources of uncertainty about emerging cyber-vulnerabilities. This work introduces a robust statistical framework for quantitative and qualitative reasoning under uncertainty about cyber-vulnerabilities and their prioritisation. Specifically, we take advantage of mid-quantile regression to deal with ordinal risk assessments, and we compare it to current alternatives for cyber-risk ranking and graded responses. For this purpose, we identify a novel accuracy measure suited for rank invariance under partial knowledge of the whole set of existing vulnerabilities. The model is tested on both simulated and real data from selected databases that support the evaluation, exploitation, or response to cyber-vulnerabilities in realistic contexts. Such datasets allow us to compare multiple models and accuracy measures, discussing the implications of partial knowledge about cyber-vulnerabilities on threat intelligence and decision-making in operational scenarios.</summary></entry><entry><title type="html">Browsing behavior exposes identities on the Web</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/BrowsingbehaviorexposesidentitiesontheWeb.html" rel="alternate" type="text/html" title="Browsing behavior exposes identities on the Web" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/BrowsingbehaviorexposesidentitiesontheWeb</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/BrowsingbehaviorexposesidentitiesontheWeb.html">&lt;p&gt;How easy is it to uniquely identify a person based solely on their web browsing behavior? Here we show that when people navigate the Web, their online traces produce fingerprints that identify them. Merely the four most visited web domains are enough to identify 95% of the individuals. These digital fingerprints are stable and render high re-identifiability. We demonstrate that we can re-identify 80% of the individuals in separate time slices of data. Such a privacy threat persists even with limited information about individuals’ browsing behavior, reinforcing existing concerns around online privacy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.15489&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marcos Oliveira, Junran Yang, Daniel Griffiths, Denis Bonnay, Juhi Kulshrestha</name></author><category term="stat.AP" /><summary type="html">How easy is it to uniquely identify a person based solely on their web browsing behavior? Here we show that when people navigate the Web, their online traces produce fingerprints that identify them. Merely the four most visited web domains are enough to identify 95% of the individuals. These digital fingerprints are stable and render high re-identifiability. We demonstrate that we can re-identify 80% of the individuals in separate time slices of data. Such a privacy threat persists even with limited information about individuals’ browsing behavior, reinforcing existing concerns around online privacy.</summary></entry><entry><title type="html">Discovering influential text using convolutional neural networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Discoveringinfluentialtextusingconvolutionalneuralnetworks.html" rel="alternate" type="text/html" title="Discovering influential text using convolutional neural networks" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Discoveringinfluentialtextusingconvolutionalneuralnetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Discoveringinfluentialtextusingconvolutionalneuralnetworks.html">&lt;p&gt;Experimental methods for estimating the impacts of text on human evaluation have been widely used in the social sciences. However, researchers in experimental settings are usually limited to testing a small number of pre-specified text treatments. While efforts to mine unstructured texts for features that causally affect outcomes have been ongoing in recent years, these models have primarily focused on the topics or specific words of text, which may not always be the mechanism of the effect. We connect these efforts with NLP interpretability techniques and present a method for flexibly discovering clusters of similar text phrases that are predictive of human reactions to texts using convolutional neural networks. When used in an experimental setting, this method can identify text treatments and their effects under certain assumptions. We apply the method to two datasets. The first enables direct validation of the model’s ability to detect phrases known to cause the outcome. The second demonstrates its ability to flexibly discover text treatments with varying textual structures. In both cases, the model learns a greater variety of text treatments compared to benchmark methods, and these text features quantitatively meet or exceed the ability of benchmark methods to predict the outcome.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10086&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Megan Ayers, Luke Sanford, Margaret Roberts, Eddie Yang</name></author><category term="stat.ME" /><summary type="html">Experimental methods for estimating the impacts of text on human evaluation have been widely used in the social sciences. However, researchers in experimental settings are usually limited to testing a small number of pre-specified text treatments. While efforts to mine unstructured texts for features that causally affect outcomes have been ongoing in recent years, these models have primarily focused on the topics or specific words of text, which may not always be the mechanism of the effect. We connect these efforts with NLP interpretability techniques and present a method for flexibly discovering clusters of similar text phrases that are predictive of human reactions to texts using convolutional neural networks. When used in an experimental setting, this method can identify text treatments and their effects under certain assumptions. We apply the method to two datasets. The first enables direct validation of the model’s ability to detect phrases known to cause the outcome. The second demonstrates its ability to flexibly discover text treatments with varying textual structures. In both cases, the model learns a greater variety of text treatments compared to benchmark methods, and these text features quantitatively meet or exceed the ability of benchmark methods to predict the outcome.</summary></entry><entry><title type="html">Estimating Changepoints in Extremal Dependence, Applied to Aviation Stock Prices During COVID-19 Pandemic</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/EstimatingChangepointsinExtremalDependenceAppliedtoAviationStockPricesDuringCOVID19Pandemic.html" rel="alternate" type="text/html" title="Estimating Changepoints in Extremal Dependence, Applied to Aviation Stock Prices During COVID-19 Pandemic" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/EstimatingChangepointsinExtremalDependenceAppliedtoAviationStockPricesDuringCOVID19Pandemic</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/EstimatingChangepointsinExtremalDependenceAppliedtoAviationStockPricesDuringCOVID19Pandemic.html">&lt;p&gt;The dependence in the tails of the joint distribution of two random variables is generally assessed using $\chi$-measure, the limiting conditional probability of one variable being extremely high given the other variable is also extremely high. This work is motivated by the structural changes in $\chi$-measure between the daily rate of return (RoR) of the two Indian airlines, IndiGo and SpiceJet, during the COVID-19 pandemic. We model the daily maximum and minimum RoR vectors (potentially transformed) using the bivariate H&quot;usler-Reiss (BHR) distribution. To estimate the changepoint in the $\chi$-measure of the BHR distribution, we explore two changepoint detection procedures based on the Likelihood Ratio Test (LRT) and Modified Information Criterion (MIC). We obtain critical values and power curves of the LRT and MIC test statistics for low through high values of $\chi$-measure. We also explore the consistency of the estimators of the changepoint based on LRT and MIC numerically. In our data application, for RoR maxima and minima, the most prominent changepoints detected by LRT and MIC are close to the announcement of the first phases of lockdown and unlock, respectively, which are realistic; thus, our study would be beneficial for portfolio optimization in the case of future pandemic situations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.13895&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arnab Hazra, Shiladitya Bose</name></author><category term="stat.AP" /><summary type="html">The dependence in the tails of the joint distribution of two random variables is generally assessed using $\chi$-measure, the limiting conditional probability of one variable being extremely high given the other variable is also extremely high. This work is motivated by the structural changes in $\chi$-measure between the daily rate of return (RoR) of the two Indian airlines, IndiGo and SpiceJet, during the COVID-19 pandemic. We model the daily maximum and minimum RoR vectors (potentially transformed) using the bivariate H&quot;usler-Reiss (BHR) distribution. To estimate the changepoint in the $\chi$-measure of the BHR distribution, we explore two changepoint detection procedures based on the Likelihood Ratio Test (LRT) and Modified Information Criterion (MIC). We obtain critical values and power curves of the LRT and MIC test statistics for low through high values of $\chi$-measure. We also explore the consistency of the estimators of the changepoint based on LRT and MIC numerically. In our data application, for RoR maxima and minima, the most prominent changepoints detected by LRT and MIC are close to the announcement of the first phases of lockdown and unlock, respectively, which are realistic; thus, our study would be beneficial for portfolio optimization in the case of future pandemic situations.</summary></entry><entry><title type="html">Estimating the linear relation between variables that are never jointly observed</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Estimatingthelinearrelationbetweenvariablesthatareneverjointlyobserved.html" rel="alternate" type="text/html" title="Estimating the linear relation between variables that are never jointly observed" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Estimatingthelinearrelationbetweenvariablesthatareneverjointlyobserved</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Estimatingthelinearrelationbetweenvariablesthatareneverjointlyobserved.html">&lt;p&gt;This work is motivated by in vivo experiments in which measurement are destructive so that the variables of interest can never be observed simultaneously when the aim is to estimate the regression coefficients of a linear regression. Assuming that the global experiment can be decomposed into sub experiments (corresponding for example to different doses) with distinct first moments, we propose different estimators of the linear regression which take account of that additional information. We consider estimators based on moments as well as estimators based optimal transport theory. These estimators are proved to be consistent as well as asymptotically Gaussian under weak hypotheses. The asymptotic variance has no explicit expression, except in some particular cases, and specific bootstrap approaches are developed to build confidence intervals for the estimated parameter. A Monte Carlo study is conducted to assess and compare the finite sample performances of the different approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.00140&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Polina Arsenteva, Mohamed Amine Benadjaoud, Hervé Cardot</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This work is motivated by in vivo experiments in which measurement are destructive so that the variables of interest can never be observed simultaneously when the aim is to estimate the regression coefficients of a linear regression. Assuming that the global experiment can be decomposed into sub experiments (corresponding for example to different doses) with distinct first moments, we propose different estimators of the linear regression which take account of that additional information. We consider estimators based on moments as well as estimators based optimal transport theory. These estimators are proved to be consistent as well as asymptotically Gaussian under weak hypotheses. The asymptotic variance has no explicit expression, except in some particular cases, and specific bootstrap approaches are developed to build confidence intervals for the estimated parameter. A Monte Carlo study is conducted to assess and compare the finite sample performances of the different approaches.</summary></entry><entry><title type="html">Extreme value methods for estimating rare events in Utopia</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/ExtremevaluemethodsforestimatingrareeventsinUtopia.html" rel="alternate" type="text/html" title="Extreme value methods for estimating rare events in Utopia" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/ExtremevaluemethodsforestimatingrareeventsinUtopia</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/ExtremevaluemethodsforestimatingrareeventsinUtopia.html">&lt;p&gt;To capture the extremal behaviour of complex environmental phenomena in practice, flexi-ble techniques for modelling tail behaviour are required. In this paper, we introduce a variety of such methods, which were used by the Lancopula Utopiversity team to tackle the EVA (2023) Conference Data Challenge. This data challenge was split into four challenges, labelled C1-C4. Challenges C1 and C2 comprise univariate problems, where the goal is to estimate extreme quantiles for a non-stationary time series exhibiting several complex features. For these, we propose a flexible modelling technique, based on generalised additive models, with diagnostics indicating generally good performance for the observed data. Challenges C3 and C4 concern multivariate problems where the focus is on estimating joint extremal probabilities. For challenge C3, we propose an extension of available models in the multivariate literature and use this framework to estimate extreme probabilities in the presence of non-stationary dependence. Finally, for challenge C4, which concerns a 50 dimensional random vector, we employ a clustering technique to achieve dimension reduction and use a conditional modelling approach to estimate extremal probabilities across independent groups of variables.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.09825&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>L. M. André, R. Campbell, E. D&apos;Arcy, A. Farrell, D. Healy, L. Kakampakou, C. Murphy, C. J. R. Murphy-Barltrop, M. Speers</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">To capture the extremal behaviour of complex environmental phenomena in practice, flexi-ble techniques for modelling tail behaviour are required. In this paper, we introduce a variety of such methods, which were used by the Lancopula Utopiversity team to tackle the EVA (2023) Conference Data Challenge. This data challenge was split into four challenges, labelled C1-C4. Challenges C1 and C2 comprise univariate problems, where the goal is to estimate extreme quantiles for a non-stationary time series exhibiting several complex features. For these, we propose a flexible modelling technique, based on generalised additive models, with diagnostics indicating generally good performance for the observed data. Challenges C3 and C4 concern multivariate problems where the focus is on estimating joint extremal probabilities. For challenge C3, we propose an extension of available models in the multivariate literature and use this framework to estimate extreme probabilities in the presence of non-stationary dependence. Finally, for challenge C4, which concerns a 50 dimensional random vector, we employ a clustering technique to achieve dimension reduction and use a conditional modelling approach to estimate extremal probabilities across independent groups of variables.</summary></entry><entry><title type="html">Graph-accelerated Markov Chain Monte Carlo using Approximate Samples</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/GraphacceleratedMarkovChainMonteCarlousingApproximateSamples.html" rel="alternate" type="text/html" title="Graph-accelerated Markov Chain Monte Carlo using Approximate Samples" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/GraphacceleratedMarkovChainMonteCarlousingApproximateSamples</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/GraphacceleratedMarkovChainMonteCarlousingApproximateSamples.html">&lt;p&gt;It has become increasingly easy nowadays to collect approximate posterior samples via fast algorithms such as variational Bayes, but concerns exist about the estimation accuracy. It is tempting to build solutions that exploit approximate samples in a canonical Markov chain Monte Carlo framework. A major barrier is that the approximate sample as a proposal tends to have a low Metropolis-Hastings acceptance rate, as the dimension increases. In this article, we propose a simple solution named graph-accelerated Markov Chain Monte Carlo. We build a graph with each node assigned to an approximate sample, then run Markov chain Monte Carlo with random walks over the graph. We optimize the graph edges to enforce small differences in posterior density/probability between nodes, while encouraging edges to have large distances in the parameter space. The graph allows us to accelerate a canonical Markov transition kernel through mixing with a large-jump Metropolis-Hastings step. The acceleration is easily applicable to existing Markov chain Monte Carlo algorithms. We theoretically quantify the rate of acceptance as dimension increases, and show the effects on improved mixing time. We demonstrate improved mixing performances for challenging problems, such as those involving multiple modes, non-convex density contour, or large-dimension latent variables.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.14186&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Leo L. Duan, Anirban Bhattacharya</name></author><category term="stat.CO" /><summary type="html">It has become increasingly easy nowadays to collect approximate posterior samples via fast algorithms such as variational Bayes, but concerns exist about the estimation accuracy. It is tempting to build solutions that exploit approximate samples in a canonical Markov chain Monte Carlo framework. A major barrier is that the approximate sample as a proposal tends to have a low Metropolis-Hastings acceptance rate, as the dimension increases. In this article, we propose a simple solution named graph-accelerated Markov Chain Monte Carlo. We build a graph with each node assigned to an approximate sample, then run Markov chain Monte Carlo with random walks over the graph. We optimize the graph edges to enforce small differences in posterior density/probability between nodes, while encouraging edges to have large distances in the parameter space. The graph allows us to accelerate a canonical Markov transition kernel through mixing with a large-jump Metropolis-Hastings step. The acceleration is easily applicable to existing Markov chain Monte Carlo algorithms. We theoretically quantify the rate of acceptance as dimension increases, and show the effects on improved mixing time. We demonstrate improved mixing performances for challenging problems, such as those involving multiple modes, non-convex density contour, or large-dimension latent variables.</summary></entry><entry><title type="html">Improved multifidelity Monte Carlo estimators based on normalizing flows and dimensionality reduction techniques</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/ImprovedmultifidelityMonteCarloestimatorsbasedonnormalizingflowsanddimensionalityreductiontechniques.html" rel="alternate" type="text/html" title="Improved multifidelity Monte Carlo estimators based on normalizing flows and dimensionality reduction techniques" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/ImprovedmultifidelityMonteCarloestimatorsbasedonnormalizingflowsanddimensionalityreductiontechniques</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/ImprovedmultifidelityMonteCarloestimatorsbasedonnormalizingflowsanddimensionalityreductiontechniques.html">&lt;p&gt;We study the problem of multifidelity uncertainty propagation for computationally expensive models. In particular, we consider the general setting where the high-fidelity and low-fidelity models have a dissimilar parameterization both in terms of number of random inputs and their probability distributions, which can be either known in closed form or provided through samples. We derive novel multifidelity Monte Carlo estimators which rely on a shared subspace between the high-fidelity and low-fidelity models where the parameters follow the same probability distribution, i.e., a standard Gaussian. We build the shared space employing normalizing flows to map different probability distributions into a common one, together with linear and nonlinear dimensionality reduction techniques, active subspaces and autoencoders, respectively, which capture the subspaces where the models vary the most. We then compose the existing low-fidelity model with these transformations and construct modified models with an increased correlation with the high-fidelity model, which therefore yield multifidelity Monte Carlo estimators with reduced variance. A series of numerical experiments illustrate the properties and advantages of our approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.12361&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrea Zanoni, Gianluca Geraci, Matteo Salvador, Karthik Menon, Alison L. Marsden, Daniele E. Schiavazzi</name></author><category term="stat.ME" /><summary type="html">We study the problem of multifidelity uncertainty propagation for computationally expensive models. In particular, we consider the general setting where the high-fidelity and low-fidelity models have a dissimilar parameterization both in terms of number of random inputs and their probability distributions, which can be either known in closed form or provided through samples. We derive novel multifidelity Monte Carlo estimators which rely on a shared subspace between the high-fidelity and low-fidelity models where the parameters follow the same probability distribution, i.e., a standard Gaussian. We build the shared space employing normalizing flows to map different probability distributions into a common one, together with linear and nonlinear dimensionality reduction techniques, active subspaces and autoencoders, respectively, which capture the subspaces where the models vary the most. We then compose the existing low-fidelity model with these transformations and construct modified models with an increased correlation with the high-fidelity model, which therefore yield multifidelity Monte Carlo estimators with reduced variance. A series of numerical experiments illustrate the properties and advantages of our approaches.</summary></entry><entry><title type="html">Large language model validity via enhanced conformal prediction methods</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Largelanguagemodelvalidityviaenhancedconformalpredictionmethods.html" rel="alternate" type="text/html" title="Large language model validity via enhanced conformal prediction methods" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Largelanguagemodelvalidityviaenhancedconformalpredictionmethods</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/Largelanguagemodelvalidityviaenhancedconformalpredictionmethods.html">&lt;p&gt;We develop new conformal inference methods for obtaining validity guarantees on the output of large language models (LLMs). Prior work in conformal language modeling identifies a subset of the text that satisfies a high-probability guarantee of correctness. These methods work by filtering claims from the LLM’s original response if a scoring function evaluated on the claim fails to exceed a threshold calibrated via split conformal prediction. Existing methods in this area suffer from two deficiencies. First, the guarantee stated is not conditionally valid. The trustworthiness of the filtering step may vary based on the topic of the response. Second, because the scoring function is imperfect, the filtering step can remove many valuable and accurate claims. We address both of these challenges via two new conformal methods. First, we generalize the conditional conformal procedure of Gibbs et al. (2023) in order to adaptively issue weaker guarantees when they are required to preserve the utility of the output. Second, we show how to systematically improve the quality of the scoring function via a novel algorithm for differentiating through the conditional conformal procedure. We demonstrate the efficacy of our approach on both synthetic and real-world datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09714&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>John J. Cherian, Isaac Gibbs, Emmanuel J. Candès</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">We develop new conformal inference methods for obtaining validity guarantees on the output of large language models (LLMs). Prior work in conformal language modeling identifies a subset of the text that satisfies a high-probability guarantee of correctness. These methods work by filtering claims from the LLM’s original response if a scoring function evaluated on the claim fails to exceed a threshold calibrated via split conformal prediction. Existing methods in this area suffer from two deficiencies. First, the guarantee stated is not conditionally valid. The trustworthiness of the filtering step may vary based on the topic of the response. Second, because the scoring function is imperfect, the filtering step can remove many valuable and accurate claims. We address both of these challenges via two new conformal methods. First, we generalize the conditional conformal procedure of Gibbs et al. (2023) in order to adaptively issue weaker guarantees when they are required to preserve the utility of the output. Second, we show how to systematically improve the quality of the scoring function via a novel algorithm for differentiating through the conditional conformal procedure. We demonstrate the efficacy of our approach on both synthetic and real-world datasets.</summary></entry><entry><title type="html">Lasso Multinomial Performance Indicators for in-play Basketball Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/LassoMultinomialPerformanceIndicatorsforinplayBasketballData.html" rel="alternate" type="text/html" title="Lasso Multinomial Performance Indicators for in-play Basketball Data" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/LassoMultinomialPerformanceIndicatorsforinplayBasketballData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/LassoMultinomialPerformanceIndicatorsforinplayBasketballData.html">&lt;p&gt;A typical approach to quantify the contribution of each player in basketball uses the plus/minus approach. Such plus/minus ratings are estimated using simple regression models and their regularised variants with response variable either the points scored or the point differences. To capture more precisely the effect of each player and the combined effects of specific lineups, more detailed possession-based play-by-play data are needed. This is the direction we take in this article, in which we investigate the performance of regularized adjusted plus/minus (RAPM) indicators estimated by different regularized models having as a response the number of points scored in each possession. Therefore, we use possession play-by-play data from all NBA games for the season 2021-22 (322,852 possessions). We initially present simple regression model-based indices starting from the implementation of ridge regression which is the standard technique in the relevant literature. We proceed with the lasso approach which has specific advantages and better performance than ridge regression when compared with selected objective validation criteria. Then, we implement regularized binary and multinomial logistic regression models to obtain more accurate performance indicators since the response is a discrete variable taking values mainly from zero to three. Our final proposal is an improved RAPM measure which is based on the expected points of a multinomial logistic regression model where each player’s contribution is weighted by his participation in the team’s possessions. The proposed indicator, called weighted expected points (wEPTS), outperforms all other RAPM measures we investigate in this study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09895&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Argyro Damoulaki, Ioannis Ntzoufras, Konstantinos Pelechrinis</name></author><category term="stat.AP" /><summary type="html">A typical approach to quantify the contribution of each player in basketball uses the plus/minus approach. Such plus/minus ratings are estimated using simple regression models and their regularised variants with response variable either the points scored or the point differences. To capture more precisely the effect of each player and the combined effects of specific lineups, more detailed possession-based play-by-play data are needed. This is the direction we take in this article, in which we investigate the performance of regularized adjusted plus/minus (RAPM) indicators estimated by different regularized models having as a response the number of points scored in each possession. Therefore, we use possession play-by-play data from all NBA games for the season 2021-22 (322,852 possessions). We initially present simple regression model-based indices starting from the implementation of ridge regression which is the standard technique in the relevant literature. We proceed with the lasso approach which has specific advantages and better performance than ridge regression when compared with selected objective validation criteria. Then, we implement regularized binary and multinomial logistic regression models to obtain more accurate performance indicators since the response is a discrete variable taking values mainly from zero to three. Our final proposal is an improved RAPM measure which is based on the expected points of a multinomial logistic regression model where each player’s contribution is weighted by his participation in the team’s possessions. The proposed indicator, called weighted expected points (wEPTS), outperforms all other RAPM measures we investigate in this study.</summary></entry><entry><title type="html">Learning High-dimensional Latent Variable Models via Doubly Stochastic Optimisation by Unadjusted Langevin</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/LearningHighdimensionalLatentVariableModelsviaDoublyStochasticOptimisationbyUnadjustedLangevin.html" rel="alternate" type="text/html" title="Learning High-dimensional Latent Variable Models via Doubly Stochastic Optimisation by Unadjusted Langevin" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/LearningHighdimensionalLatentVariableModelsviaDoublyStochasticOptimisationbyUnadjustedLangevin</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/LearningHighdimensionalLatentVariableModelsviaDoublyStochasticOptimisationbyUnadjustedLangevin.html">&lt;p&gt;Latent variable models are widely used in social and behavioural sciences, such as education, psychology, and political science. In recent years, high-dimensional latent variable models have become increasingly common for analysing large and complex data. Estimating high-dimensional latent variable models using marginal maximum likelihood is computationally demanding due to the complexity of integrals involved. To address this challenge, stochastic optimisation, which combines stochastic approximation and sampling techniques, has been shown to be effective. This method iterates between two steps – (1) sampling the latent variables from their posterior distribution based on the current parameter estimate, and (2) updating the fixed parameters using an approximate stochastic gradient constructed from the latent variable samples. In this paper, we propose a computationally more efficient stochastic optimisation algorithm. This improvement is achieved through the use of a minibatch of observations when sampling latent variables and constructing stochastic gradients, and an unadjusted Langevin sampler that utilises the gradient of the negative complete-data log-likelihood to sample latent variables. Theoretical results are established for the proposed algorithm, showing that the iterative parameter update converges to the marginal maximum likelihood estimate as the number of iterations goes to infinity. Furthermore, the proposed algorithm is shown to scale well to high-dimensional settings through simulation studies and a personality test application with 30,000 respondents, 300 items, and 30 latent dimensions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09311&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Motonori Oka, Yunxiao Chen, Irini Moustaki</name></author><category term="stat.CO" /><summary type="html">Latent variable models are widely used in social and behavioural sciences, such as education, psychology, and political science. In recent years, high-dimensional latent variable models have become increasingly common for analysing large and complex data. Estimating high-dimensional latent variable models using marginal maximum likelihood is computationally demanding due to the complexity of integrals involved. To address this challenge, stochastic optimisation, which combines stochastic approximation and sampling techniques, has been shown to be effective. This method iterates between two steps – (1) sampling the latent variables from their posterior distribution based on the current parameter estimate, and (2) updating the fixed parameters using an approximate stochastic gradient constructed from the latent variable samples. In this paper, we propose a computationally more efficient stochastic optimisation algorithm. This improvement is achieved through the use of a minibatch of observations when sampling latent variables and constructing stochastic gradients, and an unadjusted Langevin sampler that utilises the gradient of the negative complete-data log-likelihood to sample latent variables. Theoretical results are established for the proposed algorithm, showing that the iterative parameter update converges to the marginal maximum likelihood estimate as the number of iterations goes to infinity. Furthermore, the proposed algorithm is shown to scale well to high-dimensional settings through simulation studies and a personality test application with 30,000 respondents, 300 items, and 30 latent dimensions.</summary></entry><entry><title type="html">Measure This, Not That: Optimizing the Cost and Model-Based Information Content of Measurements</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/MeasureThisNotThatOptimizingtheCostandModelBasedInformationContentofMeasurements.html" rel="alternate" type="text/html" title="Measure This, Not That: Optimizing the Cost and Model-Based Information Content of Measurements" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/MeasureThisNotThatOptimizingtheCostandModelBasedInformationContentofMeasurements</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/MeasureThisNotThatOptimizingtheCostandModelBasedInformationContentofMeasurements.html">&lt;p&gt;Model-based design of experiments (MBDoE) is a powerful framework for selecting and calibrating science-based mathematical models from data. This work extends popular MBDoE workflows by proposing a convex mixed integer (non)linear programming (MINLP) problem to optimize the selection of measurements. The solver MindtPy is modified to support calculating the D-optimality objective and its gradient via an external package, \texttt{SciPy}, using the grey-box module in Pyomo. The new approach is demonstrated in two case studies: estimating highly correlated kinetics from a batch reactor and estimating transport parameters in a large-scale rotary packed bed for CO$_2$ capture. Both case studies show how examining the Pareto-optimal trade-offs between information content measured by A- and D-optimality versus measurement budget offers practical guidance for selecting measurements for scientific experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09557&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jialu Wang, Zedong Peng, Ryan Hughes, Debangsu Bhattacharyya, David E. Bernal Neira, Alexander W. Dowling</name></author><category term="stat.AP" /><summary type="html">Model-based design of experiments (MBDoE) is a powerful framework for selecting and calibrating science-based mathematical models from data. This work extends popular MBDoE workflows by proposing a convex mixed integer (non)linear programming (MINLP) problem to optimize the selection of measurements. The solver MindtPy is modified to support calculating the D-optimality objective and its gradient via an external package, \texttt{SciPy}, using the grey-box module in Pyomo. The new approach is demonstrated in two case studies: estimating highly correlated kinetics from a batch reactor and estimating transport parameters in a large-scale rotary packed bed for CO$_2$ capture. Both case studies show how examining the Pareto-optimal trade-offs between information content measured by A- and D-optimality versus measurement budget offers practical guidance for selecting measurements for scientific experiments.</summary></entry><entry><title type="html">Neural Bayes Estimators for Irregular Spatial Data using Graph Neural Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/NeuralBayesEstimatorsforIrregularSpatialDatausingGraphNeuralNetworks.html" rel="alternate" type="text/html" title="Neural Bayes Estimators for Irregular Spatial Data using Graph Neural Networks" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/NeuralBayesEstimatorsforIrregularSpatialDatausingGraphNeuralNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/NeuralBayesEstimatorsforIrregularSpatialDatausingGraphNeuralNetworks.html">&lt;p&gt;Neural Bayes estimators are neural networks that approximate Bayes estimators in a fast and likelihood-free manner. Although they are appealing to use with spatial models, where estimation is often a computational bottleneck, neural Bayes estimators in spatial applications have, to date, been restricted to data collected over a regular grid. These estimators are also currently dependent on a prescribed set of spatial locations, which means that the neural network needs to be re-trained for new data sets; this renders them impractical in many applications and impedes their widespread adoption. In this work, we employ graph neural networks to tackle the important problem of parameter point estimation from data collected over arbitrary spatial locations. In addition to extending neural Bayes estimation to irregular spatial data, our architecture leads to substantial computational benefits, since the estimator can be used with any configuration or number of locations and independent replicates, thus amortising the cost of training for a given spatial model. We also facilitate fast uncertainty quantification by training an accompanying neural Bayes estimator that approximates a set of marginal posterior quantiles. We illustrate our methodology on Gaussian and max-stable processes. Finally, we showcase our methodology on a data set of global sea-surface temperature, where we estimate the parameters of a Gaussian process model in 2161 spatial regions, each containing thousands of irregularly-spaced data points, in just a few minutes with a single graphics processing unit.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.02600&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matthew Sainsbury-Dale, Andrew Zammit-Mangion, Jordan Richards, Raphaël Huser</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Neural Bayes estimators are neural networks that approximate Bayes estimators in a fast and likelihood-free manner. Although they are appealing to use with spatial models, where estimation is often a computational bottleneck, neural Bayes estimators in spatial applications have, to date, been restricted to data collected over a regular grid. These estimators are also currently dependent on a prescribed set of spatial locations, which means that the neural network needs to be re-trained for new data sets; this renders them impractical in many applications and impedes their widespread adoption. In this work, we employ graph neural networks to tackle the important problem of parameter point estimation from data collected over arbitrary spatial locations. In addition to extending neural Bayes estimation to irregular spatial data, our architecture leads to substantial computational benefits, since the estimator can be used with any configuration or number of locations and independent replicates, thus amortising the cost of training for a given spatial model. We also facilitate fast uncertainty quantification by training an accompanying neural Bayes estimator that approximates a set of marginal posterior quantiles. We illustrate our methodology on Gaussian and max-stable processes. Finally, we showcase our methodology on a data set of global sea-surface temperature, where we estimate the parameters of a Gaussian process model in 2161 spatial regions, each containing thousands of irregularly-spaced data points, in just a few minutes with a single graphics processing unit.</summary></entry><entry><title type="html">On the Efficiency of Finely Stratified Experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/OntheEfficiencyofFinelyStratifiedExperiments.html" rel="alternate" type="text/html" title="On the Efficiency of Finely Stratified Experiments" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/OntheEfficiencyofFinelyStratifiedExperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/OntheEfficiencyofFinelyStratifiedExperiments.html">&lt;p&gt;This paper examines finely stratified designs for the efficient estimation of treatment effect parameters in randomized experiments. In such designs, units are divided into groups of fixed size, with a proportion within each group randomly assigned to a binary treatment. We focus on parameters defined using moment conditions constructed from known functions of the observed data. We establish that the naive method of moments estimator under a finely stratified design achieves the same asymptotic variance as that obtained using ex post covariate adjustment in i.i.d. designs, and further that this variance achieves the efficiency bound in a large class of designs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.15181&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuehao Bai, Jizhou Liu, Azeem M. Shaikh, Max Tabord-Meehan</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This paper examines finely stratified designs for the efficient estimation of treatment effect parameters in randomized experiments. In such designs, units are divided into groups of fixed size, with a proportion within each group randomly assigned to a binary treatment. We focus on parameters defined using moment conditions constructed from known functions of the observed data. We establish that the naive method of moments estimator under a finely stratified design achieves the same asymptotic variance as that obtained using ex post covariate adjustment in i.i.d. designs, and further that this variance achieves the efficiency bound in a large class of designs.</summary></entry><entry><title type="html">Quantifying patient and neighborhood risks for stillbirth and preterm birth in Philadelphia with a Bayesian spatial model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/QuantifyingpatientandneighborhoodrisksforstillbirthandpretermbirthinPhiladelphiawithaBayesianspatialmodel.html" rel="alternate" type="text/html" title="Quantifying patient and neighborhood risks for stillbirth and preterm birth in Philadelphia with a Bayesian spatial model" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/QuantifyingpatientandneighborhoodrisksforstillbirthandpretermbirthinPhiladelphiawithaBayesianspatialmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/QuantifyingpatientandneighborhoodrisksforstillbirthandpretermbirthinPhiladelphiawithaBayesianspatialmodel.html">&lt;p&gt;Stillbirth and preterm birth are major public health challenges. Using a Bayesian spatial model, we quantified patient-specific and neighborhood risks of stillbirth and preterm birth in the city of Philadelphia. We linked birth data from electronic health records at Penn Medicine hospitals from 2010 to 2017 with census-tract-level data from the United States Census Bureau. We found that both patient-level characteristics (e.g. self-identified race/ethnicity) and neighborhood-level characteristics (e.g. violent crime) were significantly associated with patients’ risk of stillbirth or preterm birth. Our neighborhood analysis found that higher-risk census tracts had 2.68 times the average risk of stillbirth and 2.01 times the average risk of preterm birth compared to lower-risk census tracts. Higher neighborhood rates of women in poverty or on public assistance were significantly associated with greater neighborhood risk for these outcomes, whereas higher neighborhood rates of college-educated women or women in the labor force were significantly associated with lower risk. Several of these neighborhood associations were missed by the patient-level analysis. These results suggest that neighborhood-level analyses of adverse pregnancy outcomes can reveal nuanced relationships and, thus, should be considered by epidemiologists. Our findings can potentially guide place-based public health interventions to reduce stillbirth and preterm birth rates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.04981&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Cecilia Balocchi, Ray Bai, Jessica Liu, Silvia P. Canelón, Edward I. George, Yong Chen, Mary R. Boland</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Stillbirth and preterm birth are major public health challenges. Using a Bayesian spatial model, we quantified patient-specific and neighborhood risks of stillbirth and preterm birth in the city of Philadelphia. We linked birth data from electronic health records at Penn Medicine hospitals from 2010 to 2017 with census-tract-level data from the United States Census Bureau. We found that both patient-level characteristics (e.g. self-identified race/ethnicity) and neighborhood-level characteristics (e.g. violent crime) were significantly associated with patients’ risk of stillbirth or preterm birth. Our neighborhood analysis found that higher-risk census tracts had 2.68 times the average risk of stillbirth and 2.01 times the average risk of preterm birth compared to lower-risk census tracts. Higher neighborhood rates of women in poverty or on public assistance were significantly associated with greater neighborhood risk for these outcomes, whereas higher neighborhood rates of college-educated women or women in the labor force were significantly associated with lower risk. Several of these neighborhood associations were missed by the patient-level analysis. These results suggest that neighborhood-level analyses of adverse pregnancy outcomes can reveal nuanced relationships and, thus, should be considered by epidemiologists. Our findings can potentially guide place-based public health interventions to reduce stillbirth and preterm birth rates.</summary></entry><entry><title type="html">Randomization Inference: Theory and Applications</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/RandomizationInferenceTheoryandApplications.html" rel="alternate" type="text/html" title="Randomization Inference: Theory and Applications" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/RandomizationInferenceTheoryandApplications</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/RandomizationInferenceTheoryandApplications.html">&lt;p&gt;We review approaches to statistical inference based on randomization. Permutation tests are treated as an important special case. Under a certain group invariance property, referred to as the ``randomization hypothesis,’’ randomization tests achieve exact control of the Type I error rate in finite samples. Although this unequivocal precision is very appealing, the range of problems that satisfy the randomization hypothesis is somewhat limited. We show that randomization tests are often asymptotically, or approximately, valid and efficient in settings that deviate from the conditions required for finite-sample error control. When randomization tests fail to offer even asymptotic Type 1 error control, their asymptotic validity may be restored by constructing an asymptotically pivotal test statistic. Randomization tests can then provide exact error control for tests of highly structured hypotheses with good performance in a wider class of problems. We give a detailed overview of several prominent applications of randomization tests, including two-sample permutation tests, regression, and conformal inference.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09521&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David M. Ritzwoller, Joseph P. Romano, Azeem M. Shaikh</name></author><category term="stat.ME" /><summary type="html">We review approaches to statistical inference based on randomization. Permutation tests are treated as an important special case. Under a certain group invariance property, referred to as the ``randomization hypothesis,’’ randomization tests achieve exact control of the Type I error rate in finite samples. Although this unequivocal precision is very appealing, the range of problems that satisfy the randomization hypothesis is somewhat limited. We show that randomization tests are often asymptotically, or approximately, valid and efficient in settings that deviate from the conditions required for finite-sample error control. When randomization tests fail to offer even asymptotic Type 1 error control, their asymptotic validity may be restored by constructing an asymptotically pivotal test statistic. Randomization tests can then provide exact error control for tests of highly structured hypotheses with good performance in a wider class of problems. We give a detailed overview of several prominent applications of randomization tests, including two-sample permutation tests, regression, and conformal inference.</summary></entry><entry><title type="html">Ridge Regression for Paired Comparisons: A Tractable New Approach, with Application to Premier League Football</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/RidgeRegressionforPairedComparisonsATractableNewApproachwithApplicationtoPremierLeagueFootball.html" rel="alternate" type="text/html" title="Ridge Regression for Paired Comparisons: A Tractable New Approach, with Application to Premier League Football" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/RidgeRegressionforPairedComparisonsATractableNewApproachwithApplicationtoPremierLeagueFootball</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/RidgeRegressionforPairedComparisonsATractableNewApproachwithApplicationtoPremierLeagueFootball.html">&lt;p&gt;Paired comparison models, such as Bradley-Terry and Thurstone-Mosteller, are commonly used to estimate relative strengths of pairwise compared items in tournament-style datasets. With predictive performance as primary criterion, we discuss estimation of paired comparison models with a ridge penalty. A new approach is derived which combines empirical Bayes and composite likelihoods without any need to re-fit the model, as a convenient alternative to cross-validation of the ridge tuning parameter. Simulation studies, together with application to 28 seasons of English Premier League football, demonstrate much better predictive accuracy of the new approach relative to ordinary maximum likelihood. While the application of a standard bias-reducing penalty was found to improve appreciably the performance of maximum likelihood, the ridge penalty with tuning as developed here yields greater accuracy still.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09597&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Cristiano Varin, David Firth</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Paired comparison models, such as Bradley-Terry and Thurstone-Mosteller, are commonly used to estimate relative strengths of pairwise compared items in tournament-style datasets. With predictive performance as primary criterion, we discuss estimation of paired comparison models with a ridge penalty. A new approach is derived which combines empirical Bayes and composite likelihoods without any need to re-fit the model, as a convenient alternative to cross-validation of the ridge tuning parameter. Simulation studies, together with application to 28 seasons of English Premier League football, demonstrate much better predictive accuracy of the new approach relative to ordinary maximum likelihood. While the application of a standard bias-reducing penalty was found to improve appreciably the performance of maximum likelihood, the ridge penalty with tuning as developed here yields greater accuracy still.</summary></entry><entry><title type="html">Sparse Graphical Linear Dynamical Systems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/SparseGraphicalLinearDynamicalSystems.html" rel="alternate" type="text/html" title="Sparse Graphical Linear Dynamical Systems" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/SparseGraphicalLinearDynamicalSystems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/SparseGraphicalLinearDynamicalSystems.html">&lt;p&gt;Time-series datasets are central in machine learning with applications in numerous fields of science and engineering, such as biomedicine, Earth observation, and network analysis. Extensive research exists on state-space models (SSMs), which are powerful mathematical tools that allow for probabilistic and interpretable learning on time series. Learning the model parameters in SSMs is arguably one of the most complicated tasks, and the inclusion of prior knowledge is known to both ease the interpretation but also to complicate the inferential tasks. Very recent works have attempted to incorporate a graphical perspective on some of those model parameters, but they present notable limitations that this work addresses. More generally, existing graphical modeling tools are designed to incorporate either static information, focusing on statistical dependencies among independent random variables (e.g., graphical Lasso approach), or dynamic information, emphasizing causal relationships among time series samples (e.g., graphical Granger approaches). However, there are no joint approaches combining static and dynamic graphical modeling within the context of SSMs. This work proposes a novel approach to fill this gap by introducing a joint graphical modeling framework that bridges the graphical Lasso model and a causal-based graphical approach for the linear-Gaussian SSM. We present DGLASSO (Dynamic Graphical Lasso), a new inference method within this framework that implements an efficient block alternating majorization-minimization algorithm. The algorithm’s convergence is established by departing from modern tools from nonlinear analysis. Experimental validation on various synthetic data showcases the effectiveness of the proposed model and inference algorithm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.03210&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emilie Chouzenoux, Victor Elvira</name></author><category term="stat.CO" /><summary type="html">Time-series datasets are central in machine learning with applications in numerous fields of science and engineering, such as biomedicine, Earth observation, and network analysis. Extensive research exists on state-space models (SSMs), which are powerful mathematical tools that allow for probabilistic and interpretable learning on time series. Learning the model parameters in SSMs is arguably one of the most complicated tasks, and the inclusion of prior knowledge is known to both ease the interpretation but also to complicate the inferential tasks. Very recent works have attempted to incorporate a graphical perspective on some of those model parameters, but they present notable limitations that this work addresses. More generally, existing graphical modeling tools are designed to incorporate either static information, focusing on statistical dependencies among independent random variables (e.g., graphical Lasso approach), or dynamic information, emphasizing causal relationships among time series samples (e.g., graphical Granger approaches). However, there are no joint approaches combining static and dynamic graphical modeling within the context of SSMs. This work proposes a novel approach to fill this gap by introducing a joint graphical modeling framework that bridges the graphical Lasso model and a causal-based graphical approach for the linear-Gaussian SSM. We present DGLASSO (Dynamic Graphical Lasso), a new inference method within this framework that implements an efficient block alternating majorization-minimization algorithm. The algorithm’s convergence is established by departing from modern tools from nonlinear analysis. Experimental validation on various synthetic data showcases the effectiveness of the proposed model and inference algorithm.</summary></entry><entry><title type="html">Sparse Recovery With Multiple Data Streams: A Sequential Adaptive Testing Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/SparseRecoveryWithMultipleDataStreamsASequentialAdaptiveTestingApproach.html" rel="alternate" type="text/html" title="Sparse Recovery With Multiple Data Streams: A Sequential Adaptive Testing Approach" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/SparseRecoveryWithMultipleDataStreamsASequentialAdaptiveTestingApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/SparseRecoveryWithMultipleDataStreamsASequentialAdaptiveTestingApproach.html">&lt;p&gt;Multistage design has been used in a wide range of scientific fields. By allocating sensing resources adaptively, one can effectively eliminate null locations and localize signals with a smaller study budget. We formulate a decision-theoretic framework for simultaneous multi-stage adaptive testing and study how to minimize the total number of measurements while meeting pre-specified constraints on both the false positive rate (FPR) and missed discovery rate (MDR). The new procedure, which effectively pools information across individual tests using a simultaneous multistage adaptive ranking and thresholding (SMART) approach, controls the error rates and leads to great savings in total study costs. Numerical studies confirm the effectiveness of SMART. The SMART procedure is illustrated through the analysis of large-scale A/B tests, high-throughput screening and image analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1707.07215&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Weinan Wang, Bowen Gang, Wenguang Sun</name></author><category term="stat.ME" /><summary type="html">Multistage design has been used in a wide range of scientific fields. By allocating sensing resources adaptively, one can effectively eliminate null locations and localize signals with a smaller study budget. We formulate a decision-theoretic framework for simultaneous multi-stage adaptive testing and study how to minimize the total number of measurements while meeting pre-specified constraints on both the false positive rate (FPR) and missed discovery rate (MDR). The new procedure, which effectively pools information across individual tests using a simultaneous multistage adaptive ranking and thresholding (SMART) approach, controls the error rates and leads to great savings in total study costs. Numerical studies confirm the effectiveness of SMART. The SMART procedure is illustrated through the analysis of large-scale A/B tests, high-throughput screening and image analysis.</summary></entry><entry><title type="html">Split-Apply-Combine with Dynamic Grouping</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/SplitApplyCombinewithDynamicGrouping.html" rel="alternate" type="text/html" title="Split-Apply-Combine with Dynamic Grouping" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/SplitApplyCombinewithDynamicGrouping</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/SplitApplyCombinewithDynamicGrouping.html">&lt;p&gt;Partitioning a data set by one or more of its attributes and computing an aggregate for each part is one of the most common operations in data analyses. There are use cases where the partitioning is determined dynamically by collapsing smaller subsets into larger ones, to ensure sufficient support for the computed aggregate. These use cases are not supported by software implementing split-apply-combine types of operations. This paper presents the \texttt{R} package \texttt{accumulate} that offers convenient interfaces for defining grouped aggregation where the grouping itself is dynamically determined, based on user-defined conditions on subsets, and a user-defined subset collapsing scheme. The formal underlying algorithm is described and analyzed as well.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09887&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mark P. J. van der Loo</name></author><category term="stat.CO," /><category term="stat.AP" /><summary type="html">Partitioning a data set by one or more of its attributes and computing an aggregate for each part is one of the most common operations in data analyses. There are use cases where the partitioning is determined dynamically by collapsing smaller subsets into larger ones, to ensure sufficient support for the computed aggregate. These use cases are not supported by software implementing split-apply-combine types of operations. This paper presents the \texttt{R} package \texttt{accumulate} that offers convenient interfaces for defining grouped aggregation where the grouping itself is dynamically determined, based on user-defined conditions on subsets, and a user-defined subset collapsing scheme. The formal underlying algorithm is described and analyzed as well.</summary></entry><entry><title type="html">Time Series Forecasting with Many Predictors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/TimeSeriesForecastingwithManyPredictors.html" rel="alternate" type="text/html" title="Time Series Forecasting with Many Predictors" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/TimeSeriesForecastingwithManyPredictors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/TimeSeriesForecastingwithManyPredictors.html">&lt;p&gt;We propose a novel approach for time series forecasting with many predictors, referred to as the GO-sdPCA, in this paper. The approach employs a variable selection method known as the group orthogonal greedy algorithm and the high-dimensional Akaike information criterion to mitigate the impact of irrelevant predictors. Moreover, a novel technique, called peeling, is used to boost the variable selection procedure so that many factor-relevant predictors can be included in prediction. Finally, the supervised dynamic principal component analysis (sdPCA) method is adopted to account for the dynamic information in factor recovery. In simulation studies, we found that the proposed method adapts well to unknown degrees of sparsity and factor strength, which results in good performance even when the number of relevant predictors is large compared to the sample size. Applying to economic and environmental studies, the proposed method consistently performs well compared to some commonly used benchmarks in one-step-ahead out-sample forecasts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.09625&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shuo-Chieh Huang, Ruey S. Tsay</name></author><category term="stat.ME" /><summary type="html">We propose a novel approach for time series forecasting with many predictors, referred to as the GO-sdPCA, in this paper. The approach employs a variable selection method known as the group orthogonal greedy algorithm and the high-dimensional Akaike information criterion to mitigate the impact of irrelevant predictors. Moreover, a novel technique, called peeling, is used to boost the variable selection procedure so that many factor-relevant predictors can be included in prediction. Finally, the supervised dynamic principal component analysis (sdPCA) method is adopted to account for the dynamic information in factor recovery. In simulation studies, we found that the proposed method adapts well to unknown degrees of sparsity and factor strength, which results in good performance even when the number of relevant predictors is large compared to the sample size. Applying to economic and environmental studies, the proposed method consistently performs well compared to some commonly used benchmarks in one-step-ahead out-sample forecasts.</summary></entry><entry><title type="html">What To Do (and Not to Do) with Causal Panel Analysis under Parallel Trends: Lessons from A Large Reanalysis Study</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/WhatToDoandNottoDowithCausalPanelAnalysisunderParallelTrendsLessonsfromALargeReanalysisStudy.html" rel="alternate" type="text/html" title="What To Do (and Not to Do) with Causal Panel Analysis under Parallel Trends: Lessons from A Large Reanalysis Study" /><published>2024-06-17T00:00:00+00:00</published><updated>2024-06-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/WhatToDoandNottoDowithCausalPanelAnalysisunderParallelTrendsLessonsfromALargeReanalysisStudy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/17/WhatToDoandNottoDowithCausalPanelAnalysisunderParallelTrendsLessonsfromALargeReanalysisStudy.html">&lt;p&gt;Two-way fixed effects (TWFE) models are ubiquitous in causal panel analysis in political science. However, recent methodological discussions challenge their validity in the presence of heterogeneous treatment effects (HTE) and violations of the parallel trends assumption (PTA). This burgeoning literature has introduced multiple estimators and diagnostics, leading to confusion among empirical researchers on two fronts: the reliability of existing results based on TWFE models and the current best practices. To address these concerns, we examined, replicated, and reanalyzed 37 articles from three leading political science journals that employed observational panel data with binary treatments. Using six newly introduced HTE-robust estimators, along with diagnostics tests and uncertainty measures that are robust to PTA violations, we find that only a small minority of studies are highly robust. Although HTE-robust estimates tend to be broadly consistent with TWFE estimates, discrepancies in point estimates, increased measures of uncertainty, and potential PTA violations call into question many results that were already on the margins of statistical significance. We offer recommendations for improving practice in empirical research based on these findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.15983&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Albert Chiu, Xingchen Lan, Ziyi Liu, Yiqing Xu</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Two-way fixed effects (TWFE) models are ubiquitous in causal panel analysis in political science. However, recent methodological discussions challenge their validity in the presence of heterogeneous treatment effects (HTE) and violations of the parallel trends assumption (PTA). This burgeoning literature has introduced multiple estimators and diagnostics, leading to confusion among empirical researchers on two fronts: the reliability of existing results based on TWFE models and the current best practices. To address these concerns, we examined, replicated, and reanalyzed 37 articles from three leading political science journals that employed observational panel data with binary treatments. Using six newly introduced HTE-robust estimators, along with diagnostics tests and uncertainty measures that are robust to PTA violations, we find that only a small minority of studies are highly robust. Although HTE-robust estimates tend to be broadly consistent with TWFE estimates, discrepancies in point estimates, increased measures of uncertainty, and potential PTA violations call into question many results that were already on the margins of statistical significance. We offer recommendations for improving practice in empirical research based on these findings.</summary></entry></feed>