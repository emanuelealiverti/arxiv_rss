<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-07T07:20:16+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html"></title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/2024-05-07-Investigatingtheheterogeneityofstudytwins.html" rel="alternate" type="text/html" title="" /><published>2024-05-07T07:20:16+00:00</published><updated>2024-05-07T07:20:16+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/2024-05-07-Investigatingtheheterogeneityofstudytwins</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/2024-05-07-Investigatingtheheterogeneityofstudytwins.html">&lt;p&gt;Meta-analyses are commonly performed based on random-effects models, while in certain cases one might also argue in favour of a common-effect model. One such case may be given by the example of two “study twins” that are performed according to a common (or at least very similar) protocol. Here we investigate the particular case of meta-analysis of a pair of studies, e.g. summarizing the results of two confirmatory clinical trials in phase III of a clinical development programme. Thereby, we focus on the question of to what extent homogeneity or heterogeneity may be discernible, and include an empirical investigation of published (“twin”) pairs of studies. A pair of estimates from two studies only provides very little evidence on homogeneity or heterogeneity of effects, and ad-hoc decision criteria may often be misleading.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.09884&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Aliverti</name></author></entry><entry><title type="html"></title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/2024-05-07-RejoinderonMarkedspatialpointprocessescurrentstateandextensionstopointprocessesonlinearnetworks.html" rel="alternate" type="text/html" title="" /><published>2024-05-07T07:20:16+00:00</published><updated>2024-05-07T07:20:16+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/2024-05-07-RejoinderonMarkedspatialpointprocessescurrentstateandextensionstopointprocessesonlinearnetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/2024-05-07-RejoinderonMarkedspatialpointprocessescurrentstateandextensionstopointprocessesonlinearnetworks.html">&lt;p&gt;We are grateful to all discussants for their invaluable comments, suggestions, questions, and contributions to our article. We have attentively reviewed all discussions with keen interest. In this rejoinder, our objective is to address and engage with all points raised by the discussants in a comprehensive and considerate manner. Consistently, we identify the discussants, in alphabetical order, as follows: CJK for Cronie, Jansson, and Konstantinou, DS for Stoyan, GP for Grabarnik and Pommerening, MRS for Myllym&quot;aki, Rajala, and S&quot;arkk&quot;a, and MCvL for van Lieshout throughout this rejoinder.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02343&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Aliverti</name></author></entry><entry><title type="html">A Change-Point Approach to Estimating the Proportion of False Null Hypotheses in Multiple Testing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/AChangePointApproachtoEstimatingtheProportionofFalseNullHypothesesinMultipleTesting.html" rel="alternate" type="text/html" title="A Change-Point Approach to Estimating the Proportion of False Null Hypotheses in Multiple Testing" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/AChangePointApproachtoEstimatingtheProportionofFalseNullHypothesesinMultipleTesting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/AChangePointApproachtoEstimatingtheProportionofFalseNullHypothesesinMultipleTesting.html">&lt;p&gt;For estimating the proportion of false null hypotheses in multiple testing, a family of estimators by Storey (2002) is widely used in the applied and statistical literature, with many methods suggested for selecting the parameter $\lambda$. Inspired by change-point concepts, our new approach to the latter problem first approximates the $p$-value plot with a piecewise linear function with a single change-point and then selects the $p$-value at the change-point location as $\lambda$. Simulations show that our method has among the smallest RMSE across various settings, and we extend it to address the estimation in cases of superuniform $p$-values. We provide asymptotic theory for our estimator, relying on the theory of quantile processes. Additionally, we propose an application in the change-point literature and illustrate it using high-dimensional CNV data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.10017&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anica Kostic, Piotr Fryzlewicz</name></author><category term="stat.ME" /><summary type="html">For estimating the proportion of false null hypotheses in multiple testing, a family of estimators by Storey (2002) is widely used in the applied and statistical literature, with many methods suggested for selecting the parameter $\lambda$. Inspired by change-point concepts, our new approach to the latter problem first approximates the $p$-value plot with a piecewise linear function with a single change-point and then selects the $p$-value at the change-point location as $\lambda$. Simulations show that our method has among the smallest RMSE across various settings, and we extend it to address the estimation in cases of superuniform $p$-values. We provide asymptotic theory for our estimator, relying on the theory of quantile processes. Additionally, we propose an application in the change-point literature and illustrate it using high-dimensional CNV data.</summary></entry><entry><title type="html">A Comprehensive Approach to Carbon Dioxide Emission Analysis in High Human Development Index Countries using Statistical and Machine Learning Techniques</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/AComprehensiveApproachtoCarbonDioxideEmissionAnalysisinHighHumanDevelopmentIndexCountriesusingStatisticalandMachineLearningTechniques.html" rel="alternate" type="text/html" title="A Comprehensive Approach to Carbon Dioxide Emission Analysis in High Human Development Index Countries using Statistical and Machine Learning Techniques" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/AComprehensiveApproachtoCarbonDioxideEmissionAnalysisinHighHumanDevelopmentIndexCountriesusingStatisticalandMachineLearningTechniques</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/AComprehensiveApproachtoCarbonDioxideEmissionAnalysisinHighHumanDevelopmentIndexCountriesusingStatisticalandMachineLearningTechniques.html">&lt;p&gt;Reducing Carbon dioxide (CO2) emission is vital at both global and national levels, given their significant role in exacerbating climate change. CO2 emission, stemming from a variety of industrial and economic activities, are major contributors to the greenhouse effect and global warming, posing substantial obstacles in addressing climate issues. It’s imperative to forecast CO2 emission trends and classify countries based on their emission patterns to effectively mitigate worldwide carbon emission. This paper presents an in-depth comparative study on the determinants of CO2 emission in twenty countries with high Human Development Index (HDI), exploring factors related to economy, environment, energy use, and renewable resources over a span of 25 years. The study unfolds in two distinct phases: initially, statistical techniques such as Ordinary Least Squares (OLS), fixed effects, and random effects models are applied to pinpoint significant determinants of CO2 emission. Following this, the study leverages supervised and unsupervised machine learning (ML) methods to further scrutinize and understand the factors influencing CO2 emission. Seasonal AutoRegressive Integrated Moving Average with eXogenous variables (SARIMAX), a supervised ML model, is first used to predict emission trends from historical data, offering practical insights for policy formulation. Subsequently, Dynamic Time Warping (DTW), an unsupervised learning approach, is used to group countries by similar emission patterns. The dual-phase approach utilized in this study significantly improves the accuracy of CO2 emission predictions while also providing a deeper insight into global emission trends. By adopting this thorough analytical framework, nations can develop more focused and effective carbon reduction policies, playing a vital role in the global initiative to combat climate change.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02340&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hamed Khosravi, Ahmed Shoyeb Raihan, Farzana Islam, Ashish Nimbarte, Imtiaz Ahmed</name></author><category term="stat.AP" /><summary type="html">Reducing Carbon dioxide (CO2) emission is vital at both global and national levels, given their significant role in exacerbating climate change. CO2 emission, stemming from a variety of industrial and economic activities, are major contributors to the greenhouse effect and global warming, posing substantial obstacles in addressing climate issues. It’s imperative to forecast CO2 emission trends and classify countries based on their emission patterns to effectively mitigate worldwide carbon emission. This paper presents an in-depth comparative study on the determinants of CO2 emission in twenty countries with high Human Development Index (HDI), exploring factors related to economy, environment, energy use, and renewable resources over a span of 25 years. The study unfolds in two distinct phases: initially, statistical techniques such as Ordinary Least Squares (OLS), fixed effects, and random effects models are applied to pinpoint significant determinants of CO2 emission. Following this, the study leverages supervised and unsupervised machine learning (ML) methods to further scrutinize and understand the factors influencing CO2 emission. Seasonal AutoRegressive Integrated Moving Average with eXogenous variables (SARIMAX), a supervised ML model, is first used to predict emission trends from historical data, offering practical insights for policy formulation. Subsequently, Dynamic Time Warping (DTW), an unsupervised learning approach, is used to group countries by similar emission patterns. The dual-phase approach utilized in this study significantly improves the accuracy of CO2 emission predictions while also providing a deeper insight into global emission trends. By adopting this thorough analytical framework, nations can develop more focused and effective carbon reduction policies, playing a vital role in the global initiative to combat climate change.</summary></entry><entry><title type="html">A comparison of regression models for static and dynamic prediction of a prognostic outcome during admission in electronic health care records</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Acomparisonofregressionmodelsforstaticanddynamicpredictionofaprognosticoutcomeduringadmissioninelectronichealthcarerecords.html" rel="alternate" type="text/html" title="A comparison of regression models for static and dynamic prediction of a prognostic outcome during admission in electronic health care records" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Acomparisonofregressionmodelsforstaticanddynamicpredictionofaprognosticoutcomeduringadmissioninelectronichealthcarerecords</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Acomparisonofregressionmodelsforstaticanddynamicpredictionofaprognosticoutcomeduringadmissioninelectronichealthcarerecords.html">&lt;p&gt;Objective Hospitals register information in the electronic health records (EHR) continuously until discharge or death. As such, there is no censoring for in-hospital outcomes. We aimed to compare different dynamic regression modeling approaches to predict central line-associated bloodstream infections (CLABSI) in EHR while accounting for competing events precluding CLABSI. Materials and Methods We analyzed data from 30,862 catheter episodes at University Hospitals Leuven from 2012 and 2013 to predict 7-day risk of CLABSI. Competing events are discharge and death. Static models at catheter onset included logistic, multinomial logistic, Cox, cause-specific hazard, and Fine-Gray regression. Dynamic models updated predictions daily up to 30 days after catheter onset (i.e. landmarks 0 to 30 days), and included landmark supermodel extensions of the static models, separate Fine-Gray models per landmark time, and regularized multi-task learning (RMTL). Model performance was assessed using 100 random 2:1 train-test splits. Results The Cox model performed worst of all static models in terms of area under the receiver operating characteristic curve (AUC) and calibration. Dynamic landmark supermodels reached peak AUCs between 0.741-0.747 at landmark 5. The Cox landmark supermodel had the worst AUCs (&amp;lt;=0.731) and calibration up to landmark 7. Separate Fine-Gray models per landmark performed worst for later landmarks, when the number of patients at risk was low. Discussion and Conclusion Categorical and time-to-event approaches had similar performance in the static and dynamic settings, except Cox models. Ignoring competing risks caused problems for risk prediction in the time-to-event framework (Cox), but not in the categorical framework (logistic regression).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01986&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shan Gao, Elena Albu, Hein Putter, Pieter Stijnen, Frank Rademakers, Veerle Cossey, Yves Debaveye, Christel Janssens, Ben Van Calster, Laure Wynants</name></author><category term="stat.AP" /><summary type="html">Objective Hospitals register information in the electronic health records (EHR) continuously until discharge or death. As such, there is no censoring for in-hospital outcomes. We aimed to compare different dynamic regression modeling approaches to predict central line-associated bloodstream infections (CLABSI) in EHR while accounting for competing events precluding CLABSI. Materials and Methods We analyzed data from 30,862 catheter episodes at University Hospitals Leuven from 2012 and 2013 to predict 7-day risk of CLABSI. Competing events are discharge and death. Static models at catheter onset included logistic, multinomial logistic, Cox, cause-specific hazard, and Fine-Gray regression. Dynamic models updated predictions daily up to 30 days after catheter onset (i.e. landmarks 0 to 30 days), and included landmark supermodel extensions of the static models, separate Fine-Gray models per landmark time, and regularized multi-task learning (RMTL). Model performance was assessed using 100 random 2:1 train-test splits. Results The Cox model performed worst of all static models in terms of area under the receiver operating characteristic curve (AUC) and calibration. Dynamic landmark supermodels reached peak AUCs between 0.741-0.747 at landmark 5. The Cox landmark supermodel had the worst AUCs (&amp;lt;=0.731) and calibration up to landmark 7. Separate Fine-Gray models per landmark performed worst for later landmarks, when the number of patients at risk was low. Discussion and Conclusion Categorical and time-to-event approaches had similar performance in the static and dynamic settings, except Cox models. Ignoring competing risks caused problems for risk prediction in the time-to-event framework (Cox), but not in the categorical framework (logistic regression).</summary></entry><entry><title type="html">Adolescent sports participation and health in early adulthood: An observational study</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/AdolescentsportsparticipationandhealthinearlyadulthoodAnobservationalstudy.html" rel="alternate" type="text/html" title="Adolescent sports participation and health in early adulthood: An observational study" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/AdolescentsportsparticipationandhealthinearlyadulthoodAnobservationalstudy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/AdolescentsportsparticipationandhealthinearlyadulthoodAnobservationalstudy.html">&lt;p&gt;We study the impact of teenage sports participation on early-adulthood health using longitudinal data from the National Study of Youth and Religion. We focus on two primary outcomes measured at ages 23–28 – self-rated health and total score on the PHQ9 Patient Depression Questionnaire – and control for several potential confounders related to demographics and family socioeconomic status. To probe the possibility that certain types of sports participation may have larger effects on health than others, we conduct a matched observational study at each level within a hierarchy of exposures. Our hierarchy ranges from broadly defined exposures (e.g., participation in any organized after-school activity) to narrow (e.g., participation in collision sports). We deployed an ordered testing approach that exploits the hierarchical relationships between our exposure definitions to perform our analyses while maintaining a fixed family-wise error rate. Compared to teenagers who did not participate in any after-school activities, those who participated in sports had statistically significantly better self-rated and mental health outcomes in early adulthood.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03538&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ajinkya H. Kokandakar, Yuzhou Lin, Steven Jin, Jordan Weiss, Amanda R. Rabinowitz, Reuben A. Buford May, Dylan Small, Sameer K. Deshpande</name></author><category term="stat.AP" /><summary type="html">We study the impact of teenage sports participation on early-adulthood health using longitudinal data from the National Study of Youth and Religion. We focus on two primary outcomes measured at ages 23–28 – self-rated health and total score on the PHQ9 Patient Depression Questionnaire – and control for several potential confounders related to demographics and family socioeconomic status. To probe the possibility that certain types of sports participation may have larger effects on health than others, we conduct a matched observational study at each level within a hierarchy of exposures. Our hierarchy ranges from broadly defined exposures (e.g., participation in any organized after-school activity) to narrow (e.g., participation in collision sports). We deployed an ordered testing approach that exploits the hierarchical relationships between our exposure definitions to perform our analyses while maintaining a fixed family-wise error rate. Compared to teenagers who did not participate in any after-school activities, those who participated in sports had statistically significantly better self-rated and mental health outcomes in early adulthood.</summary></entry><entry><title type="html">An algorithm for forensic toolmark comparisons</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Analgorithmforforensictoolmarkcomparisons.html" rel="alternate" type="text/html" title="An algorithm for forensic toolmark comparisons" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Analgorithmforforensictoolmarkcomparisons</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Analgorithmforforensictoolmarkcomparisons.html">&lt;p&gt;Forensic toolmark analysis traditionally relies on subjective human judgment, leading to inconsistencies and inaccuracies. The multitude of variables, including angles and directions of mark generation, further complicates comparisons. To address this, we introduce a novel approach leveraging 3D data capturing toolmarks from various angles and directions. Through algorithmic training, we objectively compare toolmark signals, revealing clustering by tool rather than angle or direction. Our method utilizes similarity matrices and density plots to establish thresholds for classification, enabling the derivation of likelihood ratios for new mark pairs. With a cross-validated sensitivity of 98% and specificity of 96%, our approach enhances the reliability of toolmark analysis. While its applicability to diverse tools and factors warrants further exploration, this empirically trained, open-source solution offers forensic examiners a standardized means to objectively compare toolmarks, potentially curbing miscarriages of justice in the legal system.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.00032&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maria Cuellar, Sheng Gao, Heike Hofmann</name></author><category term="stat.AP" /><summary type="html">Forensic toolmark analysis traditionally relies on subjective human judgment, leading to inconsistencies and inaccuracies. The multitude of variables, including angles and directions of mark generation, further complicates comparisons. To address this, we introduce a novel approach leveraging 3D data capturing toolmarks from various angles and directions. Through algorithmic training, we objectively compare toolmark signals, revealing clustering by tool rather than angle or direction. Our method utilizes similarity matrices and density plots to establish thresholds for classification, enabling the derivation of likelihood ratios for new mark pairs. With a cross-validated sensitivity of 98% and specificity of 96%, our approach enhances the reliability of toolmark analysis. While its applicability to diverse tools and factors warrants further exploration, this empirically trained, open-source solution offers forensic examiners a standardized means to objectively compare toolmarks, potentially curbing miscarriages of justice in the legal system.</summary></entry><entry><title type="html">Bayesian Functional Graphical Models with Change-Point Detection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/BayesianFunctionalGraphicalModelswithChangePointDetection.html" rel="alternate" type="text/html" title="Bayesian Functional Graphical Models with Change-Point Detection" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/BayesianFunctionalGraphicalModelswithChangePointDetection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/BayesianFunctionalGraphicalModelswithChangePointDetection.html">&lt;p&gt;Functional data analysis, which models data as realizations of random functions over a continuum, has emerged as a useful tool for time series data. Often, the goal is to infer the dynamic connections (or time-varying conditional dependencies) among multiple functions or time series. For this task, we propose a dynamic and Bayesian functional graphical model. Our modeling approach prioritizes the careful definition of an appropriate graph to identify both time-invariant and time-varying connectivity patterns. We introduce a novel block-structured sparsity prior paired with a finite basis expansion, which together yield effective shrinkage and graph selection with efficient computations via a Gibbs sampling algorithm. Crucially, the model includes (one or more) graph changepoints, which are learned jointly with all model parameters and incorporate graph dynamics. Simulation studies demonstrate excellent graph selection capabilities, with significant improvements over competing methods. We apply the proposed approach to study of dynamic connectivity patterns of sea surface temperatures in the Pacific Ocean and discovers meaningful edges.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03041&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chunshan Liu, Daniel R. Kowal, James Doss-Gollin, Marina Vannucci</name></author><category term="stat.ME" /><summary type="html">Functional data analysis, which models data as realizations of random functions over a continuum, has emerged as a useful tool for time series data. Often, the goal is to infer the dynamic connections (or time-varying conditional dependencies) among multiple functions or time series. For this task, we propose a dynamic and Bayesian functional graphical model. Our modeling approach prioritizes the careful definition of an appropriate graph to identify both time-invariant and time-varying connectivity patterns. We introduce a novel block-structured sparsity prior paired with a finite basis expansion, which together yield effective shrinkage and graph selection with efficient computations via a Gibbs sampling algorithm. Crucially, the model includes (one or more) graph changepoints, which are learned jointly with all model parameters and incorporate graph dynamics. Simulation studies demonstrate excellent graph selection capabilities, with significant improvements over competing methods. We apply the proposed approach to study of dynamic connectivity patterns of sea surface temperatures in the Pacific Ocean and discovers meaningful edges.</summary></entry><entry><title type="html">Bayesian Inference for Estimating Heat Sources through Temperature Assimilation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/BayesianInferenceforEstimatingHeatSourcesthroughTemperatureAssimilation.html" rel="alternate" type="text/html" title="Bayesian Inference for Estimating Heat Sources through Temperature Assimilation" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/BayesianInferenceforEstimatingHeatSourcesthroughTemperatureAssimilation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/BayesianInferenceforEstimatingHeatSourcesthroughTemperatureAssimilation.html">&lt;p&gt;This paper introduces a Bayesian inference framework for two-dimensional steady-state heat conduction, focusing on the estimation of unknown distributed heat sources in a thermally-conducting medium with uniform conductivity. The goal is to infer heater locations, strengths, and shapes using temperature assimilation in the Euclidean space, employing a Fourier series to represent each heater’s shape. The Markov Chain Monte Carlo (MCMC) method, incorporating the random-walk Metropolis-Hasting algorithm and parallel tempering, is utilized for posterior distribution exploration in both unbounded and wall-bounded domains. Strong correlations between heat strength and heater area prompt caution against simultaneously estimating these two quantities. It is found that multiple solutions arise in cases where the number of temperature sensors is less than the number of unknown states. Moreover, smaller heaters introduce greater uncertainty in estimated strength. The diffusive nature of heat conduction smooths out any deformations in the temperature contours, especially in the presence of multiple heaters positioned near each other, impacting convergence. In wall-bounded domains with Neumann boundary conditions, the inference of heater parameters tends to be more accurate than in unbounded domains.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02319&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hanieh Mousavi, Jeff D. Eldredge</name></author><category term="stat.AP," /><category term="stat.TH" /><summary type="html">This paper introduces a Bayesian inference framework for two-dimensional steady-state heat conduction, focusing on the estimation of unknown distributed heat sources in a thermally-conducting medium with uniform conductivity. The goal is to infer heater locations, strengths, and shapes using temperature assimilation in the Euclidean space, employing a Fourier series to represent each heater’s shape. The Markov Chain Monte Carlo (MCMC) method, incorporating the random-walk Metropolis-Hasting algorithm and parallel tempering, is utilized for posterior distribution exploration in both unbounded and wall-bounded domains. Strong correlations between heat strength and heater area prompt caution against simultaneously estimating these two quantities. It is found that multiple solutions arise in cases where the number of temperature sensors is less than the number of unknown states. Moreover, smaller heaters introduce greater uncertainty in estimated strength. The diffusive nature of heat conduction smooths out any deformations in the temperature contours, especially in the presence of multiple heaters positioned near each other, impacting convergence. In wall-bounded domains with Neumann boundary conditions, the inference of heater parameters tends to be more accurate than in unbounded domains.</summary></entry><entry><title type="html">CVXSADes: a stochastic algorithm for constructing optimal exact regression designs with single or multiple objectives</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/CVXSADesastochasticalgorithmforconstructingoptimalexactregressiondesignswithsingleormultipleobjectives.html" rel="alternate" type="text/html" title="CVXSADes: a stochastic algorithm for constructing optimal exact regression designs with single or multiple objectives" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/CVXSADesastochasticalgorithmforconstructingoptimalexactregressiondesignswithsingleormultipleobjectives</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/CVXSADesastochasticalgorithmforconstructingoptimalexactregressiondesignswithsingleormultipleobjectives.html">&lt;p&gt;We propose an algorithm to construct optimal exact designs (EDs). Most of the work in the optimal regression design literature focuses on the approximate design (AD) paradigm due to its desired properties, including the optimality verification conditions derived by Kiefer (1959, 1974). ADs may have unbalanced weights, and practitioners may have difficulty implementing them with a designated run size $n$. Some EDs are constructed using rounding methods to get an integer number of runs at each support point of an AD, but this approach may not yield optimal results. To construct EDs, one may need to perform new combinatorial constructions for each $n$, and there is no unified approach to construct them. Therefore, we develop a systematic way to construct EDs for any given $n$. Our method can transform ADs into EDs while retaining high statistical efficiency in two steps. The first step involves constructing an AD by utilizing the convex nature of many design criteria. The second step employs a simulated annealing algorithm to search for the ED stochastically. Through several applications, we demonstrate the utility of our method for various design problems. Additionally, we show that the design efficiency approaches unity as the number of design points increases.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02983&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chi-Kuang Yeh, Julie Zhou</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">We propose an algorithm to construct optimal exact designs (EDs). Most of the work in the optimal regression design literature focuses on the approximate design (AD) paradigm due to its desired properties, including the optimality verification conditions derived by Kiefer (1959, 1974). ADs may have unbalanced weights, and practitioners may have difficulty implementing them with a designated run size $n$. Some EDs are constructed using rounding methods to get an integer number of runs at each support point of an AD, but this approach may not yield optimal results. To construct EDs, one may need to perform new combinatorial constructions for each $n$, and there is no unified approach to construct them. Therefore, we develop a systematic way to construct EDs for any given $n$. Our method can transform ADs into EDs while retaining high statistical efficiency in two steps. The first step involves constructing an AD by utilizing the convex nature of many design criteria. The second step employs a simulated annealing algorithm to search for the ED stochastically. Through several applications, we demonstrate the utility of our method for various design problems. Additionally, we show that the design efficiency approaches unity as the number of design points increases.</summary></entry><entry><title type="html">Causal Inference with High-dimensional Discrete Covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/CausalInferencewithHighdimensionalDiscreteCovariates.html" rel="alternate" type="text/html" title="Causal Inference with High-dimensional Discrete Covariates" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/CausalInferencewithHighdimensionalDiscreteCovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/CausalInferencewithHighdimensionalDiscreteCovariates.html">&lt;p&gt;When estimating causal effects from observational studies, researchers often need to adjust for many covariates to deconfound the non-causal relationship between exposure and outcome, among which many covariates are discrete. The behavior of commonly used estimators in the presence of many discrete covariates is not well understood since their properties are often analyzed under structural assumptions including sparsity and smoothness, which do not apply in discrete settings. In this work, we study the estimation of causal effects in a model where the covariates required for confounding adjustment are discrete but high-dimensional, meaning the number of categories $d$ is comparable with or even larger than sample size $n$. Specifically, we show the mean squared error of commonly used regression, weighting and doubly robust estimators is bounded by $\frac{d^2}{n^2}+\frac{1}{n}$. We then prove the minimax lower bound for the average treatment effect is of order $\frac{d^2}{n^2 \log^2 n}+\frac{1}{n}$, which characterizes the fundamental difficulty of causal effect estimation in the high-dimensional discrete setting, and shows the estimators mentioned above are rate-optimal up to log-factors. We further consider additional structures that can be exploited, namely effect homogeneity and prior knowledge of the covariate distribution, and propose new estimators that enjoy faster convergence rates of order $\frac{d}{n^2} + \frac{1}{n}$, which achieve consistency in a broader regime. The results are illustrated empirically via simulation studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00118&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhenghao Zeng, Sivaraman Balakrishnan, Yanjun Han, Edward H. Kennedy</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">When estimating causal effects from observational studies, researchers often need to adjust for many covariates to deconfound the non-causal relationship between exposure and outcome, among which many covariates are discrete. The behavior of commonly used estimators in the presence of many discrete covariates is not well understood since their properties are often analyzed under structural assumptions including sparsity and smoothness, which do not apply in discrete settings. In this work, we study the estimation of causal effects in a model where the covariates required for confounding adjustment are discrete but high-dimensional, meaning the number of categories $d$ is comparable with or even larger than sample size $n$. Specifically, we show the mean squared error of commonly used regression, weighting and doubly robust estimators is bounded by $\frac{d^2}{n^2}+\frac{1}{n}$. We then prove the minimax lower bound for the average treatment effect is of order $\frac{d^2}{n^2 \log^2 n}+\frac{1}{n}$, which characterizes the fundamental difficulty of causal effect estimation in the high-dimensional discrete setting, and shows the estimators mentioned above are rate-optimal up to log-factors. We further consider additional structures that can be exploited, namely effect homogeneity and prior knowledge of the covariate distribution, and propose new estimators that enjoy faster convergence rates of order $\frac{d}{n^2} + \frac{1}{n}$, which achieve consistency in a broader regime. The results are illustrated empirically via simulation studies.</summary></entry><entry><title type="html">Causal K-Means Clustering</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/CausalKMeansClustering.html" rel="alternate" type="text/html" title="Causal K-Means Clustering" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/CausalKMeansClustering</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/CausalKMeansClustering.html">&lt;p&gt;Causal effects are often characterized with population summaries. These might provide an incomplete picture when there are heterogeneous treatment effects across subgroups. Since the subgroup structure is typically unknown, it is more challenging to identify and evaluate subgroup effects than population effects. We propose a new solution to this problem: Causal k-Means Clustering, which harnesses the widely-used k-means clustering algorithm to uncover the unknown subgroup structure. Our problem differs significantly from the conventional clustering setup since the variables to be clustered are unknown counterfactual functions. We present a plug-in estimator which is simple and readily implementable using off-the-shelf algorithms, and study its rate of convergence. We also develop a new bias-corrected estimator based on nonparametric efficiency theory and double machine learning, and show that this estimator achieves fast root-n rates and asymptotic normality in large nonparametric models. Our proposed methods are especially useful for modern outcome-wide studies with multiple treatment levels. Further, our framework is extensible to clustering with generic pseudo-outcomes, such as partially observed outcomes or otherwise unknown functions. Finally, we explore finite sample properties via simulation, and illustrate the proposed methods in a study of treatment programs for adolescent substance abuse.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03083&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kwangho Kim, Jisu Kim, Edward H. Kennedy</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Causal effects are often characterized with population summaries. These might provide an incomplete picture when there are heterogeneous treatment effects across subgroups. Since the subgroup structure is typically unknown, it is more challenging to identify and evaluate subgroup effects than population effects. We propose a new solution to this problem: Causal k-Means Clustering, which harnesses the widely-used k-means clustering algorithm to uncover the unknown subgroup structure. Our problem differs significantly from the conventional clustering setup since the variables to be clustered are unknown counterfactual functions. We present a plug-in estimator which is simple and readily implementable using off-the-shelf algorithms, and study its rate of convergence. We also develop a new bias-corrected estimator based on nonparametric efficiency theory and double machine learning, and show that this estimator achieves fast root-n rates and asymptotic normality in large nonparametric models. Our proposed methods are especially useful for modern outcome-wide studies with multiple treatment levels. Further, our framework is extensible to clustering with generic pseudo-outcomes, such as partially observed outcomes or otherwise unknown functions. Finally, we explore finite sample properties via simulation, and illustrate the proposed methods in a study of treatment programs for adolescent substance abuse.</summary></entry><entry><title type="html">Chauhan Weighted Trajectory Analysis reduces sample size requirements and expedites time-to-efficacy signals in advanced cancer clinical trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ChauhanWeightedTrajectoryAnalysisreducessamplesizerequirementsandexpeditestimetoefficacysignalsinadvancedcancerclinicaltrials.html" rel="alternate" type="text/html" title="Chauhan Weighted Trajectory Analysis reduces sample size requirements and expedites time-to-efficacy signals in advanced cancer clinical trials" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ChauhanWeightedTrajectoryAnalysisreducessamplesizerequirementsandexpeditestimetoefficacysignalsinadvancedcancerclinicaltrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ChauhanWeightedTrajectoryAnalysisreducessamplesizerequirementsandexpeditestimetoefficacysignalsinadvancedcancerclinicaltrials.html">&lt;p&gt;As Kaplan-Meier (KM) analysis is limited to single unidirectional endpoints, most advanced cancer randomized clinical trials (RCTs) are powered for either progression free survival (PFS) or overall survival (OS). This discards efficacy information carried by partial responses, complete responses, and stable disease that frequently precede progressive disease and death. Chauhan Weighted Trajectory Analysis (CWTA) is a generalization of KM that simultaneously assesses multiple rank-ordered endpoints. We hypothesized that CWTA could use this efficacy information to reduce sample size requirements and expedite efficacy signals in advanced cancer trials. We performed 100-fold and 1000-fold simulations of solid tumour systemic therapy RCTs with health statuses rank ordered from complete response (Stage 0) to death (Stage 4). At increments of sample size and hazard ratio, we compared KM PFS and OS with CWTA for (i) sample size requirements to achieve a power of 0.8 and (ii) time-to-first significant efficacy signal. CWTA consistently demonstrated greater power, and reduced sample size requirements by 18% to 35% compared to KM PFS and 14% to 20% compared to KM OS. CWTA also expedited time-to-efficacy signals 2- to 6-fold. CWTA, by incorporating all efficacy signals in the cancer treatment trajectory, provides clinically relevant reduction in required sample size and meaningfully expedites the efficacy signals of cancer treatments compared to KM PFS and KM OS. Using CWTA rather than KM as the primary trial outcome has the potential to meaningfully reduce the numbers of patients, trial duration, and costs to evaluate therapies in advanced cancer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02529&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Utkarsh Chauhan, Daylen Mackey, John R. Mackey</name></author><category term="stat.ME" /><summary type="html">As Kaplan-Meier (KM) analysis is limited to single unidirectional endpoints, most advanced cancer randomized clinical trials (RCTs) are powered for either progression free survival (PFS) or overall survival (OS). This discards efficacy information carried by partial responses, complete responses, and stable disease that frequently precede progressive disease and death. Chauhan Weighted Trajectory Analysis (CWTA) is a generalization of KM that simultaneously assesses multiple rank-ordered endpoints. We hypothesized that CWTA could use this efficacy information to reduce sample size requirements and expedite efficacy signals in advanced cancer trials. We performed 100-fold and 1000-fold simulations of solid tumour systemic therapy RCTs with health statuses rank ordered from complete response (Stage 0) to death (Stage 4). At increments of sample size and hazard ratio, we compared KM PFS and OS with CWTA for (i) sample size requirements to achieve a power of 0.8 and (ii) time-to-first significant efficacy signal. CWTA consistently demonstrated greater power, and reduced sample size requirements by 18% to 35% compared to KM PFS and 14% to 20% compared to KM OS. CWTA also expedited time-to-efficacy signals 2- to 6-fold. CWTA, by incorporating all efficacy signals in the cancer treatment trajectory, provides clinically relevant reduction in required sample size and meaningfully expedites the efficacy signals of cancer treatments compared to KM PFS and KM OS. Using CWTA rather than KM as the primary trial outcome has the potential to meaningfully reduce the numbers of patients, trial duration, and costs to evaluate therapies in advanced cancer.</summary></entry><entry><title type="html">Consistent response prediction for multilayer networks on unknown manifolds</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Consistentresponsepredictionformultilayernetworksonunknownmanifolds.html" rel="alternate" type="text/html" title="Consistent response prediction for multilayer networks on unknown manifolds" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Consistentresponsepredictionformultilayernetworksonunknownmanifolds</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Consistentresponsepredictionformultilayernetworksonunknownmanifolds.html">&lt;p&gt;Our paper deals with a collection of networks on a common set of nodes, where some of the networks are associated with responses. Assuming that the networks correspond to points on a one-dimensional manifold in a higher dimensional ambient space, we propose an algorithm to consistently predict the response at an unlabeled network. Our model involves a specific multiple random network model, namely the common subspace independent edge model, where the networks share a common invariant subspace, and the heterogeneity amongst the networks is captured by a set of low dimensional matrices. Our algorithm estimates these low dimensional matrices that capture the heterogeneity of the networks, learns the underlying manifold by isomap, and consistently predicts the response at an unlabeled network. We provide theoretical justifications for the use of our algorithm, validated by numerical simulations. Finally, we demonstrate the use of our algorithm on larval Drosophila connectome data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03225&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aranyak Acharyya, Jesús Arroyo Relión, Michael Clayton, Marta Zlatic, Youngser Park, Carey E. Priebe</name></author><category term="stat.ME" /><summary type="html">Our paper deals with a collection of networks on a common set of nodes, where some of the networks are associated with responses. Assuming that the networks correspond to points on a one-dimensional manifold in a higher dimensional ambient space, we propose an algorithm to consistently predict the response at an unlabeled network. Our model involves a specific multiple random network model, namely the common subspace independent edge model, where the networks share a common invariant subspace, and the heterogeneity amongst the networks is captured by a set of low dimensional matrices. Our algorithm estimates these low dimensional matrices that capture the heterogeneity of the networks, learns the underlying manifold by isomap, and consistently predicts the response at an unlabeled network. We provide theoretical justifications for the use of our algorithm, validated by numerical simulations. Finally, we demonstrate the use of our algorithm on larval Drosophila connectome data.</summary></entry><entry><title type="html">Copas-Heckman-type sensitivity analysis for publication bias in rare-event meta-analysis under the framework of the generalized linear mixed model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/CopasHeckmantypesensitivityanalysisforpublicationbiasinrareeventmetaanalysisundertheframeworkofthegeneralizedlinearmixedmodel.html" rel="alternate" type="text/html" title="Copas-Heckman-type sensitivity analysis for publication bias in rare-event meta-analysis under the framework of the generalized linear mixed model" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/CopasHeckmantypesensitivityanalysisforpublicationbiasinrareeventmetaanalysisundertheframeworkofthegeneralizedlinearmixedmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/CopasHeckmantypesensitivityanalysisforpublicationbiasinrareeventmetaanalysisundertheframeworkofthegeneralizedlinearmixedmodel.html">&lt;p&gt;Publication bias (PB) is one of the serious issues in meta-analysis. Many existing methods dealing with PB are based on the normal-normal (NN) random-effects model assuming normal models in both the within-study and the between-study levels. For rare-event meta-analysis where the data contain rare occurrences of event, the standard NN random-effects model may perform poorly. Instead, the generalized linear mixed effects model (GLMM) using the exact within-study model is recommended. However, no method has been proposed for dealing with PB in rare-event meta-analysis using the GLMM. In this paper, we propose sensitivity analysis methods for evaluating the impact of PB on the GLMM based on the famous Copas-Heckman-type selection model. The proposed methods can be easily implemented with the standard software coring the nonlinear mixed-effects model. We use a real-world example to show how the usefulness of the proposed methods in evaluating the potential impact of PB in meta-analysis of the log-transformed odds ratio based on the GLMM using the non-central hypergeometric or binomial distribution as the within-study model. An extension of the proposed method is also introduced for evaluating PB in meta-analysis of proportion based on the GLMM with the binomial within-study model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03603&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yi Zhou, Taojun Hu, Xiao-Hua Zhou, Satoshi Hattori</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Publication bias (PB) is one of the serious issues in meta-analysis. Many existing methods dealing with PB are based on the normal-normal (NN) random-effects model assuming normal models in both the within-study and the between-study levels. For rare-event meta-analysis where the data contain rare occurrences of event, the standard NN random-effects model may perform poorly. Instead, the generalized linear mixed effects model (GLMM) using the exact within-study model is recommended. However, no method has been proposed for dealing with PB in rare-event meta-analysis using the GLMM. In this paper, we propose sensitivity analysis methods for evaluating the impact of PB on the GLMM based on the famous Copas-Heckman-type selection model. The proposed methods can be easily implemented with the standard software coring the nonlinear mixed-effects model. We use a real-world example to show how the usefulness of the proposed methods in evaluating the potential impact of PB in meta-analysis of the log-transformed odds ratio based on the GLMM using the non-central hypergeometric or binomial distribution as the within-study model. An extension of the proposed method is also introduced for evaluating PB in meta-analysis of proportion based on the GLMM with the binomial within-study model.</summary></entry><entry><title type="html">Detecting algorithmic bias in medical-AI models using trees</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/DetectingalgorithmicbiasinmedicalAImodelsusingtrees.html" rel="alternate" type="text/html" title="Detecting algorithmic bias in medical-AI models using trees" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/DetectingalgorithmicbiasinmedicalAImodelsusingtrees</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/DetectingalgorithmicbiasinmedicalAImodelsusingtrees.html">&lt;p&gt;With the growing prevalence of machine learning and artificial intelligence-based medical decision support systems, it is equally important to ensure that these systems provide patient outcomes in a fair and equitable fashion. This paper presents an innovative framework for detecting areas of algorithmic bias in medical-AI decision support systems. Our approach efficiently identifies potential biases in medical-AI models, specifically in the context of sepsis prediction, by employing the Classification and Regression Trees (CART) algorithm. We verify our methodology by conducting a series of synthetic data experiments, showcasing its ability to estimate areas of bias in controlled settings precisely. The effectiveness of the concept is further validated by experiments using electronic medical records from Grady Memorial Hospital in Atlanta, Georgia. These tests demonstrate the practical implementation of our strategy in a clinical environment, where it can function as a vital instrument for guaranteeing fairness and equity in AI-based medical decisions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.02959&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jeffrey Smith, Andre Holder, Rishikesan Kamaleswaran, Yao Xie</name></author><category term="stat.ML," /><category term="stat.AP" /><summary type="html">With the growing prevalence of machine learning and artificial intelligence-based medical decision support systems, it is equally important to ensure that these systems provide patient outcomes in a fair and equitable fashion. This paper presents an innovative framework for detecting areas of algorithmic bias in medical-AI decision support systems. Our approach efficiently identifies potential biases in medical-AI models, specifically in the context of sepsis prediction, by employing the Classification and Regression Trees (CART) algorithm. We verify our methodology by conducting a series of synthetic data experiments, showcasing its ability to estimate areas of bias in controlled settings precisely. The effectiveness of the concept is further validated by experiments using electronic medical records from Grady Memorial Hospital in Atlanta, Georgia. These tests demonstrate the practical implementation of our strategy in a clinical environment, where it can function as a vital instrument for guaranteeing fairness and equity in AI-based medical decisions.</summary></entry><entry><title type="html">Distributed Iterative Hard Thresholding for Variable Selection in Tobit Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/DistributedIterativeHardThresholdingforVariableSelectioninTobitModels.html" rel="alternate" type="text/html" title="Distributed Iterative Hard Thresholding for Variable Selection in Tobit Models" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/DistributedIterativeHardThresholdingforVariableSelectioninTobitModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/DistributedIterativeHardThresholdingforVariableSelectioninTobitModels.html">&lt;p&gt;While extensive research has been conducted on high-dimensional data and on regression with left-censored responses, simultaneously addressing these complexities remains challenging, with only a few proposed methods available. In this paper, we utilize the Iterative Hard Thresholding (IHT) algorithm on the Tobit model in such a setting. Theoretical analysis demonstrates that our estimator converges with a near-optimal minimax rate. Additionally, we extend the method to a distributed setting, requiring only a few rounds of communication while retaining the estimation rate of the centralized version. Simulation results show that the IHT algorithm for the Tobit model achieves superior accuracy in predictions and subset selection, with the distributed estimator closely matching that of the centralized estimator. When applied to high-dimensional left-censored HIV viral load data, our method also exhibits similar superiority.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02539&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Changxin Yang, Zhongyi Zhu, Heng Lian</name></author><category term="stat.ME" /><summary type="html">While extensive research has been conducted on high-dimensional data and on regression with left-censored responses, simultaneously addressing these complexities remains challenging, with only a few proposed methods available. In this paper, we utilize the Iterative Hard Thresholding (IHT) algorithm on the Tobit model in such a setting. Theoretical analysis demonstrates that our estimator converges with a near-optimal minimax rate. Additionally, we extend the method to a distributed setting, requiring only a few rounds of communication while retaining the estimation rate of the centralized version. Simulation results show that the IHT algorithm for the Tobit model achieves superior accuracy in predictions and subset selection, with the distributed estimator closely matching that of the centralized estimator. When applied to high-dimensional left-censored HIV viral load data, our method also exhibits similar superiority.</summary></entry><entry><title type="html">Distributional Reference Class Forecasting of Corporate Sales Growth With Multiple Reference Variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/DistributionalReferenceClassForecastingofCorporateSalesGrowthWithMultipleReferenceVariables.html" rel="alternate" type="text/html" title="Distributional Reference Class Forecasting of Corporate Sales Growth With Multiple Reference Variables" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/DistributionalReferenceClassForecastingofCorporateSalesGrowthWithMultipleReferenceVariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/DistributionalReferenceClassForecastingofCorporateSalesGrowthWithMultipleReferenceVariables.html">&lt;p&gt;This paper introduces an approach to reference class selection in distributional forecasting with an application to corporate sales growth rates using several co-variates as reference variables, that are implicit predictors. The method can be used to detect expert or model-based forecasts exposed to (behavioral) bias or to forecast distributions with reference classes. These are sets of similar entities, here firms, and rank based algorithms for their selection are proposed, including an optional preprocessing data dimension reduction via principal components analysis. Forecasts are optimal if they match the underlying distribution as closely as possible. Probability integral transform values rank the forecast capability of different reference variable sets and algorithms in a backtest on a data set of 21,808 US firms over the time period 1950 - 2019. In particular, algorithms on dimension reduced variables perform well using contemporaneous balance sheet and financial market parameters along with past sales growth rates and past operating margins changes. Comparisions of actual analysts’ estimates to distributional forecasts and of historic distributional forecasts to realized sales growth illustrate the practical use of the method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03402&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Etienne Theising</name></author><category term="stat.AP" /><summary type="html">This paper introduces an approach to reference class selection in distributional forecasting with an application to corporate sales growth rates using several co-variates as reference variables, that are implicit predictors. The method can be used to detect expert or model-based forecasts exposed to (behavioral) bias or to forecast distributions with reference classes. These are sets of similar entities, here firms, and rank based algorithms for their selection are proposed, including an optional preprocessing data dimension reduction via principal components analysis. Forecasts are optimal if they match the underlying distribution as closely as possible. Probability integral transform values rank the forecast capability of different reference variable sets and algorithms in a backtest on a data set of 21,808 US firms over the time period 1950 - 2019. In particular, algorithms on dimension reduced variables perform well using contemporaneous balance sheet and financial market parameters along with past sales growth rates and past operating margins changes. Comparisions of actual analysts’ estimates to distributional forecasts and of historic distributional forecasts to realized sales growth illustrate the practical use of the method.</summary></entry><entry><title type="html">Doubly ranked tests for grouped functional data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Doublyrankedtestsforgroupedfunctionaldata.html" rel="alternate" type="text/html" title="Doubly ranked tests for grouped functional data" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Doublyrankedtestsforgroupedfunctionaldata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Doublyrankedtestsforgroupedfunctionaldata.html">&lt;p&gt;Nonparametric tests for functional data are a challenging class of tests to work with because of the potentially high dimensional nature of functional data. One of the main challenges for considering rank-based tests, like the Mann-Whitney or Wilcoxon Rank Sum tests (MWW), is that the unit of observation is a curve. Thus any rank-based test must consider ways of ranking curves. While several procedures, including depth-based methods, have recently been used to create scores for rank-based tests, these scores are not constructed under the null and often introduce additional, uncontrolled for variability. We therefore reconsider the problem of rank-based tests for functional data and develop an alternative approach that incorporates the null hypothesis throughout. Our approach first ranks realizations from the curves at each time point, then summarizes the ranks for each subject using a sufficient statistic we derive, and finally re-ranks the sufficient statistics in a procedure we refer to as a doubly ranked test. As we demonstrate, doubly rank tests are more powerful while maintaining ideal type I error in the two sample, MWW setting. We also extend our framework to more than two samples, developing a Kruskal-Wallis test for functional data which exhibits good test characteristics as well. Finally, we illustrate the use of doubly ranked tests in functional data contexts from material science, climatology, and public health policy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.14761&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mark J. Meyer</name></author><category term="stat.ME" /><summary type="html">Nonparametric tests for functional data are a challenging class of tests to work with because of the potentially high dimensional nature of functional data. One of the main challenges for considering rank-based tests, like the Mann-Whitney or Wilcoxon Rank Sum tests (MWW), is that the unit of observation is a curve. Thus any rank-based test must consider ways of ranking curves. While several procedures, including depth-based methods, have recently been used to create scores for rank-based tests, these scores are not constructed under the null and often introduce additional, uncontrolled for variability. We therefore reconsider the problem of rank-based tests for functional data and develop an alternative approach that incorporates the null hypothesis throughout. Our approach first ranks realizations from the curves at each time point, then summarizes the ranks for each subject using a sufficient statistic we derive, and finally re-ranks the sufficient statistics in a procedure we refer to as a doubly ranked test. As we demonstrate, doubly rank tests are more powerful while maintaining ideal type I error in the two sample, MWW setting. We also extend our framework to more than two samples, developing a Kruskal-Wallis test for functional data which exhibits good test characteristics as well. Finally, we illustrate the use of doubly ranked tests in functional data contexts from material science, climatology, and public health policy.</summary></entry><entry><title type="html">Efficient Weighting Schemes for Auditing Instant-Runoff Voting Elections</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/EfficientWeightingSchemesforAuditingInstantRunoffVotingElections.html" rel="alternate" type="text/html" title="Efficient Weighting Schemes for Auditing Instant-Runoff Voting Elections" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/EfficientWeightingSchemesforAuditingInstantRunoffVotingElections</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/EfficientWeightingSchemesforAuditingInstantRunoffVotingElections.html">&lt;p&gt;Various risk-limiting audit (RLA) methods have been developed for instant-runoff voting (IRV) elections. A recent method, AWAIRE, is the first efficient approach that can take advantage of but does not require cast vote records (CVRs). AWAIRE involves adaptively weighted averages of test statistics, essentially “learning” an effective set of hypotheses to test. However, the initial paper on AWAIRE only examined a few weighting schemes and parameter settings.
  We explore schemes and settings more extensively, to identify and recommend efficient choices for practice. We focus on the case where CVRs are not available, assessing performance using simulations based on real election data.
  The most effective schemes are often those that place most or all of the weight on the apparent “best” hypotheses based on already seen data. Conversely, the optimal tuning parameters tended to vary based on the election margin. Nonetheless, we quantify the performance trade-offs for different choices across varying election margins, aiding in selecting the most desirable trade-off if a default option is needed.
  A limitation of the current AWAIRE implementation is its restriction to a small number of candidates – up to six in previous implementations. One path to a more computationally efficient implementation would be to use lazy evaluation and avoid considering all possible hypotheses. Our findings suggest that such an approach could be done without substantially compromising statistical performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.15400&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexander Ek, Philip B. Stark, Peter J. Stuckey, Damjan Vukcevic</name></author><category term="stat.AP" /><summary type="html">Various risk-limiting audit (RLA) methods have been developed for instant-runoff voting (IRV) elections. A recent method, AWAIRE, is the first efficient approach that can take advantage of but does not require cast vote records (CVRs). AWAIRE involves adaptively weighted averages of test statistics, essentially “learning” an effective set of hypotheses to test. However, the initial paper on AWAIRE only examined a few weighting schemes and parameter settings. We explore schemes and settings more extensively, to identify and recommend efficient choices for practice. We focus on the case where CVRs are not available, assessing performance using simulations based on real election data. The most effective schemes are often those that place most or all of the weight on the apparent “best” hypotheses based on already seen data. Conversely, the optimal tuning parameters tended to vary based on the election margin. Nonetheless, we quantify the performance trade-offs for different choices across varying election margins, aiding in selecting the most desirable trade-off if a default option is needed. A limitation of the current AWAIRE implementation is its restriction to a small number of candidates – up to six in previous implementations. One path to a more computationally efficient implementation would be to use lazy evaluation and avoid considering all possible hypotheses. Our findings suggest that such an approach could be done without substantially compromising statistical performance.</summary></entry><entry><title type="html">Estimating Complier Average Causal Effects with Mixtures of Experts</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/EstimatingComplierAverageCausalEffectswithMixturesofExperts.html" rel="alternate" type="text/html" title="Estimating Complier Average Causal Effects with Mixtures of Experts" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/EstimatingComplierAverageCausalEffectswithMixturesofExperts</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/EstimatingComplierAverageCausalEffectswithMixturesofExperts.html">&lt;p&gt;Understanding the causal impact of medical interventions is essential in healthcare research, especially through randomized controlled trials (RCTs). Despite their prominence, challenges arise due to discrepancies between treatment allocation and actual intake, influenced by various factors like patient non-adherence or procedural errors. This paper focuses on the Complier Average Causal Effect (CACE), crucial for evaluating treatment efficacy among compliant patients. Existing methodologies often rely on assumptions such as exclusion restriction and monotonicity, which can be problematic in practice. We propose a novel approach, leveraging supervised learning architectures, to estimate CACE without depending on these assumptions. Our method involves a two-step process: first estimating compliance probabilities for patients, then using these probabilities to estimate two nuisance components relevant to CACE calculation. Building upon the principal ignorability assumption, we introduce four root-n consistent, asymptotically normal, CACE estimators, and prove that the underlying mixtures of experts’ nuisance components are identifiable. Our causal framework allows our estimation procedures to enjoy reduced mean squared errors when exclusion restriction or monotonicity assumptions hold. Through simulations and application to a breastfeeding promotion RCT, we demonstrate the method’s performance and applicability.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02779&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>François Grolleau, Céline Béji, François Petit, Raphaël Porcher</name></author><category term="stat.ME" /><summary type="html">Understanding the causal impact of medical interventions is essential in healthcare research, especially through randomized controlled trials (RCTs). Despite their prominence, challenges arise due to discrepancies between treatment allocation and actual intake, influenced by various factors like patient non-adherence or procedural errors. This paper focuses on the Complier Average Causal Effect (CACE), crucial for evaluating treatment efficacy among compliant patients. Existing methodologies often rely on assumptions such as exclusion restriction and monotonicity, which can be problematic in practice. We propose a novel approach, leveraging supervised learning architectures, to estimate CACE without depending on these assumptions. Our method involves a two-step process: first estimating compliance probabilities for patients, then using these probabilities to estimate two nuisance components relevant to CACE calculation. Building upon the principal ignorability assumption, we introduce four root-n consistent, asymptotically normal, CACE estimators, and prove that the underlying mixtures of experts’ nuisance components are identifiable. Our causal framework allows our estimation procedures to enjoy reduced mean squared errors when exclusion restriction or monotonicity assumptions hold. Through simulations and application to a breastfeeding promotion RCT, we demonstrate the method’s performance and applicability.</summary></entry><entry><title type="html">Estimating Counterfactual Matrix Means with Short Panel Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/EstimatingCounterfactualMatrixMeanswithShortPanelData.html" rel="alternate" type="text/html" title="Estimating Counterfactual Matrix Means with Short Panel Data" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/EstimatingCounterfactualMatrixMeanswithShortPanelData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/EstimatingCounterfactualMatrixMeanswithShortPanelData.html">&lt;p&gt;We develop a new, spectral approach for identifying and estimating average counterfactual outcomes under a low-rank factor model with short panel data and general outcome missingness patterns. Applications include event studies and studies of outcomes of “matches” between agents of two types, e.g. workers and firms, typically conducted under less-flexible Two-Way-Fixed-Effects (TWFE) models of outcomes. Given an infinite population of units and a finite number of outcomes, we show our approach identifies all counterfactual outcome means, including those not estimable by existing methods, if a particular graph constructed based on overlaps in observed outcomes between subpopulations is connected. Our analogous, computationally efficient estimation procedure yields consistent, asymptotically normal estimates of counterfactual outcome means under fixed-$T$ (number of outcomes), large-$N$ (sample size) asymptotics. In a semi-synthetic simulation study based on matched employer-employee data, our estimator has lower bias and only slightly higher variance than a TWFE-model-based estimator when estimating average log-wages.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.07520&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lihua Lei, Brad Ross</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We develop a new, spectral approach for identifying and estimating average counterfactual outcomes under a low-rank factor model with short panel data and general outcome missingness patterns. Applications include event studies and studies of outcomes of “matches” between agents of two types, e.g. workers and firms, typically conducted under less-flexible Two-Way-Fixed-Effects (TWFE) models of outcomes. Given an infinite population of units and a finite number of outcomes, we show our approach identifies all counterfactual outcome means, including those not estimable by existing methods, if a particular graph constructed based on overlaps in observed outcomes between subpopulations is connected. Our analogous, computationally efficient estimation procedure yields consistent, asymptotically normal estimates of counterfactual outcome means under fixed-$T$ (number of outcomes), large-$N$ (sample size) asymptotics. In a semi-synthetic simulation study based on matched employer-employee data, our estimator has lower bias and only slightly higher variance than a TWFE-model-based estimator when estimating average log-wages.</summary></entry><entry><title type="html">Exact Sampling of Spanning Trees via Fast-forwarded Random Walks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ExactSamplingofSpanningTreesviaFastforwardedRandomWalks.html" rel="alternate" type="text/html" title="Exact Sampling of Spanning Trees via Fast-forwarded Random Walks" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ExactSamplingofSpanningTreesviaFastforwardedRandomWalks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ExactSamplingofSpanningTreesviaFastforwardedRandomWalks.html">&lt;p&gt;Tree graphs are routinely used in statistics. When estimating a Bayesian model with a tree component, sampling the posterior remains a core difficulty. Existing Markov chain Monte Carlo methods tend to rely on local moves, often leading to poor mixing. A promising approach is to instead directly sample spanning trees on an auxiliary graph. Current spanning tree samplers, such as the celebrated Aldous–Broder algorithm, predominantly rely on simulating random walks that are required to visit all the nodes of the graph. Such algorithms are prone to getting stuck in certain sub-graphs. We formalize this phenomenon using the bottlenecks in the random walk’s transition probability matrix. We then propose a novel fast-forwarded cover algorithm that can break free from bottlenecks. The core idea is a marginalization argument that leads to a closed-form expression which allows for fast-forwarding to the event of visiting a new node. Unlike many existing approximation algorithms, our algorithm yields exact samples. We demonstrate the enhanced efficiency of the fast-forwarded cover algorithm, and illustrate its application in fitting a Bayesian dendrogram model on a Massachusetts crimes and communities dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03096&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Edric Tam, David B. Dunson, Leo L. Duan</name></author><category term="stat.ME" /><summary type="html">Tree graphs are routinely used in statistics. When estimating a Bayesian model with a tree component, sampling the posterior remains a core difficulty. Existing Markov chain Monte Carlo methods tend to rely on local moves, often leading to poor mixing. A promising approach is to instead directly sample spanning trees on an auxiliary graph. Current spanning tree samplers, such as the celebrated Aldous–Broder algorithm, predominantly rely on simulating random walks that are required to visit all the nodes of the graph. Such algorithms are prone to getting stuck in certain sub-graphs. We formalize this phenomenon using the bottlenecks in the random walk’s transition probability matrix. We then propose a novel fast-forwarded cover algorithm that can break free from bottlenecks. The core idea is a marginalization argument that leads to a closed-form expression which allows for fast-forwarding to the event of visiting a new node. Unlike many existing approximation algorithms, our algorithm yields exact samples. We demonstrate the enhanced efficiency of the fast-forwarded cover algorithm, and illustrate its application in fitting a Bayesian dendrogram model on a Massachusetts crimes and communities dataset.</summary></entry><entry><title type="html">Extreme Treatment Effect: Extrapolating Dose-Response Function Into Extreme Treatment Domain</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ExtremeTreatmentEffectExtrapolatingDoseResponseFunctionIntoExtremeTreatmentDomain.html" rel="alternate" type="text/html" title="Extreme Treatment Effect: Extrapolating Dose-Response Function Into Extreme Treatment Domain" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ExtremeTreatmentEffectExtrapolatingDoseResponseFunctionIntoExtremeTreatmentDomain</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ExtremeTreatmentEffectExtrapolatingDoseResponseFunctionIntoExtremeTreatmentDomain.html">&lt;p&gt;The potential outcomes framework serves as a fundamental tool for quantifying causal effects. The average dose-response function (also called the effect curve), denoted as (\mu(t)), is typically of interest when dealing with a continuous treatment variable (exposure). The focus of this work is to determine the impact of an extreme level of treatment, potentially beyond the range of observed values–that is, estimating (\mu(t)) for very large (t). Our approach is grounded in the field of statistics known as extreme value theory. We outline key assumptions for the identifiability of the extreme treatment effect. Additionally, we present a novel and consistent estimation procedure that can potentially reduce the dimension of the confounders to at most 3. This is a significant result since typically, the estimation of (\mu(t)) is very challenging due to high-dimensional confounders. In practical applications, our framework proves valuable when assessing the effects of scenarios such as drug overdoses, extreme river discharges, or extremely high temperatures on a variable of interest.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.11003&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Juraj Bodik</name></author><category term="stat.ME" /><summary type="html">The potential outcomes framework serves as a fundamental tool for quantifying causal effects. The average dose-response function (also called the effect curve), denoted as (\mu(t)), is typically of interest when dealing with a continuous treatment variable (exposure). The focus of this work is to determine the impact of an extreme level of treatment, potentially beyond the range of observed values–that is, estimating (\mu(t)) for very large (t). Our approach is grounded in the field of statistics known as extreme value theory. We outline key assumptions for the identifiability of the extreme treatment effect. Additionally, we present a novel and consistent estimation procedure that can potentially reduce the dimension of the confounders to at most 3. This is a significant result since typically, the estimation of (\mu(t)) is very challenging due to high-dimensional confounders. In practical applications, our framework proves valuable when assessing the effects of scenarios such as drug overdoses, extreme river discharges, or extremely high temperatures on a variable of interest.</summary></entry><entry><title type="html">Flexible cost-penalized Bayesian model selection: developing inclusion paths with an application to diagnosis of heart disease</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/FlexiblecostpenalizedBayesianmodelselectiondevelopinginclusionpathswithanapplicationtodiagnosisofheartdisease.html" rel="alternate" type="text/html" title="Flexible cost-penalized Bayesian model selection: developing inclusion paths with an application to diagnosis of heart disease" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/FlexiblecostpenalizedBayesianmodelselectiondevelopinginclusionpathswithanapplicationtodiagnosisofheartdisease</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/FlexiblecostpenalizedBayesianmodelselectiondevelopinginclusionpathswithanapplicationtodiagnosisofheartdisease.html">&lt;p&gt;We propose a Bayesian model selection approach that allows medical practitioners to select among predictor variables while taking their respective costs into account. Medical procedures almost always incur costs in time and/or money. These costs might exceed their usefulness for modeling the outcome of interest. We develop Bayesian model selection that uses flexible model priors to penalize costly predictors a priori and select a subset of predictors useful relative to their costs. Our approach (i) gives the practitioner control over the magnitude of cost penalization, (ii) enables the prior to scale well with sample size, and (iii) enables the creation of our proposed inclusion path visualization, which can be used to make decisions about individual candidate predictors using both probabilistic and visual tools. We demonstrate the effectiveness of our inclusion path approach and the importance of being able to adjust the magnitude of the prior’s cost penalization through a dataset pertaining to heart disease diagnosis in patients at the Cleveland Clinic Foundation, where several candidate predictors with various costs were recorded for patients, and through simulated data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.06262&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Erica M. Porter, Christopher T. Franck, Stephen Adams</name></author><category term="stat.ME" /><summary type="html">We propose a Bayesian model selection approach that allows medical practitioners to select among predictor variables while taking their respective costs into account. Medical procedures almost always incur costs in time and/or money. These costs might exceed their usefulness for modeling the outcome of interest. We develop Bayesian model selection that uses flexible model priors to penalize costly predictors a priori and select a subset of predictors useful relative to their costs. Our approach (i) gives the practitioner control over the magnitude of cost penalization, (ii) enables the prior to scale well with sample size, and (iii) enables the creation of our proposed inclusion path visualization, which can be used to make decisions about individual candidate predictors using both probabilistic and visual tools. We demonstrate the effectiveness of our inclusion path approach and the importance of being able to adjust the magnitude of the prior’s cost penalization through a dataset pertaining to heart disease diagnosis in patients at the Cleveland Clinic Foundation, where several candidate predictors with various costs were recorded for patients, and through simulated data.</summary></entry><entry><title type="html">Functional Post-Clustering Selective Inference with Applications to EHR Data Analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/FunctionalPostClusteringSelectiveInferencewithApplicationstoEHRDataAnalysis.html" rel="alternate" type="text/html" title="Functional Post-Clustering Selective Inference with Applications to EHR Data Analysis" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/FunctionalPostClusteringSelectiveInferencewithApplicationstoEHRDataAnalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/FunctionalPostClusteringSelectiveInferencewithApplicationstoEHRDataAnalysis.html">&lt;p&gt;In electronic health records (EHR) analysis, clustering patients according to patterns in their data is crucial for uncovering new subtypes of diseases. Existing medical literature often relies on classical hypothesis testing methods to test for differences in means between these clusters. Due to selection bias induced by clustering algorithms, the implementation of these classical methods on post-clustering data often leads to an inflated type-I error. In this paper, we introduce a new statistical approach that adjusts for this bias when analyzing data collected over time. Our method extends classical selective inference methods for cross-sectional data to longitudinal data. We provide theoretical guarantees for our approach with upper bounds on the selective type-I and type-II errors. We apply the method to simulated data and real-world Acute Kidney Injury (AKI) EHR datasets, thereby illustrating the advantages of our approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03042&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zihan Zhu, Xin Gai, Anru R. Zhang</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">In electronic health records (EHR) analysis, clustering patients according to patterns in their data is crucial for uncovering new subtypes of diseases. Existing medical literature often relies on classical hypothesis testing methods to test for differences in means between these clusters. Due to selection bias induced by clustering algorithms, the implementation of these classical methods on post-clustering data often leads to an inflated type-I error. In this paper, we introduce a new statistical approach that adjusts for this bias when analyzing data collected over time. Our method extends classical selective inference methods for cross-sectional data to longitudinal data. We provide theoretical guarantees for our approach with upper bounds on the selective type-I and type-II errors. We apply the method to simulated data and real-world Acute Kidney Injury (AKI) EHR datasets, thereby illustrating the advantages of our approach.</summary></entry><entry><title type="html">Generalised envelope spectrum-based signal-to-noise objectives: Formulation, optimisation and application for gear fault detection under time-varying speed conditions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/GeneralisedenvelopespectrumbasedsignaltonoiseobjectivesFormulationoptimisationandapplicationforgearfaultdetectionundertimevaryingspeedconditions.html" rel="alternate" type="text/html" title="Generalised envelope spectrum-based signal-to-noise objectives: Formulation, optimisation and application for gear fault detection under time-varying speed conditions" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/GeneralisedenvelopespectrumbasedsignaltonoiseobjectivesFormulationoptimisationandapplicationforgearfaultdetectionundertimevaryingspeedconditions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/GeneralisedenvelopespectrumbasedsignaltonoiseobjectivesFormulationoptimisationandapplicationforgearfaultdetectionundertimevaryingspeedconditions.html">&lt;p&gt;In vibration-based condition monitoring, optimal filter design improves fault detection by enhancing weak fault signatures within vibration signals. This process involves optimising a derived objective function from a defined objective. The objectives are often based on proxy health indicators to determine the filter’s parameters. However, these indicators can be compromised by irrelevant extraneous signal components and fluctuating operational conditions, affecting the filter’s efficacy. Fault detection primarily uses the fault component’s prominence in the squared envelope spectrum, quantified by a squared envelope spectrum-based signal-to-noise ratio. New optimal filter objective functions are derived from the proposed generalised envelope spectrum-based signal-to-noise objective for machines operating under variable speed conditions. Instead of optimising proxy health indicators, the optimal filter coefficients of the formulation directly maximise the squared envelope spectrum-based signal-to-noise ratio over targeted frequency bands using standard gradient-based optimisers. Four derived objective functions from the proposed objective effectively outperform five prominent methods in tests on three experimental datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00727&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Stephan Schmidt, Daniel N. Wilke, Konstantinos C. Gryllias</name></author><category term="stat.ME" /><summary type="html">In vibration-based condition monitoring, optimal filter design improves fault detection by enhancing weak fault signatures within vibration signals. This process involves optimising a derived objective function from a defined objective. The objectives are often based on proxy health indicators to determine the filter’s parameters. However, these indicators can be compromised by irrelevant extraneous signal components and fluctuating operational conditions, affecting the filter’s efficacy. Fault detection primarily uses the fault component’s prominence in the squared envelope spectrum, quantified by a squared envelope spectrum-based signal-to-noise ratio. New optimal filter objective functions are derived from the proposed generalised envelope spectrum-based signal-to-noise objective for machines operating under variable speed conditions. Instead of optimising proxy health indicators, the optimal filter coefficients of the formulation directly maximise the squared envelope spectrum-based signal-to-noise ratio over targeted frequency bands using standard gradient-based optimisers. Four derived objective functions from the proposed objective effectively outperform five prominent methods in tests on three experimental datasets.</summary></entry><entry><title type="html">Generalizing Orthogonalization for Models with Non-linearities</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/GeneralizingOrthogonalizationforModelswithNonlinearities.html" rel="alternate" type="text/html" title="Generalizing Orthogonalization for Models with Non-linearities" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/GeneralizingOrthogonalizationforModelswithNonlinearities</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/GeneralizingOrthogonalizationforModelswithNonlinearities.html">&lt;p&gt;The complexity of black-box algorithms can lead to various challenges, including the introduction of biases. These biases present immediate risks in the algorithms’ application. It was, for instance, shown that neural networks can deduce racial information solely from a patient’s X-ray scan, a task beyond the capability of medical experts. If this fact is not known to the medical expert, automatic decision-making based on this algorithm could lead to prescribing a treatment (purely) based on racial information. While current methodologies allow for the “orthogonalization” or “normalization” of neural networks with respect to such information, existing approaches are grounded in linear models. Our paper advances the discourse by introducing corrections for non-linearities such as ReLU activations. Our approach also encompasses scalar and tensor-valued predictions, facilitating its integration into neural network architectures. Through extensive experiments, we validate our method’s effectiveness in safeguarding sensitive data in generalized linear models, normalizing convolutional neural networks for metadata, and rectifying pre-existing embeddings for undesired attributes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02475&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David Rügamer, Chris Kolb, Tobias Weber, Lucas Kook, Thomas Nagler</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">The complexity of black-box algorithms can lead to various challenges, including the introduction of biases. These biases present immediate risks in the algorithms’ application. It was, for instance, shown that neural networks can deduce racial information solely from a patient’s X-ray scan, a task beyond the capability of medical experts. If this fact is not known to the medical expert, automatic decision-making based on this algorithm could lead to prescribing a treatment (purely) based on racial information. While current methodologies allow for the “orthogonalization” or “normalization” of neural networks with respect to such information, existing approaches are grounded in linear models. Our paper advances the discourse by introducing corrections for non-linearities such as ReLU activations. Our approach also encompasses scalar and tensor-valued predictions, facilitating its integration into neural network architectures. Through extensive experiments, we validate our method’s effectiveness in safeguarding sensitive data in generalized linear models, normalizing convolutional neural networks for metadata, and rectifying pre-existing embeddings for undesired attributes.</summary></entry><entry><title type="html">Grouping predictors via network-wide metrics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Groupingpredictorsvianetworkwidemetrics.html" rel="alternate" type="text/html" title="Grouping predictors via network-wide metrics" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Groupingpredictorsvianetworkwidemetrics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Groupingpredictorsvianetworkwidemetrics.html">&lt;p&gt;When multitudes of features can plausibly be associated with a response, both privacy considerations and model parsimony suggest grouping them to increase the predictive power of a regression model. Specifically, the identification of groups of predictors significantly associated with the response variable eases further downstream analysis and decision-making. This paper proposes a new data analysis methodology that utilizes the high-dimensional predictor space to construct an implicit network with weighted edges %and weights on the edges to identify significant associations between the response and the predictors. Using a population model for groups of predictors defined via network-wide metrics, a new supervised grouping algorithm is proposed to determine the correct group, with probability tending to one as the sample size diverges to infinity. For this reason, we establish several theoretical properties of the estimates of network-wide metrics. A novel model-assisted bootstrap procedure that substantially decreases computational complexity is developed, facilitating the assessment of uncertainty in the estimates of network-wide metrics. The proposed methods account for several challenges that arise in the high-dimensional data setting, including (i) a large number of predictors, (ii) uncertainty regarding the true statistical model, and (iii) model selection variability. The performance of the proposed methods is demonstrated through numerical experiments, data from sports analytics, and breast cancer data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02715&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Brandon Woosuk Park, Anand N. Vidyashankar, Tucker S. McElroy</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">When multitudes of features can plausibly be associated with a response, both privacy considerations and model parsimony suggest grouping them to increase the predictive power of a regression model. Specifically, the identification of groups of predictors significantly associated with the response variable eases further downstream analysis and decision-making. This paper proposes a new data analysis methodology that utilizes the high-dimensional predictor space to construct an implicit network with weighted edges %and weights on the edges to identify significant associations between the response and the predictors. Using a population model for groups of predictors defined via network-wide metrics, a new supervised grouping algorithm is proposed to determine the correct group, with probability tending to one as the sample size diverges to infinity. For this reason, we establish several theoretical properties of the estimates of network-wide metrics. A novel model-assisted bootstrap procedure that substantially decreases computational complexity is developed, facilitating the assessment of uncertainty in the estimates of network-wide metrics. The proposed methods account for several challenges that arise in the high-dimensional data setting, including (i) a large number of predictors, (ii) uncertainty regarding the true statistical model, and (iii) model selection variability. The performance of the proposed methods is demonstrated through numerical experiments, data from sports analytics, and breast cancer data.</summary></entry><entry><title type="html">Hölder regularity and roughness: construction and examples</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/H%C3%B6lderregularityandroughnessconstructionandexamples.html" rel="alternate" type="text/html" title="Hölder regularity and roughness: construction and examples" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/H%C3%B6lderregularityandroughnessconstructionandexamples</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/H%C3%B6lderregularityandroughnessconstructionandexamples.html">&lt;p&gt;We study how to construct a stochastic process on a finite interval with given `roughness’ and finite joint moments of marginal distributions. We first extend Ciesielski’s isomorphism along a general sequence of partitions, and provide a characterization of H&quot;older regularity of a function in terms of its Schauder coefficients. Using this characterization we provide a better (pathwise) estimator of H&quot;older exponent. As an additional application, we construct fake (fractional) Brownian motions with some path properties and finite moments of marginal distributions same as (fractional) Brownian motions. These belong to non-Gaussian families of stochastic processes which are statistically difficult to distinguish from real (fractional) Brownian motions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.13794&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Erhan Bayraktar, Purba Das, Donghan Kim</name></author><category term="stat.AP" /><summary type="html">We study how to construct a stochastic process on a finite interval with given `roughness’ and finite joint moments of marginal distributions. We first extend Ciesielski’s isomorphism along a general sequence of partitions, and provide a characterization of H&quot;older regularity of a function in terms of its Schauder coefficients. Using this characterization we provide a better (pathwise) estimator of H&quot;older exponent. As an additional application, we construct fake (fractional) Brownian motions with some path properties and finite moments of marginal distributions same as (fractional) Brownian motions. These belong to non-Gaussian families of stochastic processes which are statistically difficult to distinguish from real (fractional) Brownian motions.</summary></entry><entry><title type="html">Identifiable causal inference with noisy treatment and no side information</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Identifiablecausalinferencewithnoisytreatmentandnosideinformation.html" rel="alternate" type="text/html" title="Identifiable causal inference with noisy treatment and no side information" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Identifiablecausalinferencewithnoisytreatmentandnosideinformation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Identifiablecausalinferencewithnoisytreatmentandnosideinformation.html">&lt;p&gt;In some causal inference scenarios, the treatment variable is measured inaccurately, for instance in epidemiology or econometrics. Failure to correct for the effect of this measurement error can lead to biased causal effect estimates. Previous research has not studied methods that address this issue from a causal viewpoint while allowing for complex nonlinear dependencies and without assuming access to side information. For such a scenario, this study proposes a model that assumes a continuous treatment variable that is inaccurately measured. Building on existing results for measurement error models, we prove that our model’s causal effect estimates are identifiable, even without knowledge of the measurement error variance or other side information. Our method relies on a deep latent variable model in which Gaussian conditionals are parameterized by neural networks, and we develop an amortized importance-weighted variational objective for training the model. Empirical results demonstrate the method’s good performance with unknown measurement error. More broadly, our work extends the range of applications in which reliable causal inference can be conducted.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.10614&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Antti Pöllänen, Pekka Marttinen</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">In some causal inference scenarios, the treatment variable is measured inaccurately, for instance in epidemiology or econometrics. Failure to correct for the effect of this measurement error can lead to biased causal effect estimates. Previous research has not studied methods that address this issue from a causal viewpoint while allowing for complex nonlinear dependencies and without assuming access to side information. For such a scenario, this study proposes a model that assumes a continuous treatment variable that is inaccurately measured. Building on existing results for measurement error models, we prove that our model’s causal effect estimates are identifiable, even without knowledge of the measurement error variance or other side information. Our method relies on a deep latent variable model in which Gaussian conditionals are parameterized by neural networks, and we develop an amortized importance-weighted variational objective for training the model. Empirical results demonstrate the method’s good performance with unknown measurement error. More broadly, our work extends the range of applications in which reliable causal inference can be conducted.</summary></entry><entry><title type="html">Mixture of partially linear experts</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Mixtureofpartiallylinearexperts.html" rel="alternate" type="text/html" title="Mixture of partially linear experts" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Mixtureofpartiallylinearexperts</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Mixtureofpartiallylinearexperts.html">&lt;p&gt;In the mixture of experts model, a common assumption is the linearity between a response variable and covariates. While this assumption has theoretical and computational benefits, it may lead to suboptimal estimates by overlooking potential nonlinear relationships among the variables. To address this limitation, we propose a partially linear structure that incorporates unspecified functions to capture nonlinear relationships. We establish the identifiability of the proposed model under mild conditions and introduce a practical estimation algorithm. We present the performance of our approach through numerical studies, including simulations and real data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02905&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yeongsan Hwang, Byungtae Seo, Sangkon Oh</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">In the mixture of experts model, a common assumption is the linearity between a response variable and covariates. While this assumption has theoretical and computational benefits, it may lead to suboptimal estimates by overlooking potential nonlinear relationships among the variables. To address this limitation, we propose a partially linear structure that incorporates unspecified functions to capture nonlinear relationships. We establish the identifiability of the proposed model under mild conditions and introduce a practical estimation algorithm. We present the performance of our approach through numerical studies, including simulations and real data analysis.</summary></entry><entry><title type="html">Modeling frequency distribution above a priority in presence of IBNR</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ModelingfrequencydistributionaboveapriorityinpresenceofIBNR.html" rel="alternate" type="text/html" title="Modeling frequency distribution above a priority in presence of IBNR" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ModelingfrequencydistributionaboveapriorityinpresenceofIBNR</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ModelingfrequencydistributionaboveapriorityinpresenceofIBNR.html">&lt;p&gt;In reinsurance, Poisson and Negative binomial distributions are employed for modeling frequency. However, the incomplete data regarding reported incurred claims above a priority level presents challenges in estimation. This paper focuses on frequency estimation using Schnieper’s framework for claim numbering. We demonstrate that Schnieper’s model is consistent with a Poisson distribution for the total number of claims above a priority at each year of development, providing a robust basis for parameter estimation. Additionally, we explain how to build an alternative assumption based on a Negative binomial distribution, which yields similar results. The study includes a bootstrap procedure to manage uncertainty in parameter estimation and a case study comparing assumptions and evaluating the impact of the bootstrap approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02871&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nicolas Baradel</name></author><category term="stat.ME" /><summary type="html">In reinsurance, Poisson and Negative binomial distributions are employed for modeling frequency. However, the incomplete data regarding reported incurred claims above a priority level presents challenges in estimation. This paper focuses on frequency estimation using Schnieper’s framework for claim numbering. We demonstrate that Schnieper’s model is consistent with a Poisson distribution for the total number of claims above a priority at each year of development, providing a robust basis for parameter estimation. Additionally, we explain how to build an alternative assumption based on a Negative binomial distribution, which yields similar results. The study includes a bootstrap procedure to manage uncertainty in parameter estimation and a case study comparing assumptions and evaluating the impact of the bootstrap approach.</summary></entry><entry><title type="html">Modelling Sampling Distributions of Test Statistics with Autograd</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ModellingSamplingDistributionsofTestStatisticswithAutograd.html" rel="alternate" type="text/html" title="Modelling Sampling Distributions of Test Statistics with Autograd" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ModellingSamplingDistributionsofTestStatisticswithAutograd</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/ModellingSamplingDistributionsofTestStatisticswithAutograd.html">&lt;p&gt;Simulation-based inference methods that feature correct conditional coverage of confidence sets based on observations that have been compressed to a scalar test statistic require accurate modelling of either the p-value function or the cumulative distribution function (cdf) of the test statistic. If the model of the cdf, which is typically a deep neural network, is a function of the test statistic then the derivative of the neural network with respect to the test statistic furnishes an approximation of the sampling distribution of the test statistic. We explore whether this approach to modelling conditional 1-dimensional sampling distributions is a viable alternative to the probability density-ratio method, also known as the likelihood-ratio trick. Relatively simple, yet effective, neural network models are used whose predictive uncertainty is quantified through a variety of methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02488&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ali Al Kadhim, Harrison B. Prosper</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Simulation-based inference methods that feature correct conditional coverage of confidence sets based on observations that have been compressed to a scalar test statistic require accurate modelling of either the p-value function or the cumulative distribution function (cdf) of the test statistic. If the model of the cdf, which is typically a deep neural network, is a function of the test statistic then the derivative of the neural network with respect to the test statistic furnishes an approximation of the sampling distribution of the test statistic. We explore whether this approach to modelling conditional 1-dimensional sampling distributions is a viable alternative to the probability density-ratio method, also known as the likelihood-ratio trick. Relatively simple, yet effective, neural network models are used whose predictive uncertainty is quantified through a variety of methods.</summary></entry><entry><title type="html">Partial Identification of Individual-Level Parameters Using Aggregate Data in a Nonparametric Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/PartialIdentificationofIndividualLevelParametersUsingAggregateDatainaNonparametricModel.html" rel="alternate" type="text/html" title="Partial Identification of Individual-Level Parameters Using Aggregate Data in a Nonparametric Model" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/PartialIdentificationofIndividualLevelParametersUsingAggregateDatainaNonparametricModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/PartialIdentificationofIndividualLevelParametersUsingAggregateDatainaNonparametricModel.html">&lt;p&gt;It is well known that the relationship between variables at the individual level can be different from the relationship between those same variables aggregated over individuals. In this paper, I develop a methodology to partially identify linear combinations of conditional mean outcomes for individual-level outcomes of interest without imposing parametric assumptions when the researcher only has access to aggregate data. I construct identified sets using an optimization program that allows for researchers to impose additional shape and data restrictions. I also provide consistency results and construct an inference procedure that is valid with data that only provides marginal information about each variable. I apply the methodology to simulated and real-world data sets and find that the estimated identified sets are too wide to be useful, but become narrower as more assumptions are imposed and data aggregated at a finer level is available.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.07236&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sarah Moon</name></author><category term="stat.ME" /><summary type="html">It is well known that the relationship between variables at the individual level can be different from the relationship between those same variables aggregated over individuals. In this paper, I develop a methodology to partially identify linear combinations of conditional mean outcomes for individual-level outcomes of interest without imposing parametric assumptions when the researcher only has access to aggregate data. I construct identified sets using an optimization program that allows for researchers to impose additional shape and data restrictions. I also provide consistency results and construct an inference procedure that is valid with data that only provides marginal information about each variable. I apply the methodology to simulated and real-world data sets and find that the estimated identified sets are too wide to be useful, but become narrower as more assumptions are imposed and data aggregated at a finer level is available.</summary></entry><entry><title type="html">Percentage Coefficient (bp) – Effect Size Analysis (Theory Paper 1)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/PercentageCoefficientbpEffectSizeAnalysisTheoryPaper1.html" rel="alternate" type="text/html" title="Percentage Coefficient (bp) – Effect Size Analysis (Theory Paper 1)" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/PercentageCoefficientbpEffectSizeAnalysisTheoryPaper1</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/PercentageCoefficientbpEffectSizeAnalysisTheoryPaper1.html">&lt;p&gt;Percentage coefficient (bp) has emerged in recent publications as an additional and alternative estimator of effect size for regression analysis. This paper retraces the theory behind the estimator. It’s posited that an estimator must first serve the fundamental function of enabling researchers and readers to comprehend an estimand, the target of estimation. It may then serve the instrumental function of enabling researchers and readers to compare two or more estimands. Defined as the regression coefficient when dependent variable (DV) and independent variable (IV) are both on conceptual 0-1 percentage scales, percentage coefficients (bp) feature 1) clearly comprehendible interpretation and 2) equitable scales for comparison. The coefficient (bp) serves the two functions effectively and efficiently. It thus serves needs unserved by other indicators, such as raw coefficient (bw) and standardized beta.
  Another premise of the functionalist theory is that “effect” is not a monolithic concept. Rather, it is a collection of concepts, each of which measures a component of the conglomerate called “effect”, thereby serving a subfunction. Regression coefficient (b), for example, indicates the unit change in DV associated with a one-unit increase in IV, thereby measuring one aspect called unit effect, aka efficiency. Percentage coefficient (bp) indicates the percentage change in DV associated with a whole scale increase in IV. It is not meant to be an all-encompassing indicator of an all-encompassing concept, but rather a comprehendible and comparable indicator of efficiency, a key aspect of effect.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19495&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinshu Zhao , Dianshi Moses Li , Ze Zack Lai , Piper Liping Liu , Song Harris Ao , Fei You</name></author><category term="stat.AP," /><category term="stat.ME," /><category term="stat.OT" /><summary type="html">Percentage coefficient (bp) has emerged in recent publications as an additional and alternative estimator of effect size for regression analysis. This paper retraces the theory behind the estimator. It’s posited that an estimator must first serve the fundamental function of enabling researchers and readers to comprehend an estimand, the target of estimation. It may then serve the instrumental function of enabling researchers and readers to compare two or more estimands. Defined as the regression coefficient when dependent variable (DV) and independent variable (IV) are both on conceptual 0-1 percentage scales, percentage coefficients (bp) feature 1) clearly comprehendible interpretation and 2) equitable scales for comparison. The coefficient (bp) serves the two functions effectively and efficiently. It thus serves needs unserved by other indicators, such as raw coefficient (bw) and standardized beta. Another premise of the functionalist theory is that “effect” is not a monolithic concept. Rather, it is a collection of concepts, each of which measures a component of the conglomerate called “effect”, thereby serving a subfunction. Regression coefficient (b), for example, indicates the unit change in DV associated with a one-unit increase in IV, thereby measuring one aspect called unit effect, aka efficiency. Percentage coefficient (bp) indicates the percentage change in DV associated with a whole scale increase in IV. It is not meant to be an all-encompassing indicator of an all-encompassing concept, but rather a comprehendible and comparable indicator of efficiency, a key aspect of effect.</summary></entry><entry><title type="html">Permutation time irreversibility in sleep electroencephalograms: Dependence on sleep stage and the effect of equal values</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/PermutationtimeirreversibilityinsleepelectroencephalogramsDependenceonsleepstageandtheeffectofequalvalues.html" rel="alternate" type="text/html" title="Permutation time irreversibility in sleep electroencephalograms: Dependence on sleep stage and the effect of equal values" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/PermutationtimeirreversibilityinsleepelectroencephalogramsDependenceonsleepstageandtheeffectofequalvalues</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/PermutationtimeirreversibilityinsleepelectroencephalogramsDependenceonsleepstageandtheeffectofequalvalues.html">&lt;p&gt;Time irreversibility (TIR) refers to the manifestation of nonequilibrium brain activity influenced by various physiological conditions; however, the influence of sleep on electroencephalogram (EEG) TIR has not been sufficiently investigated. In this paper, a comprehensive study on permutation TIR (pTIR) of EEG data under different sleep stages is conducted. Two basic ordinal patterns (i.e., the original and amplitude permutations) are distinguished to simplify sleep EEGs, and then the influences of equal values and forbidden permutation on pTIR are elucidated. To detect pTIR of brain electric signals, 5 groups of EEGs in the awake, stages I, II, III, and rapid eye movement (REM) stages are collected from the public Polysomnographic Database in PhysioNet. Test results suggested that the pTIR of sleep EEGs significantly decreases as the sleep stage increases (p&amp;lt;0.001), with the awake and REM EEGs, demonstrating greater differences than others. Comparative analysis and numerical simulations support the importance of equal values. Distribution of equal states, a simple quantification of amplitude fluctuations, significantly increases with the sleep stage (p&amp;lt;0.001). If these equalities are ignored, incorrect probabilistic differences may arise in the forward-backward and symmetric permutations of TIR, leading to contradictory results; moreover, the ascending and descending orders for symmetric permutations also lead different outcomes in sleep EEGs. Overall, pTIR in sleep EEGs contributes to our understanding of quantitative TIR and classification of sleep EEGs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02802&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wenpo Yao</name></author><category term="stat.CO" /><summary type="html">Time irreversibility (TIR) refers to the manifestation of nonequilibrium brain activity influenced by various physiological conditions; however, the influence of sleep on electroencephalogram (EEG) TIR has not been sufficiently investigated. In this paper, a comprehensive study on permutation TIR (pTIR) of EEG data under different sleep stages is conducted. Two basic ordinal patterns (i.e., the original and amplitude permutations) are distinguished to simplify sleep EEGs, and then the influences of equal values and forbidden permutation on pTIR are elucidated. To detect pTIR of brain electric signals, 5 groups of EEGs in the awake, stages I, II, III, and rapid eye movement (REM) stages are collected from the public Polysomnographic Database in PhysioNet. Test results suggested that the pTIR of sleep EEGs significantly decreases as the sleep stage increases (p&amp;lt;0.001), with the awake and REM EEGs, demonstrating greater differences than others. Comparative analysis and numerical simulations support the importance of equal values. Distribution of equal states, a simple quantification of amplitude fluctuations, significantly increases with the sleep stage (p&amp;lt;0.001). If these equalities are ignored, incorrect probabilistic differences may arise in the forward-backward and symmetric permutations of TIR, leading to contradictory results; moreover, the ascending and descending orders for symmetric permutations also lead different outcomes in sleep EEGs. Overall, pTIR in sleep EEGs contributes to our understanding of quantitative TIR and classification of sleep EEGs.</summary></entry><entry><title type="html">Power-Enhanced Two-Sample Mean Tests for High-Dimensional Compositional Data with Application to Microbiome Data Analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/PowerEnhancedTwoSampleMeanTestsforHighDimensionalCompositionalDatawithApplicationtoMicrobiomeDataAnalysis.html" rel="alternate" type="text/html" title="Power-Enhanced Two-Sample Mean Tests for High-Dimensional Compositional Data with Application to Microbiome Data Analysis" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/PowerEnhancedTwoSampleMeanTestsforHighDimensionalCompositionalDatawithApplicationtoMicrobiomeDataAnalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/PowerEnhancedTwoSampleMeanTestsforHighDimensionalCompositionalDatawithApplicationtoMicrobiomeDataAnalysis.html">&lt;p&gt;Testing differences in mean vectors is a fundamental task in the analysis of high-dimensional compositional data. Existing methods may suffer from low power if the underlying signal pattern is in a situation that does not favor the deployed test. In this work, we develop two-sample power-enhanced mean tests for high-dimensional compositional data based on the combination of $p$-values, which integrates strengths from two popular types of tests: the maximum-type test and the quadratic-type test. We provide rigorous theoretical guarantees on the proposed tests, showing accurate Type-I error rate control and enhanced testing power. Our method boosts the testing power towards a broader alternative space, which yields robust performance across a wide range of signal pattern settings. Our theory also contributes to the literature on power enhancement and Gaussian approximation for high-dimensional hypothesis testing. We demonstrate the performance of our method on both simulated data and real-world microbiome data, showing that our proposed approach improves the testing power substantially compared to existing methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02551&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Danning Li, Lingzhou Xue, Haoyi Yang, Xiufan Yu</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.TH" /><summary type="html">Testing differences in mean vectors is a fundamental task in the analysis of high-dimensional compositional data. Existing methods may suffer from low power if the underlying signal pattern is in a situation that does not favor the deployed test. In this work, we develop two-sample power-enhanced mean tests for high-dimensional compositional data based on the combination of $p$-values, which integrates strengths from two popular types of tests: the maximum-type test and the quadratic-type test. We provide rigorous theoretical guarantees on the proposed tests, showing accurate Type-I error rate control and enhanced testing power. Our method boosts the testing power towards a broader alternative space, which yields robust performance across a wide range of signal pattern settings. Our theory also contributes to the literature on power enhancement and Gaussian approximation for high-dimensional hypothesis testing. We demonstrate the performance of our method on both simulated data and real-world microbiome data, showing that our proposed approach improves the testing power substantially compared to existing methods.</summary></entry><entry><title type="html">Regime Identification for Improving Causal Analysis in Non-stationary Timeseries</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/RegimeIdentificationforImprovingCausalAnalysisinNonstationaryTimeseries.html" rel="alternate" type="text/html" title="Regime Identification for Improving Causal Analysis in Non-stationary Timeseries" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/RegimeIdentificationforImprovingCausalAnalysisinNonstationaryTimeseries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/RegimeIdentificationforImprovingCausalAnalysisinNonstationaryTimeseries.html">&lt;p&gt;Time series data from real-world systems often display non-stationary behavior, indicating varying statistical characteristics over time. This inherent variability poses significant challenges in deciphering the underlying structural relationships within the data, particularly in correlation and causality analyses, model stability, etc. Recognizing distinct segments or regimes within multivariate time series data, characterized by relatively stable behavior and consistent statistical properties over extended periods, becomes crucial. In this study, we apply the regime identification (RegID) technique to multivariate time series, fundamentally designed to unveil locally stationary segments within data. The distinguishing features between regimes are identified using covariance matrices in a Riemannian space. We aim to highlight how regime identification contributes to improving the discovery of causal structures from multivariate non-stationary time series data. Our experiments, encompassing both synthetic and real-world datasets, highlight the effectiveness of regime-wise time series causal analysis. We validate our approach by first demonstrating improved causal structure discovery using synthetic data where the ground truth causal relationships are known. Subsequently, we apply this methodology to climate-ecosystem dataset, showcasing its applicability in real-world scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02315&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wasim Ahmad, Maha Shadaydeh, Joachim Denzler</name></author><category term="stat.AP" /><summary type="html">Time series data from real-world systems often display non-stationary behavior, indicating varying statistical characteristics over time. This inherent variability poses significant challenges in deciphering the underlying structural relationships within the data, particularly in correlation and causality analyses, model stability, etc. Recognizing distinct segments or regimes within multivariate time series data, characterized by relatively stable behavior and consistent statistical properties over extended periods, becomes crucial. In this study, we apply the regime identification (RegID) technique to multivariate time series, fundamentally designed to unveil locally stationary segments within data. The distinguishing features between regimes are identified using covariance matrices in a Riemannian space. We aim to highlight how regime identification contributes to improving the discovery of causal structures from multivariate non-stationary time series data. Our experiments, encompassing both synthetic and real-world datasets, highlight the effectiveness of regime-wise time series causal analysis. We validate our approach by first demonstrating improved causal structure discovery using synthetic data where the ground truth causal relationships are known. Subsequently, we apply this methodology to climate-ecosystem dataset, showcasing its applicability in real-world scenarios.</summary></entry><entry><title type="html">Reversibility of elliptical slice sampling revisited</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Reversibilityofellipticalslicesamplingrevisited.html" rel="alternate" type="text/html" title="Reversibility of elliptical slice sampling revisited" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Reversibilityofellipticalslicesamplingrevisited</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Reversibilityofellipticalslicesamplingrevisited.html">&lt;p&gt;We extend elliptical slice sampling, a Markov chain transition kernel suggested in Murray, Adams and MacKay 2010, to infinite-dimensional separable Hilbert spaces and discuss its well-definedness. We point to a regularity requirement, provide an alternative proof of the desirable reversibility property and show that it induces a positive semi-definite Markov operator. Crucial within the proof of the formerly mentioned results is the analysis of a shrinkage Markov chain that may be interesting on its own.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.02426&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mareike Hasenpflug, Viacheslav Telezhnikov, Daniel Rudolf</name></author><category term="stat.CO," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">We extend elliptical slice sampling, a Markov chain transition kernel suggested in Murray, Adams and MacKay 2010, to infinite-dimensional separable Hilbert spaces and discuss its well-definedness. We point to a regularity requirement, provide an alternative proof of the desirable reversibility property and show that it induces a positive semi-definite Markov operator. Crucial within the proof of the formerly mentioned results is the analysis of a shrinkage Markov chain that may be interesting on its own.</summary></entry><entry><title type="html">Singularity and Error Analysis of a Simple Quaternion Estimator</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SingularityandErrorAnalysisofaSimpleQuaternionEstimator.html" rel="alternate" type="text/html" title="Singularity and Error Analysis of a Simple Quaternion Estimator" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SingularityandErrorAnalysisofaSimpleQuaternionEstimator</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SingularityandErrorAnalysisofaSimpleQuaternionEstimator.html">&lt;p&gt;A novel single-frame quaternion estimator processing two vector observations is introduced. The singular cases are examined, and appropriate rotational solutions are provided. Additionally, an alternative method involving sequential rotation is introduced to manage these singularities. The simplicity of the estimator enables clear physical insights and a closed-form expression for the bias as a function of the quaternion error covariance matrix. The covariance could be approximated up to second order with respect to the underlying measurement noise assuming arbitrary probability distribution. The current note relaxes the second-order assumption and provides an expression for the error covariance that is exact to the fourth order, under the assumption of Gaussian distribution. A comprehensive derivation of the individual components of the quaternion additive error covariance matrix is presented. This not only provides increased accuracy but also alleviates issues related to singularity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.01150&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Caitong Peng, Daniel Choukroun</name></author><category term="stat.ME" /><summary type="html">A novel single-frame quaternion estimator processing two vector observations is introduced. The singular cases are examined, and appropriate rotational solutions are provided. Additionally, an alternative method involving sequential rotation is introduced to manage these singularities. The simplicity of the estimator enables clear physical insights and a closed-form expression for the bias as a function of the quaternion error covariance matrix. The covariance could be approximated up to second order with respect to the underlying measurement noise assuming arbitrary probability distribution. The current note relaxes the second-order assumption and provides an expression for the error covariance that is exact to the fourth order, under the assumption of Gaussian distribution. A comprehensive derivation of the individual components of the quaternion additive error covariance matrix is presented. This not only provides increased accuracy but also alleviates issues related to singularity.</summary></entry><entry><title type="html">Soft Phenotyping for Sepsis via EHR Time-aware Soft Clustering</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SoftPhenotypingforSepsisviaEHRTimeawareSoftClustering.html" rel="alternate" type="text/html" title="Soft Phenotyping for Sepsis via EHR Time-aware Soft Clustering" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SoftPhenotypingforSepsisviaEHRTimeawareSoftClustering</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SoftPhenotypingforSepsisviaEHRTimeawareSoftClustering.html">&lt;p&gt;Objective: Sepsis is one of the most serious hospital conditions associated with high mortality. Sepsis is the result of a dysregulated immune response to infection that can lead to multiple organ dysfunction and death. Due to the wide variability in the causes of sepsis, clinical presentation, and the recovery trajectories, identifying sepsis sub-phenotypes is crucial to advance our understanding of sepsis characterization, to choose targeted treatments and optimal timing of interventions, and to improve prognostication. Prior studies have described different sub-phenotypes of sepsis using organ-specific characteristics. These studies applied clustering algorithms to electronic health records (EHRs) to identify disease sub-phenotypes. However, prior approaches did not capture temporal information and made uncertain assumptions about the relationships among the sub-phenotypes for clustering procedures.
  Methods: We developed a time-aware soft clustering algorithm guided by clinical variables to identify sepsis sub-phenotypes using data available in the EHR.
  Results: We identified six novel sepsis hybrid sub-phenotypes and evaluated them for medical plausibility. In addition, we built an early-warning sepsis prediction model using logistic regression.
  Conclusion: Our results suggest that these novel sepsis hybrid sub-phenotypes are promising to provide more accurate information on sepsis-related organ dysfunction and sepsis recovery trajectories which can be important to inform management decisions and sepsis prognosis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.08629&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shiyi Jiang, Xin Gai, Miriam Treggiari, William W. Stead, Yuankang Zhao, C. David Page, Anru R. Zhang</name></author><category term="stat.AP" /><summary type="html">Objective: Sepsis is one of the most serious hospital conditions associated with high mortality. Sepsis is the result of a dysregulated immune response to infection that can lead to multiple organ dysfunction and death. Due to the wide variability in the causes of sepsis, clinical presentation, and the recovery trajectories, identifying sepsis sub-phenotypes is crucial to advance our understanding of sepsis characterization, to choose targeted treatments and optimal timing of interventions, and to improve prognostication. Prior studies have described different sub-phenotypes of sepsis using organ-specific characteristics. These studies applied clustering algorithms to electronic health records (EHRs) to identify disease sub-phenotypes. However, prior approaches did not capture temporal information and made uncertain assumptions about the relationships among the sub-phenotypes for clustering procedures. Methods: We developed a time-aware soft clustering algorithm guided by clinical variables to identify sepsis sub-phenotypes using data available in the EHR. Results: We identified six novel sepsis hybrid sub-phenotypes and evaluated them for medical plausibility. In addition, we built an early-warning sepsis prediction model using logistic regression. Conclusion: Our results suggest that these novel sepsis hybrid sub-phenotypes are promising to provide more accurate information on sepsis-related organ dysfunction and sepsis recovery trajectories which can be important to inform management decisions and sepsis prognosis.</summary></entry><entry><title type="html">Some Statistical and Data Challenges When Building Early-Stage Digital Experimentation and Measurement Capabilities</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SomeStatisticalandDataChallengesWhenBuildingEarlyStageDigitalExperimentationandMeasurementCapabilities.html" rel="alternate" type="text/html" title="Some Statistical and Data Challenges When Building Early-Stage Digital Experimentation and Measurement Capabilities" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SomeStatisticalandDataChallengesWhenBuildingEarlyStageDigitalExperimentationandMeasurementCapabilities</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SomeStatisticalandDataChallengesWhenBuildingEarlyStageDigitalExperimentationandMeasurementCapabilities.html">&lt;p&gt;Digital experimentation and measurement (DEM) capabilities – the knowledge and tools necessary to run experiments with digital products, services, or experiences and measure their impact – are fast becoming part of the standard toolkit of digital/data-driven organisations in guiding business decisions. Many large technology companies report having mature DEM capabilities, and several businesses have been established purely to manage experiments for others. Given the growing evidence that data-driven organisations tend to outperform their non-data-driven counterparts, there has never been a greater need for organisations to build/acquire DEM capabilities to thrive in the current digital era.
  This thesis presents several novel approaches to statistical and data challenges for organisations building DEM capabilities. We focus on the fundamentals associated with building DEM capabilities, which lead to a richer understanding of the underlying assumptions and thus enable us to develop more appropriate capabilities. We address why one should engage in DEM by quantifying the benefits and risks of acquiring DEM capabilities. This is done using a ranking under lower uncertainty model, enabling one to construct a business case. We also examine what ingredients are necessary to run digital experiments. In addition to clarifying the existing literature around statistical tests, datasets, and methods in experimental design and causal inference, we construct an additional dataset and detailed case studies on applying state-of-the-art methods. Finally, we investigate when a digital experiment design would outperform another, leading to an evaluation framework that compares competing designs’ data efficiency.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03579&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>C. H. Bryan Liu</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Digital experimentation and measurement (DEM) capabilities – the knowledge and tools necessary to run experiments with digital products, services, or experiences and measure their impact – are fast becoming part of the standard toolkit of digital/data-driven organisations in guiding business decisions. Many large technology companies report having mature DEM capabilities, and several businesses have been established purely to manage experiments for others. Given the growing evidence that data-driven organisations tend to outperform their non-data-driven counterparts, there has never been a greater need for organisations to build/acquire DEM capabilities to thrive in the current digital era. This thesis presents several novel approaches to statistical and data challenges for organisations building DEM capabilities. We focus on the fundamentals associated with building DEM capabilities, which lead to a richer understanding of the underlying assumptions and thus enable us to develop more appropriate capabilities. We address why one should engage in DEM by quantifying the benefits and risks of acquiring DEM capabilities. This is done using a ranking under lower uncertainty model, enabling one to construct a business case. We also examine what ingredients are necessary to run digital experiments. In addition to clarifying the existing literature around statistical tests, datasets, and methods in experimental design and causal inference, we construct an additional dataset and detailed case studies on applying state-of-the-art methods. Finally, we investigate when a digital experiment design would outperform another, leading to an evaluation framework that compares competing designs’ data efficiency.</summary></entry><entry><title type="html">Spatial Heterogeneous Additive Partial Linear Model: A Joint Approach of Bivariate Spline and Forest Lasso</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SpatialHeterogeneousAdditivePartialLinearModelAJointApproachofBivariateSplineandForestLasso.html" rel="alternate" type="text/html" title="Spatial Heterogeneous Additive Partial Linear Model: A Joint Approach of Bivariate Spline and Forest Lasso" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SpatialHeterogeneousAdditivePartialLinearModelAJointApproachofBivariateSplineandForestLasso</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SpatialHeterogeneousAdditivePartialLinearModelAJointApproachofBivariateSplineandForestLasso.html">&lt;p&gt;Identifying spatial heterogeneous patterns has attracted a surge of research interest in recent years, due to its important applications in various scientific and engineering fields. In practice the spatially heterogeneous components are often mixed with components which are spatially smooth, making the task of identifying the heterogeneous regions more challenging. In this paper, we develop an efficient clustering approach to identify the model heterogeneity of the spatial additive partial linear model. Specifically, we aim to detect the spatially contiguous clusters based on the regression coefficients while introducing a spatially varying intercept to deal with the smooth spatial effect. On the one hand, to approximate the spatial varying intercept, we use the method of bivariate spline over triangulation, which can effectively handle the data from a complex domain. On the other hand, a novel fusion penalty termed the forest lasso is proposed to reveal the spatial clustering pattern. Our proposed fusion penalty has advantages in both the estimation and computation efficiencies when dealing with large spatial data. Theoretically properties of our estimator are established, and simulation results show that our approach can achieve more accurate estimation with a limited computation cost compared with the existing approaches. To illustrate its practical use, we apply our approach to analyze the spatial pattern of the relationship between land surface temperature measured by satellites and air temperature measured by ground stations in the United States.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.11579&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xin Zhang, Shan Yu, Zhengyuan Zhu, Xin Wang</name></author><category term="stat.ME" /><summary type="html">Identifying spatial heterogeneous patterns has attracted a surge of research interest in recent years, due to its important applications in various scientific and engineering fields. In practice the spatially heterogeneous components are often mixed with components which are spatially smooth, making the task of identifying the heterogeneous regions more challenging. In this paper, we develop an efficient clustering approach to identify the model heterogeneity of the spatial additive partial linear model. Specifically, we aim to detect the spatially contiguous clusters based on the regression coefficients while introducing a spatially varying intercept to deal with the smooth spatial effect. On the one hand, to approximate the spatial varying intercept, we use the method of bivariate spline over triangulation, which can effectively handle the data from a complex domain. On the other hand, a novel fusion penalty termed the forest lasso is proposed to reveal the spatial clustering pattern. Our proposed fusion penalty has advantages in both the estimation and computation efficiencies when dealing with large spatial data. Theoretically properties of our estimator are established, and simulation results show that our approach can achieve more accurate estimation with a limited computation cost compared with the existing approaches. To illustrate its practical use, we apply our approach to analyze the spatial pattern of the relationship between land surface temperature measured by satellites and air temperature measured by ground stations in the United States.</summary></entry><entry><title type="html">Square-Root Higher-Order Unscented Estimators for Robust Orbit Determination</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SquareRootHigherOrderUnscentedEstimatorsforRobustOrbitDetermination.html" rel="alternate" type="text/html" title="Square-Root Higher-Order Unscented Estimators for Robust Orbit Determination" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SquareRootHigherOrderUnscentedEstimatorsforRobustOrbitDetermination</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/SquareRootHigherOrderUnscentedEstimatorsforRobustOrbitDetermination.html">&lt;p&gt;Orbit determination (OD) is a fundamental problem in space surveillance and tracking, crucial for ensuring the safety of space assets. Real-world ground-based optical tracking scenarios often involve challenges such as limited measurement time, short visible arcs, and the presence of outliers, leading to sparse and non-Gaussian observational data. Additionally, the highly perturbative and nonlinear orbit dynamics of resident space objects (RSOs) in low Earth orbit (LEO) add further complexity to the OD problem.
  This paper introduces a novel variant of the higher-order unscented Kalman estimator (HOUSE) called $w$-HOUSE, which employs a square-root formulation and addresses the challenges posed by nonlinear and non-Gaussian OD problems. The effectiveness of $w$-HOUSE was demonstrated through synthetic and real-world measurements, specifically outlier-contaminated angle-only measurements collected for the Sentinel 6A satellite flying in LEO. Comparative analyses are conducted with the original HOUSE (referred to as $\delta$-HOUSE), unscented Kalman filters (UKF), conjugate unscented transformation (CUT) filters, and precise orbit determination solutions estimated via onboard global navigation satellite systems measurements.
  The results reveal that the proposed $w$-HOUSE filter exhibits greater robustness when dealing with varying values of the dependent parameter compared to the original $\delta$-HOUSE. Moreover, it surpasses all other filters in terms of positioning accuracy, achieving three-dimensional root-mean-square errors of less than 60 m in a three-day scenario. This research suggests that the new $w$-HOUSE filter represents a viable alternative to UKF and CUT filters, offering improved positioning performance in handling the nonlinear and non-Gaussian OD problem associated with LEO RSOs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.10452&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yang Yang</name></author><category term="stat.AP" /><summary type="html">Orbit determination (OD) is a fundamental problem in space surveillance and tracking, crucial for ensuring the safety of space assets. Real-world ground-based optical tracking scenarios often involve challenges such as limited measurement time, short visible arcs, and the presence of outliers, leading to sparse and non-Gaussian observational data. Additionally, the highly perturbative and nonlinear orbit dynamics of resident space objects (RSOs) in low Earth orbit (LEO) add further complexity to the OD problem. This paper introduces a novel variant of the higher-order unscented Kalman estimator (HOUSE) called $w$-HOUSE, which employs a square-root formulation and addresses the challenges posed by nonlinear and non-Gaussian OD problems. The effectiveness of $w$-HOUSE was demonstrated through synthetic and real-world measurements, specifically outlier-contaminated angle-only measurements collected for the Sentinel 6A satellite flying in LEO. Comparative analyses are conducted with the original HOUSE (referred to as $\delta$-HOUSE), unscented Kalman filters (UKF), conjugate unscented transformation (CUT) filters, and precise orbit determination solutions estimated via onboard global navigation satellite systems measurements. The results reveal that the proposed $w$-HOUSE filter exhibits greater robustness when dealing with varying values of the dependent parameter compared to the original $\delta$-HOUSE. Moreover, it surpasses all other filters in terms of positioning accuracy, achieving three-dimensional root-mean-square errors of less than 60 m in a three-day scenario. This research suggests that the new $w$-HOUSE filter represents a viable alternative to UKF and CUT filters, offering improved positioning performance in handling the nonlinear and non-Gaussian OD problem associated with LEO RSOs.</summary></entry><entry><title type="html">Stability of a Generalized Debiased Lasso with Applications to Resampling-Based Variable Selection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/StabilityofaGeneralizedDebiasedLassowithApplicationstoResamplingBasedVariableSelection.html" rel="alternate" type="text/html" title="Stability of a Generalized Debiased Lasso with Applications to Resampling-Based Variable Selection" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/StabilityofaGeneralizedDebiasedLassowithApplicationstoResamplingBasedVariableSelection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/StabilityofaGeneralizedDebiasedLassowithApplicationstoResamplingBasedVariableSelection.html">&lt;p&gt;Suppose that we first apply the Lasso to a design matrix, and then update one of its columns. In general, the signs of the Lasso coefficients may change, and there is no closed-form expression for updating the Lasso solution exactly. In this work, we propose an approximate formula for updating a debiased Lasso coefficient. We provide general nonasymptotic error bounds in terms of the norms and correlations of a given design matrix’s columns, and then prove asymptotic convergence results for the case of a random design matrix with i.i.d.\ sub-Gaussian row vectors and i.i.d.\ Gaussian noise. Notably, the approximate formula is asymptotically correct for most coordinates in the proportional growth regime, under the mild assumption that each row of the design matrix is sub-Gaussian with a covariance matrix having a bounded condition number. Our proof only requires certain concentration and anti-concentration properties to control various error terms and the number of sign changes. In contrast, rigorously establishing distributional limit properties (e.g.\ Gaussian limits for the debiased Lasso) under similarly general assumptions has been considered open problem in the universality theory. As applications, we show that the approximate formula allows us to reduce the computation complexity of variable selection algorithms that require solving multiple Lasso problems, such as the conditional randomization test and a variant of the knockoff filter.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03063&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jingbo Liu</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Suppose that we first apply the Lasso to a design matrix, and then update one of its columns. In general, the signs of the Lasso coefficients may change, and there is no closed-form expression for updating the Lasso solution exactly. In this work, we propose an approximate formula for updating a debiased Lasso coefficient. We provide general nonasymptotic error bounds in terms of the norms and correlations of a given design matrix’s columns, and then prove asymptotic convergence results for the case of a random design matrix with i.i.d.\ sub-Gaussian row vectors and i.i.d.\ Gaussian noise. Notably, the approximate formula is asymptotically correct for most coordinates in the proportional growth regime, under the mild assumption that each row of the design matrix is sub-Gaussian with a covariance matrix having a bounded condition number. Our proof only requires certain concentration and anti-concentration properties to control various error terms and the number of sign changes. In contrast, rigorously establishing distributional limit properties (e.g.\ Gaussian limits for the debiased Lasso) under similarly general assumptions has been considered open problem in the universality theory. As applications, we show that the approximate formula allows us to reduce the computation complexity of variable selection algorithms that require solving multiple Lasso problems, such as the conditional randomization test and a variant of the knockoff filter.</summary></entry><entry><title type="html">Statistical Edge Detection And UDF Learning For Shape Representation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/StatisticalEdgeDetectionAndUDFLearningForShapeRepresentation.html" rel="alternate" type="text/html" title="Statistical Edge Detection And UDF Learning For Shape Representation" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/StatisticalEdgeDetectionAndUDFLearningForShapeRepresentation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/StatisticalEdgeDetectionAndUDFLearningForShapeRepresentation.html">&lt;p&gt;In the field of computer vision, the numerical encoding of 3D surfaces is crucial. It is classical to represent surfaces with their Signed Distance Functions (SDFs) or Unsigned Distance Functions (UDFs). For tasks like representation learning, surface classification, or surface reconstruction, this function can be learned by a neural network, called Neural Distance Function. This network, and in particular its weights, may serve as a parametric and implicit representation for the surface. The network must represent the surface as accurately as possible. In this paper, we propose a method for learning UDFs that improves the fidelity of the obtained Neural UDF to the original 3D surface. The key idea of our method is to concentrate the learning effort of the Neural UDF on surface edges.  More precisely, we show that sampling more training points around surface edges allows better local accuracy of the trained Neural UDF, and thus improves the global expressiveness of the Neural UDF in terms of Hausdorff distance. To detect surface edges, we propose a new statistical method based on the calculation of a $p$-value at each point on the surface. Our method is shown to detect surface edges more accurately than a commonly used local geometric descriptor.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03381&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Virgile Foy , Fabrice Gamboa , Reda Chhaibi</name></author><category term="stat.AP" /><summary type="html">In the field of computer vision, the numerical encoding of 3D surfaces is crucial. It is classical to represent surfaces with their Signed Distance Functions (SDFs) or Unsigned Distance Functions (UDFs). For tasks like representation learning, surface classification, or surface reconstruction, this function can be learned by a neural network, called Neural Distance Function. This network, and in particular its weights, may serve as a parametric and implicit representation for the surface. The network must represent the surface as accurately as possible. In this paper, we propose a method for learning UDFs that improves the fidelity of the obtained Neural UDF to the original 3D surface. The key idea of our method is to concentrate the learning effort of the Neural UDF on surface edges. More precisely, we show that sampling more training points around surface edges allows better local accuracy of the trained Neural UDF, and thus improves the global expressiveness of the Neural UDF in terms of Hausdorff distance. To detect surface edges, we propose a new statistical method based on the calculation of a $p$-value at each point on the surface. Our method is shown to detect surface edges more accurately than a commonly used local geometric descriptor.</summary></entry><entry><title type="html">Statistical Principles for Platform Trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/StatisticalPrinciplesforPlatformTrials.html" rel="alternate" type="text/html" title="Statistical Principles for Platform Trials" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/StatisticalPrinciplesforPlatformTrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/StatisticalPrinciplesforPlatformTrials.html">&lt;p&gt;While within a clinical study there may be multiple doses and endpoints, across different studies each study will result in either an approval or a lack of approval of the drug compound studied. The False Approval Rate (FAR) is the proportion of drug compounds that lack efficacy incorrectly approved by regulators. (In the U.S., compounds that have efficacy and are approved are not involved in the FAR consideration, according to our reading of the relevant U.S. Congressional statute).
  While Tukey’s (1953) Error Rate Familwise (ERFw) is meant to be applied within a clinical study, Tukey’s (1953) Error Rate per Family (ERpF), defined alongside ERFw,is meant to be applied across studies. We show that controlling Error Rate Familwise (ERFw) within a clinical study at 5% in turn controls Error Rate per Family (ERpF) across studies at 5-per-100, regardless of whether the studies are correlated or not. Further, we show that ongoing regulatory practice, the additive multiplicity adjustment method of controlling ERpF, is controlling False Approval Rate FAR exactly (not conservatively) at 5-per-100 (even for Platform trials).
  In contrast, if a regulatory agency chooses to control the False Discovery Rate (FDR) across studies at 5% instead, then this change in policy from ERpF control to FDR control will result in incorrectly approving drug compounds that lack efficacy at a rate higher than 5-per-100, because in essence it gives the industry additional rewards for successfully developing compounds that have efficacy and are approved. Seems to us the discussion of such a change in policy would be at a level higher than merely statistical, needing harmonizsation/harmonization. (In the U.S., policy is set by the Congress.)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.12728&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinping Cui, Emily Ouyang, Yi Liu, Jingjing Schneider, Hong Tian, Bushi Wang, Jason C. Hsu</name></author><category term="stat.ME" /><summary type="html">While within a clinical study there may be multiple doses and endpoints, across different studies each study will result in either an approval or a lack of approval of the drug compound studied. The False Approval Rate (FAR) is the proportion of drug compounds that lack efficacy incorrectly approved by regulators. (In the U.S., compounds that have efficacy and are approved are not involved in the FAR consideration, according to our reading of the relevant U.S. Congressional statute). While Tukey’s (1953) Error Rate Familwise (ERFw) is meant to be applied within a clinical study, Tukey’s (1953) Error Rate per Family (ERpF), defined alongside ERFw,is meant to be applied across studies. We show that controlling Error Rate Familwise (ERFw) within a clinical study at 5% in turn controls Error Rate per Family (ERpF) across studies at 5-per-100, regardless of whether the studies are correlated or not. Further, we show that ongoing regulatory practice, the additive multiplicity adjustment method of controlling ERpF, is controlling False Approval Rate FAR exactly (not conservatively) at 5-per-100 (even for Platform trials). In contrast, if a regulatory agency chooses to control the False Discovery Rate (FDR) across studies at 5% instead, then this change in policy from ERpF control to FDR control will result in incorrectly approving drug compounds that lack efficacy at a rate higher than 5-per-100, because in essence it gives the industry additional rewards for successfully developing compounds that have efficacy and are approved. Seems to us the discussion of such a change in policy would be at a level higher than merely statistical, needing harmonizsation/harmonization. (In the U.S., policy is set by the Congress.)</summary></entry><entry><title type="html">Strang Splitting for Parametric Inference in Second-order Stochastic Differential Equations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/StrangSplittingforParametricInferenceinSecondorderStochasticDifferentialEquations.html" rel="alternate" type="text/html" title="Strang Splitting for Parametric Inference in Second-order Stochastic Differential Equations" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/StrangSplittingforParametricInferenceinSecondorderStochasticDifferentialEquations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/StrangSplittingforParametricInferenceinSecondorderStochasticDifferentialEquations.html">&lt;p&gt;We address parameter estimation in second-order stochastic differential equations (SDEs), prevalent in physics, biology, and ecology. Second-order SDE is converted to a first-order system by introducing an auxiliary velocity variable raising two main challenges. First, the system is hypoelliptic since the noise affects only the velocity, making the Euler-Maruyama estimator ill-conditioned. To overcome that, we propose an estimator based on the Strang splitting scheme. Second, since the velocity is rarely observed we adjust the estimator for partial observations. We present four estimators for complete and partial observations, using full likelihood or only velocity marginal likelihood. These estimators are intuitive, easy to implement, and computationally fast, and we prove their consistency and asymptotic normality. Our analysis demonstrates that using full likelihood with complete observations reduces the asymptotic variance of the diffusion estimator. With partial observations, the asymptotic variance increases due to information loss but remains unaffected by the likelihood choice. However, a numerical study on the Kramers oscillator reveals that using marginal likelihood for partial observations yields less biased estimators. We apply our approach to paleoclimate data from the Greenland ice core and fit it to the Kramers oscillator model, capturing transitions between metastable states reflecting observed climatic conditions during glacial eras.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03606&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Predrag Pilipovic, Adeline Samson, Susanne Ditlevsen</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We address parameter estimation in second-order stochastic differential equations (SDEs), prevalent in physics, biology, and ecology. Second-order SDE is converted to a first-order system by introducing an auxiliary velocity variable raising two main challenges. First, the system is hypoelliptic since the noise affects only the velocity, making the Euler-Maruyama estimator ill-conditioned. To overcome that, we propose an estimator based on the Strang splitting scheme. Second, since the velocity is rarely observed we adjust the estimator for partial observations. We present four estimators for complete and partial observations, using full likelihood or only velocity marginal likelihood. These estimators are intuitive, easy to implement, and computationally fast, and we prove their consistency and asymptotic normality. Our analysis demonstrates that using full likelihood with complete observations reduces the asymptotic variance of the diffusion estimator. With partial observations, the asymptotic variance increases due to information loss but remains unaffected by the likelihood choice. However, a numerical study on the Kramers oscillator reveals that using marginal likelihood for partial observations yields less biased estimators. We apply our approach to paleoclimate data from the Greenland ice core and fit it to the Kramers oscillator model, capturing transitions between metastable states reflecting observed climatic conditions during glacial eras.</summary></entry><entry><title type="html">The Analysis of Criminal Recidivism: A Hierarchical Model-Based Approach for the Analysis of Zero-Inflated, Spatially Correlated recurrent events Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/TheAnalysisofCriminalRecidivismAHierarchicalModelBasedApproachfortheAnalysisofZeroInflatedSpatiallyCorrelatedrecurrenteventsData.html" rel="alternate" type="text/html" title="The Analysis of Criminal Recidivism: A Hierarchical Model-Based Approach for the Analysis of Zero-Inflated, Spatially Correlated recurrent events Data" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/TheAnalysisofCriminalRecidivismAHierarchicalModelBasedApproachfortheAnalysisofZeroInflatedSpatiallyCorrelatedrecurrenteventsData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/TheAnalysisofCriminalRecidivismAHierarchicalModelBasedApproachfortheAnalysisofZeroInflatedSpatiallyCorrelatedrecurrenteventsData.html">&lt;p&gt;The life course perspective in criminology has become prominent last years, offering valuable insights into various patterns of criminal offending and pathways. The study of criminal trajectories aims to understand the beginning, persistence and desistence in crime, providing intriguing explanations about these moments in life. Central to this analysis is the identification of patterns in the frequency of criminal victimization and recidivism, along with the factors that contribute to them. Specifically, this work introduces a new class of models that overcome limitations in traditional methods used to analyze criminal recidivism. These models are designed for recurrent events data characterized by excess of zeros and spatial correlation. They extend the Non-Homogeneous Poisson Process, incorporating spatial dependence in the model through random effects, enabling the analysis of associations among individuals within the same spatial stratum. To deal with the excess of zeros in the data, a zero-inflated Poisson mixed model was incorporated. In addition to parametric models following the Power Law process for baseline intensity functions, we propose flexible semi-parametric versions approximating the intensity function using Bernstein Polynomials. The Bayesian approach offers advantages such as incorporating external evidence and modeling specific correlations between random effects and observed data. The performance of these models was evaluated in a simulation study with various scenarios, and we applied them to analyze criminal recidivism data in the Metropolitan Region of Belo Horizonte, Brazil. The results provide a detailed analysis of high-risk areas for recurrent crimes and the behavior of recidivism rates over time. This research significantly enhances our understanding of criminal trajectories, paving the way for more effective strategies in combating criminal recidivism.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02666&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alisson C. C. Silva, Fábio N. Demarqui, Bráulio F. Silva, Marcos O. Prates</name></author><category term="stat.ME" /><summary type="html">The life course perspective in criminology has become prominent last years, offering valuable insights into various patterns of criminal offending and pathways. The study of criminal trajectories aims to understand the beginning, persistence and desistence in crime, providing intriguing explanations about these moments in life. Central to this analysis is the identification of patterns in the frequency of criminal victimization and recidivism, along with the factors that contribute to them. Specifically, this work introduces a new class of models that overcome limitations in traditional methods used to analyze criminal recidivism. These models are designed for recurrent events data characterized by excess of zeros and spatial correlation. They extend the Non-Homogeneous Poisson Process, incorporating spatial dependence in the model through random effects, enabling the analysis of associations among individuals within the same spatial stratum. To deal with the excess of zeros in the data, a zero-inflated Poisson mixed model was incorporated. In addition to parametric models following the Power Law process for baseline intensity functions, we propose flexible semi-parametric versions approximating the intensity function using Bernstein Polynomials. The Bayesian approach offers advantages such as incorporating external evidence and modeling specific correlations between random effects and observed data. The performance of these models was evaluated in a simulation study with various scenarios, and we applied them to analyze criminal recidivism data in the Metropolitan Region of Belo Horizonte, Brazil. The results provide a detailed analysis of high-risk areas for recurrent crimes and the behavior of recidivism rates over time. This research significantly enhances our understanding of criminal trajectories, paving the way for more effective strategies in combating criminal recidivism.</summary></entry><entry><title type="html">The SIDO Performance Model for League of Legends</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/TheSIDOPerformanceModelforLeagueofLegends.html" rel="alternate" type="text/html" title="The SIDO Performance Model for League of Legends" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/TheSIDOPerformanceModelforLeagueofLegends</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/TheSIDOPerformanceModelforLeagueofLegends.html">&lt;p&gt;League of Legends (LoL) has been a dominant esport for a decade, yet the inherent complexity of the game has stymied the creation of analytical measures of player skill and performance. Current industry standards are limited to easy-to-procure individual player statistics that are incomplete and lacking context as they do not take into account teamplay or game state. We present a unified performance model for League of Legends which blends together measures of a player’s contribution within the context of their team, insights from traditional sports metrics such as the Plus-Minus model, and the intricacies of LoL as a complex team invasion sport. Using hierarchical Bayesian models, we outline the use of gold and damage dealt as a measure of skill, detailing players’ impact on their own-, their allies’- and their enemies’ statistics throughout the course of the game. Our results showcase the model’s increased efficacy in separating professional players when compared to a Plus-Minus model and to current esports industry standards, while metric quality is rigorously assessed for discrimination, independence, and stability. Readers might also find additional qualitative analytics which explore champion proficiency and the impact of collaborative team-play. Future work is proposed to refine and expand the SIDO performance model, offering a comprehensive framework for esports analytics in team performance management, scouting and research realms.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.04873&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Amy X. Zhang, Parth Naidu</name></author><category term="stat.AP" /><summary type="html">League of Legends (LoL) has been a dominant esport for a decade, yet the inherent complexity of the game has stymied the creation of analytical measures of player skill and performance. Current industry standards are limited to easy-to-procure individual player statistics that are incomplete and lacking context as they do not take into account teamplay or game state. We present a unified performance model for League of Legends which blends together measures of a player’s contribution within the context of their team, insights from traditional sports metrics such as the Plus-Minus model, and the intricacies of LoL as a complex team invasion sport. Using hierarchical Bayesian models, we outline the use of gold and damage dealt as a measure of skill, detailing players’ impact on their own-, their allies’- and their enemies’ statistics throughout the course of the game. Our results showcase the model’s increased efficacy in separating professional players when compared to a Plus-Minus model and to current esports industry standards, while metric quality is rigorously assessed for discrimination, independence, and stability. Readers might also find additional qualitative analytics which explore champion proficiency and the impact of collaborative team-play. Future work is proposed to refine and expand the SIDO performance model, offering a comprehensive framework for esports analytics in team performance management, scouting and research realms.</summary></entry><entry><title type="html">The risks of risk assessment: causal blind spots when using prediction models for treatment decisions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Therisksofriskassessmentcausalblindspotswhenusingpredictionmodelsfortreatmentdecisions.html" rel="alternate" type="text/html" title="The risks of risk assessment: causal blind spots when using prediction models for treatment decisions" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Therisksofriskassessmentcausalblindspotswhenusingpredictionmodelsfortreatmentdecisions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/Therisksofriskassessmentcausalblindspotswhenusingpredictionmodelsfortreatmentdecisions.html">&lt;p&gt;Prediction models are increasingly proposed for guiding treatment decisions, but most fail to address the special role of treatments, leading to inappropriate use. This paper highlights the limitations of using standard prediction models for treatment decision support. We identify &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;causal blind spots&apos; in three common approaches to handling treatments in prediction modelling: including treatment as a predictor, restricting data based on treatment status and ignoring treatments. When predictions are used to inform treatment decisions, confounders, colliders and mediators, as well as changes in treatment protocols over time may lead to misinformed decision-making. We illustrate potential harmful consequences in several medical applications. We advocate for an extension of guidelines for development, reporting and evaluation of prediction models to ensure that the intended use of the model is matched to an appropriate risk estimand. When prediction models are intended to inform treatment decisions, prediction models should specify upfront the treatment decisions they aim to support and target a prediction estimand in line with that goal. This requires a shift towards developing predictions under the specific treatment options under consideration (&lt;/code&gt;predictions under interventions’). Predictions under interventions need causal reasoning and inference techniques during development and validation. We argue that this will improve the efficacy of prediction models in guiding treatment decisions and prevent potential negative effects on patient outcomes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.17366&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nan van Geloven, Ruth H Keogh, Wouter van Amsterdam, Giovanni Cinà, Jesse H. Krijthe, Niels Peek, Kim Luijken, Sara Magliacane, Pawe{\l} Morzywo{\l}ek, Thijs van Ommen, Hein Putter, Matthew Sperrin, Junfeng Wang, Daniala L. Weir, Vanessa Didelez</name></author><category term="stat.ME" /><summary type="html">Prediction models are increasingly proposed for guiding treatment decisions, but most fail to address the special role of treatments, leading to inappropriate use. This paper highlights the limitations of using standard prediction models for treatment decision support. We identify causal blind spots&apos; in three common approaches to handling treatments in prediction modelling: including treatment as a predictor, restricting data based on treatment status and ignoring treatments. When predictions are used to inform treatment decisions, confounders, colliders and mediators, as well as changes in treatment protocols over time may lead to misinformed decision-making. We illustrate potential harmful consequences in several medical applications. We advocate for an extension of guidelines for development, reporting and evaluation of prediction models to ensure that the intended use of the model is matched to an appropriate risk estimand. When prediction models are intended to inform treatment decisions, prediction models should specify upfront the treatment decisions they aim to support and target a prediction estimand in line with that goal. This requires a shift towards developing predictions under the specific treatment options under consideration (predictions under interventions’). Predictions under interventions need causal reasoning and inference techniques during development and validation. We argue that this will improve the efficacy of prediction models in guiding treatment decisions and prevent potential negative effects on patient outcomes.</summary></entry><entry><title type="html">Towards Causal Foundation Model: on Duality between Causal Inference and Attention</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/TowardsCausalFoundationModelonDualitybetweenCausalInferenceandAttention.html" rel="alternate" type="text/html" title="Towards Causal Foundation Model: on Duality between Causal Inference and Attention" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/TowardsCausalFoundationModelonDualitybetweenCausalInferenceandAttention</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/TowardsCausalFoundationModelonDualitybetweenCausalInferenceandAttention.html">&lt;p&gt;Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach CInA effectively generalizes to out-of-distribution datasets and various real-world datasets, matching or even surpassing traditional per-dataset causal inference methodologies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.00809&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiaqi Zhang, Joel Jennings, Agrin Hilmkil, Nick Pawlowski, Cheng Zhang, Chao Ma</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for complex tasks. We propose a novel, theoretically sound method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that our approach CInA effectively generalizes to out-of-distribution datasets and various real-world datasets, matching or even surpassing traditional per-dataset causal inference methodologies.</summary></entry><entry><title type="html">Towards Causal Interpretation of Sexual Orientation in Regression Analysis: Applications and Challenges</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/TowardsCausalInterpretationofSexualOrientationinRegressionAnalysisApplicationsandChallenges.html" rel="alternate" type="text/html" title="Towards Causal Interpretation of Sexual Orientation in Regression Analysis: Applications and Challenges" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/TowardsCausalInterpretationofSexualOrientationinRegressionAnalysisApplicationsandChallenges</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/TowardsCausalInterpretationofSexualOrientationinRegressionAnalysisApplicationsandChallenges.html">&lt;p&gt;This study presents an approach to analyze health disparities in Sexual and Gender Minority (SGM) populations, with a focus on the role of social support levels as an example to allow causal interpretations of regression models. We advocate for precisely defining the exposure variable and incorporating mediators into analyses, to address the limitations of comparing counterfactual outcomes solely between SGM and heterosexual populations. We define sexual orientation into domains (attraction, behavior, and identity), and emphasize a consideration of these elements either separately or together, depending on the research question. We also introduce social support measured before and after the disclosure of sexual orientation to facilitate inference. We illustrate this approach by examining the association between SGM status and depression diagnosis with data from the 2020 and 2021 National Health Interview Survey. We find a direct effect of SGM status on depression (OR: 3.07, 95% CI: 2.64 - 3.58) and no indirect effect through social support (OR: 1.07, 95% CI: 0.87-1.31). Our research emphasizes the necessity of the comprehensive measurement of sexual orientation and a focus on intervenable variables like social support in order to empower SGM communities and address SGM related health inequalities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02322&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Junjie Lu, Zhongyi Guo, David H. Rehkopf</name></author><category term="stat.AP" /><summary type="html">This study presents an approach to analyze health disparities in Sexual and Gender Minority (SGM) populations, with a focus on the role of social support levels as an example to allow causal interpretations of regression models. We advocate for precisely defining the exposure variable and incorporating mediators into analyses, to address the limitations of comparing counterfactual outcomes solely between SGM and heterosexual populations. We define sexual orientation into domains (attraction, behavior, and identity), and emphasize a consideration of these elements either separately or together, depending on the research question. We also introduce social support measured before and after the disclosure of sexual orientation to facilitate inference. We illustrate this approach by examining the association between SGM status and depression diagnosis with data from the 2020 and 2021 National Health Interview Survey. We find a direct effect of SGM status on depression (OR: 3.07, 95% CI: 2.64 - 3.58) and no indirect effect through social support (OR: 1.07, 95% CI: 0.87-1.31). Our research emphasizes the necessity of the comprehensive measurement of sexual orientation and a focus on intervenable variables like social support in order to empower SGM communities and address SGM related health inequalities.</summary></entry><entry><title type="html">Unscented Trajectory Optimization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/UnscentedTrajectoryOptimization.html" rel="alternate" type="text/html" title="Unscented Trajectory Optimization" /><published>2024-05-07T00:00:00+00:00</published><updated>2024-05-07T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/UnscentedTrajectoryOptimization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/07/UnscentedTrajectoryOptimization.html">&lt;p&gt;In a nutshell, unscented trajectory optimization is the generation of optimal trajectories through the use of an unscented transform. Although unscented trajectory optimization was introduced by the authors about a decade ago, it is reintroduced in this paper as a special instantiation of tychastic optimal control theory. Tychastic optimal control theory (from \textit{Tyche}, the Greek goddess of chance) avoids the use of a Brownian motion and the resulting It\^{o} calculus even though it uses random variables across the entire spectrum of a problem formulation. This approach circumvents the enormous technical and numerical challenges associated with stochastic trajectory optimization. Furthermore, it is shown how a tychastic optimal control problem that involves nonlinear transformations of the expectation operator can be quickly instantiated using an unscented transform. These nonlinear transformations are particularly useful in managing trajectory dispersions be it associated with path constraints or targeted values of final-time conditions. This paper also presents a systematic and rapid process for formulating and computing the most desirable tychastic trajectory using an unscented transform. Numerical examples are used to illustrate how unscented trajectory optimization may be used for risk reduction and mission recovery caused by uncertainties and failures.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02753&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>I. M. Ross, R. J. Proulx, M. Karpenko</name></author><category term="stat.CO," /><category term="stat.TH" /><summary type="html">In a nutshell, unscented trajectory optimization is the generation of optimal trajectories through the use of an unscented transform. Although unscented trajectory optimization was introduced by the authors about a decade ago, it is reintroduced in this paper as a special instantiation of tychastic optimal control theory. Tychastic optimal control theory (from \textit{Tyche}, the Greek goddess of chance) avoids the use of a Brownian motion and the resulting It\^{o} calculus even though it uses random variables across the entire spectrum of a problem formulation. This approach circumvents the enormous technical and numerical challenges associated with stochastic trajectory optimization. Furthermore, it is shown how a tychastic optimal control problem that involves nonlinear transformations of the expectation operator can be quickly instantiated using an unscented transform. These nonlinear transformations are particularly useful in managing trajectory dispersions be it associated with path constraints or targeted values of final-time conditions. This paper also presents a systematic and rapid process for formulating and computing the most desirable tychastic trajectory using an unscented transform. Numerical examples are used to illustrate how unscented trajectory optimization may be used for risk reduction and mission recovery caused by uncertainties and failures.</summary></entry></feed>