<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-05T07:14:55+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html"></title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/2024-06-05-ABayesiannonlinearstationarymodelwithmultiplefrequenciesforbusinesscycleanalysis.html" rel="alternate" type="text/html" title="" /><published>2024-06-05T07:14:55+00:00</published><updated>2024-06-05T07:14:55+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/2024-06-05-ABayesiannonlinearstationarymodelwithmultiplefrequenciesforbusinesscycleanalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/2024-06-05-ABayesiannonlinearstationarymodelwithmultiplefrequenciesforbusinesscycleanalysis.html">&lt;p&gt;We design a novel, nonlinear single-source-of-error model for analysis of multiple business cycles. The model’s specification is intended to capture key empirical characteristics of business cycle data by allowing for simultaneous cycles of different types and lengths, as well as time-variable amplitude and phase shift. The model is shown to feature relevant theoretical properties, including stationarity and pseudo-cyclical autocovariance function, and enables a decomposition of overall cyclic fluctuations into separate frequency-specific components. We develop a Bayesian framework for estimation and inference in the model, along with an MCMC procedure for posterior sampling, combining the Gibbs sampler and the Metropolis-Hastings algorithm, suitably adapted to address encountered numerical issues. Empirical results obtained from the model applied to the Polish GDP growth rates imply co-existence of two types of economic fluctuations: the investment and inventory cycles, and support the stochastic variability of the amplitude and phase shift, also capturing some business cycle asymmetries. Finally, the Bayesian framework enables a fully probabilistic inference on the business cycle clocks and dating, which seems the most relevant approach in view of economic uncertainties.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02321&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Aliverti</name></author></entry><entry><title type="html">A Practical Approach for Exploring Granger Connectivity in High-Dimensional Networks of Time Series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/APracticalApproachforExploringGrangerConnectivityinHighDimensionalNetworksofTimeSeries.html" rel="alternate" type="text/html" title="A Practical Approach for Exploring Granger Connectivity in High-Dimensional Networks of Time Series" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/APracticalApproachforExploringGrangerConnectivityinHighDimensionalNetworksofTimeSeries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/APracticalApproachforExploringGrangerConnectivityinHighDimensionalNetworksofTimeSeries.html">&lt;p&gt;This manuscript presents a novel method for discovering effective connectivity between specified pairs of nodes in a high-dimensional network of time series. To accurately perform Granger causality analysis from the first node to the second node, it is essential to eliminate the influence of all other nodes within the network. The approach proposed is to create a low-dimensional representation of all other nodes in the network using frequency-domain-based dynamic principal component analysis (spectral DPCA). The resulting scores are subsequently removed from the first and second nodes of interest, thus eliminating the confounding effect of other nodes within the high-dimensional network. To conduct hypothesis testing on Granger causality, we propose a permutation-based causality test. This test enhances the accuracy of our findings when the error structures are non-Gaussian. The approach has been validated in extensive simulation studies, which demonstrate the efficacy of the methodology as a tool for causality analysis in complex time series networks. The proposed methodology has also been demonstrated to be both expedient and viable on real datasets, with particular success observed on multichannel EEG networks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02360&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sipan Aslan, Hernando Ombao</name></author><category term="stat.AP," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">This manuscript presents a novel method for discovering effective connectivity between specified pairs of nodes in a high-dimensional network of time series. To accurately perform Granger causality analysis from the first node to the second node, it is essential to eliminate the influence of all other nodes within the network. The approach proposed is to create a low-dimensional representation of all other nodes in the network using frequency-domain-based dynamic principal component analysis (spectral DPCA). The resulting scores are subsequently removed from the first and second nodes of interest, thus eliminating the confounding effect of other nodes within the high-dimensional network. To conduct hypothesis testing on Granger causality, we propose a permutation-based causality test. This test enhances the accuracy of our findings when the error structures are non-Gaussian. The approach has been validated in extensive simulation studies, which demonstrate the efficacy of the methodology as a tool for causality analysis in complex time series networks. The proposed methodology has also been demonstrated to be both expedient and viable on real datasets, with particular success observed on multichannel EEG networks.</summary></entry><entry><title type="html">Active Learning for a Recursive Non-Additive Emulator for Multi-Fidelity Computer Experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ActiveLearningforaRecursiveNonAdditiveEmulatorforMultiFidelityComputerExperiments.html" rel="alternate" type="text/html" title="Active Learning for a Recursive Non-Additive Emulator for Multi-Fidelity Computer Experiments" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ActiveLearningforaRecursiveNonAdditiveEmulatorforMultiFidelityComputerExperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ActiveLearningforaRecursiveNonAdditiveEmulatorforMultiFidelityComputerExperiments.html">&lt;p&gt;Computer simulations have become essential for analyzing complex systems, but high-fidelity simulations often come with significant computational costs. To tackle this challenge, multi-fidelity computer experiments have emerged as a promising approach that leverages both low-fidelity and high-fidelity simulations, enhancing both the accuracy and efficiency of the analysis. In this paper, we introduce a new and flexible statistical model, the Recursive Non-Additive (RNA) emulator, that integrates the data from multi-fidelity computer experiments. Unlike conventional multi-fidelity emulation approaches that rely on an additive auto-regressive structure, the proposed RNA emulator recursively captures the relationships between multi-fidelity data using Gaussian process priors without making the additive assumption, allowing the model to accommodate more complex data patterns. Importantly, we derive the posterior predictive mean and variance of the emulator, which can be efficiently computed in a closed-form manner, leading to significant improvements in computational efficiency. Additionally, based on this emulator, we introduce four active learning strategies that optimize the balance between accuracy and simulation costs to guide the selection of the fidelity level and input locations for the next simulation run. We demonstrate the effectiveness of the proposed approach in a suite of synthetic examples and a real-world problem. An R package RNAmf for the proposed methodology is provided on CRAN.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.11772&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Junoh Heo, Chih-Li Sung</name></author><category term="stat.ME" /><summary type="html">Computer simulations have become essential for analyzing complex systems, but high-fidelity simulations often come with significant computational costs. To tackle this challenge, multi-fidelity computer experiments have emerged as a promising approach that leverages both low-fidelity and high-fidelity simulations, enhancing both the accuracy and efficiency of the analysis. In this paper, we introduce a new and flexible statistical model, the Recursive Non-Additive (RNA) emulator, that integrates the data from multi-fidelity computer experiments. Unlike conventional multi-fidelity emulation approaches that rely on an additive auto-regressive structure, the proposed RNA emulator recursively captures the relationships between multi-fidelity data using Gaussian process priors without making the additive assumption, allowing the model to accommodate more complex data patterns. Importantly, we derive the posterior predictive mean and variance of the emulator, which can be efficiently computed in a closed-form manner, leading to significant improvements in computational efficiency. Additionally, based on this emulator, we introduce four active learning strategies that optimize the balance between accuracy and simulation costs to guide the selection of the fidelity level and input locations for the next simulation run. We demonstrate the effectiveness of the proposed approach in a suite of synthetic examples and a real-world problem. An R package RNAmf for the proposed methodology is provided on CRAN.</summary></entry><entry><title type="html">Adaptive Online Experimental Design for Causal Discovery</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/AdaptiveOnlineExperimentalDesignforCausalDiscovery.html" rel="alternate" type="text/html" title="Adaptive Online Experimental Design for Causal Discovery" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/AdaptiveOnlineExperimentalDesignforCausalDiscovery</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/AdaptiveOnlineExperimentalDesignforCausalDiscovery.html">&lt;p&gt;Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.11548&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Muhammad Qasim Elahi, Lai Wei, Murat Kocaoglu, Mahsa Ghasemi</name></author><category term="stat.AP" /><summary type="html">Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.</summary></entry><entry><title type="html">Analyzing trends for agricultural decision support system using twitter data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Analyzingtrendsforagriculturaldecisionsupportsystemusingtwitterdata.html" rel="alternate" type="text/html" title="Analyzing trends for agricultural decision support system using twitter data" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Analyzingtrendsforagriculturaldecisionsupportsystemusingtwitterdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Analyzingtrendsforagriculturaldecisionsupportsystemusingtwitterdata.html">&lt;p&gt;The trends and reactions of the general public towards global events can be analyzed using data from social platforms, including Twitter. The number of tweets has been reported to help detect variations in communication traffic within subsets like countries, age groups and industries. Similarly, publicly accessible data and (in particular) data from social media about agricultural issues provide a great opportunity for obtaining instantaneous snapshots of farmer opinions and a method to track changes in opinion through temporal analysis. In this paper we hypothesize that the presence of keywords like precision agriculture, digital agriculture, Internet of Things (IoT), BigData, remote sensing, GPS, etc., in tweets could serve as an indicator of discussions centered around interest in modern farming practices. We extracted relevant tweets using keywords such as IoT, BigData and Geographical Information System (GIS), and then analyzed their geographical origin and frequency of their mention. We analyzed the Twitter data for the period of 1st -11th January 2018 to understand these trends and the factors affecting them. These factors, such as special events, projects, biogeography, etc., were further analyzed using tweet sources and trending hashtags from the database. The regions with the highest interest in the keywords were United States, Egypt, Brazil, Japan and China. A comparison of frequency of keywords revealed IoT as the most tweeted word (77.6%) in the downloaded data. The most used language was English followed by Spanish, Japanese and French. Periodical tweets on IoT from an account handled by IoT project on Twitter and Seminars on IoT in January in Santa Catarina (Brazil) were found to be the underlying factors for the observed trends.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00577&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sneha Jha, Dharmendra Saraswat, Mark D. Ward</name></author><category term="stat.AP" /><summary type="html">The trends and reactions of the general public towards global events can be analyzed using data from social platforms, including Twitter. The number of tweets has been reported to help detect variations in communication traffic within subsets like countries, age groups and industries. Similarly, publicly accessible data and (in particular) data from social media about agricultural issues provide a great opportunity for obtaining instantaneous snapshots of farmer opinions and a method to track changes in opinion through temporal analysis. In this paper we hypothesize that the presence of keywords like precision agriculture, digital agriculture, Internet of Things (IoT), BigData, remote sensing, GPS, etc., in tweets could serve as an indicator of discussions centered around interest in modern farming practices. We extracted relevant tweets using keywords such as IoT, BigData and Geographical Information System (GIS), and then analyzed their geographical origin and frequency of their mention. We analyzed the Twitter data for the period of 1st -11th January 2018 to understand these trends and the factors affecting them. These factors, such as special events, projects, biogeography, etc., were further analyzed using tweet sources and trending hashtags from the database. The regions with the highest interest in the keywords were United States, Egypt, Brazil, Japan and China. A comparison of frequency of keywords revealed IoT as the most tweeted word (77.6%) in the downloaded data. The most used language was English followed by Spanish, Japanese and French. Periodical tweets on IoT from an account handled by IoT project on Twitter and Seminars on IoT in January in Santa Catarina (Brazil) were found to be the underlying factors for the observed trends.</summary></entry><entry><title type="html">An efficient Wasserstein-distance approach for reconstructing jump-diffusion processes using parameterized neural networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/AnefficientWassersteindistanceapproachforreconstructingjumpdiffusionprocessesusingparameterizedneuralnetworks.html" rel="alternate" type="text/html" title="An efficient Wasserstein-distance approach for reconstructing jump-diffusion processes using parameterized neural networks" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/AnefficientWassersteindistanceapproachforreconstructingjumpdiffusionprocessesusingparameterizedneuralnetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/AnefficientWassersteindistanceapproachforreconstructingjumpdiffusionprocessesusingparameterizedneuralnetworks.html">&lt;p&gt;We analyze the Wasserstein distance ($W$-distance) between two probability distributions associated with two multidimensional jump-diffusion processes. Specifically, we analyze a temporally decoupled squared $W_2$-distance, which provides both upper and lower bounds associated with the discrepancies in the drift, diffusion, and jump amplitude functions between the two jump-diffusion processes. Then, we propose a temporally decoupled squared $W_2$-distance method for efficiently reconstructing unknown jump-diffusion processes from data using parameterized neural networks. We further show its performance can be enhanced by utilizing prior information on the drift function of the jump-diffusion process. The effectiveness of our proposed reconstruction method is demonstrated across several examples and applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01653&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mingtao Xia, Xiangting Li, Qijing Shen, Tom Chou</name></author><category term="stat.ML," /><category term="stat.AP," /><category term="stat.ME" /><summary type="html">We analyze the Wasserstein distance ($W$-distance) between two probability distributions associated with two multidimensional jump-diffusion processes. Specifically, we analyze a temporally decoupled squared $W_2$-distance, which provides both upper and lower bounds associated with the discrepancies in the drift, diffusion, and jump amplitude functions between the two jump-diffusion processes. Then, we propose a temporally decoupled squared $W_2$-distance method for efficiently reconstructing unknown jump-diffusion processes from data using parameterized neural networks. We further show its performance can be enhanced by utilizing prior information on the drift function of the jump-diffusion process. The effectiveness of our proposed reconstruction method is demonstrated across several examples and applications.</summary></entry><entry><title type="html">An efficient solution to Hidden Markov Models on trees with coupled branches</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/AnefficientsolutiontoHiddenMarkovModelsontreeswithcoupledbranches.html" rel="alternate" type="text/html" title="An efficient solution to Hidden Markov Models on trees with coupled branches" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/AnefficientsolutiontoHiddenMarkovModelsontreeswithcoupledbranches</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/AnefficientsolutiontoHiddenMarkovModelsontreeswithcoupledbranches.html">&lt;p&gt;Hidden Markov Models (HMMs) are powerful tools for modeling sequential data, where the underlying states evolve in a stochastic manner and are only indirectly observable. Traditional HMM approaches are well-established for linear sequences, and have been extended to other structures such as trees. In this paper, we extend the framework of HMMs on trees to address scenarios where the tree-like structure of the data includes coupled branches – a common feature in biological systems where entities within the same lineage exhibit dependent characteristics. We develop a dynamic programming algorithm that efficiently solves the likelihood, decoding, and parameter learning problems for tree-based HMMs with coupled branches. Our approach scales polynomially with the number of states and nodes, making it computationally feasible for a wide range of applications and does not suffer from the underflow problem. We demonstrate our algorithm by applying it to simulated data and propose self-consistency checks for validating the assumptions of the model used for inference. This work not only advances the theoretical understanding of HMMs on trees but also provides a practical tool for analyzing complex biological data where dependencies between branches cannot be ignored.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01663&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Farzan Vafa, Sahand Hormoz</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Hidden Markov Models (HMMs) are powerful tools for modeling sequential data, where the underlying states evolve in a stochastic manner and are only indirectly observable. Traditional HMM approaches are well-established for linear sequences, and have been extended to other structures such as trees. In this paper, we extend the framework of HMMs on trees to address scenarios where the tree-like structure of the data includes coupled branches – a common feature in biological systems where entities within the same lineage exhibit dependent characteristics. We develop a dynamic programming algorithm that efficiently solves the likelihood, decoding, and parameter learning problems for tree-based HMMs with coupled branches. Our approach scales polynomially with the number of states and nodes, making it computationally feasible for a wide range of applications and does not suffer from the underflow problem. We demonstrate our algorithm by applying it to simulated data and propose self-consistency checks for validating the assumptions of the model used for inference. This work not only advances the theoretical understanding of HMMs on trees but also provides a practical tool for analyzing complex biological data where dependencies between branches cannot be ignored.</summary></entry><entry><title type="html">Bayesian Linear Models: A compact general set of results</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/BayesianLinearModelsAcompactgeneralsetofresults.html" rel="alternate" type="text/html" title="Bayesian Linear Models: A compact general set of results" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/BayesianLinearModelsAcompactgeneralsetofresults</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/BayesianLinearModelsAcompactgeneralsetofresults.html">&lt;p&gt;I present all the details in calculating the posterior distribution of the conjugate Normal-Gamma prior in Bayesian Linear Models (BLM), including correlated observations, prediction, model selection and comments on efficient numeric implementations. A Python implementation is also presented. These have been presented and available in many books and texts but, I believe, a general compact and simple presentation is always welcome and not always simple to find. Since correlated observations are also included, these results may also be useful for time series analysis and spacial statistics. Other particular cases presented include regression, Gaussian processes and Bayesian Dynamic Models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01819&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>J Andres Christen</name></author><category term="stat.ME" /><summary type="html">I present all the details in calculating the posterior distribution of the conjugate Normal-Gamma prior in Bayesian Linear Models (BLM), including correlated observations, prediction, model selection and comments on efficient numeric implementations. A Python implementation is also presented. These have been presented and available in many books and texts but, I believe, a general compact and simple presentation is always welcome and not always simple to find. Since correlated observations are also included, these results may also be useful for time series analysis and spacial statistics. Other particular cases presented include regression, Gaussian processes and Bayesian Dynamic Models.</summary></entry><entry><title type="html">Causal Discovery with Fewer Conditional Independence Tests</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/CausalDiscoverywithFewerConditionalIndependenceTests.html" rel="alternate" type="text/html" title="Causal Discovery with Fewer Conditional Independence Tests" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/CausalDiscoverywithFewerConditionalIndependenceTests</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/CausalDiscoverywithFewerConditionalIndependenceTests.html">&lt;p&gt;Many questions in science center around the fundamental problem of understanding causal relationships. However, most constraint-based causal discovery algorithms, including the well-celebrated PC algorithm, often incur an exponential number of conditional independence (CI) tests, posing limitations in various applications. Addressing this, our work focuses on characterizing what can be learned about the underlying causal graph with a reduced number of CI tests. We show that it is possible to a learn a coarser representation of the hidden causal graph with a polynomial number of tests. This coarser representation, named Causal Consistent Partition Graph (CCPG), comprises of a partition of the vertices and a directed graph defined over its components. CCPG satisfies consistency of orientations and additional constraints which favor finer partitions. Furthermore, it reduces to the underlying causal graph when the causal graph is identifiable. As a consequence, our results offer the first efficient algorithm for recovering the true causal graph with a polynomial number of tests, in special cases where the causal graph is fully identifiable through observational data and potentially additional interventions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01823&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kirankumar Shiragur, Jiaqi Zhang, Caroline Uhler</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Many questions in science center around the fundamental problem of understanding causal relationships. However, most constraint-based causal discovery algorithms, including the well-celebrated PC algorithm, often incur an exponential number of conditional independence (CI) tests, posing limitations in various applications. Addressing this, our work focuses on characterizing what can be learned about the underlying causal graph with a reduced number of CI tests. We show that it is possible to a learn a coarser representation of the hidden causal graph with a polynomial number of tests. This coarser representation, named Causal Consistent Partition Graph (CCPG), comprises of a partition of the vertices and a directed graph defined over its components. CCPG satisfies consistency of orientations and additional constraints which favor finer partitions. Furthermore, it reduces to the underlying causal graph when the causal graph is identifiable. As a consequence, our results offer the first efficient algorithm for recovering the true causal graph with a polynomial number of tests, in special cases where the causal graph is fully identifiable through observational data and potentially additional interventions.</summary></entry><entry><title type="html">Causal Effect Identification in LiNGAM Models with Latent Confounders</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/CausalEffectIdentificationinLiNGAMModelswithLatentConfounders.html" rel="alternate" type="text/html" title="Causal Effect Identification in LiNGAM Models with Latent Confounders" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/CausalEffectIdentificationinLiNGAMModelswithLatentConfounders</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/CausalEffectIdentificationinLiNGAMModelswithLatentConfounders.html">&lt;p&gt;We study the generic identifiability of causal effects in linear non-Gaussian acyclic models (LiNGAM) with latent variables. We consider the problem in two main settings: When the causal graph is known a priori, and when it is unknown. In both settings, we provide a complete graphical characterization of the identifiable direct or total causal effects among observed variables. Moreover, we propose efficient algorithms to certify the graphical conditions. Finally, we propose an adaptation of the reconstruction independent component analysis (RICA) algorithm that estimates the causal effects from the observational data given the causal graph. Experimental results show the effectiveness of the proposed method in estimating the causal effects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02049&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Daniele Tramontano, Yaroslav Kivva, Saber Salehkaleybar, Mathias Drton, Negar Kiyavash</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">We study the generic identifiability of causal effects in linear non-Gaussian acyclic models (LiNGAM) with latent variables. We consider the problem in two main settings: When the causal graph is known a priori, and when it is unknown. In both settings, we provide a complete graphical characterization of the identifiable direct or total causal effects among observed variables. Moreover, we propose efficient algorithms to certify the graphical conditions. Finally, we propose an adaptation of the reconstruction independent component analysis (RICA) algorithm that estimates the causal effects from the observational data given the causal graph. Experimental results show the effectiveness of the proposed method in estimating the causal effects.</summary></entry><entry><title type="html">Causal Inference for a Hidden Treatment</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/CausalInferenceforaHiddenTreatment.html" rel="alternate" type="text/html" title="Causal Inference for a Hidden Treatment" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/CausalInferenceforaHiddenTreatment</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/CausalInferenceforaHiddenTreatment.html">&lt;p&gt;In many empirical settings, directly observing a treatment variable may be infeasible although an error-prone surrogate measurement of the latter will often be available. Causal inference based solely on the observed surrogate measurement of the hidden treatment may be particularly challenging without an additional assumption or auxiliary data. To address this issue, we propose a method that carefully incorporates the surrogate measurement together with a proxy of the hidden treatment to identify its causal effect on any scale for which identification would in principle be feasible had contrary to fact the treatment been observed error-free. Beyond identification, we provide general semiparametric theory for causal effects identified using our approach, and we derive a large class of semiparametric estimators with an appealing multiple robustness property. A significant obstacle to our approach is the estimation of nuisance functions involving the hidden treatment, which prevents the direct application of standard machine learning algorithms. To resolve this, we introduce a novel semiparametric EM algorithm, thus adding a practical dimension to our theoretical contributions. This methodology can be adapted to analyze a large class of causal parameters in the proposed hidden treatment model, including the population average treatment effect, the effect of treatment on the treated, quantile treatment effects, and causal effects under marginal structural models. We examine the finite-sample performance of our method using simulations and an application which aims to estimate the causal effect of Alzheimer’s disease on hippocampal volume using data from the Alzheimer’s Disease Neuroimaging Initiative.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09080&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ying Zhou, Eric Tchetgen Tchetgen</name></author><category term="stat.ME" /><summary type="html">In many empirical settings, directly observing a treatment variable may be infeasible although an error-prone surrogate measurement of the latter will often be available. Causal inference based solely on the observed surrogate measurement of the hidden treatment may be particularly challenging without an additional assumption or auxiliary data. To address this issue, we propose a method that carefully incorporates the surrogate measurement together with a proxy of the hidden treatment to identify its causal effect on any scale for which identification would in principle be feasible had contrary to fact the treatment been observed error-free. Beyond identification, we provide general semiparametric theory for causal effects identified using our approach, and we derive a large class of semiparametric estimators with an appealing multiple robustness property. A significant obstacle to our approach is the estimation of nuisance functions involving the hidden treatment, which prevents the direct application of standard machine learning algorithms. To resolve this, we introduce a novel semiparametric EM algorithm, thus adding a practical dimension to our theoretical contributions. This methodology can be adapted to analyze a large class of causal parameters in the proposed hidden treatment model, including the population average treatment effect, the effect of treatment on the treated, quantile treatment effects, and causal effects under marginal structural models. We examine the finite-sample performance of our method using simulations and an application which aims to estimate the causal effect of Alzheimer’s disease on hippocampal volume using data from the Alzheimer’s Disease Neuroimaging Initiative.</summary></entry><entry><title type="html">CoLa-DCE – Concept-guided Latent Diffusion Counterfactual Explanations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/CoLaDCEConceptguidedLatentDiffusionCounterfactualExplanations.html" rel="alternate" type="text/html" title="CoLa-DCE – Concept-guided Latent Diffusion Counterfactual Explanations" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/CoLaDCEConceptguidedLatentDiffusionCounterfactualExplanations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/CoLaDCEConceptguidedLatentDiffusionCounterfactualExplanations.html">&lt;p&gt;Recent advancements in generative AI have introduced novel prospects and practical implementations. Especially diffusion models show their strength in generating diverse and, at the same time, realistic features, positioning them well for generating counterfactual explanations for computer vision models. Answering “what if” questions of what needs to change to make an image classifier change its prediction, counterfactual explanations align well with human understanding and consequently help in making model behavior more comprehensible. Current methods succeed in generating authentic counterfactuals, but lack transparency as feature changes are not directly perceivable. To address this limitation, we introduce Concept-guided Latent Diffusion Counterfactual Explanations (CoLa-DCE). CoLa-DCE generates concept-guided counterfactuals for any classifier with a high degree of control regarding concept selection and spatial conditioning. The counterfactuals comprise an increased granularity through minimal feature changes. The reference feature visualization ensures better comprehensibility, while the feature localization provides increased transparency of “where” changed “what”. We demonstrate the advantages of our approach in minimality and comprehensibility across multiple image classification models and datasets and provide insights into how our CoLa-DCE explanations help comprehend model errors like misclassification cases.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01649&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Franz Motzkus, Christian Hellert, Ute Schmid</name></author><category term="stat.ME" /><summary type="html">Recent advancements in generative AI have introduced novel prospects and practical implementations. Especially diffusion models show their strength in generating diverse and, at the same time, realistic features, positioning them well for generating counterfactual explanations for computer vision models. Answering “what if” questions of what needs to change to make an image classifier change its prediction, counterfactual explanations align well with human understanding and consequently help in making model behavior more comprehensible. Current methods succeed in generating authentic counterfactuals, but lack transparency as feature changes are not directly perceivable. To address this limitation, we introduce Concept-guided Latent Diffusion Counterfactual Explanations (CoLa-DCE). CoLa-DCE generates concept-guided counterfactuals for any classifier with a high degree of control regarding concept selection and spatial conditioning. The counterfactuals comprise an increased granularity through minimal feature changes. The reference feature visualization ensures better comprehensibility, while the feature localization provides increased transparency of “where” changed “what”. We demonstrate the advantages of our approach in minimality and comprehensibility across multiple image classification models and datasets and provide insights into how our CoLa-DCE explanations help comprehend model errors like misclassification cases.</summary></entry><entry><title type="html">Compositional dynamic modelling for causal prediction in multivariate time series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Compositionaldynamicmodellingforcausalpredictioninmultivariatetimeseries.html" rel="alternate" type="text/html" title="Compositional dynamic modelling for causal prediction in multivariate time series" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Compositionaldynamicmodellingforcausalpredictioninmultivariatetimeseries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Compositionaldynamicmodellingforcausalpredictioninmultivariatetimeseries.html">&lt;p&gt;Theoretical developments in sequential Bayesian analysis of multivariate dynamic models underlie new methodology for causal prediction. This extends the utility of existing models with computationally efficient methodology, enabling routine exploration of Bayesian counterfactual analyses with multiple selected time series as synthetic controls. Methodological contributions also define the concept of outcome adaptive modelling to monitor and inferentially respond to changes in experimental time series following interventions designed to explore causal effects. The benefits of sequential analyses with time-varying parameter models for causal investigations are inherited in this broader setting. A case study in commercial causal analysis– involving retail revenue outcomes related to marketing interventions– highlights the methodological advances.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02320&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kevin Li, Graham Tierney, Christoph Hellmayr, Mike West</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Theoretical developments in sequential Bayesian analysis of multivariate dynamic models underlie new methodology for causal prediction. This extends the utility of existing models with computationally efficient methodology, enabling routine exploration of Bayesian counterfactual analyses with multiple selected time series as synthetic controls. Methodological contributions also define the concept of outcome adaptive modelling to monitor and inferentially respond to changes in experimental time series following interventions designed to explore causal effects. The benefits of sequential analyses with time-varying parameter models for causal investigations are inherited in this broader setting. A case study in commercial causal analysis– involving retail revenue outcomes related to marketing interventions– highlights the methodological advances.</summary></entry><entry><title type="html">Conditional Aalen–Johansen estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ConditionalAalenJohansenestimation.html" rel="alternate" type="text/html" title="Conditional Aalen–Johansen estimation" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ConditionalAalenJohansenestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ConditionalAalenJohansenestimation.html">&lt;p&gt;The conditional Aalen–Johansen estimator, a general-purpose non-parametric estimator of conditional state occupation probabilities, is introduced. The estimator is applicable for any finite-state jump process and supports conditioning on external as well as internal covariate information. The conditioning feature permits for a much more detailed analysis of the distributional characteristics of the process. The estimator reduces to the conditional Kaplan–Meier estimator in the special case of a survival model and also englobes other, more recent, landmark estimators when covariates are discrete. Strong uniform consistency and asymptotic normality are established under lax moment conditions on the multivariate counting process, allowing in particular for an unbounded number of transitions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.02119&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Martin Bladt, Christian Furrer</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">The conditional Aalen–Johansen estimator, a general-purpose non-parametric estimator of conditional state occupation probabilities, is introduced. The estimator is applicable for any finite-state jump process and supports conditioning on external as well as internal covariate information. The conditioning feature permits for a much more detailed analysis of the distributional characteristics of the process. The estimator reduces to the conditional Kaplan–Meier estimator in the special case of a survival model and also englobes other, more recent, landmark estimators when covariates are discrete. Strong uniform consistency and asymptotic normality are established under lax moment conditions on the multivariate counting process, allowing in particular for an unbounded number of transitions.</summary></entry><entry><title type="html">Contextual Dynamic Pricing: Algorithms, Optimality, and Local Differential Privacy Constraints</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ContextualDynamicPricingAlgorithmsOptimalityandLocalDifferentialPrivacyConstraints.html" rel="alternate" type="text/html" title="Contextual Dynamic Pricing: Algorithms, Optimality, and Local Differential Privacy Constraints" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ContextualDynamicPricingAlgorithmsOptimalityandLocalDifferentialPrivacyConstraints</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ContextualDynamicPricingAlgorithmsOptimalityandLocalDifferentialPrivacyConstraints.html">&lt;p&gt;We study the contextual dynamic pricing problem where a firm sells products to $T$ sequentially arriving consumers that behave according to an unknown demand model. The firm aims to maximize its revenue, i.e. minimize its regret over a clairvoyant that knows the model in advance. The demand model is a generalized linear model (GLM), allowing for a stochastic feature vector in $\mathbb R^d$ that encodes product and consumer information. We first show that the optimal regret upper bound is of order $\sqrt{dT}$, up to a logarithmic factor, improving upon existing upper bounds in the literature by a $\sqrt{d}$ factor. This sharper rate is materialised by two algorithms: a confidence bound-type (supCB) algorithm and an explore-then-commit (ETC) algorithm. A key insight of our theoretical result is an intrinsic connection between dynamic pricing and the contextual multi-armed bandit problem with many arms based on a careful discretization. We further study contextual dynamic pricing under the local differential privacy (LDP) constraints. In particular, we propose a stochastic gradient descent based ETC algorithm that achieves an optimal regret upper bound of order $d\sqrt{T}/\epsilon$, up to a logarithmic factor, where $\epsilon&amp;gt;0$ is the privacy parameter. The regret upper bounds with and without LDP constraints are accompanied by newly constructed minimax lower bounds, which further characterize the cost of privacy. Extensive numerical experiments and a real data application on online lending are conducted to illustrate the efficiency and practical value of the proposed algorithms in dynamic pricing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02424&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zifeng Zhao, Feiyu Jiang, Yi Yu</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We study the contextual dynamic pricing problem where a firm sells products to $T$ sequentially arriving consumers that behave according to an unknown demand model. The firm aims to maximize its revenue, i.e. minimize its regret over a clairvoyant that knows the model in advance. The demand model is a generalized linear model (GLM), allowing for a stochastic feature vector in $\mathbb R^d$ that encodes product and consumer information. We first show that the optimal regret upper bound is of order $\sqrt{dT}$, up to a logarithmic factor, improving upon existing upper bounds in the literature by a $\sqrt{d}$ factor. This sharper rate is materialised by two algorithms: a confidence bound-type (supCB) algorithm and an explore-then-commit (ETC) algorithm. A key insight of our theoretical result is an intrinsic connection between dynamic pricing and the contextual multi-armed bandit problem with many arms based on a careful discretization. We further study contextual dynamic pricing under the local differential privacy (LDP) constraints. In particular, we propose a stochastic gradient descent based ETC algorithm that achieves an optimal regret upper bound of order $d\sqrt{T}/\epsilon$, up to a logarithmic factor, where $\epsilon&amp;gt;0$ is the privacy parameter. The regret upper bounds with and without LDP constraints are accompanied by newly constructed minimax lower bounds, which further characterize the cost of privacy. Extensive numerical experiments and a real data application on online lending are conducted to illustrate the efficiency and practical value of the proposed algorithms in dynamic pricing.</summary></entry><entry><title type="html">Development of Bayesian Component Failure Models in E1 HEMP Grid Analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/DevelopmentofBayesianComponentFailureModelsinE1HEMPGridAnalysis.html" rel="alternate" type="text/html" title="Development of Bayesian Component Failure Models in E1 HEMP Grid Analysis" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/DevelopmentofBayesianComponentFailureModelsinE1HEMPGridAnalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/DevelopmentofBayesianComponentFailureModelsinE1HEMPGridAnalysis.html">&lt;p&gt;Combined electric power system and High-Altitude Electromagnetic Pulse (HEMP) models are being developed to determine the effect of a HEMP on the US power grid. The work relies primarily on deterministic methods; however, it is computationally untenable to evaluate the E1 HEMP response of large numbers of grid components distributed across a large interconnection. Further, the deterministic assessment of these components’ failures are largely unachievable. E1 HEMP laboratory testing of the components is accomplished, but is expensive, leaving few data points to construct failure models of grid components exposed to E1 HEMP. The use of Bayesian priors, developed using the subject matter expertise, combined with the minimal test data in a Bayesian inference process, provides the basis for the development of more robust and cost-effective statistical component failure models. These can be used with minimal computational burden in a simulation environment such as sampling of Cumulative Distribution Functions (CDFs).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01923&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Niladri Das, Ross Guttromson, Tommie A. Catanach</name></author><category term="stat.AP" /><summary type="html">Combined electric power system and High-Altitude Electromagnetic Pulse (HEMP) models are being developed to determine the effect of a HEMP on the US power grid. The work relies primarily on deterministic methods; however, it is computationally untenable to evaluate the E1 HEMP response of large numbers of grid components distributed across a large interconnection. Further, the deterministic assessment of these components’ failures are largely unachievable. E1 HEMP laboratory testing of the components is accomplished, but is expensive, leaving few data points to construct failure models of grid components exposed to E1 HEMP. The use of Bayesian priors, developed using the subject matter expertise, combined with the minimal test data in a Bayesian inference process, provides the basis for the development of more robust and cost-effective statistical component failure models. These can be used with minimal computational burden in a simulation environment such as sampling of Cumulative Distribution Functions (CDFs).</summary></entry><entry><title type="html">Diffusion Boosted Trees</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/DiffusionBoostedTrees.html" rel="alternate" type="text/html" title="Diffusion Boosted Trees" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/DiffusionBoostedTrees</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/DiffusionBoostedTrees.html">&lt;p&gt;Combining the merits of both denoising diffusion probabilistic models and gradient boosting, the diffusion boosting paradigm is introduced for tackling supervised learning problems. We develop Diffusion Boosted Trees (DBT), which can be viewed as both a new denoising diffusion generative model parameterized by decision trees (one single tree for each diffusion timestep), and a new boosting algorithm that combines the weak learners into a strong learner of conditional distributions without making explicit parametric assumptions on their density forms. We demonstrate through experiments the advantages of DBT over deep neural network-based diffusion models as well as the competence of DBT on real-world regression tasks, and present a business application (fraud detection) of DBT for classification on tabular data with the ability of learning to defer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01813&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xizewen Han, Mingyuan Zhou</name></author><category term="stat.ML," /><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Combining the merits of both denoising diffusion probabilistic models and gradient boosting, the diffusion boosting paradigm is introduced for tackling supervised learning problems. We develop Diffusion Boosted Trees (DBT), which can be viewed as both a new denoising diffusion generative model parameterized by decision trees (one single tree for each diffusion timestep), and a new boosting algorithm that combines the weak learners into a strong learner of conditional distributions without making explicit parametric assumptions on their density forms. We demonstrate through experiments the advantages of DBT over deep neural network-based diffusion models as well as the competence of DBT on real-world regression tasks, and present a business application (fraud detection) of DBT for classification on tabular data with the ability of learning to defer.</summary></entry><entry><title type="html">Disentangling Structural Breaks in Factor Models for Macroeconomic Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/DisentanglingStructuralBreaksinFactorModelsforMacroeconomicData.html" rel="alternate" type="text/html" title="Disentangling Structural Breaks in Factor Models for Macroeconomic Data" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/DisentanglingStructuralBreaksinFactorModelsforMacroeconomicData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/DisentanglingStructuralBreaksinFactorModelsforMacroeconomicData.html">&lt;p&gt;Through a routine normalization of the factor variance, standard methods for estimating factor models in macroeconomics do not distinguish between breaks of the factor variance and factor loadings. We argue that it is important to distinguish between structural breaks in the factor variance and loadings within factor models commonly employed in macroeconomics as both can lead to markedly different interpretations when viewed via the lens of the underlying dynamic factor model. We then develop a projection-based decomposition that leads to two standard and easy-to-implement Wald tests to disentangle structural breaks in the factor variance and factor loadings. Applying our procedure to U.S. macroeconomic data, we find evidence of both types of breaks associated with the Great Moderation and the Great Recession. Through our projection-based decomposition, we estimate that the Great Moderation is associated with an over 60% reduction in the total factor variance, highlighting the relevance of disentangling breaks in the factor structure.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.00178&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bonsoo Koo, Benjamin Wong, Ze-Yu Zhong</name></author><category term="stat.ME" /><summary type="html">Through a routine normalization of the factor variance, standard methods for estimating factor models in macroeconomics do not distinguish between breaks of the factor variance and factor loadings. We argue that it is important to distinguish between structural breaks in the factor variance and loadings within factor models commonly employed in macroeconomics as both can lead to markedly different interpretations when viewed via the lens of the underlying dynamic factor model. We then develop a projection-based decomposition that leads to two standard and easy-to-implement Wald tests to disentangle structural breaks in the factor variance and factor loadings. Applying our procedure to U.S. macroeconomic data, we find evidence of both types of breaks associated with the Great Moderation and the Great Recession. Through our projection-based decomposition, we estimate that the Great Moderation is associated with an over 60% reduction in the total factor variance, highlighting the relevance of disentangling breaks in the factor structure.</summary></entry><entry><title type="html">Distributional bias compromises leave-one-out cross-validation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Distributionalbiascompromisesleaveoneoutcrossvalidation.html" rel="alternate" type="text/html" title="Distributional bias compromises leave-one-out cross-validation" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Distributionalbiascompromisesleaveoneoutcrossvalidation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Distributionalbiascompromisesleaveoneoutcrossvalidation.html">&lt;p&gt;Cross-validation is a common method for estimating the predictive performance of machine learning models. In a data-scarce regime, where one typically wishes to maximize the number of instances used for training the model, an approach called “leave-one-out cross-validation” is often used. In this design, a separate model is built for predicting each data instance after training on all other instances. Since this results in a single test data point available per model trained, predictions are aggregated across the entire dataset to calculate common rank-based performance metrics such as the area under the receiver operating characteristic or precision-recall curves. In this work, we demonstrate that this approach creates a negative correlation between the average label of each training fold and the label of its corresponding test instance, a phenomenon that we term distributional bias. As machine learning models tend to regress to the mean of their training data, this distributional bias tends to negatively impact performance evaluation and hyperparameter optimization. We show that this effect generalizes to leave-P-out cross-validation and persists across a wide range of modeling and evaluation approaches, and that it can lead to a bias against stronger regularization. To address this, we propose a generalizable rebalanced cross-validation approach that corrects for distributional bias. We demonstrate that our approach improves cross-validation performance evaluation in synthetic simulations and in several published leave-one-out analyses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01652&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>George I. Austin, Itsik Pe&apos;er, Tal Korem</name></author><category term="stat.ME" /><summary type="html">Cross-validation is a common method for estimating the predictive performance of machine learning models. In a data-scarce regime, where one typically wishes to maximize the number of instances used for training the model, an approach called “leave-one-out cross-validation” is often used. In this design, a separate model is built for predicting each data instance after training on all other instances. Since this results in a single test data point available per model trained, predictions are aggregated across the entire dataset to calculate common rank-based performance metrics such as the area under the receiver operating characteristic or precision-recall curves. In this work, we demonstrate that this approach creates a negative correlation between the average label of each training fold and the label of its corresponding test instance, a phenomenon that we term distributional bias. As machine learning models tend to regress to the mean of their training data, this distributional bias tends to negatively impact performance evaluation and hyperparameter optimization. We show that this effect generalizes to leave-P-out cross-validation and persists across a wide range of modeling and evaluation approaches, and that it can lead to a bias against stronger regularization. To address this, we propose a generalizable rebalanced cross-validation approach that corrects for distributional bias. We demonstrate that our approach improves cross-validation performance evaluation in synthetic simulations and in several published leave-one-out analyses.</summary></entry><entry><title type="html">Efficient Fourier representations of families of Gaussian processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/EfficientFourierrepresentationsoffamiliesofGaussianprocesses.html" rel="alternate" type="text/html" title="Efficient Fourier representations of families of Gaussian processes" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/EfficientFourierrepresentationsoffamiliesofGaussianprocesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/EfficientFourierrepresentationsoffamiliesofGaussianprocesses.html">&lt;p&gt;We introduce a class of algorithms for constructing Fourier representations of Gaussian processes in $1$ dimension that are valid over ranges of hyperparameter values. The scaling and frequencies of the Fourier basis functions are evaluated numerically via generalized quadratures. The representations introduced allow for $O(m^3)$ inference, independent of $N$, for all hyperparameters in the user-specified range after $O(N + m^2\log{m})$ precomputation where $N$, the number of data points, is usually significantly larger than $m$, the number of basis functions. Inference independent of $N$ for various hyperparameters is facilitated by generalized quadratures, and the $O(N + m^2\log{m})$ precomputation is achieved with the non-uniform FFT. Numerical results are provided for Mat&apos;ern kernels with $\nu \in [3/2, 7/2]$ and lengthscale $\rho \in [0.1, 0.5]$ and squared-exponential kernels with lengthscale $\rho \in [0.1, 0.5]$. The algorithms of this paper generalize mathematically to higher dimensions, though they suffer from the standard curse of dimensionality.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2109.14081&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Philip Greengard</name></author><category term="stat.CO" /><summary type="html">We introduce a class of algorithms for constructing Fourier representations of Gaussian processes in $1$ dimension that are valid over ranges of hyperparameter values. The scaling and frequencies of the Fourier basis functions are evaluated numerically via generalized quadratures. The representations introduced allow for $O(m^3)$ inference, independent of $N$, for all hyperparameters in the user-specified range after $O(N + m^2\log{m})$ precomputation where $N$, the number of data points, is usually significantly larger than $m$, the number of basis functions. Inference independent of $N$ for various hyperparameters is facilitated by generalized quadratures, and the $O(N + m^2\log{m})$ precomputation is achieved with the non-uniform FFT. Numerical results are provided for Mat&apos;ern kernels with $\nu \in [3/2, 7/2]$ and lengthscale $\rho \in [0.1, 0.5]$ and squared-exponential kernels with lengthscale $\rho \in [0.1, 0.5]$. The algorithms of this paper generalize mathematically to higher dimensions, though they suffer from the standard curse of dimensionality.</summary></entry><entry><title type="html">Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/FeatureAttributionwithNecessityandSufficiencyviaDualstagePerturbationTestforCausalExplanation.html" rel="alternate" type="text/html" title="Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/FeatureAttributionwithNecessityandSufficiencyviaDualstagePerturbationTestforCausalExplanation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/FeatureAttributionwithNecessityandSufficiencyviaDualstagePerturbationTestforCausalExplanation.html">&lt;p&gt;We investigate the problem of explainability for machine learning models, focusing on Feature Attribution Methods (FAMs) that evaluate feature importance through perturbation tests. Despite their utility, FAMs struggle to distinguish the contributions of different features, when their prediction changes are similar after perturbation. To enhance FAMs’ discriminative power, we introduce Feature Attribution with Necessity and Sufficiency (FANS), which find a neighborhood of the input such that perturbing samples within this neighborhood have a high Probability of being Necessity and Sufficiency (PNS) cause for the change in predictions, and use this PNS as the importance of the feature. Specifically, FANS compute this PNS via a heuristic strategy for estimating the neighborhood and a perturbation test involving two stages (factual and interventional) for counterfactual reasoning. To generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution. We demonstrate that FANS outperforms existing attribution methods on six benchmarks. Please refer to the source code via \url{https://github.com/DMIRLAB-Group/FANS}.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.08845&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xuexin Chen, Ruichu Cai, Zhengting Huang, Yuxuan Zhu, Julien Horwood, Zhifeng Hao, Zijian Li, Jose Miguel Hernandez-Lobato</name></author><category term="stat.ME" /><summary type="html">We investigate the problem of explainability for machine learning models, focusing on Feature Attribution Methods (FAMs) that evaluate feature importance through perturbation tests. Despite their utility, FAMs struggle to distinguish the contributions of different features, when their prediction changes are similar after perturbation. To enhance FAMs’ discriminative power, we introduce Feature Attribution with Necessity and Sufficiency (FANS), which find a neighborhood of the input such that perturbing samples within this neighborhood have a high Probability of being Necessity and Sufficiency (PNS) cause for the change in predictions, and use this PNS as the importance of the feature. Specifically, FANS compute this PNS via a heuristic strategy for estimating the neighborhood and a perturbation test involving two stages (factual and interventional) for counterfactual reasoning. To generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution. We demonstrate that FANS outperforms existing attribution methods on six benchmarks. Please refer to the source code via \url{https://github.com/DMIRLAB-Group/FANS}.</summary></entry><entry><title type="html">Generalised Bayes Linear Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/GeneralisedBayesLinearInference.html" rel="alternate" type="text/html" title="Generalised Bayes Linear Inference" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/GeneralisedBayesLinearInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/GeneralisedBayesLinearInference.html">&lt;p&gt;Motivated by big data and the vast parameter spaces in modern machine learning models, optimisation approaches to Bayesian inference have seen a surge in popularity in recent years. In this paper, we address the connection between the popular new methods termed generalised Bayesian inference and Bayes linear methods. We propose a further generalisation to Bayesian inference that unifies these and other recent approaches by considering the Bayesian inference problem as one of finding the closest point in a particular solution space to a data generating process, where these notions differ depending on user-specified geometries and foundational belief systems. Motivated by this framework, we propose a generalisation to Bayes linear approaches that enables fast and principled inferences that obey the coherence requirements implied by domain restrictions on random quantities. We demonstrate the efficacy of generalised Bayes linear inference on a number of examples, including monotonic regression and inference for spatial counts. This paper is accompanied by an R package available at github.com/astfalckl/bayeslinear.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14145&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lachlan Astfalck, Cassandra Bird, Daniel Williamson</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Motivated by big data and the vast parameter spaces in modern machine learning models, optimisation approaches to Bayesian inference have seen a surge in popularity in recent years. In this paper, we address the connection between the popular new methods termed generalised Bayesian inference and Bayes linear methods. We propose a further generalisation to Bayesian inference that unifies these and other recent approaches by considering the Bayesian inference problem as one of finding the closest point in a particular solution space to a data generating process, where these notions differ depending on user-specified geometries and foundational belief systems. Motivated by this framework, we propose a generalisation to Bayes linear approaches that enables fast and principled inferences that obey the coherence requirements implied by domain restrictions on random quantities. We demonstrate the efficacy of generalised Bayes linear inference on a number of examples, including monotonic regression and inference for spatial counts. This paper is accompanied by an R package available at github.com/astfalckl/bayeslinear.</summary></entry><entry><title type="html">How should parallel cluster randomized trials with a baseline period be analyzed? A survey of estimands and common estimators</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/HowshouldparallelclusterrandomizedtrialswithabaselineperiodbeanalyzedAsurveyofestimandsandcommonestimators.html" rel="alternate" type="text/html" title="How should parallel cluster randomized trials with a baseline period be analyzed? A survey of estimands and common estimators" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/HowshouldparallelclusterrandomizedtrialswithabaselineperiodbeanalyzedAsurveyofestimandsandcommonestimators</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/HowshouldparallelclusterrandomizedtrialswithabaselineperiodbeanalyzedAsurveyofestimandsandcommonestimators.html">&lt;p&gt;The parallel cluster randomized trial with baseline (PB-CRT) is a common variant of the standard parallel cluster randomized trial (P-CRT) that maintains parallel randomization but additionally allows for both within and between-cluster comparisons. We define two estimands of interest in the context of PB-CRTs, the participant-average treatment effect (pATE) and cluster-average treatment effect (cATE), to address participant and cluster-level hypotheses. Previous work has indicated that under informative cluster sizes, commonly used mixed-effects models may yield inconsistent estimators for the estimands of interest. In this work, we theoretically derive the convergence of the unweighted and inverse cluster-period size weighted (i.) independence estimating equation, (ii.) fixed-effects model, (iii.) exchangeable mixed-effects model, and (iv.) nested-exchangeable mixed-effects model treatment effect estimators in a PB-CRT with continuous outcomes. We report a simulation study to evaluate the bias and inference with these different treatment effect estimators and their corresponding model-based or jackknife variance estimators. We then re-analyze a PB-CRT examining the effects of community youth teams on improving mental health among adolescent girls in rural eastern India. We demonstrate that the unweighted and weighted independence estimating equation and fixed-effects model regularly yield consistent estimators for the pATE and cATE estimands, whereas the mixed-effects models yield inconsistent estimators under informative cluster sizes. However, we demonstrate that unlike the nested-exchangeable mixed-effects model and corresponding analyses in P-CRTs, the exchangeable mixed-effects model is surprisingly robust to bias in many PB-CRT scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02028&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kenneth Menglin Lee, Fan Li</name></author><category term="stat.ME" /><summary type="html">The parallel cluster randomized trial with baseline (PB-CRT) is a common variant of the standard parallel cluster randomized trial (P-CRT) that maintains parallel randomization but additionally allows for both within and between-cluster comparisons. We define two estimands of interest in the context of PB-CRTs, the participant-average treatment effect (pATE) and cluster-average treatment effect (cATE), to address participant and cluster-level hypotheses. Previous work has indicated that under informative cluster sizes, commonly used mixed-effects models may yield inconsistent estimators for the estimands of interest. In this work, we theoretically derive the convergence of the unweighted and inverse cluster-period size weighted (i.) independence estimating equation, (ii.) fixed-effects model, (iii.) exchangeable mixed-effects model, and (iv.) nested-exchangeable mixed-effects model treatment effect estimators in a PB-CRT with continuous outcomes. We report a simulation study to evaluate the bias and inference with these different treatment effect estimators and their corresponding model-based or jackknife variance estimators. We then re-analyze a PB-CRT examining the effects of community youth teams on improving mental health among adolescent girls in rural eastern India. We demonstrate that the unweighted and weighted independence estimating equation and fixed-effects model regularly yield consistent estimators for the pATE and cATE estimands, whereas the mixed-effects models yield inconsistent estimators under informative cluster sizes. However, we demonstrate that unlike the nested-exchangeable mixed-effects model and corresponding analyses in P-CRTs, the exchangeable mixed-effects model is surprisingly robust to bias in many PB-CRT scenarios.</summary></entry><entry><title type="html">Identifying Sample Size and Accuracy and Precision of the Estimators in Case-Crossover Analysis with Distributed Lags of Heteroskedastic Time-Varying Continuous Exposures Measured with Simple or Complex Error</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/IdentifyingSampleSizeandAccuracyandPrecisionoftheEstimatorsinCaseCrossoverAnalysiswithDistributedLagsofHeteroskedasticTimeVaryingContinuousExposuresMeasuredwithSimpleorComplexError.html" rel="alternate" type="text/html" title="Identifying Sample Size and Accuracy and Precision of the Estimators in Case-Crossover Analysis with Distributed Lags of Heteroskedastic Time-Varying Continuous Exposures Measured with Simple or Complex Error" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/IdentifyingSampleSizeandAccuracyandPrecisionoftheEstimatorsinCaseCrossoverAnalysiswithDistributedLagsofHeteroskedasticTimeVaryingContinuousExposuresMeasuredwithSimpleorComplexError</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/IdentifyingSampleSizeandAccuracyandPrecisionoftheEstimatorsinCaseCrossoverAnalysiswithDistributedLagsofHeteroskedasticTimeVaryingContinuousExposuresMeasuredwithSimpleorComplexError.html">&lt;p&gt;Power analyses help investigators design robust and reproducible research. Understanding of determinants of statistical power is helpful in interpreting results in publications and in analysis for causal inference. Case-crossover analysis, a matched case-control analysis, is widely used to estimate health effects of short-term exposures. Despite its widespread use, understanding of sample size, statistical power, and the accuracy and precision of the estimator in real-world data settings is very limited. First, the variance of exposures that exhibit spatiotemporal patterns may be heteroskedastic (e.g., air pollution and temperature exposures, impacted by climate change). Second, distributed lags of the exposure variable may be used to identify critical exposure time-windows. Third, exposure measurement error is not uncommon, impacting the accuracy and/or precision of the estimator, depending on the measurement error mechanism. Exposure measurement errors result in covariate measurement errors of distributed lags. All these issues complicate the understanding. Therefore, I developed approximation equations for sample size, estimates of the estimators and standard errors, and identified conditions for applications. I discussed polynomials for non-linear effect estimation. I analyzed air pollution estimates in the United States (U.S.), developed by U.S. Environmental Protection Agency to examine errors, and conducted statistical simulations. Overall, sample size can be calculated based on external information about exposure variable validation, without validation data in hand. For estimators of distributed lags, calculations may perform well if residual confounding due to covariate measurement error is not severe. This condition may sometimes be difficult to identify without validation data, suggesting investigators should consider validation research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02369&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Honghyok Kim</name></author><category term="stat.ME" /><summary type="html">Power analyses help investigators design robust and reproducible research. Understanding of determinants of statistical power is helpful in interpreting results in publications and in analysis for causal inference. Case-crossover analysis, a matched case-control analysis, is widely used to estimate health effects of short-term exposures. Despite its widespread use, understanding of sample size, statistical power, and the accuracy and precision of the estimator in real-world data settings is very limited. First, the variance of exposures that exhibit spatiotemporal patterns may be heteroskedastic (e.g., air pollution and temperature exposures, impacted by climate change). Second, distributed lags of the exposure variable may be used to identify critical exposure time-windows. Third, exposure measurement error is not uncommon, impacting the accuracy and/or precision of the estimator, depending on the measurement error mechanism. Exposure measurement errors result in covariate measurement errors of distributed lags. All these issues complicate the understanding. Therefore, I developed approximation equations for sample size, estimates of the estimators and standard errors, and identified conditions for applications. I discussed polynomials for non-linear effect estimation. I analyzed air pollution estimates in the United States (U.S.), developed by U.S. Environmental Protection Agency to examine errors, and conducted statistical simulations. Overall, sample size can be calculated based on external information about exposure variable validation, without validation data in hand. For estimators of distributed lags, calculations may perform well if residual confounding due to covariate measurement error is not severe. This condition may sometimes be difficult to identify without validation data, suggesting investigators should consider validation research.</summary></entry><entry><title type="html">Impacts of Climate Change on Mortality: An extrapolation of temperature effects based on time series data in France</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ImpactsofClimateChangeonMortalityAnextrapolationoftemperatureeffectsbasedontimeseriesdatainFrance.html" rel="alternate" type="text/html" title="Impacts of Climate Change on Mortality: An extrapolation of temperature effects based on time series data in France" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ImpactsofClimateChangeonMortalityAnextrapolationoftemperatureeffectsbasedontimeseriesdatainFrance</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ImpactsofClimateChangeonMortalityAnextrapolationoftemperatureeffectsbasedontimeseriesdatainFrance.html">&lt;p&gt;Most contemporary mortality models rely on extrapolating trends or past events. However, population dynamics will be significantly impacted by climate change, notably the influence of temperatures on mortality. In this paper, we introduce a novel approach to incorporate temperature effects on projected mortality using a multi-population mortality model. This method combines a stochastic mortality model with a climate epidemiology model, predicting mortality variations due to daily temperature fluctuations, be it excesses or insufficiencies. The significance of this approach lies in its ability to disrupt mortality projections by utilizing temperature forecasts from climate models and to assess the impact of this unaccounted risk factor in conventional mortality models. We illustrate this proposed mortality model using French data stratified by gender, focusing on past temperatures and mortality. Utilizing climate model predictions across various IPCC scenarios, we investigate gains and loss in life expectancy linked to temperatures and the additional mortality induced by extreme heatwaves, and quantify them by assessing this new risk factor in prediction intervals. Furthermore, we analyze the geographical differences across the Metropolitan France.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02054&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Quentin Guibert , Gaëlle Pincemin , Frédéric Planchet</name></author><category term="stat.AP" /><summary type="html">Most contemporary mortality models rely on extrapolating trends or past events. However, population dynamics will be significantly impacted by climate change, notably the influence of temperatures on mortality. In this paper, we introduce a novel approach to incorporate temperature effects on projected mortality using a multi-population mortality model. This method combines a stochastic mortality model with a climate epidemiology model, predicting mortality variations due to daily temperature fluctuations, be it excesses or insufficiencies. The significance of this approach lies in its ability to disrupt mortality projections by utilizing temperature forecasts from climate models and to assess the impact of this unaccounted risk factor in conventional mortality models. We illustrate this proposed mortality model using French data stratified by gender, focusing on past temperatures and mortality. Utilizing climate model predictions across various IPCC scenarios, we investigate gains and loss in life expectancy linked to temperatures and the additional mortality induced by extreme heatwaves, and quantify them by assessing this new risk factor in prediction intervals. Furthermore, we analyze the geographical differences across the Metropolitan France.</summary></entry><entry><title type="html">Jacobi Prior: An Alternative Bayesian Method for Supervised Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/JacobiPriorAnAlternativeBayesianMethodforSupervisedLearning.html" rel="alternate" type="text/html" title="Jacobi Prior: An Alternative Bayesian Method for Supervised Learning" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/JacobiPriorAnAlternativeBayesianMethodforSupervisedLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/JacobiPriorAnAlternativeBayesianMethodforSupervisedLearning.html">&lt;p&gt;The `Jacobi prior’ is an alternative Bayesian method for predictive models. It performs better than well-known methods such as Lasso, Ridge, Elastic Net, and MCMC-based Horse-Shoe Prior, particularly in terms of prediction accuracy and run-time. This method is implemented for Gaussian process classification, adeptly handling a nonlinear decision boundary. The Jacobi prior demonstrates its capability to manage partitioned data across global servers, making it highly useful in distributed computing environments. Additionally, we show that the Jacobi prior is more than a hundred times faster than these methods while maintaining similar predictive accuracy. As the method is both fast and accurate, it is advantageous for organisations looking to reduce their environmental impact and meet ESG standards. To demonstrate the effectiveness of the Jacobi prior, we conducted a detailed simulation study with four experiments focusing on statistical consistency, accuracy, and speed. We also present two empirical studies: the first evaluates credit risk by analysing default probability using data from the U.S. Small Business Administration (SBA), and the second uses the Jacobi prior for classifying stars, quasars, and galaxies in a three-class problem using multinomial logit regression on data from the Sloan Digital Sky Survey. Different filters were used as features in this study. All codes and datasets for this paper are available in the following GitHub repository : https://github.com/sourish-cmi/Jacobi-Prior/&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.11345&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sourish Das, Shouvik Sardar</name></author><category term="stat.ME" /><summary type="html">The `Jacobi prior’ is an alternative Bayesian method for predictive models. It performs better than well-known methods such as Lasso, Ridge, Elastic Net, and MCMC-based Horse-Shoe Prior, particularly in terms of prediction accuracy and run-time. This method is implemented for Gaussian process classification, adeptly handling a nonlinear decision boundary. The Jacobi prior demonstrates its capability to manage partitioned data across global servers, making it highly useful in distributed computing environments. Additionally, we show that the Jacobi prior is more than a hundred times faster than these methods while maintaining similar predictive accuracy. As the method is both fast and accurate, it is advantageous for organisations looking to reduce their environmental impact and meet ESG standards. To demonstrate the effectiveness of the Jacobi prior, we conducted a detailed simulation study with four experiments focusing on statistical consistency, accuracy, and speed. We also present two empirical studies: the first evaluates credit risk by analysing default probability using data from the U.S. Small Business Administration (SBA), and the second uses the Jacobi prior for classifying stars, quasars, and galaxies in a three-class problem using multinomial logit regression on data from the Sloan Digital Sky Survey. Different filters were used as features in this study. All codes and datasets for this paper are available in the following GitHub repository : https://github.com/sourish-cmi/Jacobi-Prior/</summary></entry><entry><title type="html">LongBet: Heterogeneous Treatment Effect Estimation in Panel Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/LongBetHeterogeneousTreatmentEffectEstimationinPanelData.html" rel="alternate" type="text/html" title="LongBet: Heterogeneous Treatment Effect Estimation in Panel Data" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/LongBetHeterogeneousTreatmentEffectEstimationinPanelData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/LongBetHeterogeneousTreatmentEffectEstimationinPanelData.html">&lt;p&gt;This paper introduces a novel approach for estimating heterogeneous treatment effects of binary treatment in panel data, particularly focusing on short panel data with large cross-sectional data and observed confoundings. In contrast to traditional literature in difference-in-differences method that often relies on the parallel trend assumption, our proposed model does not necessitate such an assumption. Instead, it leverages observed confoundings to impute potential outcomes and identify treatment effects. The method presented is a Bayesian semi-parametric approach based on the Bayesian causal forest model, which is extended here to suit panel data settings. The approach offers the advantage of the Bayesian approach to provides uncertainty quantification on the estimates. Simulation studies demonstrate its performance with and without the presence of parallel trend. Additionally, our proposed model enables the estimation of conditional average treatment effects, a capability that is rarely available in panel data settings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02530&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Meijia Wang, Ignacio Martinez, P. Richard Hahn</name></author><category term="stat.ME" /><summary type="html">This paper introduces a novel approach for estimating heterogeneous treatment effects of binary treatment in panel data, particularly focusing on short panel data with large cross-sectional data and observed confoundings. In contrast to traditional literature in difference-in-differences method that often relies on the parallel trend assumption, our proposed model does not necessitate such an assumption. Instead, it leverages observed confoundings to impute potential outcomes and identify treatment effects. The method presented is a Bayesian semi-parametric approach based on the Bayesian causal forest model, which is extended here to suit panel data settings. The approach offers the advantage of the Bayesian approach to provides uncertainty quantification on the estimates. Simulation studies demonstrate its performance with and without the presence of parallel trend. Additionally, our proposed model enables the estimation of conditional average treatment effects, a capability that is rarely available in panel data settings.</summary></entry><entry><title type="html">Long-term foehn reconstruction combining unsupervised and supervised learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Longtermfoehnreconstructioncombiningunsupervisedandsupervisedlearning.html" rel="alternate" type="text/html" title="Long-term foehn reconstruction combining unsupervised and supervised learning" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Longtermfoehnreconstructioncombiningunsupervisedandsupervisedlearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Longtermfoehnreconstructioncombiningunsupervisedandsupervisedlearning.html">&lt;p&gt;Foehn winds, characterized by abrupt temperature increases and wind speed changes, significantly impact regions on the leeward side of mountain ranges, e.g., by spreading wildfires. Understanding how foehn occurrences change under climate change is crucial. Unfortunately, foehn cannot be measured directly but has to be inferred from meteorological measurements employing suitable classification schemes. Hence, this approach is typically limited to specific periods for which the necessary data are available. We present a novel approach for reconstructing historical foehn occurrences using a combination of unsupervised and supervised probabilistic statistical learning methods. We utilize in-situ measurements (available for recent decades) to train an unsupervised learner (finite mixture model) for automatic foehn classification. These labeled data are then linked to reanalysis data (covering longer periods) using a supervised learner (lasso or boosting). This allows to reconstruct past foehn probabilities based solely on reanalysis data. Applying this method to ERA5 reanalysis data for six stations across Switzerland and Austria achieves accurate hourly reconstructions of north and south foehn occurrence, respectively, dating back to 1940. This paves the way for investigating how seasonal foehn patterns have evolved over the past 83 years, providing valuable insights into climate change impacts on these critical wind events.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01818&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Reto Stauffer, Achim Zeileis, Georg J. Mayr</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Foehn winds, characterized by abrupt temperature increases and wind speed changes, significantly impact regions on the leeward side of mountain ranges, e.g., by spreading wildfires. Understanding how foehn occurrences change under climate change is crucial. Unfortunately, foehn cannot be measured directly but has to be inferred from meteorological measurements employing suitable classification schemes. Hence, this approach is typically limited to specific periods for which the necessary data are available. We present a novel approach for reconstructing historical foehn occurrences using a combination of unsupervised and supervised probabilistic statistical learning methods. We utilize in-situ measurements (available for recent decades) to train an unsupervised learner (finite mixture model) for automatic foehn classification. These labeled data are then linked to reanalysis data (covering longer periods) using a supervised learner (lasso or boosting). This allows to reconstruct past foehn probabilities based solely on reanalysis data. Applying this method to ERA5 reanalysis data for six stations across Switzerland and Austria achieves accurate hourly reconstructions of north and south foehn occurrence, respectively, dating back to 1940. This paves the way for investigating how seasonal foehn patterns have evolved over the past 83 years, providing valuable insights into climate change impacts on these critical wind events.</summary></entry><entry><title type="html">Majority Vote for Distributed Differentially Private Sign Selection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/MajorityVoteforDistributedDifferentiallyPrivateSignSelection.html" rel="alternate" type="text/html" title="Majority Vote for Distributed Differentially Private Sign Selection" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/MajorityVoteforDistributedDifferentiallyPrivateSignSelection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/MajorityVoteforDistributedDifferentiallyPrivateSignSelection.html">&lt;p&gt;Privacy-preserving data analysis has become more prevalent in recent years. In this study, we propose a distributed group differentially private Majority Vote mechanism, for the sign selection problem in a distributed setup. To achieve this, we apply the iterative peeling to the stability function and use the exponential mechanism to recover the signs. For enhanced applicability, we study the private sign selection for mean estimation and linear regression problems, in distributed systems. Our method recovers the support and signs with the optimal signal-to-noise ratio as in the non-private scenario, which is better than contemporary works of private variable selections. Moreover, the sign selection consistency is justified by theoretical guarantees. Simulation studies are conducted to demonstrate the effectiveness of the proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2209.04419&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Weidong Liu, Jiyuan Tu, Xiaojun Mao, Xi Chen</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Privacy-preserving data analysis has become more prevalent in recent years. In this study, we propose a distributed group differentially private Majority Vote mechanism, for the sign selection problem in a distributed setup. To achieve this, we apply the iterative peeling to the stability function and use the exponential mechanism to recover the signs. For enhanced applicability, we study the private sign selection for mean estimation and linear regression problems, in distributed systems. Our method recovers the support and signs with the optimal signal-to-noise ratio as in the non-private scenario, which is better than contemporary works of private variable selections. Moreover, the sign selection consistency is justified by theoretical guarantees. Simulation studies are conducted to demonstrate the effectiveness of the proposed method.</summary></entry><entry><title type="html">Markov Chain Monte Carlo with Gaussian Process Emulation for a 1D Hemodynamics Model of CTEPH</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/MarkovChainMonteCarlowithGaussianProcessEmulationfora1DHemodynamicsModelofCTEPH.html" rel="alternate" type="text/html" title="Markov Chain Monte Carlo with Gaussian Process Emulation for a 1D Hemodynamics Model of CTEPH" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/MarkovChainMonteCarlowithGaussianProcessEmulationfora1DHemodynamicsModelofCTEPH</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/MarkovChainMonteCarlowithGaussianProcessEmulationfora1DHemodynamicsModelofCTEPH.html">&lt;p&gt;Microvascular disease is a contributor to persistent pulmonary hypertension in those with chronic thromboembolic pulmonary hypertension (CTEPH). The heterogenous nature of the micro and macrovascular defects motivates the use of personalized computational models, which can predict flow dynamics within multiple generations of the arterial tree and into the microvasculature. Our study uses computational hemodynamics models and Gaussian processes for rapid, subject-specific calibration using retrospective data from a large animal model of CTEPH. Our subject-specific predictions shed light on microvascular dysfunction and arterial wall shear stress changes in CTEPH.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01599&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Amirreza Kachabi, Mitchel J. Colebank, Sofia Altieri Correa, Naomi C. Chesler</name></author><category term="stat.AP" /><summary type="html">Microvascular disease is a contributor to persistent pulmonary hypertension in those with chronic thromboembolic pulmonary hypertension (CTEPH). The heterogenous nature of the micro and macrovascular defects motivates the use of personalized computational models, which can predict flow dynamics within multiple generations of the arterial tree and into the microvasculature. Our study uses computational hemodynamics models and Gaussian processes for rapid, subject-specific calibration using retrospective data from a large animal model of CTEPH. Our subject-specific predictions shed light on microvascular dysfunction and arterial wall shear stress changes in CTEPH.</summary></entry><entry><title type="html">Measuring the Dispersion of Discrete Distributions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/MeasuringtheDispersionofDiscreteDistributions.html" rel="alternate" type="text/html" title="Measuring the Dispersion of Discrete Distributions" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/MeasuringtheDispersionofDiscreteDistributions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/MeasuringtheDispersionofDiscreteDistributions.html">&lt;p&gt;Measuring dispersion is among the most fundamental and ubiquitous concepts in statistics, both in applied and theoretical contexts. In order to ensure that dispersion measures like the standard deviation indeed capture the dispersion of any given distribution, they are by definition required to preserve a stochastic order of dispersion. The most basic order that functions as a foundation underneath the concept of dispersion measures is the so-called dispersive order. However, that order is incompatible with almost all discrete distributions, including all lattice distributions and most empirical distributions. Thus, there is no guarantee that popular measures properly capture the dispersion of these distributions.
  In this paper, discrete adaptations of the dispersive order are derived and analyzed. Their derivation is directly informed by key properties of the dispersive order in order to obtain a foundation for the measurement of discrete dispersion that is as similar as possible to the continuous setting. Two slightly different orders are obtained that both have numerous properties that the original dispersive order also has. Their behaviour on well-known families of lattice distribution is generally as expected if the parameter differences are large enough. Most popular dispersion measures preserve both discrete dispersive orders, which rigorously ensures that they are also meaningful in discrete settings. However, the interquantile range preserves neither discrete order, yielding that it should not be used to measure the dispersion of discrete distributions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02124&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andreas Eberl, Bernhard Klar</name></author><category term="stat.ME" /><summary type="html">Measuring dispersion is among the most fundamental and ubiquitous concepts in statistics, both in applied and theoretical contexts. In order to ensure that dispersion measures like the standard deviation indeed capture the dispersion of any given distribution, they are by definition required to preserve a stochastic order of dispersion. The most basic order that functions as a foundation underneath the concept of dispersion measures is the so-called dispersive order. However, that order is incompatible with almost all discrete distributions, including all lattice distributions and most empirical distributions. Thus, there is no guarantee that popular measures properly capture the dispersion of these distributions. In this paper, discrete adaptations of the dispersive order are derived and analyzed. Their derivation is directly informed by key properties of the dispersive order in order to obtain a foundation for the measurement of discrete dispersion that is as similar as possible to the continuous setting. Two slightly different orders are obtained that both have numerous properties that the original dispersive order also has. Their behaviour on well-known families of lattice distribution is generally as expected if the parameter differences are large enough. Most popular dispersion measures preserve both discrete dispersive orders, which rigorously ensures that they are also meaningful in discrete settings. However, the interquantile range preserves neither discrete order, yielding that it should not be used to measure the dispersion of discrete distributions.</summary></entry><entry><title type="html">Model Assessment and Selection under Temporal Distribution Shift</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ModelAssessmentandSelectionunderTemporalDistributionShift.html" rel="alternate" type="text/html" title="Model Assessment and Selection under Temporal Distribution Shift" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ModelAssessmentandSelectionunderTemporalDistributionShift</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/ModelAssessmentandSelectionunderTemporalDistributionShift.html">&lt;p&gt;We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs. To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model. This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates. Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.08672&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elise Han, Chengpiao Huang, Kaizheng Wang</name></author><category term="stat.ME" /><summary type="html">We investigate model assessment and selection in a changing environment, by synthesizing datasets from both the current time period and historical epochs. To tackle unknown and potentially arbitrary temporal distribution shift, we develop an adaptive rolling window approach to estimate the generalization error of a given model. This strategy also facilitates the comparison between any two candidate models by estimating the difference of their generalization errors. We further integrate pairwise comparisons into a single-elimination tournament, achieving near-optimal model selection from a collection of candidates. Theoretical analyses and numerical experiments demonstrate the adaptivity of our proposed methods to the non-stationarity in data.</summary></entry><entry><title type="html">Monitoring overall survival in pivotal trials in indolent cancers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Monitoringoverallsurvivalinpivotaltrialsinindolentcancers.html" rel="alternate" type="text/html" title="Monitoring overall survival in pivotal trials in indolent cancers" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Monitoringoverallsurvivalinpivotaltrialsinindolentcancers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Monitoringoverallsurvivalinpivotaltrialsinindolentcancers.html">&lt;p&gt;Indolent cancers are characterized by long overall survival (OS) times. Therefore, powering a clinical trial to provide definitive assessment of the effects of an experimental intervention on OS in a reasonable timeframe is generally infeasible. Instead, the primary outcome in many pivotal trials is an intermediate clinical response such as progression-free survival (PFS). In several recently reported pivotal trials of interventions for indolent cancers that yielded promising results on an intermediate outcome, however, more mature data or post-approval trials showed concerning OS trends. These problematic results have prompted a keen interest in quantitative approaches for monitoring OS that can support regulatory decision-making related to the risk of an unacceptably large detrimental effect on OS. For example, the US Food and Drug Administration, the American Association for Cancer Research, and the American Statistical Association recently organized a one-day multi-stakeholder workshop entitled ‘Overall Survival in Oncology Clinical Trials’. In this paper, we propose OS monitoring guidelines tailored for the setting of indolent cancers. Our pragmatic approach is modeled, in part, on the monitoring guidelines the FDA has used in cardiovascular safety trials conducted in Type 2 Diabetes Mellitus. We illustrate proposals through application to several examples informed by actual case studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.20658&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thomas R Fleming, Lisa V Hampson, Bharani Bharani-Dharan, Frank Bretz, Arunava Chakravartty, Thibaud Coroller, Evanthia Koukouli, Janet Wittes, Nigel Yateman, Emmanuel Zuber</name></author><category term="stat.AP" /><summary type="html">Indolent cancers are characterized by long overall survival (OS) times. Therefore, powering a clinical trial to provide definitive assessment of the effects of an experimental intervention on OS in a reasonable timeframe is generally infeasible. Instead, the primary outcome in many pivotal trials is an intermediate clinical response such as progression-free survival (PFS). In several recently reported pivotal trials of interventions for indolent cancers that yielded promising results on an intermediate outcome, however, more mature data or post-approval trials showed concerning OS trends. These problematic results have prompted a keen interest in quantitative approaches for monitoring OS that can support regulatory decision-making related to the risk of an unacceptably large detrimental effect on OS. For example, the US Food and Drug Administration, the American Association for Cancer Research, and the American Statistical Association recently organized a one-day multi-stakeholder workshop entitled ‘Overall Survival in Oncology Clinical Trials’. In this paper, we propose OS monitoring guidelines tailored for the setting of indolent cancers. Our pragmatic approach is modeled, in part, on the monitoring guidelines the FDA has used in cardiovascular safety trials conducted in Type 2 Diabetes Mellitus. We illustrate proposals through application to several examples informed by actual case studies.</summary></entry><entry><title type="html">Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/MultiplyRobustEstimationforLocalDistributionShiftswithMultipleDomains.html" rel="alternate" type="text/html" title="Multiply Robust Estimation for Local Distribution Shifts with Multiple Domains" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/MultiplyRobustEstimationforLocalDistributionShiftswithMultipleDomains</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/MultiplyRobustEstimationforLocalDistributionShiftswithMultipleDomains.html">&lt;p&gt;Distribution shifts are ubiquitous in real-world machine learning applications, posing a challenge to the generalization of models trained on one data distribution to another. We focus on scenarios where data distributions vary across multiple segments of the entire population and only make local assumptions about the differences between training and test (deployment) distributions within each segment. We propose a two-stage multiply robust estimation method to improve model performance on each individual segment for tabular data analysis. The method involves fitting a linear combination of the based models, learned using clusters of training data from multiple segments, followed by a refinement step for each segment. Our method is designed to be implemented with commonly used off-the-shelf machine learning models. We establish theoretical guarantees on the generalization bound of the method on the test risk. With extensive experiments on synthetic and real datasets, we demonstrate that the proposed method substantially improves over existing alternatives in prediction accuracy and robustness on both regression and classification tasks. We also assess its effectiveness on a user city prediction dataset from Meta.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.14145&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Steven Wilkins-Reeves, Xu Chen, Qi Ma, Christine Agarwal, Aude Hofleitner</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Distribution shifts are ubiquitous in real-world machine learning applications, posing a challenge to the generalization of models trained on one data distribution to another. We focus on scenarios where data distributions vary across multiple segments of the entire population and only make local assumptions about the differences between training and test (deployment) distributions within each segment. We propose a two-stage multiply robust estimation method to improve model performance on each individual segment for tabular data analysis. The method involves fitting a linear combination of the based models, learned using clusters of training data from multiple segments, followed by a refinement step for each segment. Our method is designed to be implemented with commonly used off-the-shelf machine learning models. We establish theoretical guarantees on the generalization bound of the method on the test risk. With extensive experiments on synthetic and real datasets, we demonstrate that the proposed method substantially improves over existing alternatives in prediction accuracy and robustness on both regression and classification tasks. We also assess its effectiveness on a user city prediction dataset from Meta.</summary></entry><entry><title type="html">Optimal Stock Portfolio Selection with a Multivariate Hidden Markov Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/OptimalStockPortfolioSelectionwithaMultivariateHiddenMarkovModel.html" rel="alternate" type="text/html" title="Optimal Stock Portfolio Selection with a Multivariate Hidden Markov Model" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/OptimalStockPortfolioSelectionwithaMultivariateHiddenMarkovModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/OptimalStockPortfolioSelectionwithaMultivariateHiddenMarkovModel.html">&lt;p&gt;The underlying market trends that drive stock price fluctuations are often referred to in terms of bull and bear markets. Optimal stock portfolio selection methods need to take into account these market trends; however, the bull and bear market states tend to be unobserved and can only be assigned retrospectively. We fit a linked hidden Markov model (LHMM) to relative stock price changes for S&amp;amp;P 500 stocks from 2011–2016 based on weekly closing values. The LHMM consists of a multivariate state process whose individual components correspond to HMMs for each of the 12 sectors of the S\&amp;amp;P 500 stocks. The state processes are linked using a Gaussian copula so that the states of the component chains are correlated at any given time point. The LHMM allows us to capture more heterogeneity in the underlying market dynamics for each sector. In this study, stock performances are evaluated in terms of capital gains using the LHMM by utilizing historical stock price data. Based on the fitted LHMM, optimal stock portfolios are constructed to maximize capital gain while balancing reward and risk. Under out-of-sample testing, the annual capital gain for the portfolios for 2016–2017 are calculated. Portfolios constructed using the LHMM are able to generate returns comparable to the S&amp;amp;P 500 index.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02297&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Reetam Majumder, Qing Ji, Nagaraj K. Neerchal</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The underlying market trends that drive stock price fluctuations are often referred to in terms of bull and bear markets. Optimal stock portfolio selection methods need to take into account these market trends; however, the bull and bear market states tend to be unobserved and can only be assigned retrospectively. We fit a linked hidden Markov model (LHMM) to relative stock price changes for S&amp;amp;P 500 stocks from 2011–2016 based on weekly closing values. The LHMM consists of a multivariate state process whose individual components correspond to HMMs for each of the 12 sectors of the S\&amp;amp;P 500 stocks. The state processes are linked using a Gaussian copula so that the states of the component chains are correlated at any given time point. The LHMM allows us to capture more heterogeneity in the underlying market dynamics for each sector. In this study, stock performances are evaluated in terms of capital gains using the LHMM by utilizing historical stock price data. Based on the fitted LHMM, optimal stock portfolios are constructed to maximize capital gain while balancing reward and risk. Under out-of-sample testing, the annual capital gain for the portfolios for 2016–2017 are calculated. Portfolios constructed using the LHMM are able to generate returns comparable to the S&amp;amp;P 500 index.</summary></entry><entry><title type="html">Orthogonal Causal Calibration</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/OrthogonalCausalCalibration.html" rel="alternate" type="text/html" title="Orthogonal Causal Calibration" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/OrthogonalCausalCalibration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/OrthogonalCausalCalibration.html">&lt;p&gt;Estimates of causal parameters such as conditional average treatment effects and conditional quantile treatment effects play an important role in real-world decision making. Given this importance, one should ensure these estimators are calibrated. While there is a rich literature on calibrating estimators of non-causal parameters, very few methods have been derived for calibrating estimators of causal parameters, or more generally estimators of quantities involving nuisance parameters.
  In this work, we provide a general framework for calibrating predictors involving nuisance estimation. We consider a notion of calibration defined with respect to an arbitrary, nuisance-dependent loss $\ell$, under which we say an estimator $\theta$ is calibrated if its predictions cannot be changed on any level set to decrease loss. We prove generic upper bounds on the calibration error of any causal parameter estimate $\theta$ with respect to any loss $\ell$ using a concept called Neyman Orthogonality. Our bounds involve two decoupled terms - one measuring the error in estimating the unknown nuisance parameters, and the other representing the calibration error in a hypothetical world where the learned nuisance estimates were true. We use our bound to analyze the convergence of two sample splitting algorithms for causal calibration. One algorithm, which applies to universally orthogonalizable loss functions, transforms the data into generalized pseudo-outcomes and applies an off-the-shelf calibration procedure. The other algorithm, which applies to conditionally orthogonalizable loss functions, extends the classical uniform mass binning algorithm to include nuisance estimation. Our results are exceedingly general, showing that essentially any existing calibration algorithm can be used in causal settings, with additional loss only arising from errors in nuisance estimation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01933&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Justin Whitehouse, Christopher Jung, Vasilis Syrgkanis, Bryan Wilder, Zhiwei Steven Wu</name></author><category term="stat.ML," /><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Estimates of causal parameters such as conditional average treatment effects and conditional quantile treatment effects play an important role in real-world decision making. Given this importance, one should ensure these estimators are calibrated. While there is a rich literature on calibrating estimators of non-causal parameters, very few methods have been derived for calibrating estimators of causal parameters, or more generally estimators of quantities involving nuisance parameters. In this work, we provide a general framework for calibrating predictors involving nuisance estimation. We consider a notion of calibration defined with respect to an arbitrary, nuisance-dependent loss $\ell$, under which we say an estimator $\theta$ is calibrated if its predictions cannot be changed on any level set to decrease loss. We prove generic upper bounds on the calibration error of any causal parameter estimate $\theta$ with respect to any loss $\ell$ using a concept called Neyman Orthogonality. Our bounds involve two decoupled terms - one measuring the error in estimating the unknown nuisance parameters, and the other representing the calibration error in a hypothetical world where the learned nuisance estimates were true. We use our bound to analyze the convergence of two sample splitting algorithms for causal calibration. One algorithm, which applies to universally orthogonalizable loss functions, transforms the data into generalized pseudo-outcomes and applies an off-the-shelf calibration procedure. The other algorithm, which applies to conditionally orthogonalizable loss functions, extends the classical uniform mass binning algorithm to include nuisance estimation. Our results are exceedingly general, showing that essentially any existing calibration algorithm can be used in causal settings, with additional loss only arising from errors in nuisance estimation.</summary></entry><entry><title type="html">PASOA- PArticle baSed Bayesian Optimal Adaptive design</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/PASOAPArticlebaSedBayesianOptimalAdaptivedesign.html" rel="alternate" type="text/html" title="PASOA- PArticle baSed Bayesian Optimal Adaptive design" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/PASOAPArticlebaSedBayesianOptimalAdaptivedesign</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/PASOAPArticlebaSedBayesianOptimalAdaptivedesign.html">&lt;p&gt;We propose a new procedure named PASOA, for Bayesian experimental design, that performs sequential design optimization by simultaneously providing accurate estimates of successive posterior distributions for parameter inference. The sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and Sequential Monte Carlo (SMC) samplers to maximise the Expected Information Gain (EIG). As larger information gains are obtained for larger distances between successive posterior distributions, this EIG objective may worsen classical SMC performance. To handle this issue, tempering is proposed to have both a large information gain and an accurate SMC sampling, that we show is crucial for performance. This novel combination of stochastic optimization and tempered SMC allows to jointly handle design optimization and parameter inference. We provide a proof that the obtained optimal design estimators benefit from some consistency property. Numerical experiments confirm the potential of the approach, which outperforms other recent existing procedures.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.07160&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jacopo Iollo, Christophe Heinkelé, Pierre Alliez, Florence Forbes</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">We propose a new procedure named PASOA, for Bayesian experimental design, that performs sequential design optimization by simultaneously providing accurate estimates of successive posterior distributions for parameter inference. The sequential design process is carried out via a contrastive estimation principle, using stochastic optimization and Sequential Monte Carlo (SMC) samplers to maximise the Expected Information Gain (EIG). As larger information gains are obtained for larger distances between successive posterior distributions, this EIG objective may worsen classical SMC performance. To handle this issue, tempering is proposed to have both a large information gain and an accurate SMC sampling, that we show is crucial for performance. This novel combination of stochastic optimization and tempered SMC allows to jointly handle design optimization and parameter inference. We provide a proof that the obtained optimal design estimators benefit from some consistency property. Numerical experiments confirm the potential of the approach, which outperforms other recent existing procedures.</summary></entry><entry><title type="html">Principal Sub-manifolds</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/PrincipalSubmanifolds.html" rel="alternate" type="text/html" title="Principal Sub-manifolds" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/PrincipalSubmanifolds</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/PrincipalSubmanifolds.html">&lt;p&gt;We propose a novel method of finding principal components in multivariate data sets that lie on an embedded nonlinear Riemannian manifold within a higher-dimensional space. Our aim is to extend the geometric interpretation of PCA, while being able to capture non-geodesic modes of variation in the data. We introduce the concept of a principal sub-manifold, a manifold passing through a reference point, and at any point on the manifold extending in the direction of highest variation in the space spanned by the eigenvectors of the local tangent space PCA. Compared to recent work for the case where the sub-manifold is of dimension one Panaretos et al. (2014)$-$essentially a curve lying on the manifold attempting to capture one-dimensional variation$-$the current setting is much more general. The principal sub-manifold is therefore an extension of the principal flow, accommodating to capture higher dimensional variation in the data. We show the principal sub-manifold yields the ball spanned by the usual principal components in Euclidean space. By means of examples, we illustrate how to find, use and interpret a principal sub-manifold and we present an application in shape analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1604.04318&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhigang Yao, Benjamin Eltzner, Tung Pham</name></author><category term="stat.ME" /><summary type="html">We propose a novel method of finding principal components in multivariate data sets that lie on an embedded nonlinear Riemannian manifold within a higher-dimensional space. Our aim is to extend the geometric interpretation of PCA, while being able to capture non-geodesic modes of variation in the data. We introduce the concept of a principal sub-manifold, a manifold passing through a reference point, and at any point on the manifold extending in the direction of highest variation in the space spanned by the eigenvectors of the local tangent space PCA. Compared to recent work for the case where the sub-manifold is of dimension one Panaretos et al. (2014)$-$essentially a curve lying on the manifold attempting to capture one-dimensional variation$-$the current setting is much more general. The principal sub-manifold is therefore an extension of the principal flow, accommodating to capture higher dimensional variation in the data. We show the principal sub-manifold yields the ball spanned by the usual principal components in Euclidean space. By means of examples, we illustrate how to find, use and interpret a principal sub-manifold and we present an application in shape analysis.</summary></entry><entry><title type="html">Random measure priors in Bayesian recovery from sketches</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/RandommeasurepriorsinBayesianrecoveryfromsketches.html" rel="alternate" type="text/html" title="Random measure priors in Bayesian recovery from sketches" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/RandommeasurepriorsinBayesianrecoveryfromsketches</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/RandommeasurepriorsinBayesianrecoveryfromsketches.html">&lt;p&gt;This paper introduces a Bayesian nonparametric approach to frequency recovery from lossy-compressed discrete data, leveraging all information contained in a sketch obtained through random hashing. By modeling the data points as random samples from an unknown discrete distribution endowed with a Poisson-Kingman prior, we derive the posterior distribution of a symbol’s empirical frequency given the sketch. This leads to principled frequency estimates through mean functionals, e.g., the posterior mean, median and mode. We highlight applications of this general result to Dirichlet process and Pitman-Yor process priors. Notably, we prove that the former prior uniquely satisfies a sufficiency property that simplifies the posterior distribution, while the latter enables a convenient large-sample asymptotic approximation. Additionally, we extend our approach to the problem of cardinality recovery, estimating the number of distinct symbols in the sketched dataset. Our approach to frequency recovery also adapts to a more general &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;traits&apos;&apos; setting, where each data point has integer levels of association with multiple symbols, typically referred to as&lt;/code&gt;traits’’. By employing a generalized Indian buffet process, we compute the posterior distribution of a trait’s frequency using both the Poisson and Bernoulli distributions for the trait association levels, respectively yielding exact and approximate posterior frequency distributions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.15029&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mario Beraha, Stefano Favaro, Matteo Sesia</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This paper introduces a Bayesian nonparametric approach to frequency recovery from lossy-compressed discrete data, leveraging all information contained in a sketch obtained through random hashing. By modeling the data points as random samples from an unknown discrete distribution endowed with a Poisson-Kingman prior, we derive the posterior distribution of a symbol’s empirical frequency given the sketch. This leads to principled frequency estimates through mean functionals, e.g., the posterior mean, median and mode. We highlight applications of this general result to Dirichlet process and Pitman-Yor process priors. Notably, we prove that the former prior uniquely satisfies a sufficiency property that simplifies the posterior distribution, while the latter enables a convenient large-sample asymptotic approximation. Additionally, we extend our approach to the problem of cardinality recovery, estimating the number of distinct symbols in the sketched dataset. Our approach to frequency recovery also adapts to a more general traits&apos;&apos; setting, where each data point has integer levels of association with multiple symbols, typically referred to astraits’’. By employing a generalized Indian buffet process, we compute the posterior distribution of a trait’s frequency using both the Poisson and Bernoulli distributions for the trait association levels, respectively yielding exact and approximate posterior frequency distributions.</summary></entry><entry><title type="html">Resampling methods for private statistical inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Resamplingmethodsforprivatestatisticalinference.html" rel="alternate" type="text/html" title="Resampling methods for private statistical inference" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Resamplingmethodsforprivatestatisticalinference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Resamplingmethodsforprivatestatisticalinference.html">&lt;p&gt;We consider the task of constructing confidence intervals with differential privacy. We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple “little” bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. For a fixed differential privacy parameter $\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the sample size $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and logistic regression with both real and synthetic data. Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\gtrsim 10$ times) confidence intervals than previous approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.07131&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Karan Chadha, John Duchi, Rohith Kuditipudi</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">We consider the task of constructing confidence intervals with differential privacy. We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple “little” bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. For a fixed differential privacy parameter $\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the sample size $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and logistic regression with both real and synthetic data. Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\gtrsim 10$ times) confidence intervals than previous approaches.</summary></entry><entry><title type="html">Robust Data-driven Prescriptiveness Optimization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/RobustDatadrivenPrescriptivenessOptimization.html" rel="alternate" type="text/html" title="Robust Data-driven Prescriptiveness Optimization" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/RobustDatadrivenPrescriptivenessOptimization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/RobustDatadrivenPrescriptivenessOptimization.html">&lt;p&gt;The abundance of data has led to the emergence of a variety of optimization techniques that attempt to leverage available side information to provide more anticipative decisions. The wide range of methods and contexts of application have motivated the design of a universal unitless measure of performance known as the coefficient of prescriptiveness. This coefficient was designed to quantify both the quality of contextual decisions compared to a reference one and the prescriptive power of side information. To identify policies that maximize the former in a data-driven context, this paper introduces a distributionally robust contextual optimization model where the coefficient of prescriptiveness substitutes for the classical empirical risk minimization objective. We present a bisection algorithm to solve this model, which relies on solving a series of linear programs when the distributional ambiguity set has an appropriate nested form and polyhedral structure. Studying a contextual shortest path problem, we evaluate the robustness of the resulting policies against alternative methods when the out-of-sample dataset is subject to varying amounts of distribution shift.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.05937&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mehran Poursoltani, Erick Delage, Angelos Georghiou</name></author><category term="stat.ME" /><summary type="html">The abundance of data has led to the emergence of a variety of optimization techniques that attempt to leverage available side information to provide more anticipative decisions. The wide range of methods and contexts of application have motivated the design of a universal unitless measure of performance known as the coefficient of prescriptiveness. This coefficient was designed to quantify both the quality of contextual decisions compared to a reference one and the prescriptive power of side information. To identify policies that maximize the former in a data-driven context, this paper introduces a distributionally robust contextual optimization model where the coefficient of prescriptiveness substitutes for the classical empirical risk minimization objective. We present a bisection algorithm to solve this model, which relies on solving a series of linear programs when the distributional ambiguity set has an appropriate nested form and polyhedral structure. Studying a contextual shortest path problem, we evaluate the robustness of the resulting policies against alternative methods when the out-of-sample dataset is subject to varying amounts of distribution shift.</summary></entry><entry><title type="html">Semi-Supervised Learning guided by the Generalized Bayes Rule under Soft Revision</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/SemiSupervisedLearningguidedbytheGeneralizedBayesRuleunderSoftRevision.html" rel="alternate" type="text/html" title="Semi-Supervised Learning guided by the Generalized Bayes Rule under Soft Revision" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/SemiSupervisedLearningguidedbytheGeneralizedBayesRuleunderSoftRevision</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/SemiSupervisedLearningguidedbytheGeneralizedBayesRuleunderSoftRevision.html">&lt;p&gt;We provide a theoretical and computational investigation of the Gamma-Maximin method with soft revision, which was recently proposed as a robust criterion for pseudo-label selection (PLS) in semi-supervised learning. Opposed to traditional methods for PLS we use credal sets of priors (“generalized Bayes”) to represent the epistemic modeling uncertainty. These latter are then updated by the Gamma-Maximin method with soft revision. We eventually select pseudo-labeled data that are most likely in light of the least favorable distribution from the so updated credal set. We formalize the task of finding optimal pseudo-labeled data w.r.t. the Gamma-Maximin method with soft revision as an optimization problem. A concrete implementation for the class of logistic models then allows us to compare the predictive power of the method with competing approaches. It is observed that the Gamma-Maximin method with soft revision can achieve very promising results, especially when the proportion of labeled data is low.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15294&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Stefan Dietrich, Julian Rodemann, Christoph Jansen</name></author><category term="stat.ML," /><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We provide a theoretical and computational investigation of the Gamma-Maximin method with soft revision, which was recently proposed as a robust criterion for pseudo-label selection (PLS) in semi-supervised learning. Opposed to traditional methods for PLS we use credal sets of priors (“generalized Bayes”) to represent the epistemic modeling uncertainty. These latter are then updated by the Gamma-Maximin method with soft revision. We eventually select pseudo-labeled data that are most likely in light of the least favorable distribution from the so updated credal set. We formalize the task of finding optimal pseudo-labeled data w.r.t. the Gamma-Maximin method with soft revision as an optimization problem. A concrete implementation for the class of logistic models then allows us to compare the predictive power of the method with competing approaches. It is observed that the Gamma-Maximin method with soft revision can achieve very promising results, especially when the proportion of labeled data is low.</summary></entry><entry><title type="html">Semiparametric Estimation of the Shape of the Limiting Bivariate Point Cloud</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/SemiparametricEstimationoftheShapeoftheLimitingBivariatePointCloud.html" rel="alternate" type="text/html" title="Semiparametric Estimation of the Shape of the Limiting Bivariate Point Cloud" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/SemiparametricEstimationoftheShapeoftheLimitingBivariatePointCloud</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/SemiparametricEstimationoftheShapeoftheLimitingBivariatePointCloud.html">&lt;p&gt;We propose a model to flexibly estimate joint tail properties by exploiting the convergence of an appropriately scaled point cloud onto a compact limit set. Characteristics of the shape of the limit set correspond to key tail dependence properties. We directly model the shape of the limit set using Bezier splines, which allow flexible and parsimonious specification of shapes in two dimensions. We then fit the Bezier splines to data in pseudo-polar coordinates using Markov chain Monte Carlo sampling, utilizing a limiting approximation to the conditional likelihood of the radii given angles. By imposing appropriate constraints on the parameters of the Bezier splines, we guarantee that each posterior sample is a valid limit set boundary, allowing direct posterior analysis of any quantity derived from the shape of the curve. Furthermore, we obtain interpretable inference on the asymptotic dependence class by using mixture priors with point masses on the corner of the unit box. Finally, we apply our model to bivariate datasets of extremes of variables related to fire risk and air pollution.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.13257&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Reetam Majumder, Benjamin A. Shaby, Brian J. Reich, Daniel Cooley</name></author><category term="stat.ME" /><summary type="html">We propose a model to flexibly estimate joint tail properties by exploiting the convergence of an appropriately scaled point cloud onto a compact limit set. Characteristics of the shape of the limit set correspond to key tail dependence properties. We directly model the shape of the limit set using Bezier splines, which allow flexible and parsimonious specification of shapes in two dimensions. We then fit the Bezier splines to data in pseudo-polar coordinates using Markov chain Monte Carlo sampling, utilizing a limiting approximation to the conditional likelihood of the radii given angles. By imposing appropriate constraints on the parameters of the Bezier splines, we guarantee that each posterior sample is a valid limit set boundary, allowing direct posterior analysis of any quantity derived from the shape of the curve. Furthermore, we obtain interpretable inference on the asymptotic dependence class by using mixture priors with point masses on the corner of the unit box. Finally, we apply our model to bivariate datasets of extremes of variables related to fire risk and air pollution.</summary></entry><entry><title type="html">Stochastic Gradient MCMC for Massive Geostatistical Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/StochasticGradientMCMCforMassiveGeostatisticalData.html" rel="alternate" type="text/html" title="Stochastic Gradient MCMC for Massive Geostatistical Data" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/StochasticGradientMCMCforMassiveGeostatisticalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/StochasticGradientMCMCforMassiveGeostatisticalData.html">&lt;p&gt;Gaussian processes (GPs) are commonly used for prediction and inference for spatial data analyses. However, since estimation and prediction tasks have cubic time and quadratic memory complexity in number of locations, GPs are difficult to scale to large spatial datasets. The Vecchia approximation induces sparsity in the dependence structure and is one of several methods proposed to scale GP inference. Our work adds to the substantial research in this area by developing a stochastic gradient Markov chain Monte Carlo (SGMCMC) framework for efficient computation in GPs. At each step, the algorithm subsamples a minibatch of locations and subsequently updates process parameters through a Vecchia-approximated GP likelihood. Since the Vecchia-approximated GP has a time complexity that is linear in the number of locations, this results in scalable estimation in GPs. Through simulation studies, we demonstrate that SGMCMC is competitive with state-of-the-art scalable GP algorithms in terms of computational time and parameter estimation. An application of our method is also provided using the Argo dataset of ocean temperature measurements.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04531&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mohamed A. Abba, Brian J. Reich, Reetam Majumder, Brandon Feng</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Gaussian processes (GPs) are commonly used for prediction and inference for spatial data analyses. However, since estimation and prediction tasks have cubic time and quadratic memory complexity in number of locations, GPs are difficult to scale to large spatial datasets. The Vecchia approximation induces sparsity in the dependence structure and is one of several methods proposed to scale GP inference. Our work adds to the substantial research in this area by developing a stochastic gradient Markov chain Monte Carlo (SGMCMC) framework for efficient computation in GPs. At each step, the algorithm subsamples a minibatch of locations and subsequently updates process parameters through a Vecchia-approximated GP likelihood. Since the Vecchia-approximated GP has a time complexity that is linear in the number of locations, this results in scalable estimation in GPs. Through simulation studies, we demonstrate that SGMCMC is competitive with state-of-the-art scalable GP algorithms in terms of computational time and parameter estimation. An application of our method is also provided using the Argo dataset of ocean temperature measurements.</summary></entry><entry><title type="html">Survival Data Simulation With the R Package rsurv</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/SurvivalDataSimulationWiththeRPackagersurv.html" rel="alternate" type="text/html" title="Survival Data Simulation With the R Package rsurv" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/SurvivalDataSimulationWiththeRPackagersurv</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/SurvivalDataSimulationWiththeRPackagersurv.html">&lt;p&gt;In this paper we propose a novel R package, called rsurv, developed for general survival data simulation purposes. The package is built under a new approach to simulate survival data that depends heavily on the use of dplyr verbs. The proposed package allows simulations of survival data from a wide range of regression models, including accelerated failure time (AFT), proportional hazards (PH), proportional odds (PO), accelerated hazard (AH), Yang and Prentice (YP), and extended hazard (EH) models. The package rsurv also stands out by its ability to generate survival data from an unlimited number of baseline distributions provided that an implementation of the quantile function of the chosen baseline distribution is available in R. Another nice feature of the package rsurv lies in the fact that linear predictors are specified using R formulas, facilitating the inclusion of categorical variables, interaction terms and offset variables. The functions implemented in the package rsurv can also be employed to simulate survival data with more complex structures, such as survival data with different types of censoring mechanisms, survival data with cure fraction, survival data with random effects (frailties), multivarite survival data, and competing risks survival data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01750&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fábio N. Demarqui</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">In this paper we propose a novel R package, called rsurv, developed for general survival data simulation purposes. The package is built under a new approach to simulate survival data that depends heavily on the use of dplyr verbs. The proposed package allows simulations of survival data from a wide range of regression models, including accelerated failure time (AFT), proportional hazards (PH), proportional odds (PO), accelerated hazard (AH), Yang and Prentice (YP), and extended hazard (EH) models. The package rsurv also stands out by its ability to generate survival data from an unlimited number of baseline distributions provided that an implementation of the quantile function of the chosen baseline distribution is available in R. Another nice feature of the package rsurv lies in the fact that linear predictors are specified using R formulas, facilitating the inclusion of categorical variables, interaction terms and offset variables. The functions implemented in the package rsurv can also be employed to simulate survival data with more complex structures, such as survival data with different types of censoring mechanisms, survival data with cure fraction, survival data with random effects (frailties), multivarite survival data, and competing risks survival data.</summary></entry><entry><title type="html">Towards Causal Foundation Model: on Duality between Causal Inference and Attention</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/TowardsCausalFoundationModelonDualitybetweenCausalInferenceandAttention.html" rel="alternate" type="text/html" title="Towards Causal Foundation Model: on Duality between Causal Inference and Attention" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/TowardsCausalFoundationModelonDualitybetweenCausalInferenceandAttention</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/TowardsCausalFoundationModelonDualitybetweenCausalInferenceandAttention.html">&lt;p&gt;Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for treatment effect estimations. We propose a novel, theoretically justified method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that CInA effectively generalizes to out-of-distribution datasets and various real-world datasets, matching or even surpassing traditional per-dataset methodologies. These results provide compelling evidence that our method has the potential to serve as a stepping stone for the development of causal foundation models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.00809&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiaqi Zhang, Joel Jennings, Agrin Hilmkil, Nick Pawlowski, Cheng Zhang, Chao Ma</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Foundation models have brought changes to the landscape of machine learning, demonstrating sparks of human-level intelligence across a diverse array of tasks. However, a gap persists in complex tasks such as causal inference, primarily due to challenges associated with intricate reasoning steps and high numerical precision requirements. In this work, we take a first step towards building causally-aware foundation models for treatment effect estimations. We propose a novel, theoretically justified method called Causal Inference with Attention (CInA), which utilizes multiple unlabeled datasets to perform self-supervised causal learning, and subsequently enables zero-shot causal inference on unseen tasks with new data. This is based on our theoretical results that demonstrate the primal-dual connection between optimal covariate balancing and self-attention, facilitating zero-shot causal inference through the final layer of a trained transformer-type architecture. We demonstrate empirically that CInA effectively generalizes to out-of-distribution datasets and various real-world datasets, matching or even surpassing traditional per-dataset methodologies. These results provide compelling evidence that our method has the potential to serve as a stepping stone for the development of causal foundation models.</summary></entry><entry><title type="html">Variable importance measure for spatial machine learning models with application to air pollution exposure prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Variableimportancemeasureforspatialmachinelearningmodelswithapplicationtoairpollutionexposureprediction.html" rel="alternate" type="text/html" title="Variable importance measure for spatial machine learning models with application to air pollution exposure prediction" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Variableimportancemeasureforspatialmachinelearningmodelswithapplicationtoairpollutionexposureprediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Variableimportancemeasureforspatialmachinelearningmodelswithapplicationtoairpollutionexposureprediction.html">&lt;p&gt;Exposure assessment is fundamental to air pollution cohort studies. The objective is to predict air pollution exposures for study subjects at locations without data in order to optimize our ability to learn about health effects of air pollution. In addition to generating accurate predictions to minimize exposure measurement error, understanding the mechanism captured by the model is another crucial aspect that may not always be straightforward due to the complex nature of machine learning methods, as well as the lack of unifying notions of variable importance. This is further complicated in air pollution modeling by the presence of spatial correlation. We tackle these challenges in two datasets: sulfur (S) from regulatory United States national PM2.5 sub-species data and ultrafine particles (UFP) from a new Seattle-area traffic-related air pollution dataset. Our key contribution is a leave-one-out approach for variable importance that leads to interpretable and comparable measures for a broad class of models with separable mean and covariance components. We illustrate our approach with several spatial machine learning models, and it clearly highlights the difference in model mechanisms, even for those producing similar predictions. We leverage insights from this variable importance measure to assess the relative utilities of two exposure models for S and UFP that have similar out-of-sample prediction accuracies but appear to draw on different types of spatial information to make predictions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01982&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Si Cheng, Magali N. Blanco, Lianne Sheppard, Ali Shojaie, Adam Szpiro</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Exposure assessment is fundamental to air pollution cohort studies. The objective is to predict air pollution exposures for study subjects at locations without data in order to optimize our ability to learn about health effects of air pollution. In addition to generating accurate predictions to minimize exposure measurement error, understanding the mechanism captured by the model is another crucial aspect that may not always be straightforward due to the complex nature of machine learning methods, as well as the lack of unifying notions of variable importance. This is further complicated in air pollution modeling by the presence of spatial correlation. We tackle these challenges in two datasets: sulfur (S) from regulatory United States national PM2.5 sub-species data and ultrafine particles (UFP) from a new Seattle-area traffic-related air pollution dataset. Our key contribution is a leave-one-out approach for variable importance that leads to interpretable and comparable measures for a broad class of models with separable mean and covariance components. We illustrate our approach with several spatial machine learning models, and it clearly highlights the difference in model mechanisms, even for those producing similar predictions. We leverage insights from this variable importance measure to assess the relative utilities of two exposure models for S and UFP that have similar out-of-sample prediction accuracies but appear to draw on different types of spatial information to make predictions.</summary></entry><entry><title type="html">Variance-reduced sampling importance resampling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Variancereducedsamplingimportanceresampling.html" rel="alternate" type="text/html" title="Variance-reduced sampling importance resampling" /><published>2024-06-05T00:00:00+00:00</published><updated>2024-06-05T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Variancereducedsamplingimportanceresampling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/05/Variancereducedsamplingimportanceresampling.html">&lt;p&gt;The sampling importance resampling method is widely utilized in various fields, such as numerical integration and statistical simulation. In this paper, two modified methods are presented by incorporating two variance reduction techniques commonly used in Monte Carlo simulation, namely antithetic sampling and Latin hypercube sampling, into the process of sampling importance resampling method respectively. Theoretical evidence is provided to demonstrate that the proposed methods significantly reduce estimation errors compared to the original approach. Furthermore, the effectiveness and advantages of the proposed methods are validated through both numerical studies and real data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01864&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yao Xiao, Kang Fu, Kun Li</name></author><category term="stat.CO" /><summary type="html">The sampling importance resampling method is widely utilized in various fields, such as numerical integration and statistical simulation. In this paper, two modified methods are presented by incorporating two variance reduction techniques commonly used in Monte Carlo simulation, namely antithetic sampling and Latin hypercube sampling, into the process of sampling importance resampling method respectively. Theoretical evidence is provided to demonstrate that the proposed methods significantly reduce estimation errors compared to the original approach. Furthermore, the effectiveness and advantages of the proposed methods are validated through both numerical studies and real data analysis.</summary></entry></feed>