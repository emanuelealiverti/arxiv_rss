<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-03T07:14:05+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Bayesian joint model of multiple nonlinear longitudinal and competing risks outcomes for dynamic prediction in multiple myeloma: joint estimation and corrected two-stage approaches</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ABayesianjointmodelofmultiplenonlinearlongitudinalandcompetingrisksoutcomesfordynamicpredictioninmultiplemyelomajointestimationandcorrectedtwostageapproaches.html" rel="alternate" type="text/html" title="A Bayesian joint model of multiple nonlinear longitudinal and competing risks outcomes for dynamic prediction in multiple myeloma: joint estimation and corrected two-stage approaches" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ABayesianjointmodelofmultiplenonlinearlongitudinalandcompetingrisksoutcomesfordynamicpredictioninmultiplemyelomajointestimationandcorrectedtwostageapproaches</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ABayesianjointmodelofmultiplenonlinearlongitudinalandcompetingrisksoutcomesfordynamicpredictioninmultiplemyelomajointestimationandcorrectedtwostageapproaches.html">&lt;p&gt;Predicting cancer-associated clinical events is challenging in oncology. In Multiple Myeloma (MM), a cancer of plasma cells, disease progression is determined by changes in biomarkers, such as serum concentration of the paraprotein secreted by plasma cells (M-protein). Therefore, the time-dependent behaviour of M-protein and the transition across lines of therapy (LoT) that may be a consequence of disease progression should be accounted for in statistical models to predict relevant clinical outcomes. Furthermore, it is important to understand the contribution of the patterns of longitudinal biomarkers, upon each LoT initiation, to time-to-death or time-to-next-LoT. Motivated by these challenges, we propose a Bayesian joint model for trajectories of multiple longitudinal biomarkers, such as M-protein, and the competing risks of death and transition to next LoT. Additionally, we explore two estimation approaches for our joint model: simultaneous estimation of all parameters (joint estimation) and sequential estimation of parameters using a corrected two-stage strategy aiming to reduce computational time. Our proposed model and estimation methods are applied to a retrospective cohort study from a real-world database of patients diagnosed with MM in the US from January 2015 to February 2022. We split the data into training and test sets in order to validate the joint model using both estimation approaches and make dynamic predictions of times until clinical events of interest, informed by longitudinally measured biomarkers and baseline variables available up to the time of prediction.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20418&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Danilo Alvares, Jessica K. Barrett, François Mercier, Spyros Roumpanis, Sean Yiu, Felipe Castro, Jochen Schulze, Yajing Zhu</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Predicting cancer-associated clinical events is challenging in oncology. In Multiple Myeloma (MM), a cancer of plasma cells, disease progression is determined by changes in biomarkers, such as serum concentration of the paraprotein secreted by plasma cells (M-protein). Therefore, the time-dependent behaviour of M-protein and the transition across lines of therapy (LoT) that may be a consequence of disease progression should be accounted for in statistical models to predict relevant clinical outcomes. Furthermore, it is important to understand the contribution of the patterns of longitudinal biomarkers, upon each LoT initiation, to time-to-death or time-to-next-LoT. Motivated by these challenges, we propose a Bayesian joint model for trajectories of multiple longitudinal biomarkers, such as M-protein, and the competing risks of death and transition to next LoT. Additionally, we explore two estimation approaches for our joint model: simultaneous estimation of all parameters (joint estimation) and sequential estimation of parameters using a corrected two-stage strategy aiming to reduce computational time. Our proposed model and estimation methods are applied to a retrospective cohort study from a real-world database of patients diagnosed with MM in the US from January 2015 to February 2022. We split the data into training and test sets in order to validate the joint model using both estimation approaches and make dynamic predictions of times until clinical events of interest, informed by longitudinally measured biomarkers and baseline variables available up to the time of prediction.</summary></entry><entry><title type="html">A Novel Two-stage Deming Regression Framework with Applications to Association Analysis between Clinical Risks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ANovelTwostageDemingRegressionFrameworkwithApplicationstoAssociationAnalysisbetweenClinicalRisks.html" rel="alternate" type="text/html" title="A Novel Two-stage Deming Regression Framework with Applications to Association Analysis between Clinical Risks" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ANovelTwostageDemingRegressionFrameworkwithApplicationstoAssociationAnalysisbetweenClinicalRisks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ANovelTwostageDemingRegressionFrameworkwithApplicationstoAssociationAnalysisbetweenClinicalRisks.html">&lt;p&gt;In healthcare, clinical risks are crucial for treatment decisions, yet the analysis of their associations is often overlooked. This gap is particularly significant when balancing risks that are weighed against each other, as in the case of atrial fibrillation (AF) patients facing stroke and bleeding risks with anticoagulant medication. While traditional regression models are ill-suited for this task due to standard errors in risk estimation, a novel two-stage Deming regression framework is proposed to address this issue, offering a more accurate tool for analyzing associations between variables observed with errors of known or estimated variances. The first stage is to obtain the variable values with variances of errors either by estimation or observation, followed by the second stage that fits a Deming regression model potentially subject to a transformation. The second stage accounts for the uncertainties associated with both independent and response variables, including known or estimated variances and additional unknown variances from the model. The complexity arising from different scenarios of uncertainty is handled by existing and advanced variations of Deming regression models. An important practical application is to support personalized treatment recommendations based on clinical risk associations that were identified by the proposed framework. The model’s effectiveness is demonstrated by applying it to a real-world dataset of AF-diagnosed patients to explore the relationship between stroke and bleeding risks, providing crucial guidance for making informed decisions regarding anticoagulant medication. Furthermore, the model’s versatility in addressing data containing multiple sources of uncertainty such as privacy-protected data suggests promising avenues for future research in regression analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20992&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yajie Duan, Javier Cabrera, Davit Sargsyan</name></author><category term="stat.AP" /><summary type="html">In healthcare, clinical risks are crucial for treatment decisions, yet the analysis of their associations is often overlooked. This gap is particularly significant when balancing risks that are weighed against each other, as in the case of atrial fibrillation (AF) patients facing stroke and bleeding risks with anticoagulant medication. While traditional regression models are ill-suited for this task due to standard errors in risk estimation, a novel two-stage Deming regression framework is proposed to address this issue, offering a more accurate tool for analyzing associations between variables observed with errors of known or estimated variances. The first stage is to obtain the variable values with variances of errors either by estimation or observation, followed by the second stage that fits a Deming regression model potentially subject to a transformation. The second stage accounts for the uncertainties associated with both independent and response variables, including known or estimated variances and additional unknown variances from the model. The complexity arising from different scenarios of uncertainty is handled by existing and advanced variations of Deming regression models. An important practical application is to support personalized treatment recommendations based on clinical risk associations that were identified by the proposed framework. The model’s effectiveness is demonstrated by applying it to a real-world dataset of AF-diagnosed patients to explore the relationship between stroke and bleeding risks, providing crucial guidance for making informed decisions regarding anticoagulant medication. Furthermore, the model’s versatility in addressing data containing multiple sources of uncertainty such as privacy-protected data suggests promising avenues for future research in regression analysis.</summary></entry><entry><title type="html">Adapting Quantile Mapping to Bias Correct Solar Radiation Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/AdaptingQuantileMappingtoBiasCorrectSolarRadiationData.html" rel="alternate" type="text/html" title="Adapting Quantile Mapping to Bias Correct Solar Radiation Data" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/AdaptingQuantileMappingtoBiasCorrectSolarRadiationData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/AdaptingQuantileMappingtoBiasCorrectSolarRadiationData.html">&lt;p&gt;Bias correction is a common pre-processing step applied to climate model data before it is used for further analysis. This article introduces an efficient adaptation of a well-established bias-correction method - quantile mapping - for global horizontal irradiance (GHI) that ensures corrected data is physically plausible through incorporating measurements of clearsky GHI. The proposed quantile mapping method is fit on reanalysis data to first bias correct for regional climate models (RCMs) and is tested on RCMs forced by general circulation models (GCMs) to understand existing biases directly from GCMs. Additionally, we adapt a functional analysis of variance methodology that analyzes sources of remaining biases after implementing the proposed quantile mapping method and considered biases by climate region. This analysis is applied to four sets of climate model output from NA-CORDEX and compared against data from the National Solar Radiation Database produced by the National Renewable Energy Lab.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20352&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maggie D. Bailey, Douglas W. Nychka, Manajit Sengupta, Soutir Bandyopadhyay</name></author><category term="stat.AP" /><summary type="html">Bias correction is a common pre-processing step applied to climate model data before it is used for further analysis. This article introduces an efficient adaptation of a well-established bias-correction method - quantile mapping - for global horizontal irradiance (GHI) that ensures corrected data is physically plausible through incorporating measurements of clearsky GHI. The proposed quantile mapping method is fit on reanalysis data to first bias correct for regional climate models (RCMs) and is tested on RCMs forced by general circulation models (GCMs) to understand existing biases directly from GCMs. Additionally, we adapt a functional analysis of variance methodology that analyzes sources of remaining biases after implementing the proposed quantile mapping method and considered biases by climate region. This analysis is applied to four sets of climate model output from NA-CORDEX and compared against data from the National Solar Radiation Database produced by the National Renewable Energy Lab.</summary></entry><entry><title type="html">Approximate Factor Models for Functional Time Series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ApproximateFactorModelsforFunctionalTimeSeries.html" rel="alternate" type="text/html" title="Approximate Factor Models for Functional Time Series" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ApproximateFactorModelsforFunctionalTimeSeries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ApproximateFactorModelsforFunctionalTimeSeries.html">&lt;p&gt;We propose a novel approximate factor model tailored for analyzing time-dependent curve data. Our model decomposes such data into two distinct components: a low-dimensional predictable factor component and an unpredictable error term. These components are identified through the autocovariance structure of the underlying functional time series. The model parameters are consistently estimated using the eigencomponents of a cumulative autocovariance operator and an information criterion is proposed to determine the appropriate number of factors. The methodology is applied to yield curve modeling and forecasting. Our results indicate that more than three factors are required to characterize the dynamics of the term structure of bond yields.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2201.02532&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sven Otto, Nazarii Salish</name></author><category term="stat.ME" /><summary type="html">We propose a novel approximate factor model tailored for analyzing time-dependent curve data. Our model decomposes such data into two distinct components: a low-dimensional predictable factor component and an unpredictable error term. These components are identified through the autocovariance structure of the underlying functional time series. The model parameters are consistently estimated using the eigencomponents of a cumulative autocovariance operator and an information criterion is proposed to determine the appropriate number of factors. The methodology is applied to yield curve modeling and forecasting. Our results indicate that more than three factors are required to characterize the dynamics of the term structure of bond yields.</summary></entry><entry><title type="html">Asymptotic utility of spectral anonymization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Asymptoticutilityofspectralanonymization.html" rel="alternate" type="text/html" title="Asymptotic utility of spectral anonymization" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Asymptoticutilityofspectralanonymization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Asymptoticutilityofspectralanonymization.html">&lt;p&gt;In the contemporary data landscape characterized by multi-source data collection and third-party sharing, ensuring individual privacy stands as a critical concern. While various anonymization methods exist, their utility preservation and privacy guarantees remain challenging to quantify. In this work, we address this gap by studying the utility and privacy of the spectral anonymization (SA) algorithm, particularly in an asymptotic framework. Unlike conventional anonymization methods that directly modify the original data, SA operates by perturbing the data in a spectral basis and subsequently reverting them to their original basis. Alongside the original version $\mathcal{P}$-SA, employing random permutation transformation, we introduce two novel SA variants: $\mathcal{J}$-spectral anonymization and $\mathcal{O}$-spectral anonymization, which employ sign-change and orthogonal matrix transformations, respectively. We show how well, under some practical assumptions, these SA algorithms preserve the first and second moments of the original data. Our results reveal, in particular, that the asymptotic efficiency of all three SA algorithms in covariance estimation is exactly 50% when compared to the original data. To assess the applicability of these asymptotic results in practice, we conduct a simulation study with finite data and also evaluate the privacy protection offered by these algorithms using distance-based record linkage. Our research reveals that while no method exhibits clear superiority in finite-sample utility, $\mathcal{O}$-SA distinguishes itself for its exceptional privacy preservation, never producing identical records, albeit with increased computational complexity. Conversely, $\mathcal{P}$-SA emerges as a computationally efficient alternative, demonstrating unmatched efficiency in mean estimation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20779&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Katariina Perkonoja, Joni Virta</name></author><category term="stat.ME" /><summary type="html">In the contemporary data landscape characterized by multi-source data collection and third-party sharing, ensuring individual privacy stands as a critical concern. While various anonymization methods exist, their utility preservation and privacy guarantees remain challenging to quantify. In this work, we address this gap by studying the utility and privacy of the spectral anonymization (SA) algorithm, particularly in an asymptotic framework. Unlike conventional anonymization methods that directly modify the original data, SA operates by perturbing the data in a spectral basis and subsequently reverting them to their original basis. Alongside the original version $\mathcal{P}$-SA, employing random permutation transformation, we introduce two novel SA variants: $\mathcal{J}$-spectral anonymization and $\mathcal{O}$-spectral anonymization, which employ sign-change and orthogonal matrix transformations, respectively. We show how well, under some practical assumptions, these SA algorithms preserve the first and second moments of the original data. Our results reveal, in particular, that the asymptotic efficiency of all three SA algorithms in covariance estimation is exactly 50% when compared to the original data. To assess the applicability of these asymptotic results in practice, we conduct a simulation study with finite data and also evaluate the privacy protection offered by these algorithms using distance-based record linkage. Our research reveals that while no method exhibits clear superiority in finite-sample utility, $\mathcal{O}$-SA distinguishes itself for its exceptional privacy preservation, never producing identical records, albeit with increased computational complexity. Conversely, $\mathcal{P}$-SA emerges as a computationally efficient alternative, demonstrating unmatched efficiency in mean estimation.</summary></entry><entry><title type="html">Bayesian Deep Generative Models for Replicated Networks with Multiscale Overlapping Clusters</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/BayesianDeepGenerativeModelsforReplicatedNetworkswithMultiscaleOverlappingClusters.html" rel="alternate" type="text/html" title="Bayesian Deep Generative Models for Replicated Networks with Multiscale Overlapping Clusters" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/BayesianDeepGenerativeModelsforReplicatedNetworkswithMultiscaleOverlappingClusters</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/BayesianDeepGenerativeModelsforReplicatedNetworkswithMultiscaleOverlappingClusters.html">&lt;p&gt;Our interest is in replicated network data with multiple networks observed across the same set of nodes. Examples include brain connection networks, in which nodes corresponds to brain regions and replicates to different individuals, and ecological networks, in which nodes correspond to species and replicates to samples collected at different locations and/or times. Our goal is to infer a hierarchical structure of the nodes at a population level, while performing multi-resolution clustering of the individual replicates. In brain connectomics, the focus is on inferring common relationships among the brain regions, while characterizing inter-individual variability in an easily interpretable manner. To accomplish this, we propose a Bayesian hierarchical model, while providing theoretical support in terms of identifiability and posterior consistency, and design efficient methods for posterior computation. We provide novel technical tools for proving model identifiability, which are of independent interest. Our simulations and application to brain connectome data provide support for the proposed methodology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20936&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuren Zhou, Yuqi Gu, David B. Dunson</name></author><category term="stat.ME" /><summary type="html">Our interest is in replicated network data with multiple networks observed across the same set of nodes. Examples include brain connection networks, in which nodes corresponds to brain regions and replicates to different individuals, and ecological networks, in which nodes correspond to species and replicates to samples collected at different locations and/or times. Our goal is to infer a hierarchical structure of the nodes at a population level, while performing multi-resolution clustering of the individual replicates. In brain connectomics, the focus is on inferring common relationships among the brain regions, while characterizing inter-individual variability in an easily interpretable manner. To accomplish this, we propose a Bayesian hierarchical model, while providing theoretical support in terms of identifiability and posterior consistency, and design efficient methods for posterior computation. We provide novel technical tools for proving model identifiability, which are of independent interest. Our simulations and application to brain connectome data provide support for the proposed methodology.</summary></entry><entry><title type="html">Bayesian Estimation of Hierarchical Linear Models from Incomplete Data: Cluster-Level Interaction Effects and Small Sample Sizes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/BayesianEstimationofHierarchicalLinearModelsfromIncompleteDataClusterLevelInteractionEffectsandSmallSampleSizes.html" rel="alternate" type="text/html" title="Bayesian Estimation of Hierarchical Linear Models from Incomplete Data: Cluster-Level Interaction Effects and Small Sample Sizes" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/BayesianEstimationofHierarchicalLinearModelsfromIncompleteDataClusterLevelInteractionEffectsandSmallSampleSizes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/BayesianEstimationofHierarchicalLinearModelsfromIncompleteDataClusterLevelInteractionEffectsandSmallSampleSizes.html">&lt;p&gt;We consider Bayesian estimation of a hierarchical linear model (HLM) from small sample sizes where 37 patient-physician encounters are repeatedly measured at four time points. The continuous response $Y$ and continuous covariates $C$ are partially observed and assumed missing at random. With $C$ having linear effects, the HLM may be efficiently estimated by available methods. When $C$ includes cluster-level covariates having interactive or other nonlinear effects given small sample sizes, however, maximum likelihood estimation is suboptimal, and existing Gibbs samplers are based on a Bayesian joint distribution compatible with the HLM, but impute missing values of $C$ by a Metropolis algorithm via a proposal density having a constant variance while the target conditional distribution has a nonconstant variance. Therefore, the samplers are not guaranteed to be compatible with the joint distribution and, thus, not guaranteed to always produce unbiased estimation of the HLM. We introduce a compatible Gibbs sampler that imputes parameters and missing values directly from the exact conditional distributions. We analyze repeated measurements from patient-physician encounters by our sampler, and compare our estimators with those of existing methods by simulation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.21020&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Dongho Shin, Yongyun Shin, Nao Hagiwara</name></author><category term="stat.ME" /><summary type="html">We consider Bayesian estimation of a hierarchical linear model (HLM) from small sample sizes where 37 patient-physician encounters are repeatedly measured at four time points. The continuous response $Y$ and continuous covariates $C$ are partially observed and assumed missing at random. With $C$ having linear effects, the HLM may be efficiently estimated by available methods. When $C$ includes cluster-level covariates having interactive or other nonlinear effects given small sample sizes, however, maximum likelihood estimation is suboptimal, and existing Gibbs samplers are based on a Bayesian joint distribution compatible with the HLM, but impute missing values of $C$ by a Metropolis algorithm via a proposal density having a constant variance while the target conditional distribution has a nonconstant variance. Therefore, the samplers are not guaranteed to be compatible with the joint distribution and, thus, not guaranteed to always produce unbiased estimation of the HLM. We introduce a compatible Gibbs sampler that imputes parameters and missing values directly from the exact conditional distributions. We analyze repeated measurements from patient-physician encounters by our sampler, and compare our estimators with those of existing methods by simulation.</summary></entry><entry><title type="html">Bayesian Nonparametric Quasi Likelihood</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/BayesianNonparametricQuasiLikelihood.html" rel="alternate" type="text/html" title="Bayesian Nonparametric Quasi Likelihood" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/BayesianNonparametricQuasiLikelihood</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/BayesianNonparametricQuasiLikelihood.html">&lt;p&gt;A recent trend in Bayesian research has been revisiting generalizations of the likelihood that enable Bayesian inference without requiring the specification of a model for the data generating mechanism. This paper focuses on a Bayesian nonparametric extension of Wedderburn’s quasi-likelihood, using Bayesian additive regression trees to model the mean function. Here, the analyst posits only a structural relationship between the mean and variance of the outcome. We show that this approach provides a unified, computationally efficient, framework for extending Bayesian decision tree ensembles to many new settings, including simplex-valued and heavily heteroskedastic data. We also introduce Bayesian strategies for inferring the dispersion parameter of the quasi-likelihood, a task which is complicated by the fact that the quasi-likelihood itself does not contain information about this parameter; despite these challenges, we are able to inject updates for the dispersion parameter into a Markov chain Monte Carlo inference scheme in a way that, in the parametric setting, leads to a Bernstein-von Mises result for the stationary distribution of the resulting Markov chain. We illustrate the utility of our approach on a variety of both synthetic and non-synthetic datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20601&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Antonio R. Linero</name></author><category term="stat.ME," /><category term="stat.OT" /><summary type="html">A recent trend in Bayesian research has been revisiting generalizations of the likelihood that enable Bayesian inference without requiring the specification of a model for the data generating mechanism. This paper focuses on a Bayesian nonparametric extension of Wedderburn’s quasi-likelihood, using Bayesian additive regression trees to model the mean function. Here, the analyst posits only a structural relationship between the mean and variance of the outcome. We show that this approach provides a unified, computationally efficient, framework for extending Bayesian decision tree ensembles to many new settings, including simplex-valued and heavily heteroskedastic data. We also introduce Bayesian strategies for inferring the dispersion parameter of the quasi-likelihood, a task which is complicated by the fact that the quasi-likelihood itself does not contain information about this parameter; despite these challenges, we are able to inject updates for the dispersion parameter into a Markov chain Monte Carlo inference scheme in a way that, in the parametric setting, leads to a Bernstein-von Mises result for the stationary distribution of the resulting Markov chain. We illustrate the utility of our approach on a variety of both synthetic and non-synthetic datasets.</summary></entry><entry><title type="html">Comparison of Point Process Learning and its special case Takacs-Fiksel estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ComparisonofPointProcessLearninganditsspecialcaseTakacsFikselestimation.html" rel="alternate" type="text/html" title="Comparison of Point Process Learning and its special case Takacs-Fiksel estimation" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ComparisonofPointProcessLearninganditsspecialcaseTakacsFikselestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ComparisonofPointProcessLearninganditsspecialcaseTakacsFikselestimation.html">&lt;p&gt;Recently, Cronie et al. (2024) introduced the notion of cross-validation for point processes and a new statistical methodology called Point Process Learning (PPL). In PPL one splits a point process/pattern into a training and a validation set, and then predicts the latter from the former through a parametrised Papangelou conditional intensity. The model parameters are estimated by minimizing a point process prediction error; this notion was introduced as the second building block of PPL. It was shown that PPL outperforms the state-of-the-art in both kernel intensity estimation and estimation of the parameters of the Gibbs hard-core process. In the latter case, the state-of-the-art was represented by pseudolikelihood estimation. In this paper we study PPL in relation to Takacs-Fiksel estimation, of which pseudolikelihood is a special case. We show that Takacs-Fiksel estimation is a special case of PPL in the sense that PPL with a specific loss function asymptotically reduces to Takacs-Fiksel estimation if we let the cross-validation regime tend to leave-one-out cross-validation. Moreover, PPL involves a certain type of hyperparameter given by a weight function which ensures that the prediction errors have expectation zero if and only if we have the correct parametrisation. We show that the weight function takes an explicit but intractable form for general Gibbs models. Consequently, we propose different approaches to estimate the weight function in practice. In order to assess how the general PPL setup performs in relation to its special case Takacs-Fiksel estimation, we conduct a simulation study where we find that for common Gibbs models we can find loss functions and hyperparameters so that PPL typically outperforms Takacs-Fiksel estimation significantly in terms of mean square error. Here, the hyperparameters are the cross-validation parameters and the weight function estimate.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19523&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Julia Jansson, Ottmar Cronie</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Recently, Cronie et al. (2024) introduced the notion of cross-validation for point processes and a new statistical methodology called Point Process Learning (PPL). In PPL one splits a point process/pattern into a training and a validation set, and then predicts the latter from the former through a parametrised Papangelou conditional intensity. The model parameters are estimated by minimizing a point process prediction error; this notion was introduced as the second building block of PPL. It was shown that PPL outperforms the state-of-the-art in both kernel intensity estimation and estimation of the parameters of the Gibbs hard-core process. In the latter case, the state-of-the-art was represented by pseudolikelihood estimation. In this paper we study PPL in relation to Takacs-Fiksel estimation, of which pseudolikelihood is a special case. We show that Takacs-Fiksel estimation is a special case of PPL in the sense that PPL with a specific loss function asymptotically reduces to Takacs-Fiksel estimation if we let the cross-validation regime tend to leave-one-out cross-validation. Moreover, PPL involves a certain type of hyperparameter given by a weight function which ensures that the prediction errors have expectation zero if and only if we have the correct parametrisation. We show that the weight function takes an explicit but intractable form for general Gibbs models. Consequently, we propose different approaches to estimate the weight function in practice. In order to assess how the general PPL setup performs in relation to its special case Takacs-Fiksel estimation, we conduct a simulation study where we find that for common Gibbs models we can find loss functions and hyperparameters so that PPL typically outperforms Takacs-Fiksel estimation significantly in terms of mean square error. Here, the hyperparameters are the cross-validation parameters and the weight function estimate.</summary></entry><entry><title type="html">Correlated Dynamics in Marketing Sensitivities</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/CorrelatedDynamicsinMarketingSensitivities.html" rel="alternate" type="text/html" title="Correlated Dynamics in Marketing Sensitivities" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/CorrelatedDynamicsinMarketingSensitivities</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/CorrelatedDynamicsinMarketingSensitivities.html">&lt;p&gt;Understanding individual customers’ sensitivities to prices, promotions, brands, and other marketing mix elements is fundamental to a wide swath of marketing problems. An important but understudied aspect of this problem is the dynamic nature of these sensitivities, which change over time and vary across individuals. Prior work has developed methods for capturing such dynamic heterogeneity within product categories, but neglected the possibility of correlated dynamics across categories. In this work, we introduce a framework to capture such correlated dynamics using a hierarchical dynamic factor model, where individual preference parameters are influenced by common cross-category dynamic latent factors, estimated through Bayesian nonparametric Gaussian processes. We apply our model to grocery purchase data, and find that a surprising degree of dynamic heterogeneity can be accounted for by only a few global trends. We also characterize the patterns in how consumers’ sensitivities evolve across categories. Managerially, the proposed framework not only enhances predictive accuracy by leveraging cross-category data, but enables more precise estimation of quantities of interest, like price elasticity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.11702&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ryan Dew, Yuhao Fan</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Understanding individual customers’ sensitivities to prices, promotions, brands, and other marketing mix elements is fundamental to a wide swath of marketing problems. An important but understudied aspect of this problem is the dynamic nature of these sensitivities, which change over time and vary across individuals. Prior work has developed methods for capturing such dynamic heterogeneity within product categories, but neglected the possibility of correlated dynamics across categories. In this work, we introduce a framework to capture such correlated dynamics using a hierarchical dynamic factor model, where individual preference parameters are influenced by common cross-category dynamic latent factors, estimated through Bayesian nonparametric Gaussian processes. We apply our model to grocery purchase data, and find that a surprising degree of dynamic heterogeneity can be accounted for by only a few global trends. We also characterize the patterns in how consumers’ sensitivities evolve across categories. Managerially, the proposed framework not only enhances predictive accuracy by leveraging cross-category data, but enables more precise estimation of quantities of interest, like price elasticity.</summary></entry><entry><title type="html">Cross-Temporal Forecast Reconciliation at Digital Platforms with Machine Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/CrossTemporalForecastReconciliationatDigitalPlatformswithMachineLearning.html" rel="alternate" type="text/html" title="Cross-Temporal Forecast Reconciliation at Digital Platforms with Machine Learning" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/CrossTemporalForecastReconciliationatDigitalPlatformswithMachineLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/CrossTemporalForecastReconciliationatDigitalPlatformswithMachineLearning.html">&lt;p&gt;Platform businesses operate on a digital core and their decision making requires high-dimensional accurate forecast streams at different levels of cross-sectional (e.g., geographical regions) and temporal aggregation (e.g., minutes to days). It also necessitates coherent forecasts across all levels of the hierarchy to ensure aligned decision making across different planning units such as pricing, product, controlling and strategy. Given that platform data streams feature complex characteristics and interdependencies, we introduce a non-linear hierarchical forecast reconciliation method that produces cross-temporal reconciled forecasts in a direct and automated way through the use of popular machine learning methods. The method is sufficiently fast to allow forecast-based high-frequency decision making that platforms require. We empirically test our framework on unique, large-scale streaming datasets from a leading on-demand delivery platform in Europe and a bicycle sharing system in New York City.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.09033&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jeroen Rombouts, Marie Ternes, Ines Wilms</name></author><category term="stat.AP," /><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Platform businesses operate on a digital core and their decision making requires high-dimensional accurate forecast streams at different levels of cross-sectional (e.g., geographical regions) and temporal aggregation (e.g., minutes to days). It also necessitates coherent forecasts across all levels of the hierarchy to ensure aligned decision making across different planning units such as pricing, product, controlling and strategy. Given that platform data streams feature complex characteristics and interdependencies, we introduce a non-linear hierarchical forecast reconciliation method that produces cross-temporal reconciled forecasts in a direct and automated way through the use of popular machine learning methods. The method is sufficiently fast to allow forecast-based high-frequency decision making that platforms require. We empirically test our framework on unique, large-scale streaming datasets from a leading on-demand delivery platform in Europe and a bicycle sharing system in New York City.</summary></entry><entry><title type="html">Data Fusion for Heterogeneous Treatment Effect Estimation with Multi-Task Gaussian Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/DataFusionforHeterogeneousTreatmentEffectEstimationwithMultiTaskGaussianProcesses.html" rel="alternate" type="text/html" title="Data Fusion for Heterogeneous Treatment Effect Estimation with Multi-Task Gaussian Processes" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/DataFusionforHeterogeneousTreatmentEffectEstimationwithMultiTaskGaussianProcesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/DataFusionforHeterogeneousTreatmentEffectEstimationwithMultiTaskGaussianProcesses.html">&lt;p&gt;Bridging the gap between internal and external validity is crucial for heterogeneous treatment effect estimation. Randomised controlled trials (RCTs), favoured for their internal validity due to randomisation, often encounter challenges in generalising findings due to strict eligibility criteria. Observational studies on the other hand, provide external validity advantages through larger and more representative samples but suffer from compromised internal validity due to unmeasured confounding. Motivated by these complementary characteristics, we propose a novel Bayesian nonparametric approach leveraging multi-task Gaussian processes to integrate data from both RCTs and observational studies. In particular, we introduce a parameter which controls the degree of borrowing between the datasets and prevents the observational dataset from dominating the estimation. The value of the parameter can be either user-set or chosen through a data-adaptive procedure. Our approach outperforms other methods in point predictions across the covariate support of the observational study, and furthermore provides a calibrated measure of uncertainty for the estimated treatment effects, which is crucial when extrapolating. We demonstrate the robust performance of our approach in diverse scenarios through multiple simulation studies and a real-world education randomised trial.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20957&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Evangelos Dimitriou, Edwin Fong, Karla Diaz-Ordaz, Brieuc Lehmann</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Bridging the gap between internal and external validity is crucial for heterogeneous treatment effect estimation. Randomised controlled trials (RCTs), favoured for their internal validity due to randomisation, often encounter challenges in generalising findings due to strict eligibility criteria. Observational studies on the other hand, provide external validity advantages through larger and more representative samples but suffer from compromised internal validity due to unmeasured confounding. Motivated by these complementary characteristics, we propose a novel Bayesian nonparametric approach leveraging multi-task Gaussian processes to integrate data from both RCTs and observational studies. In particular, we introduce a parameter which controls the degree of borrowing between the datasets and prevents the observational dataset from dominating the estimation. The value of the parameter can be either user-set or chosen through a data-adaptive procedure. Our approach outperforms other methods in point predictions across the covariate support of the observational study, and furthermore provides a calibrated measure of uncertainty for the estimated treatment effects, which is crucial when extrapolating. We demonstrate the robust performance of our approach in diverse scenarios through multiple simulation studies and a real-world education randomised trial.</summary></entry><entry><title type="html">Differentially Private Boxplots</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/DifferentiallyPrivateBoxplots.html" rel="alternate" type="text/html" title="Differentially Private Boxplots" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/DifferentiallyPrivateBoxplots</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/DifferentiallyPrivateBoxplots.html">&lt;p&gt;Despite the potential of differentially private data visualization to harmonize data analysis and privacy, research in this area remains relatively underdeveloped. Boxplots are a widely popular visualization used for summarizing a dataset and for comparison of multiple datasets. Consequentially, we introduce a differentially private boxplot. We evaluate its effectiveness for displaying location, scale, skewness and tails of a given empirical distribution. In our theoretical exposition, we show that the location and scale of the boxplot are estimated with optimal sample complexity, and the skewness and tails are estimated consistently. In simulations, we show that this boxplot performs similarly to a non-private boxplot, and it outperforms a boxplot naively constructed from existing differentially private quantile algorithms. Additionally, we conduct a real data analysis of Airbnb listings, which shows that comparable analysis can be achieved through differentially private boxplot visualization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20415&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kelly Ramsay, Jairo Diaz-Rodriguez</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.OT" /><summary type="html">Despite the potential of differentially private data visualization to harmonize data analysis and privacy, research in this area remains relatively underdeveloped. Boxplots are a widely popular visualization used for summarizing a dataset and for comparison of multiple datasets. Consequentially, we introduce a differentially private boxplot. We evaluate its effectiveness for displaying location, scale, skewness and tails of a given empirical distribution. In our theoretical exposition, we show that the location and scale of the boxplot are estimated with optimal sample complexity, and the skewness and tails are estimated consistently. In simulations, we show that this boxplot performs similarly to a non-private boxplot, and it outperforms a boxplot naively constructed from existing differentially private quantile algorithms. Additionally, we conduct a real data analysis of Airbnb listings, which shows that comparable analysis can be achieved through differentially private boxplot visualization.</summary></entry><entry><title type="html">Ensemble-localized Kernel Density Estimation with Applications to the Ensemble Gaussian Mixture Filter</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/EnsemblelocalizedKernelDensityEstimationwithApplicationstotheEnsembleGaussianMixtureFilter.html" rel="alternate" type="text/html" title="Ensemble-localized Kernel Density Estimation with Applications to the Ensemble Gaussian Mixture Filter" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/EnsemblelocalizedKernelDensityEstimationwithApplicationstotheEnsembleGaussianMixtureFilter</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/EnsemblelocalizedKernelDensityEstimationwithApplicationstotheEnsembleGaussianMixtureFilter.html">&lt;p&gt;The ensemble Gaussian mixture filter (EnGMF) is a non-linear filter suited to data assimilation of highly non-Gaussian and non-linear models that has practical utility in the case of a small number of samples, and theoretical convergence to full Bayesian inference in the ensemble limit. We aim to increase the utility of the EnGMF by introducing an ensemble-local notion of covariance into the kernel density estimation (KDE) step for the prior distribution. We prove that in the Gaussian case, our new ensemble-localized KDE technique is exactly the same as more traditional KDE techniques. We also show an example of a non-Gaussian distribution that can fail to be approximated by canonical KDE methods, but can be approximated well by our new KDE technique. We showcase our new KDE technique on a simple bivariate problem, showing that it has nice qualitative and quantitative properties, and significantly improves the estimate of the prior and posterior distributions for all ensemble sizes tested. We additionally show the utility of the proposed methodology for sequential filtering for the Lorenz ‘63 equations, achieving a significant reduction in error, and less conservative behavior in the uncertainty estimate with respect to traditional techniques.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.14143&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrey A. Popov, Enrico M. Zucchelli, Renato Zanetti</name></author><category term="stat.AP" /><summary type="html">The ensemble Gaussian mixture filter (EnGMF) is a non-linear filter suited to data assimilation of highly non-Gaussian and non-linear models that has practical utility in the case of a small number of samples, and theoretical convergence to full Bayesian inference in the ensemble limit. We aim to increase the utility of the EnGMF by introducing an ensemble-local notion of covariance into the kernel density estimation (KDE) step for the prior distribution. We prove that in the Gaussian case, our new ensemble-localized KDE technique is exactly the same as more traditional KDE techniques. We also show an example of a non-Gaussian distribution that can fail to be approximated by canonical KDE methods, but can be approximated well by our new KDE technique. We showcase our new KDE technique on a simple bivariate problem, showing that it has nice qualitative and quantitative properties, and significantly improves the estimate of the prior and posterior distributions for all ensemble sizes tested. We additionally show the utility of the proposed methodology for sequential filtering for the Lorenz ‘63 equations, achieving a significant reduction in error, and less conservative behavior in the uncertainty estimate with respect to traditional techniques.</summary></entry><entry><title type="html">Extremile scalar-on-function regression with application to climate scenarios</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Extremilescalaronfunctionregressionwithapplicationtoclimatescenarios.html" rel="alternate" type="text/html" title="Extremile scalar-on-function regression with application to climate scenarios" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Extremilescalaronfunctionregressionwithapplicationtoclimatescenarios</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Extremilescalaronfunctionregressionwithapplicationtoclimatescenarios.html">&lt;p&gt;Extremiles provide a generalization of quantiles which are not only robust, but also have an intrinsic link with extreme value theory. This paper introduces an extremile regression model tailored for functional covariate spaces. The estimation procedure turns out to be a weighted version of local linear scalar-on-function regression, where now a double kernel approach plays a crucial role. Asymptotic expressions for the bias and variance are established, applicable to both decreasing bandwidth sequences and automatically selected bandwidths. The methodology is then investigated in detail through a simulation study. Furthermore, we highlight the applicability of the model through the analysis of data sourced from the CH2018 Swiss climate scenarios project, offering insights into its ability to serve as a modern tool to quantify climate behaviour.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20817&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maria Laura Battagliola, Martin Bladt</name></author><category term="stat.ME" /><summary type="html">Extremiles provide a generalization of quantiles which are not only robust, but also have an intrinsic link with extreme value theory. This paper introduces an extremile regression model tailored for functional covariate spaces. The estimation procedure turns out to be a weighted version of local linear scalar-on-function regression, where now a double kernel approach plays a crucial role. Asymptotic expressions for the bias and variance are established, applicable to both decreasing bandwidth sequences and automatically selected bandwidths. The methodology is then investigated in detail through a simulation study. Furthermore, we highlight the applicability of the model through the analysis of data sourced from the CH2018 Swiss climate scenarios project, offering insights into its ability to serve as a modern tool to quantify climate behaviour.</summary></entry><entry><title type="html">Fast Bayesian Basis Selection for Functional Data Representation with Correlated Errors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/FastBayesianBasisSelectionforFunctionalDataRepresentationwithCorrelatedErrors.html" rel="alternate" type="text/html" title="Fast Bayesian Basis Selection for Functional Data Representation with Correlated Errors" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/FastBayesianBasisSelectionforFunctionalDataRepresentationwithCorrelatedErrors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/FastBayesianBasisSelectionforFunctionalDataRepresentationwithCorrelatedErrors.html">&lt;p&gt;Functional data analysis (FDA) finds widespread application across various fields, due to data being recorded continuously over a time interval or at several discrete points. Since the data is not observed at every point but rather across a dense grid, smoothing techniques are often employed to convert the observed data into functions. In this work, we propose a novel Bayesian approach for selecting basis functions for smoothing one or multiple curves simultaneously. Our method differentiates from other Bayesian approaches in two key ways: (i) by accounting for correlated errors and (ii) by developing a variational EM algorithm instead of a Gibbs sampler. Simulation studies demonstrate that our method effectively identifies the true underlying structure of the data across various scenarios and it is applicable to different types of functional data. Our variational EM algorithm not only recovers the basis coefficients and the correct set of basis functions but also estimates the existing within-curve correlation. When applied to the motorcycle dataset, our method demonstrates comparable, and in some cases superior, performance in terms of adjusted $R^2$ compared to other techniques such as regression splines, Bayesian LASSO and LASSO. Additionally, when assuming independence among observations within a curve, our method, utilizing only a variational Bayes algorithm, is in the order of thousands faster than a Gibbs sampler on average. Our proposed method is implemented in R and codes are available at https://github.com/acarolcruz/VB-Bases-Selection.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20758&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ana Carolina da Cruz, Camila P. E. de Souza, Pedro H. T. O. Sousa</name></author><category term="stat.ME" /><summary type="html">Functional data analysis (FDA) finds widespread application across various fields, due to data being recorded continuously over a time interval or at several discrete points. Since the data is not observed at every point but rather across a dense grid, smoothing techniques are often employed to convert the observed data into functions. In this work, we propose a novel Bayesian approach for selecting basis functions for smoothing one or multiple curves simultaneously. Our method differentiates from other Bayesian approaches in two key ways: (i) by accounting for correlated errors and (ii) by developing a variational EM algorithm instead of a Gibbs sampler. Simulation studies demonstrate that our method effectively identifies the true underlying structure of the data across various scenarios and it is applicable to different types of functional data. Our variational EM algorithm not only recovers the basis coefficients and the correct set of basis functions but also estimates the existing within-curve correlation. When applied to the motorcycle dataset, our method demonstrates comparable, and in some cases superior, performance in terms of adjusted $R^2$ compared to other techniques such as regression splines, Bayesian LASSO and LASSO. Additionally, when assuming independence among observations within a curve, our method, utilizing only a variational Bayes algorithm, is in the order of thousands faster than a Gibbs sampler on average. Our proposed method is implemented in R and codes are available at https://github.com/acarolcruz/VB-Bases-Selection.</summary></entry><entry><title type="html">Fast Forecasting of Unstable Data Streams for On-Demand Service Platforms</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/FastForecastingofUnstableDataStreamsforOnDemandServicePlatforms.html" rel="alternate" type="text/html" title="Fast Forecasting of Unstable Data Streams for On-Demand Service Platforms" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/FastForecastingofUnstableDataStreamsforOnDemandServicePlatforms</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/FastForecastingofUnstableDataStreamsforOnDemandServicePlatforms.html">&lt;p&gt;On-demand service platforms face a challenging problem of forecasting a large collection of high-frequency regional demand data streams that exhibit instabilities. This paper develops a novel forecast framework that is fast and scalable, and automatically assesses changing environments without human intervention. We empirically test our framework on a large-scale demand data set from a leading on-demand delivery platform in Europe, and find strong performance gains from using our framework against several industry benchmarks, across all geographical regions, loss functions, and both pre- and post-Covid periods. We translate forecast gains to economic impacts for this on-demand service platform by computing financial gains and reductions in computing costs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.01887&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yu Jeffrey Hu, Jeroen Rombouts, Ines Wilms</name></author><category term="stat.AP" /><summary type="html">On-demand service platforms face a challenging problem of forecasting a large collection of high-frequency regional demand data streams that exhibit instabilities. This paper develops a novel forecast framework that is fast and scalable, and automatically assesses changing environments without human intervention. We empirically test our framework on a large-scale demand data set from a leading on-demand delivery platform in Europe, and find strong performance gains from using our framework against several industry benchmarks, across all geographical regions, loss functions, and both pre- and post-Covid periods. We translate forecast gains to economic impacts for this on-demand service platform by computing financial gains and reductions in computing costs.</summary></entry><entry><title type="html">Fast leave-one-cluster-out cross-validation by clustered Network Information Criteria (NICc)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/FastleaveoneclusteroutcrossvalidationbyclusteredNetworkInformationCriteriaNICc.html" rel="alternate" type="text/html" title="Fast leave-one-cluster-out cross-validation by clustered Network Information Criteria (NICc)" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/FastleaveoneclusteroutcrossvalidationbyclusteredNetworkInformationCriteriaNICc</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/FastleaveoneclusteroutcrossvalidationbyclusteredNetworkInformationCriteriaNICc.html">&lt;p&gt;This paper introduced a clustered estimator of the Network Information Criterion (NICc) to approximate leave-one-cluster-out cross-validated deviance, which can be used as an alternative to cluster-based cross-validation when modeling clustered data. Stone proved that Akaike Information Criterion (AIC) is an asymptotic equivalence to leave-one-observation-out cross-validation if the parametric model is true. Ripley pointed out that the Network Information Criterion (NIC) derived in Stone’s proof, is a better approximation to leave-one-observation-out cross-validation when the model is not true. For clustered data, we derived a clustered estimator of NIC, referred to as NICc, by substituting the Fisher information matrix in NIC with its estimator that adjusts for clustering. This adjustment imposes a larger penalty in NICc than the unclustered estimator of NIC when modeling clustered data, thereby preventing overfitting more effectively. In a simulation study and an empirical example, we used linear and logistic regression to model clustered data with Gaussian or binomial response, respectively. We showed that NICc is a better approximation to leave-one-cluster-out deviance and prevents overfitting more effectively than AIC and Bayesian Information Criterion (BIC). NICc leads to more accurate model selection, as determined by cluster-based cross-validation, compared to AIC and BIC.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20400&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiaxing Qiu, Douglas E. Lake, Teague R. Henry</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">This paper introduced a clustered estimator of the Network Information Criterion (NICc) to approximate leave-one-cluster-out cross-validated deviance, which can be used as an alternative to cluster-based cross-validation when modeling clustered data. Stone proved that Akaike Information Criterion (AIC) is an asymptotic equivalence to leave-one-observation-out cross-validation if the parametric model is true. Ripley pointed out that the Network Information Criterion (NIC) derived in Stone’s proof, is a better approximation to leave-one-observation-out cross-validation when the model is not true. For clustered data, we derived a clustered estimator of NIC, referred to as NICc, by substituting the Fisher information matrix in NIC with its estimator that adjusts for clustering. This adjustment imposes a larger penalty in NICc than the unclustered estimator of NIC when modeling clustered data, thereby preventing overfitting more effectively. In a simulation study and an empirical example, we used linear and logistic regression to model clustered data with Gaussian or binomial response, respectively. We showed that NICc is a better approximation to leave-one-cluster-out deviance and prevents overfitting more effectively than AIC and Bayesian Information Criterion (BIC). NICc leads to more accurate model selection, as determined by cluster-based cross-validation, compared to AIC and BIC.</summary></entry><entry><title type="html">Fixed-budget optimal designs for multi-fidelity computer experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Fixedbudgetoptimaldesignsformultifidelitycomputerexperiments.html" rel="alternate" type="text/html" title="Fixed-budget optimal designs for multi-fidelity computer experiments" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Fixedbudgetoptimaldesignsformultifidelitycomputerexperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Fixedbudgetoptimaldesignsformultifidelitycomputerexperiments.html">&lt;p&gt;This work focuses on the design of experiments of multi-fidelity computer experiments. We consider the autoregressive Gaussian process model proposed by Kennedy and O’Hagan (2000) and the optimal nested design that maximizes the prediction accuracy subject to a budget constraint. An approximate solution is identified through the idea of multi-level approximation and recent error bounds of Gaussian process regression. The proposed (approximately) optimal designs admit a simple analytical form. We prove that, to achieve the same prediction accuracy, the proposed optimal multi-fidelity design requires much lower computational cost than any single-fidelity design in the asymptotic sense. Numerical studies confirm this theoretical assertion.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20644&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Gecheng Chen, Rui Tuo</name></author><category term="stat.ME" /><summary type="html">This work focuses on the design of experiments of multi-fidelity computer experiments. We consider the autoregressive Gaussian process model proposed by Kennedy and O’Hagan (2000) and the optimal nested design that maximizes the prediction accuracy subject to a budget constraint. An approximate solution is identified through the idea of multi-level approximation and recent error bounds of Gaussian process regression. The proposed (approximately) optimal designs admit a simple analytical form. We prove that, to achieve the same prediction accuracy, the proposed optimal multi-fidelity design requires much lower computational cost than any single-fidelity design in the asymptotic sense. Numerical studies confirm this theoretical assertion.</summary></entry><entry><title type="html">G-Transformer for Conditional Average Potential Outcome Estimation over Time</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/GTransformerforConditionalAveragePotentialOutcomeEstimationoverTime.html" rel="alternate" type="text/html" title="G-Transformer for Conditional Average Potential Outcome Estimation over Time" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/GTransformerforConditionalAveragePotentialOutcomeEstimationoverTime</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/GTransformerforConditionalAveragePotentialOutcomeEstimationoverTime.html">&lt;p&gt;Estimating potential outcomes for treatments over time based on observational data is important for personalized decision-making in medicine. Yet, existing neural methods for this task suffer from either (a) bias or (b) large variance. In order to address both limitations, we introduce the G-transformer (GT). Our GT is a novel, neural end-to-end model designed for unbiased, low-variance estimation of conditional average potential outcomes (CAPOs) over time. Specifically, our GT is the first neural model to perform regression-based iterative G-computation for CAPOs in the time-varying setting. We evaluate the effectiveness of our GT across various experiments. In sum, this work represents a significant step towards personalized decision-making from electronic health records.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.21012&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Konstantin Hess, Dennis Frauen, Valentyn Melnychuk, Stefan Feuerriegel</name></author><category term="stat.ME" /><summary type="html">Estimating potential outcomes for treatments over time based on observational data is important for personalized decision-making in medicine. Yet, existing neural methods for this task suffer from either (a) bias or (b) large variance. In order to address both limitations, we introduce the G-transformer (GT). Our GT is a novel, neural end-to-end model designed for unbiased, low-variance estimation of conditional average potential outcomes (CAPOs) over time. Specifically, our GT is the first neural model to perform regression-based iterative G-computation for CAPOs in the time-varying setting. We evaluate the effectiveness of our GT across various experiments. In sum, this work represents a significant step towards personalized decision-making from electronic health records.</summary></entry><entry><title type="html">IncomeSCM: From tabular data set to time-series simulator and causal estimation benchmark</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/IncomeSCMFromtabulardatasettotimeseriessimulatorandcausalestimationbenchmark.html" rel="alternate" type="text/html" title="IncomeSCM: From tabular data set to time-series simulator and causal estimation benchmark" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/IncomeSCMFromtabulardatasettotimeseriessimulatorandcausalestimationbenchmark</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/IncomeSCMFromtabulardatasettotimeseriessimulatorandcausalestimationbenchmark.html">&lt;p&gt;Evaluating observational estimators of causal effects demands information that is rarely available: unconfounded interventions and outcomes from the population of interest, created either by randomization or adjustment. As a result, it is customary to fall back on simulators when creating benchmark tasks. Simulators offer great control but are often too simplistic to make challenging tasks, either because they are hand-designed and lack the nuances of real-world data, or because they are fit to observational data without structural constraints. In this work, we propose a general, repeatable strategy for turning observational data into sequential structural causal models and challenging estimation tasks by following two simple principles: 1) fitting real-world data where possible, and 2) creating complexity by composing simple, hand-designed mechanisms. We implement these ideas in a highly configurable software package and apply it to the well-known Adult income data set to construct the \tt IncomeSCM simulator. From this, we devise multiple estimation tasks and sample data sets to compare established estimators of causal effects. The tasks present a suitable challenge, with effect estimates varying greatly in quality between methods, despite similar performance in the modeling of factual outcomes, highlighting the need for dedicated causal estimators and model selection criteria.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16069&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fredrik D. Johansson</name></author><category term="stat.ME" /><summary type="html">Evaluating observational estimators of causal effects demands information that is rarely available: unconfounded interventions and outcomes from the population of interest, created either by randomization or adjustment. As a result, it is customary to fall back on simulators when creating benchmark tasks. Simulators offer great control but are often too simplistic to make challenging tasks, either because they are hand-designed and lack the nuances of real-world data, or because they are fit to observational data without structural constraints. In this work, we propose a general, repeatable strategy for turning observational data into sequential structural causal models and challenging estimation tasks by following two simple principles: 1) fitting real-world data where possible, and 2) creating complexity by composing simple, hand-designed mechanisms. We implement these ideas in a highly configurable software package and apply it to the well-known Adult income data set to construct the \tt IncomeSCM simulator. From this, we devise multiple estimation tasks and sample data sets to compare established estimators of causal effects. The tasks present a suitable challenge, with effect estimates varying greatly in quality between methods, despite similar performance in the modeling of factual outcomes, highlighting the need for dedicated causal estimators and model selection criteria.</summary></entry><entry><title type="html">Introducing sgboost: A Practical Guide and Implementation of sparse-group boosting in R</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/IntroducingsgboostAPracticalGuideandImplementationofsparsegroupboostinginR.html" rel="alternate" type="text/html" title="Introducing sgboost: A Practical Guide and Implementation of sparse-group boosting in R" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/IntroducingsgboostAPracticalGuideandImplementationofsparsegroupboostinginR</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/IntroducingsgboostAPracticalGuideandImplementationofsparsegroupboostinginR.html">&lt;p&gt;This paper introduces the sgboost package in R, which implements sparse-group boosting for modeling high-dimensional data with natural groupings in covariates. Sparse-group boosting offers a flexible approach for both group and individual variable selection, reducing overfitting and enhancing model interpretability. The package uses regularization techniques based on the degrees of freedom of individual and group base-learners, and is designed to be used in conjunction with the mboost package. Through comparisons with existing methods and demonstration of its unique functionalities, this paper provides a practical guide on utilizing sparse-group boosting in R, accompanied by code examples to facilitate its application in various research domains. Overall, this paper serves as a valuable resource for researchers and practitioners seeking to use sparse-group boosting for efficient and interpretable high-dimensional data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.21037&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fabian Obster, Christian Heumann</name></author><category term="stat.AP," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">This paper introduces the sgboost package in R, which implements sparse-group boosting for modeling high-dimensional data with natural groupings in covariates. Sparse-group boosting offers a flexible approach for both group and individual variable selection, reducing overfitting and enhancing model interpretability. The package uses regularization techniques based on the degrees of freedom of individual and group base-learners, and is designed to be used in conjunction with the mboost package. Through comparisons with existing methods and demonstration of its unique functionalities, this paper provides a practical guide on utilizing sparse-group boosting in R, accompanied by code examples to facilitate its application in various research domains. Overall, this paper serves as a valuable resource for researchers and practitioners seeking to use sparse-group boosting for efficient and interpretable high-dimensional data analysis.</summary></entry><entry><title type="html">Numerical Generalized Randomized HMC processes for restricted domains</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/NumericalGeneralizedRandomizedHMCprocessesforrestricteddomains.html" rel="alternate" type="text/html" title="Numerical Generalized Randomized HMC processes for restricted domains" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/NumericalGeneralizedRandomizedHMCprocessesforrestricteddomains</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/NumericalGeneralizedRandomizedHMCprocessesforrestricteddomains.html">&lt;p&gt;We propose a generic approach for numerically efficient simulation from analytically intractable distributions with constrained support. Our approach relies upon Generalized Randomized Hamiltonian Monte Carlo (GRHMC) processes and combines these with a randomized transition kernel that appropriately adjusts the Hamiltonian flow at the boundary of the constrained domain, ensuring that it remains within the domain. The numerical implementation of this constrained GRHMC process exploits the sparsity of the randomized transition kernel and the specific structure of the constraints so that the proposed approach is numerically accurate, computationally fast and operational even in high-dimensional applications. We illustrate this approach with posterior distributions of several Bayesian models with challenging parameter domain constraints in applications to real-word data sets. Building on the capability of GRHMC processes to efficiently explore otherwise challenging and high-dimensional posteriors, the proposed method expands the set of Bayesian models that can be analyzed by using the standard Markov-Chain Monte-Carlo (MCMC) methodology, As such, it can advance the development and use of Bayesian models with useful constrained priors, which are difficult to handle with existing methods. The article is accompanied by an R-package (\url{https://github.com/torekleppe/pdmphmc}), which allows for automatically implementing GRHMC processes for arbitrary target distributions and domain constraints.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.14492&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tore Selland Kleppe, Roman Liesenfeld</name></author><category term="stat.CO" /><summary type="html">We propose a generic approach for numerically efficient simulation from analytically intractable distributions with constrained support. Our approach relies upon Generalized Randomized Hamiltonian Monte Carlo (GRHMC) processes and combines these with a randomized transition kernel that appropriately adjusts the Hamiltonian flow at the boundary of the constrained domain, ensuring that it remains within the domain. The numerical implementation of this constrained GRHMC process exploits the sparsity of the randomized transition kernel and the specific structure of the constraints so that the proposed approach is numerically accurate, computationally fast and operational even in high-dimensional applications. We illustrate this approach with posterior distributions of several Bayesian models with challenging parameter domain constraints in applications to real-word data sets. Building on the capability of GRHMC processes to efficiently explore otherwise challenging and high-dimensional posteriors, the proposed method expands the set of Bayesian models that can be analyzed by using the standard Markov-Chain Monte-Carlo (MCMC) methodology, As such, it can advance the development and use of Bayesian models with useful constrained priors, which are difficult to handle with existing methods. The article is accompanied by an R-package (\url{https://github.com/torekleppe/pdmphmc}), which allows for automatically implementing GRHMC processes for arbitrary target distributions and domain constraints.</summary></entry><entry><title type="html">Parameter identification in linear non-Gaussian causal models under general confounding</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ParameteridentificationinlinearnonGaussiancausalmodelsundergeneralconfounding.html" rel="alternate" type="text/html" title="Parameter identification in linear non-Gaussian causal models under general confounding" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ParameteridentificationinlinearnonGaussiancausalmodelsundergeneralconfounding</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ParameteridentificationinlinearnonGaussiancausalmodelsundergeneralconfounding.html">&lt;p&gt;Linear non-Gaussian causal models postulate that each random variable is a linear function of parent variables and non-Gaussian exogenous error terms. We study identification of the linear coefficients when such models contain latent variables. Our focus is on the commonly studied acyclic setting, where each model corresponds to a directed acyclic graph (DAG). For this case, prior literature has demonstrated that connections to overcomplete independent component analysis yield effective criteria to decide parameter identifiability in latent variable models. However, this connection is based on the assumption that the observed variables linearly depend on the latent variables. Departing from this assumption, we treat models that allow for arbitrary non-linear latent confounding. Our main result is a graphical criterion that is necessary and sufficient for deciding the generic identifiability of direct causal effects. Moreover, we provide an algorithmic implementation of the criterion with a run time that is polynomial in the number of observed variables. Finally, we report on estimation heuristics based on the identification result, explore a generalization to models with feedback loops, and provide new results on the identifiability of the causal graph.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20856&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Daniele Tramontano, Mathias Drton, Jalal Etesami</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Linear non-Gaussian causal models postulate that each random variable is a linear function of parent variables and non-Gaussian exogenous error terms. We study identification of the linear coefficients when such models contain latent variables. Our focus is on the commonly studied acyclic setting, where each model corresponds to a directed acyclic graph (DAG). For this case, prior literature has demonstrated that connections to overcomplete independent component analysis yield effective criteria to decide parameter identifiability in latent variable models. However, this connection is based on the assumption that the observed variables linearly depend on the latent variables. Departing from this assumption, we treat models that allow for arbitrary non-linear latent confounding. Our main result is a graphical criterion that is necessary and sufficient for deciding the generic identifiability of direct causal effects. Moreover, we provide an algorithmic implementation of the criterion with a run time that is polynomial in the number of observed variables. Finally, we report on estimation heuristics based on the identification result, explore a generalization to models with feedback loops, and provide new results on the identifiability of the causal graph.</summary></entry><entry><title type="html">Resampling methods for Private Statistical Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ResamplingmethodsforPrivateStatisticalInference.html" rel="alternate" type="text/html" title="Resampling methods for Private Statistical Inference" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ResamplingmethodsforPrivateStatisticalInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/ResamplingmethodsforPrivateStatisticalInference.html">&lt;p&gt;We consider the task of constructing confidence intervals with differential privacy. We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple “little” bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. For a fixed differential privacy parameter $\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the sample size $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and logistic regression with both real and synthetic data. Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\gtrsim 10$ times) confidence intervals than previous approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.07131&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Karan Chadha, John Duchi, Rohith Kuditipudi</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">We consider the task of constructing confidence intervals with differential privacy. We propose two private variants of the non-parametric bootstrap, which privately compute the median of the results of multiple “little” bootstraps run on partitions of the data and give asymptotic bounds on the coverage error of the resulting confidence intervals. For a fixed differential privacy parameter $\epsilon$, our methods enjoy the same error rates as that of the non-private bootstrap to within logarithmic factors in the sample size $n$. We empirically validate the performance of our methods for mean estimation, median estimation, and logistic regression with both real and synthetic data. Our methods achieve similar coverage accuracy to existing methods (and non-private baselines) while providing notably shorter ($\gtrsim 10$ times) confidence intervals than previous approaches.</summary></entry><entry><title type="html">Statistical inference for case-control logistic regression via integrating external summary data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Statisticalinferenceforcasecontrollogisticregressionviaintegratingexternalsummarydata.html" rel="alternate" type="text/html" title="Statistical inference for case-control logistic regression via integrating external summary data" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Statisticalinferenceforcasecontrollogisticregressionviaintegratingexternalsummarydata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Statisticalinferenceforcasecontrollogisticregressionviaintegratingexternalsummarydata.html">&lt;p&gt;Case-control sampling is a commonly used retrospective sampling design to alleviate imbalanced structure of binary data. When fitting the logistic regression model with case-control data, although the slope parameter of the model can be consistently estimated, the intercept parameter is not identifiable, and the marginal case proportion is not estimatable, either. We consider the situations in which besides the case-control data from the main study, called internal study, there also exists summary-level information from related external studies. An empirical likelihood based approach is proposed to make inference for the logistic model by incorporating the internal case-control data and external information. We show that the intercept parameter is identifiable with the help of external information, and then all the regression parameters as well as the marginal case proportion can be estimated consistently. The proposed method also accounts for the possible variability in external studies. The resultant estimators are shown to be asymptotically normally distributed. The asymptotic variance-covariance matrix can be consistently estimated by the case-control data. The optimal way to utilized external information is discussed. Simulation studies are conducted to verify the theoretical findings. A real data set is analyzed for illustration.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20655&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hengchao Shi, Xinyi Liu, Ming Zheng, Wen Yu</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Case-control sampling is a commonly used retrospective sampling design to alleviate imbalanced structure of binary data. When fitting the logistic regression model with case-control data, although the slope parameter of the model can be consistently estimated, the intercept parameter is not identifiable, and the marginal case proportion is not estimatable, either. We consider the situations in which besides the case-control data from the main study, called internal study, there also exists summary-level information from related external studies. An empirical likelihood based approach is proposed to make inference for the logistic model by incorporating the internal case-control data and external information. We show that the intercept parameter is identifiable with the help of external information, and then all the regression parameters as well as the marginal case proportion can be estimated consistently. The proposed method also accounts for the possible variability in external studies. The resultant estimators are shown to be asymptotically normally distributed. The asymptotic variance-covariance matrix can be consistently estimated by the case-control data. The optimal way to utilized external information is discussed. Simulation studies are conducted to verify the theoretical findings. A real data set is analyzed for illustration.</summary></entry><entry><title type="html">Synthesis estimators for positivity violations with a continuous covariate</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Synthesisestimatorsforpositivityviolationswithacontinuouscovariate.html" rel="alternate" type="text/html" title="Synthesis estimators for positivity violations with a continuous covariate" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Synthesisestimatorsforpositivityviolationswithacontinuouscovariate</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/Synthesisestimatorsforpositivityviolationswithacontinuouscovariate.html">&lt;p&gt;Studies intended to estimate the effect of a treatment, like randomized trials, may not be sampled from the desired target population. To correct for this discrepancy, estimates can be transported to the target population. Methods for transporting between populations are often premised on a positivity assumption, such that all relevant covariate patterns in one population are also present in the other. However, eligibility criteria, particularly in the case of trials, can result in violations of positivity when transporting to external populations. To address nonpositivity, a synthesis of statistical and mathematical models can be considered. This approach integrates multiple data sources (e.g. trials, observational, pharmacokinetic studies) to estimate treatment effects, leveraging mathematical models to handle positivity violations. This approach was previously demonstrated for positivity violations by a single binary covariate. Here, we extend the synthesis approach for positivity violations with a continuous covariate. For estimation, two novel augmented inverse probability weighting estimators are proposed. Both estimators are contrasted with other common approaches for addressing nonpositivity. Empirical performance is compared via Monte Carlo simulation. Finally, the competing approaches are illustrated with an example in the context of two-drug versus one-drug antiretroviral therapy on CD4 T cell counts among women with HIV.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.09388&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Paul N Zivich, Jessie K Edwards, Bonnie E Shook-Sa, Eric T Lofgren, Justin Lessler, Stephen R Cole</name></author><category term="stat.ME" /><summary type="html">Studies intended to estimate the effect of a treatment, like randomized trials, may not be sampled from the desired target population. To correct for this discrepancy, estimates can be transported to the target population. Methods for transporting between populations are often premised on a positivity assumption, such that all relevant covariate patterns in one population are also present in the other. However, eligibility criteria, particularly in the case of trials, can result in violations of positivity when transporting to external populations. To address nonpositivity, a synthesis of statistical and mathematical models can be considered. This approach integrates multiple data sources (e.g. trials, observational, pharmacokinetic studies) to estimate treatment effects, leveraging mathematical models to handle positivity violations. This approach was previously demonstrated for positivity violations by a single binary covariate. Here, we extend the synthesis approach for positivity violations with a continuous covariate. For estimation, two novel augmented inverse probability weighting estimators are proposed. Both estimators are contrasted with other common approaches for addressing nonpositivity. Empirical performance is compared via Monte Carlo simulation. Finally, the competing approaches are illustrated with an example in the context of two-drug versus one-drug antiretroviral therapy on CD4 T cell counts among women with HIV.</summary></entry><entry><title type="html">Tomographic reconstruction of a disease transmission landscape via GPS recorded random paths</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/TomographicreconstructionofadiseasetransmissionlandscapeviaGPSrecordedrandompaths.html" rel="alternate" type="text/html" title="Tomographic reconstruction of a disease transmission landscape via GPS recorded random paths" /><published>2024-06-03T00:00:00+00:00</published><updated>2024-06-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/TomographicreconstructionofadiseasetransmissionlandscapeviaGPSrecordedrandompaths</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/03/TomographicreconstructionofadiseasetransmissionlandscapeviaGPSrecordedrandompaths.html">&lt;p&gt;Identifying areas in a landscape where individuals have higher probability of becoming infected with a pathogen is a crucial step towards disease management. We perform a novel epidemiological tomography for the estimation of landscape propensity to disease infection, using GPS animal tracks in a manner analogous to tomographic techniques in Positron Emission Tomography. Our study data consists of individual tracks of white-tailed deer (Odocoileus virginianus) and three exotic Cervidae species moving freely in a high-fenced game preserve over given time periods. A serological test was performed on each individual to measure antibody concentration of epizootic hemorrhagic disease viruses (EHDV) at the beginning and at the end of each tracking period. EHDV is a vector-borne viral disease indirectly transmitted between ruminant hosts by biting midges. We model the data as a binomial linear inverse problem, where spatial coherence is enforced with a total variation regularization. The smoothness of the reconstructed propensity map is selected by the quantile universal threshold, which can also test the null hypothesis that the propensity map is spatially constant. We apply our method to simulated and real data, showing good statistical properties during simulations and consistent results and interpretations compared to intensive field estimations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.04455&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jairo Diaz-Rodriguez, Juan Pablo Gomez, Jeremy P. Orange, Nathan D. Burkett-Cadena, Samantha M. Wisely, Jason K. Blackburn, Sylvain Sardy</name></author><category term="stat.AP" /><summary type="html">Identifying areas in a landscape where individuals have higher probability of becoming infected with a pathogen is a crucial step towards disease management. We perform a novel epidemiological tomography for the estimation of landscape propensity to disease infection, using GPS animal tracks in a manner analogous to tomographic techniques in Positron Emission Tomography. Our study data consists of individual tracks of white-tailed deer (Odocoileus virginianus) and three exotic Cervidae species moving freely in a high-fenced game preserve over given time periods. A serological test was performed on each individual to measure antibody concentration of epizootic hemorrhagic disease viruses (EHDV) at the beginning and at the end of each tracking period. EHDV is a vector-borne viral disease indirectly transmitted between ruminant hosts by biting midges. We model the data as a binomial linear inverse problem, where spatial coherence is enforced with a total variation regularization. The smoothness of the reconstructed propensity map is selected by the quantile universal threshold, which can also test the null hypothesis that the propensity map is spatially constant. We apply our method to simulated and real data, showing good statistical properties during simulations and consistent results and interpretations compared to intensive field estimations.</summary></entry></feed>