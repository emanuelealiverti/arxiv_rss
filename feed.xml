<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-22T07:13:36+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html"></title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/2024-05-22-Thecaseforspecifyingtheidealtargettrial.html" rel="alternate" type="text/html" title="" /><published>2024-05-22T07:13:36+00:00</published><updated>2024-05-22T07:13:36+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/2024-05-22-Thecaseforspecifyingtheidealtargettrial</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/2024-05-22-Thecaseforspecifyingtheidealtargettrial.html">&lt;p&gt;The target trial is an increasingly popular conceptual device for guiding the design and analysis of observational studies that seek to perform causal inference. As tends to occur with concepts like this, there is variability in how certain aspects of the approach are understood, which may lead to potentially consequential differences in how the approach is taught, implemented, and interpreted in practice. In this paper, we consider two of these aspects: how the target trial should be specified, and relatedly, how the target trial fits within a formal causal inference framework. We first describe two challenges with what we call the “aligned” approach to target trial specification, which is common in evaluations of medical interventions using healthcare databases and seeks to specify a target trial that is closely aligned to the observational data so that all protocol components apart from randomization can be closely emulated. We then argue how these challenges can be circumvented by an approach that focusses on specifying the “ideal” target trial: a trial with certain idealized aspects that ensure it is relevant to answer the research question. In essence, this approach is applicable in a broader range of settings and enables systematic assessment of all potential sources of causal bias. Importantly, this view provides clarification for how the target trial approach fits within a formal causal inference framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10026&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Aliverti</name></author></entry><entry><title type="html">A Global Wavelet Based Bootstrapped Test of Covariance Stationarity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AGlobalWaveletBasedBootstrappedTestofCovarianceStationarity.html" rel="alternate" type="text/html" title="A Global Wavelet Based Bootstrapped Test of Covariance Stationarity" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AGlobalWaveletBasedBootstrappedTestofCovarianceStationarity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AGlobalWaveletBasedBootstrappedTestofCovarianceStationarity.html">&lt;p&gt;We propose a covariance stationarity test for an otherwise dependent and possibly globally non-stationary time series. We work in a generalized version of the new setting in Jin, Wang and Wang (2015), who exploit Walsh (1923) functions in order to compare sub-sample covariances with the full sample counterpart. They impose strict stationarity under the null, only consider linear processes under either hypothesis in order to achieve a parametric estimator for an inverted high dimensional asymptotic covariance matrix, and do not consider any other orthonormal basis. Conversely, we work with a general orthonormal basis under mild conditions that include Haar wavelet and Walsh functions; and we allow for linear or nonlinear processes with possibly non-iid innovations. This is important in macroeconomics and finance where nonlinear feedback and random volatility occur in many settings. We completely sidestep asymptotic covariance matrix estimation and inversion by bootstrapping a max-correlation difference statistic, where the maximum is taken over the correlation lag $h$ and basis generated sub-sample counter $k$ (the number of systematic samples). We achieve a higher feasible rate of increase for the maximum lag and counter $\mathcal{H}&lt;em&gt;{T}$ and $\mathcal{K}&lt;/em&gt;{T}$. Of particular note, our test is capable of detecting breaks in variance, and distant, or very mild, deviations from stationarity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.14086&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jonathan B. Hill, Tianqi Li</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We propose a covariance stationarity test for an otherwise dependent and possibly globally non-stationary time series. We work in a generalized version of the new setting in Jin, Wang and Wang (2015), who exploit Walsh (1923) functions in order to compare sub-sample covariances with the full sample counterpart. They impose strict stationarity under the null, only consider linear processes under either hypothesis in order to achieve a parametric estimator for an inverted high dimensional asymptotic covariance matrix, and do not consider any other orthonormal basis. Conversely, we work with a general orthonormal basis under mild conditions that include Haar wavelet and Walsh functions; and we allow for linear or nonlinear processes with possibly non-iid innovations. This is important in macroeconomics and finance where nonlinear feedback and random volatility occur in many settings. We completely sidestep asymptotic covariance matrix estimation and inversion by bootstrapping a max-correlation difference statistic, where the maximum is taken over the correlation lag $h$ and basis generated sub-sample counter $k$ (the number of systematic samples). We achieve a higher feasible rate of increase for the maximum lag and counter $\mathcal{H}{T}$ and $\mathcal{K}{T}$. Of particular note, our test is capable of detecting breaks in variance, and distant, or very mild, deviations from stationarity.</summary></entry><entry><title type="html">A Metric-based Principal Curve Approach for Learning One-dimensional Manifold</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AMetricbasedPrincipalCurveApproachforLearningOnedimensionalManifold.html" rel="alternate" type="text/html" title="A Metric-based Principal Curve Approach for Learning One-dimensional Manifold" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AMetricbasedPrincipalCurveApproachforLearningOnedimensionalManifold</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AMetricbasedPrincipalCurveApproachforLearningOnedimensionalManifold.html">&lt;p&gt;Principal curve is a well-known statistical method oriented in manifold learning using concepts from differential geometry. In this paper, we propose a novel metric-based principal curve (MPC) method that learns one-dimensional manifold of spatial data. Synthetic datasets Real applications using MNIST dataset show that our method can learn the one-dimensional manifold well in terms of the shape.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12390&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elvis Han Cui, Sisi Shao</name></author><category term="stat.ML," /><category term="stat.AP" /><summary type="html">Principal curve is a well-known statistical method oriented in manifold learning using concepts from differential geometry. In this paper, we propose a novel metric-based principal curve (MPC) method that learns one-dimensional manifold of spatial data. Synthetic datasets Real applications using MNIST dataset show that our method can learn the one-dimensional manifold well in terms of the shape.</summary></entry><entry><title type="html">A Non-Parametric Box-Cox Approach to Robustifying High-Dimensional Linear Hypothesis Testing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ANonParametricBoxCoxApproachtoRobustifyingHighDimensionalLinearHypothesisTesting.html" rel="alternate" type="text/html" title="A Non-Parametric Box-Cox Approach to Robustifying High-Dimensional Linear Hypothesis Testing" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ANonParametricBoxCoxApproachtoRobustifyingHighDimensionalLinearHypothesisTesting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ANonParametricBoxCoxApproachtoRobustifyingHighDimensionalLinearHypothesisTesting.html">&lt;p&gt;The mainstream theory of hypothesis testing in high-dimensional regression typically assumes the underlying true model is a low-dimensional linear regression model, yet the Box-Cox transformation is a regression technique commonly used to mitigate anomalies like non-additivity and heteroscedasticity. This paper introduces a more flexible framework, the non-parametric Box-Cox model with unspecified transformation, to address model mis-specification in high-dimensional linear hypothesis testing while preserving the interpretation of regression coefficients. Model estimation and computation in high dimensions poses challenges beyond traditional sparse penalization methods. We propose the constrained partial penalized composite probit regression method for sparse estimation and investigate its statistical properties. Additionally, we present a computationally efficient algorithm using augmented Lagrangian and coordinate majorization descent for solving regularization problems with folded concave penalization and linear constraints. For testing linear hypotheses, we propose the partial penalized composite likelihood ratio test, score test and Wald test, and show that their limiting distributions under null and local alternatives follow generalized chi-squared distributions with the same degrees of freedom and noncentral parameter. Extensive simulation studies are conducted to examine the finite sample performance of the proposed tests. Our analysis of supermarket data illustrates potential discrepancies between our testing procedures and standard high-dimensional methods, highlighting the importance of our robustified approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12816&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>He Zhou, Hui Zou</name></author><category term="stat.ME" /><summary type="html">The mainstream theory of hypothesis testing in high-dimensional regression typically assumes the underlying true model is a low-dimensional linear regression model, yet the Box-Cox transformation is a regression technique commonly used to mitigate anomalies like non-additivity and heteroscedasticity. This paper introduces a more flexible framework, the non-parametric Box-Cox model with unspecified transformation, to address model mis-specification in high-dimensional linear hypothesis testing while preserving the interpretation of regression coefficients. Model estimation and computation in high dimensions poses challenges beyond traditional sparse penalization methods. We propose the constrained partial penalized composite probit regression method for sparse estimation and investigate its statistical properties. Additionally, we present a computationally efficient algorithm using augmented Lagrangian and coordinate majorization descent for solving regularization problems with folded concave penalization and linear constraints. For testing linear hypotheses, we propose the partial penalized composite likelihood ratio test, score test and Wald test, and show that their limiting distributions under null and local alternatives follow generalized chi-squared distributions with the same degrees of freedom and noncentral parameter. Extensive simulation studies are conducted to examine the finite sample performance of the proposed tests. Our analysis of supermarket data illustrates potential discrepancies between our testing procedures and standard high-dimensional methods, highlighting the importance of our robustified approach.</summary></entry><entry><title type="html">A canonical polyadic tensor basis for fast Bayesian estimation of multi-subject fMRI activation patterns</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AcanonicalpolyadictensorbasisforfastBayesianestimationofmultisubjectfMRIactivationpatterns.html" rel="alternate" type="text/html" title="A canonical polyadic tensor basis for fast Bayesian estimation of multi-subject fMRI activation patterns" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AcanonicalpolyadictensorbasisforfastBayesianestimationofmultisubjectfMRIactivationpatterns</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AcanonicalpolyadictensorbasisforfastBayesianestimationofmultisubjectfMRIactivationpatterns.html">&lt;p&gt;Task-evoked functional magnetic resonance imaging studies, such as the Human Connectome Project (HCP), are a powerful tool for exploring how brain activity is influenced by cognitive tasks like memory retention, decision-making, and language processing. A fast Bayesian function-on-scalar model is proposed for estimating population-level activation maps linked to the working memory task. The model is based on the canonical polyadic (CP) tensor decomposition of coefficient maps obtained for each subject. This decomposition effectively yields a tensor basis capable of extracting both common features and subject-specific features from the coefficient maps. These subject-specific features, in turn, are modeled as a function of covariates of interest using a Bayesian model that accounts for the correlation of the CP-extracted features. The dimensionality reduction achieved with the tensor basis allows for a fast MCMC estimation of population-level activation maps. This model is applied to one hundred unrelated subjects from the HCP dataset, yielding significant insights into brain signatures associated with working memory.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12325&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michelle F. Miranda</name></author><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Task-evoked functional magnetic resonance imaging studies, such as the Human Connectome Project (HCP), are a powerful tool for exploring how brain activity is influenced by cognitive tasks like memory retention, decision-making, and language processing. A fast Bayesian function-on-scalar model is proposed for estimating population-level activation maps linked to the working memory task. The model is based on the canonical polyadic (CP) tensor decomposition of coefficient maps obtained for each subject. This decomposition effectively yields a tensor basis capable of extracting both common features and subject-specific features from the coefficient maps. These subject-specific features, in turn, are modeled as a function of covariates of interest using a Bayesian model that accounts for the correlation of the CP-extracted features. The dimensionality reduction achieved with the tensor basis allows for a fast MCMC estimation of population-level activation maps. This model is applied to one hundred unrelated subjects from the HCP dataset, yielding significant insights into brain signatures associated with working memory.</summary></entry><entry><title type="html">An Introduction on Solar Imaging Data Analytic Challenges and Opportunities for Statisticians</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AnIntroductiononSolarImagingDataAnalyticChallengesandOpportunitiesforStatisticians.html" rel="alternate" type="text/html" title="An Introduction on Solar Imaging Data Analytic Challenges and Opportunities for Statisticians" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AnIntroductiononSolarImagingDataAnalyticChallengesandOpportunitiesforStatisticians</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AnIntroductiononSolarImagingDataAnalyticChallengesandOpportunitiesforStatisticians.html">&lt;p&gt;We give a gentle introduction to solar imaging data, focusing on challenges and opportunities of data-driven approaches for solar eruptions. The various solar phenomena prediction problems that might benefit from statistical methods are presented. Available data and software will be described. State-of-art solar eruption forecasting with data driven approaches are summarized and discussed. Based on the characteristics of the datasets and state-of-art approaches, we point out several promising directions to explore from statistical modeling and computational perspectives.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12331&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yang Chen, Ward Manchester, Meng Jin, Alexei Pevtsov</name></author><category term="stat.AP" /><summary type="html">We give a gentle introduction to solar imaging data, focusing on challenges and opportunities of data-driven approaches for solar eruptions. The various solar phenomena prediction problems that might benefit from statistical methods are presented. Available data and software will be described. State-of-art solar eruption forecasting with data driven approaches are summarized and discussed. Based on the characteristics of the datasets and state-of-art approaches, we point out several promising directions to explore from statistical modeling and computational perspectives.</summary></entry><entry><title type="html">An analysis of factors impacting team strengths in the Australian Football League using time-variant Bradley-Terry models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AnanalysisoffactorsimpactingteamstrengthsintheAustralianFootballLeagueusingtimevariantBradleyTerrymodels.html" rel="alternate" type="text/html" title="An analysis of factors impacting team strengths in the Australian Football League using time-variant Bradley-Terry models" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AnanalysisoffactorsimpactingteamstrengthsintheAustralianFootballLeagueusingtimevariantBradleyTerrymodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AnanalysisoffactorsimpactingteamstrengthsintheAustralianFootballLeagueusingtimevariantBradleyTerrymodels.html">&lt;p&gt;Australian Rules Football is a field invasion game where two teams attempt to score the highest points to win. Complex machine learning algorithms have been developed to predict match outcomes post-game, but their lack of interpretability hampers an understanding of the factors that affect a team’s performance. Using data from the male competition of the Australian Football League, seasons 2015 to 2023, we estimate team strengths and the factors impacting them by fitting flexible Bradley-Terry models. We successfully identify teams significantly stronger or weaker than the average, with stronger teams placing higher in the previous seasons’ ladder and leading the activity in the Forward 50 zone, goal shots and scoring over their opponents. Playing at home is confirmed to create an advantage regardless of team strengths. The ability of the model to predict game results in advance is tested, with models accounting for team-specific, time-variant features predicting up to 71.5% of outcomes. Therefore, our approach can provide an interpretable understanding of team strengths and competitive game predictions, making it optimal for data-driven strategies and training.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12588&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carlos Rafael Gonzalez Soffner, Manuele Leonelli</name></author><category term="stat.AP" /><summary type="html">Australian Rules Football is a field invasion game where two teams attempt to score the highest points to win. Complex machine learning algorithms have been developed to predict match outcomes post-game, but their lack of interpretability hampers an understanding of the factors that affect a team’s performance. Using data from the male competition of the Australian Football League, seasons 2015 to 2023, we estimate team strengths and the factors impacting them by fitting flexible Bradley-Terry models. We successfully identify teams significantly stronger or weaker than the average, with stronger teams placing higher in the previous seasons’ ladder and leading the activity in the Forward 50 zone, goal shots and scoring over their opponents. Playing at home is confirmed to create an advantage regardless of team strengths. The ability of the model to predict game results in advance is tested, with models accounting for team-specific, time-variant features predicting up to 71.5% of outcomes. Therefore, our approach can provide an interpretable understanding of team strengths and competitive game predictions, making it optimal for data-driven strategies and training.</summary></entry><entry><title type="html">A spectral based goodness-of-fit test for stochastic block models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/Aspectralbasedgoodnessoffittestforstochasticblockmodels.html" rel="alternate" type="text/html" title="A spectral based goodness-of-fit test for stochastic block models" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/Aspectralbasedgoodnessoffittestforstochasticblockmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/Aspectralbasedgoodnessoffittestforstochasticblockmodels.html">&lt;p&gt;Community detection is a fundamental problem in complex network data analysis. Though many methods have been proposed, most existing methods require the number of communities to be the known parameter, which is not in practice. In this paper, we propose a novel goodness-of-fit test for the stochastic block model. The test statistic is based on the linear spectral of the adjacency matrix. Under the null hypothesis, we prove that the linear spectral statistic converges in distribution to $N(0,1)$. Some recent results in generalized Wigner matrices are used to prove the main theorems. Numerical experiments and real world data examples illustrate that our proposed linear spectral statistic has good performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.14508&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Qianyong Wu, Jiang Hu</name></author><category term="stat.ME" /><summary type="html">Community detection is a fundamental problem in complex network data analysis. Though many methods have been proposed, most existing methods require the number of communities to be the known parameter, which is not in practice. In this paper, we propose a novel goodness-of-fit test for the stochastic block model. The test statistic is based on the linear spectral of the adjacency matrix. Under the null hypothesis, we prove that the linear spectral statistic converges in distribution to $N(0,1)$. Some recent results in generalized Wigner matrices are used to prove the main theorems. Numerical experiments and real world data examples illustrate that our proposed linear spectral statistic has good performance.</summary></entry><entry><title type="html">Asymptotic Properties of Matthews Correlation Coefficient</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AsymptoticPropertiesofMatthewsCorrelationCoefficient.html" rel="alternate" type="text/html" title="Asymptotic Properties of Matthews Correlation Coefficient" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AsymptoticPropertiesofMatthewsCorrelationCoefficient</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/AsymptoticPropertiesofMatthewsCorrelationCoefficient.html">&lt;p&gt;Evaluating classifications is crucial in statistics and machine learning, as it influences decision-making across various fields, such as patient prognosis and therapy in critical conditions. The Matthews correlation coefficient (MCC) is recognized as a performance metric with high reliability, offering a balanced measurement even in the presence of class imbalances. Despite its importance, there remains a notable lack of comprehensive research on the statistical inference of MCC. This deficiency often leads to studies merely validating and comparing MCC point estimates, a practice that, while common, overlooks the statistical significance and reliability of results. Addressing this research gap, our paper introduces and evaluates several methods to construct asymptotic confidence intervals for the single MCC and the differences between MCCs in paired designs. Through simulations across various scenarios, we evaluate the finite-sample behavior of these methods and compare their performances. Furthermore, through real data analysis, we illustrate the potential utility of our findings in comparing binary classifiers, highlighting the possible contributions of our research in this field.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12622&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuki Itaya, Jun Tamura, Kenichi Hayashi, Kouji Yamamoto</name></author><category term="stat.ME" /><summary type="html">Evaluating classifications is crucial in statistics and machine learning, as it influences decision-making across various fields, such as patient prognosis and therapy in critical conditions. The Matthews correlation coefficient (MCC) is recognized as a performance metric with high reliability, offering a balanced measurement even in the presence of class imbalances. Despite its importance, there remains a notable lack of comprehensive research on the statistical inference of MCC. This deficiency often leads to studies merely validating and comparing MCC point estimates, a practice that, while common, overlooks the statistical significance and reliability of results. Addressing this research gap, our paper introduces and evaluates several methods to construct asymptotic confidence intervals for the single MCC and the differences between MCCs in paired designs. Through simulations across various scenarios, we evaluate the finite-sample behavior of these methods and compare their performances. Furthermore, through real data analysis, we illustrate the potential utility of our findings in comparing binary classifiers, highlighting the possible contributions of our research in this field.</summary></entry><entry><title type="html">Bayesian Geostatistics Using Predictive Stacking</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/BayesianGeostatisticsUsingPredictiveStacking.html" rel="alternate" type="text/html" title="Bayesian Geostatistics Using Predictive Stacking" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/BayesianGeostatisticsUsingPredictiveStacking</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/BayesianGeostatisticsUsingPredictiveStacking.html">&lt;p&gt;We develop Bayesian predictive stacking for geostatistical models, where the primary inferential objective is to provide inference on the latent spatial random field and conduct spatial predictions at arbitrary locations. We exploit analytically tractable posterior distributions for regression coefficients of predictors and the realizations of the spatial process conditional upon process parameters. We subsequently combine such inference by stacking these models across the range of values of the hyper-parameters. We devise stacking of means and posterior densities in a manner that is computationally efficient without resorting to iterative algorithms such as Markov chain Monte Carlo (MCMC) and can exploit the benefits of parallel computations. We offer novel theoretical insights into the resulting inference within an infill asymptotic paradigm and through empirical results showing that stacked inference is comparable to full sampling-based Bayesian inference at a significantly lower computational cost.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.12414&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lu Zhang, Wenpin Tang, Sudipto Banerjee</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">We develop Bayesian predictive stacking for geostatistical models, where the primary inferential objective is to provide inference on the latent spatial random field and conduct spatial predictions at arbitrary locations. We exploit analytically tractable posterior distributions for regression coefficients of predictors and the realizations of the spatial process conditional upon process parameters. We subsequently combine such inference by stacking these models across the range of values of the hyper-parameters. We devise stacking of means and posterior densities in a manner that is computationally efficient without resorting to iterative algorithms such as Markov chain Monte Carlo (MCMC) and can exploit the benefits of parallel computations. We offer novel theoretical insights into the resulting inference within an infill asymptotic paradigm and through empirical results showing that stacked inference is comparable to full sampling-based Bayesian inference at a significantly lower computational cost.</summary></entry><entry><title type="html">Change Point Detection for High-dimensional Linear Models: A General Tail-adaptive Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ChangePointDetectionforHighdimensionalLinearModelsAGeneralTailadaptiveApproach.html" rel="alternate" type="text/html" title="Change Point Detection for High-dimensional Linear Models: A General Tail-adaptive Approach" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ChangePointDetectionforHighdimensionalLinearModelsAGeneralTailadaptiveApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ChangePointDetectionforHighdimensionalLinearModelsAGeneralTailadaptiveApproach.html">&lt;p&gt;We propose a novel approach for detecting change points in high-dimensional linear regression models. Unlike previous research that relied on strict Gaussian/sub-Gaussian error assumptions and had prior knowledge of change points, we propose a tail-adaptive method for change point detection and estimation. We use a weighted combination of composite quantile and least squared losses to build a new loss function, allowing us to leverage information from both conditional means and quantiles. For change point testing, we develop a family of individual testing statistics with different weights to account for unknown tail structures. These individual tests are further aggregated to construct a powerful tail-adaptive test for sparse regression coefficient changes. For change point estimation, we propose a family of argmax-based individual estimators. We provide theoretical justifications for the validity of these tests and change point estimators. Additionally, we introduce a new algorithm for detecting multiple change points in a tail-adaptive manner using the wild binary segmentation. Extensive numerical results show the effectiveness of our method. Lastly, an R package called ``TailAdaptiveCpt” is developed to implement our algorithms.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2207.11532&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bin Liu, Zhengling Qi, Xinsheng Zhang, Yufeng Liu</name></author><category term="stat.ME" /><summary type="html">We propose a novel approach for detecting change points in high-dimensional linear regression models. Unlike previous research that relied on strict Gaussian/sub-Gaussian error assumptions and had prior knowledge of change points, we propose a tail-adaptive method for change point detection and estimation. We use a weighted combination of composite quantile and least squared losses to build a new loss function, allowing us to leverage information from both conditional means and quantiles. For change point testing, we develop a family of individual testing statistics with different weights to account for unknown tail structures. These individual tests are further aggregated to construct a powerful tail-adaptive test for sparse regression coefficient changes. For change point estimation, we propose a family of argmax-based individual estimators. We provide theoretical justifications for the validity of these tests and change point estimators. Additionally, we introduce a new algorithm for detecting multiple change points in a tail-adaptive manner using the wild binary segmentation. Extensive numerical results show the effectiveness of our method. Lastly, an R package called ``TailAdaptiveCpt” is developed to implement our algorithms.</summary></entry><entry><title type="html">Considerations for Single-Arm Trials to Support Accelerated Approval of Oncology Drugs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ConsiderationsforSingleArmTrialstoSupportAcceleratedApprovalofOncologyDrugs.html" rel="alternate" type="text/html" title="Considerations for Single-Arm Trials to Support Accelerated Approval of Oncology Drugs" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ConsiderationsforSingleArmTrialstoSupportAcceleratedApprovalofOncologyDrugs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ConsiderationsforSingleArmTrialstoSupportAcceleratedApprovalofOncologyDrugs.html">&lt;p&gt;In the last two decades, single-arm trials (SATs) have been effectively used to study anticancer therapies in well-defined patient populations using durable response rates as an objective and interpretable clinical endpoints. With a growing trend of regulatory accelerated approval (AA) requiring randomized controlled trials (RCTs), some confusions have arisen about the roles of SATs in AA. This paper is intended to elucidate conditions under which an SAT may be considered reasonable for AA. Specifically, the paper describes (1) two necessary conditions for designing an SAT, (2) three sufficient conditions that help either optimize the study design or interpret the study results, (3) four conditions that demonstrate substantial evidence of clinical benefits of the drug, and (4) a plan of a confirmatory RCT to verify the clinical benefits. Some further considerations are discussed to help design a scientifically sound SAT and communicate with regulatory agencies. Conditions presented in this paper may serve as a set of references for sponsors using SATs for regulatory decision.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12437&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Feinan Lu, Tao Wang, Ying Lu, Jie Chen</name></author><category term="stat.AP" /><summary type="html">In the last two decades, single-arm trials (SATs) have been effectively used to study anticancer therapies in well-defined patient populations using durable response rates as an objective and interpretable clinical endpoints. With a growing trend of regulatory accelerated approval (AA) requiring randomized controlled trials (RCTs), some confusions have arisen about the roles of SATs in AA. This paper is intended to elucidate conditions under which an SAT may be considered reasonable for AA. Specifically, the paper describes (1) two necessary conditions for designing an SAT, (2) three sufficient conditions that help either optimize the study design or interpret the study results, (3) four conditions that demonstrate substantial evidence of clinical benefits of the drug, and (4) a plan of a confirmatory RCT to verify the clinical benefits. Some further considerations are discussed to help design a scientifically sound SAT and communicate with regulatory agencies. Conditions presented in this paper may serve as a set of references for sponsors using SATs for regulatory decision.</summary></entry><entry><title type="html">Determine the Number of States in Hidden Markov Models via Marginal Likelihood</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/DeterminetheNumberofStatesinHiddenMarkovModelsviaMarginalLikelihood.html" rel="alternate" type="text/html" title="Determine the Number of States in Hidden Markov Models via Marginal Likelihood" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/DeterminetheNumberofStatesinHiddenMarkovModelsviaMarginalLikelihood</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/DeterminetheNumberofStatesinHiddenMarkovModelsviaMarginalLikelihood.html">&lt;p&gt;Hidden Markov models (HMM) have been widely used by scientists to model stochastic systems: the underlying process is a discrete Markov chain and the observations are noisy realizations of the underlying process. Determining the number of hidden states for an HMM is a model selection problem, which is yet to be satisfactorily solved, especially for the popular Gaussian HMM with heterogeneous covariance. In this paper, we propose a consistent method for determining the number of hidden states of HMM based on the marginal likelihood, which is obtained by integrating out both the parameters and hidden states. Moreover, we show that the model selection problem of HMM includes the order selection problem of finite mixture models as a special case. We give rigorous proof of the consistency of the proposed marginal likelihood method and provide an efficient computation method for practical implementation. We numerically compare the proposed method with the Bayesian information criterion (BIC), demonstrating the effectiveness of the proposed marginal likelihood method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12343&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yang Chen, Cheng-Der Fuh, Chu-Lan Michael Kao</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Hidden Markov models (HMM) have been widely used by scientists to model stochastic systems: the underlying process is a discrete Markov chain and the observations are noisy realizations of the underlying process. Determining the number of hidden states for an HMM is a model selection problem, which is yet to be satisfactorily solved, especially for the popular Gaussian HMM with heterogeneous covariance. In this paper, we propose a consistent method for determining the number of hidden states of HMM based on the marginal likelihood, which is obtained by integrating out both the parameters and hidden states. Moreover, we show that the model selection problem of HMM includes the order selection problem of finite mixture models as a special case. We give rigorous proof of the consistency of the proposed marginal likelihood method and provide an efficient computation method for practical implementation. We numerically compare the proposed method with the Bayesian information criterion (BIC), demonstrating the effectiveness of the proposed marginal likelihood method.</summary></entry><entry><title type="html">EKM: An exact, polynomial-time algorithm for the $K$-medoids problem</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/EKMAnexactpolynomialtimealgorithmfortheKmedoidsproblem.html" rel="alternate" type="text/html" title="EKM: An exact, polynomial-time algorithm for the $K$-medoids problem" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/EKMAnexactpolynomialtimealgorithmfortheKmedoidsproblem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/EKMAnexactpolynomialtimealgorithmfortheKmedoidsproblem.html">&lt;p&gt;The $K$-medoids problem is a challenging combinatorial clustering task, widely used in data analysis applications. While numerous algorithms have been proposed to solve this problem, none of these are able to obtain an exact (globally optimal) solution for the problem in polynomial time. In this paper, we present EKM: a novel algorithm for solving this problem exactly with worst-case $O\left(N^{K+1}\right)$ time complexity. EKM is developed according to recent advances in transformational programming and combinatorial generation, using formal program derivation steps. The derived algorithm is provably correct by construction. We demonstrate the effectiveness of our algorithm by comparing it against various approximate methods on numerous real-world datasets. We show that the wall-clock run time of our algorithm matches the worst-case time complexity analysis on synthetic datasets, clearly outperforming the exponential time complexity of benchmark branch-and-bound based MIP solvers. To our knowledge, this is the first, rigorously-proven polynomial time, practical algorithm for this ubiquitous problem.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12237&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xi He, Max A. Little</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">The $K$-medoids problem is a challenging combinatorial clustering task, widely used in data analysis applications. While numerous algorithms have been proposed to solve this problem, none of these are able to obtain an exact (globally optimal) solution for the problem in polynomial time. In this paper, we present EKM: a novel algorithm for solving this problem exactly with worst-case $O\left(N^{K+1}\right)$ time complexity. EKM is developed according to recent advances in transformational programming and combinatorial generation, using formal program derivation steps. The derived algorithm is provably correct by construction. We demonstrate the effectiveness of our algorithm by comparing it against various approximate methods on numerous real-world datasets. We show that the wall-clock run time of our algorithm matches the worst-case time complexity analysis on synthetic datasets, clearly outperforming the exponential time complexity of benchmark branch-and-bound based MIP solvers. To our knowledge, this is the first, rigorously-proven polynomial time, practical algorithm for this ubiquitous problem.</summary></entry><entry><title type="html">Efficient modeling of sub-kilometer surface wind with Gaussian processes and neural networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/EfficientmodelingofsubkilometersurfacewindwithGaussianprocessesandneuralnetworks.html" rel="alternate" type="text/html" title="Efficient modeling of sub-kilometer surface wind with Gaussian processes and neural networks" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/EfficientmodelingofsubkilometersurfacewindwithGaussianprocessesandneuralnetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/EfficientmodelingofsubkilometersurfacewindwithGaussianprocessesandneuralnetworks.html">&lt;p&gt;Accurately representing surface weather at the sub-kilometer scale is crucial for optimal decision-making in a wide range of applications. This motivates the use of statistical techniques to provide accurate and calibrated probabilistic predictions at a lower cost compared to numerical simulations. Wind represents a particularly challenging variable to model due to its high spatial and temporal variability. This paper presents a novel approach that integrates Gaussian processes (GPs) and neural networks to model surface wind gusts, leveraging multiple data sources, including numerical weather prediction (NWP) models, digital elevation models (DEM), and in-situ measurements. Results demonstrate the added value of modeling the multivariate covariance structure of the variable of interest, as opposed to only applying a univariate probabilistic regression approach. Modeling the covariance enables the optimal integration of observed measurements from ground stations, which is shown to reduce the continuous ranked probability score compared to the baseline. Moreover, it allows the direct generation of realistic fields that are also marginally calibrated, aided by scalable techniques such as Random Fourier Features (RFF) and pathwise conditioning. We discuss the effect of different modeling choices, as well as different degrees of approximation, and present our results for a case study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12614&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Francesco Zanetta, Daniele Nerini, Matteo Buzzi, Henry Moss</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Accurately representing surface weather at the sub-kilometer scale is crucial for optimal decision-making in a wide range of applications. This motivates the use of statistical techniques to provide accurate and calibrated probabilistic predictions at a lower cost compared to numerical simulations. Wind represents a particularly challenging variable to model due to its high spatial and temporal variability. This paper presents a novel approach that integrates Gaussian processes (GPs) and neural networks to model surface wind gusts, leveraging multiple data sources, including numerical weather prediction (NWP) models, digital elevation models (DEM), and in-situ measurements. Results demonstrate the added value of modeling the multivariate covariance structure of the variable of interest, as opposed to only applying a univariate probabilistic regression approach. Modeling the covariance enables the optimal integration of observed measurements from ground stations, which is shown to reduce the continuous ranked probability score compared to the baseline. Moreover, it allows the direct generation of realistic fields that are also marginally calibrated, aided by scalable techniques such as Random Fourier Features (RFF) and pathwise conditioning. We discuss the effect of different modeling choices, as well as different degrees of approximation, and present our results for a case study.</summary></entry><entry><title type="html">Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data: Insights from Item Response Theory</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/EstimatingHeterogeneousTreatmentEffectswithItemLevelOutcomeDataInsightsfromItemResponseTheory.html" rel="alternate" type="text/html" title="Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data: Insights from Item Response Theory" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/EstimatingHeterogeneousTreatmentEffectswithItemLevelOutcomeDataInsightsfromItemResponseTheory</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/EstimatingHeterogeneousTreatmentEffectswithItemLevelOutcomeDataInsightsfromItemResponseTheory.html">&lt;p&gt;Analyses of heterogeneous treatment effects (HTE) are common in applied causal inference research. However, when outcomes are latent variables assessed via psychometric instruments such as educational tests, standard methods ignore the potential HTE that may exist among the individual items of the outcome measure. Failing to account for “item-level” HTE (IL-HTE) can lead to both estimated standard errors that are too small and identification challenges in the estimation of treatment-by-covariate interaction effects. We demonstrate how Item Response Theory (IRT) models that estimate a treatment effect for each assessment item can both address these challenges and provide new insights into HTE generally. This study articulates the theoretical rationale for the IL-HTE model and demonstrates its practical value using data from 20 randomized controlled trials containing 2.3 million item responses in economics, education, and health research. Our results show that the IL-HTE model reveals item-level variation masked by average treatment effects, provides more accurate statistical inference, allows for estimates of the generalizability of causal effects, resolves identification problems in the estimation of interaction effects, and provides estimates of standardized treatment effect sizes corrected for attenuation due to measurement error.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00161&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joshua B. Gilbert, Zachary Himmelsbach, James Soland, Mridul Joshi, Benjamin W. Domingue</name></author><category term="stat.ME" /><summary type="html">Analyses of heterogeneous treatment effects (HTE) are common in applied causal inference research. However, when outcomes are latent variables assessed via psychometric instruments such as educational tests, standard methods ignore the potential HTE that may exist among the individual items of the outcome measure. Failing to account for “item-level” HTE (IL-HTE) can lead to both estimated standard errors that are too small and identification challenges in the estimation of treatment-by-covariate interaction effects. We demonstrate how Item Response Theory (IRT) models that estimate a treatment effect for each assessment item can both address these challenges and provide new insights into HTE generally. This study articulates the theoretical rationale for the IL-HTE model and demonstrates its practical value using data from 20 randomized controlled trials containing 2.3 million item responses in economics, education, and health research. Our results show that the IL-HTE model reveals item-level variation masked by average treatment effects, provides more accurate statistical inference, allows for estimates of the generalizability of causal effects, resolves identification problems in the estimation of interaction effects, and provides estimates of standardized treatment effect sizes corrected for attenuation due to measurement error.</summary></entry><entry><title type="html">Evaluating Binary Outcome Classifiers Estimated from Survey Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/EvaluatingBinaryOutcomeClassifiersEstimatedfromSurveyData.html" rel="alternate" type="text/html" title="Evaluating Binary Outcome Classifiers Estimated from Survey Data" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/EvaluatingBinaryOutcomeClassifiersEstimatedfromSurveyData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/EvaluatingBinaryOutcomeClassifiersEstimatedfromSurveyData.html">&lt;p&gt;Surveys are commonly used to facilitate research in epidemiology, health, and the social and behavioral sciences. Often, these surveys are not simple random samples, and respondents are given weights reflecting their probability of selection into the survey. It is well known that analysts can use these survey weights to produce unbiased estimates of population quantities like totals. In this article, we show that survey weights also can be beneficial for evaluating the quality of predictive models when splitting data into training and test sets. In particular, we characterize model assessment statistics, such as sensitivity and specificity, as finite population quantities, and compute survey-weighted estimates of these quantities with sample test data comprising a random subset of the original data.Using simulations with data from the National Survey on Drug Use and Health and the National Comorbidity Survey, we show that unweighted metrics estimated with sample test data can misrepresent population performance, but weighted metrics appropriately adjust for the complex sampling design. We also show that this conclusion holds for models trained using upsampling for mitigating class imbalance. The results suggest that weighted metrics should be used when evaluating performance on sample test data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.00596&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Adway S. Wadekar, Jerome P. Reiter</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Surveys are commonly used to facilitate research in epidemiology, health, and the social and behavioral sciences. Often, these surveys are not simple random samples, and respondents are given weights reflecting their probability of selection into the survey. It is well known that analysts can use these survey weights to produce unbiased estimates of population quantities like totals. In this article, we show that survey weights also can be beneficial for evaluating the quality of predictive models when splitting data into training and test sets. In particular, we characterize model assessment statistics, such as sensitivity and specificity, as finite population quantities, and compute survey-weighted estimates of these quantities with sample test data comprising a random subset of the original data.Using simulations with data from the National Survey on Drug Use and Health and the National Comorbidity Survey, we show that unweighted metrics estimated with sample test data can misrepresent population performance, but weighted metrics appropriately adjust for the complex sampling design. We also show that this conclusion holds for models trained using upsampling for mitigating class imbalance. The results suggest that weighted metrics should be used when evaluating performance on sample test data.</summary></entry><entry><title type="html">Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/FeatureAttributionwithNecessityandSufficiencyviaDualstagePerturbationTestforCausalExplanation.html" rel="alternate" type="text/html" title="Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/FeatureAttributionwithNecessityandSufficiencyviaDualstagePerturbationTestforCausalExplanation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/FeatureAttributionwithNecessityandSufficiencyviaDualstagePerturbationTestforCausalExplanation.html">&lt;p&gt;We investigate the problem of explainability in machine learning. To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations. However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation. In order to enhance the ability of FAMs to distinguish different features’ contributions in this challenging setting, we propose to utilize the Probability of Necessity and Sufficiency (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance. Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional). In practice, to generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution. We demonstrate that FANS outperforms existing attribution methods on six benchmarks. Our source code is available at \url{https://github.com/DMIRLAB-Group/FANS}.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.08845&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xuexin Chen, Ruichu Cai, Zhengting Huang, Yuxuan Zhu, Julien Horwood, Zhifeng Hao, Zijian Li, Jose Miguel Hernandez-Lobato</name></author><category term="stat.ME" /><summary type="html">We investigate the problem of explainability in machine learning. To address this problem, Feature Attribution Methods (FAMs) measure the contribution of each feature through a perturbation test, where the difference in prediction is compared under different perturbations. However, such perturbation tests may not accurately distinguish the contributions of different features, when their change in prediction is the same after perturbation. In order to enhance the ability of FAMs to distinguish different features’ contributions in this challenging setting, we propose to utilize the Probability of Necessity and Sufficiency (PNS) that perturbing a feature is a necessary and sufficient cause for the prediction to change as a measure of feature importance. Our approach, Feature Attribution with Necessity and Sufficiency (FANS), computes the PNS via a perturbation test involving two stages (factual and interventional). In practice, to generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution. We demonstrate that FANS outperforms existing attribution methods on six benchmarks. Our source code is available at \url{https://github.com/DMIRLAB-Group/FANS}.</summary></entry><entry><title type="html">Improving Ego-Cluster for Network Effect Measurement</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ImprovingEgoClusterforNetworkEffectMeasurement.html" rel="alternate" type="text/html" title="Improving Ego-Cluster for Network Effect Measurement" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ImprovingEgoClusterforNetworkEffectMeasurement</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ImprovingEgoClusterforNetworkEffectMeasurement.html">&lt;p&gt;The network effect, wherein one user’s activity impacts another user, is common in social network platforms. Many new features in social networks are specifically designed to create a network effect, enhancing user engagement. For instance, content creators tend to produce more when their articles and posts receive positive feedback from followers. This paper discusses a new cluster-level experimentation methodology for measuring creator-side metrics in the context of A/B experiments. The methodology is designed to address cases where the experiment randomization unit and the metric measurement unit differ. It is a crucial part of LinkedIn’s overall strategy to foster a robust creator community and ecosystem. The method is developed based on widely-cited research at LinkedIn but significantly improves the efficiency and flexibility of the clustering algorithm. This improvement results in a stronger capability for measuring creator-side metrics and an increased velocity for creator-related experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.05945&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wentao Su, Weitao Duan</name></author><category term="stat.ME" /><summary type="html">The network effect, wherein one user’s activity impacts another user, is common in social network platforms. Many new features in social networks are specifically designed to create a network effect, enhancing user engagement. For instance, content creators tend to produce more when their articles and posts receive positive feedback from followers. This paper discusses a new cluster-level experimentation methodology for measuring creator-side metrics in the context of A/B experiments. The methodology is designed to address cases where the experiment randomization unit and the metric measurement unit differ. It is a crucial part of LinkedIn’s overall strategy to foster a robust creator community and ecosystem. The method is developed based on widely-cited research at LinkedIn but significantly improves the efficiency and flexibility of the clustering algorithm. This improvement results in a stronger capability for measuring creator-side metrics and an increased velocity for creator-related experiments.</summary></entry><entry><title type="html">Leveraging text data for causal inference using electronic health records</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/Leveragingtextdataforcausalinferenceusingelectronichealthrecords.html" rel="alternate" type="text/html" title="Leveraging text data for causal inference using electronic health records" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/Leveragingtextdataforcausalinferenceusingelectronichealthrecords</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/Leveragingtextdataforcausalinferenceusingelectronichealthrecords.html">&lt;p&gt;In studies that rely on data from electronic health records (EHRs), unstructured text data such as clinical progress notes offer a rich source of information about patient characteristics and care that may be missing from structured data. Despite the prevalence of text in clinical research, these data are often ignored for the purposes of quantitative analysis due their complexity. This paper presents a unified framework for leveraging text data to support causal inference with electronic health data at multiple stages of analysis. In particular, we consider how natural language processing and statistical text analysis can be combined with standard inferential techniques to address common challenges due to missing data, confounding bias, and treatment effect heterogeneity. Through an application to a recent EHR study investigating the effects of a non-randomized medical intervention on patient outcomes, we show how incorporating text data in a traditional matching analysis can help strengthen the validity of an estimated treatment effect and identify patient subgroups that may benefit most from treatment. We believe these methods have the potential to expand the scope of secondary analysis of clinical data to domains where structured EHR data is limited, such as in developing countries. To this end, we provide code and open-source replication materials to encourage adoption and broader exploration of these techniques in clinical research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.03687&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Reagan Mozer, Aaron R. Kaufman, Leo A. Celi, Luke Miratrix</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">In studies that rely on data from electronic health records (EHRs), unstructured text data such as clinical progress notes offer a rich source of information about patient characteristics and care that may be missing from structured data. Despite the prevalence of text in clinical research, these data are often ignored for the purposes of quantitative analysis due their complexity. This paper presents a unified framework for leveraging text data to support causal inference with electronic health data at multiple stages of analysis. In particular, we consider how natural language processing and statistical text analysis can be combined with standard inferential techniques to address common challenges due to missing data, confounding bias, and treatment effect heterogeneity. Through an application to a recent EHR study investigating the effects of a non-randomized medical intervention on patient outcomes, we show how incorporating text data in a traditional matching analysis can help strengthen the validity of an estimated treatment effect and identify patient subgroups that may benefit most from treatment. We believe these methods have the potential to expand the scope of secondary analysis of clinical data to domains where structured EHR data is limited, such as in developing countries. To this end, we provide code and open-source replication materials to encourage adoption and broader exploration of these techniques in clinical research.</summary></entry><entry><title type="html">One-step data-driven generative model via Schrödinger Bridge</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/OnestepdatadrivengenerativemodelviaSchr%C3%B6dingerBridge.html" rel="alternate" type="text/html" title="One-step data-driven generative model via Schrödinger Bridge" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/OnestepdatadrivengenerativemodelviaSchr%C3%B6dingerBridge</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/OnestepdatadrivengenerativemodelviaSchr%C3%B6dingerBridge.html">&lt;p&gt;Generating samples from a probability distribution is a fundamental task in machine learning and statistics. This article proposes a novel scheme for sampling from a distribution for which the probability density $\mu({\bf x})$ for ${\bf x}\in{\mathbb{R}}^d$ is unknown, but finite independent samples are given. We focus on constructing a Schr&quot;odinger Bridge (SB) diffusion process on finite horizon $t\in[0,1]$ which induces a probability evolution starting from a fixed point at $t=0$ and ending with the desired target distribution $\mu({\bf x})$ at $t=1$. The diffusion process is characterized by a stochastic differential equation whose drift function can be solely estimated from data samples through a simple one-step procedure. Compared to the classical iterative schemes developed for the SB problem, the methodology of this article is quite simple, efficient, and computationally inexpensive as it does not require the training of neural network and thus circumvents many of the challenges in building the network architecture. The performance of our new generative model is evaluated through a series of numerical experiments on multi-modal low-dimensional simulated data and high-dimensional benchmark image data. Experimental results indicate that the synthetic samples generated from our SB Bridge based algorithm are comparable with the samples generated from the state-of-the-art methods in the field. Our formulation opens up new opportunities for developing efficient diffusion models that can be directly applied to large scale real-world data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12453&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hanwen Huang</name></author><category term="stat.CO" /><summary type="html">Generating samples from a probability distribution is a fundamental task in machine learning and statistics. This article proposes a novel scheme for sampling from a distribution for which the probability density $\mu({\bf x})$ for ${\bf x}\in{\mathbb{R}}^d$ is unknown, but finite independent samples are given. We focus on constructing a Schr&quot;odinger Bridge (SB) diffusion process on finite horizon $t\in[0,1]$ which induces a probability evolution starting from a fixed point at $t=0$ and ending with the desired target distribution $\mu({\bf x})$ at $t=1$. The diffusion process is characterized by a stochastic differential equation whose drift function can be solely estimated from data samples through a simple one-step procedure. Compared to the classical iterative schemes developed for the SB problem, the methodology of this article is quite simple, efficient, and computationally inexpensive as it does not require the training of neural network and thus circumvents many of the challenges in building the network architecture. The performance of our new generative model is evaluated through a series of numerical experiments on multi-modal low-dimensional simulated data and high-dimensional benchmark image data. Experimental results indicate that the synthetic samples generated from our SB Bridge based algorithm are comparable with the samples generated from the state-of-the-art methods in the field. Our formulation opens up new opportunities for developing efficient diffusion models that can be directly applied to large scale real-world data.</summary></entry><entry><title type="html">On the Injectivity of Euler Integral Transforms with Hyperplanes and Quadric Hypersurfaces</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/OntheInjectivityofEulerIntegralTransformswithHyperplanesandQuadricHypersurfaces.html" rel="alternate" type="text/html" title="On the Injectivity of Euler Integral Transforms with Hyperplanes and Quadric Hypersurfaces" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/OntheInjectivityofEulerIntegralTransformswithHyperplanesandQuadricHypersurfaces</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/OntheInjectivityofEulerIntegralTransformswithHyperplanesandQuadricHypersurfaces.html">&lt;p&gt;The Euler characteristic transform (ECT) is an integral transform used widely in topological data analysis. Previous efforts by Curry et al. and Ghrist et al. have independently shown that the ECT is injective on all compact definable sets. In this work, we first study the injectivity of the ECT on definable sets that are not necessarily compact and prove a complete classification of constructible functions that the Euler characteristic transform is not injective on. We then introduce the quadric Euler characteristic transform (QECT) as a natural generalization of the ECT by detecting definable shapes with quadric hypersurfaces rather than hyperplanes. We also discuss some criteria for the injectivity of QECT.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.10002&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mattie Ji</name></author><category term="stat.ME" /><summary type="html">The Euler characteristic transform (ECT) is an integral transform used widely in topological data analysis. Previous efforts by Curry et al. and Ghrist et al. have independently shown that the ECT is injective on all compact definable sets. In this work, we first study the injectivity of the ECT on definable sets that are not necessarily compact and prove a complete classification of constructible functions that the Euler characteristic transform is not injective on. We then introduce the quadric Euler characteristic transform (QECT) as a natural generalization of the ECT by detecting definable shapes with quadric hypersurfaces rather than hyperplanes. We also discuss some criteria for the injectivity of QECT.</summary></entry><entry><title type="html">Parameter estimation in Comparative Judgement</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ParameterestimationinComparativeJudgement.html" rel="alternate" type="text/html" title="Parameter estimation in Comparative Judgement" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ParameterestimationinComparativeJudgement</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ParameterestimationinComparativeJudgement.html">&lt;p&gt;Comparative Judgement is an assessment method where item ratings are estimated based on rankings of subsets of the items. These rankings are typically pairwise, with ratings taken to be the estimated parameters from fitting a Bradley-Terry model. Likelihood penalization is often employed. Adaptive scheduling of the comparisons can increase the efficiency of the assessment. We show that the most commonly used penalty is not the best-performing penalty under adaptive scheduling and can lead to substantial bias in parameter estimates. We demonstrate this using simulated and real data and provide a theoretical explanation for the relative performance of the penalties considered. Further, we propose a superior approach based on bootstrapping. It is shown to produce better parameter estimates for adaptive schedules and to be robust to variations in underlying strength distributions and initial penalization method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12694&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ian Hamilton, Nick Tawn</name></author><category term="stat.ME" /><summary type="html">Comparative Judgement is an assessment method where item ratings are estimated based on rankings of subsets of the items. These rankings are typically pairwise, with ratings taken to be the estimated parameters from fitting a Bradley-Terry model. Likelihood penalization is often employed. Adaptive scheduling of the comparisons can increase the efficiency of the assessment. We show that the most commonly used penalty is not the best-performing penalty under adaptive scheduling and can lead to substantial bias in parameter estimates. We demonstrate this using simulated and real data and provide a theoretical explanation for the relative performance of the penalties considered. Further, we propose a superior approach based on bootstrapping. It is shown to produce better parameter estimates for adaptive schedules and to be robust to variations in underlying strength distributions and initial penalization method.</summary></entry><entry><title type="html">Particle swarm optimization with Applications to Maximum Likelihood Estimation and Penalized Negative Binomial Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ParticleswarmoptimizationwithApplicationstoMaximumLikelihoodEstimationandPenalizedNegativeBinomialRegression.html" rel="alternate" type="text/html" title="Particle swarm optimization with Applications to Maximum Likelihood Estimation and Penalized Negative Binomial Regression" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ParticleswarmoptimizationwithApplicationstoMaximumLikelihoodEstimationandPenalizedNegativeBinomialRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ParticleswarmoptimizationwithApplicationstoMaximumLikelihoodEstimationandPenalizedNegativeBinomialRegression.html">&lt;p&gt;General purpose optimization routines such as nlminb, optim (R) or nlmixed (SAS) are frequently used to estimate model parameters in nonstandard distributions. This paper presents Particle Swarm Optimization (PSO), as an alternative to many of the current algorithms used in statistics. We find that PSO can not only reproduce the same results as the above routines, it can also produce results that are more optimal or when others cannot converge. In the latter case, it can also identify the source of the problem or problems. We highlight advantages of using PSO using four examples, where: (1) some parameters in a generalized distribution are unidentified using PSO when it is not apparent or computationally manifested using routines in R or SAS; (2) PSO can produce estimation results for the log-binomial regressions when current routines may not; (3) PSO provides flexibility in the link function for binomial regression with LASSO penalty, which is unsupported by standard packages like GLM and GENMOD in Stata and SAS, respectively, and (4) PSO provides superior MLE estimates for an EE-IW distribution compared with those from the traditional statistical methods that rely on moments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12386&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sisi Shao, Junhyung Park, Weng Kee Wong</name></author><category term="stat.ML," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">General purpose optimization routines such as nlminb, optim (R) or nlmixed (SAS) are frequently used to estimate model parameters in nonstandard distributions. This paper presents Particle Swarm Optimization (PSO), as an alternative to many of the current algorithms used in statistics. We find that PSO can not only reproduce the same results as the above routines, it can also produce results that are more optimal or when others cannot converge. In the latter case, it can also identify the source of the problem or problems. We highlight advantages of using PSO using four examples, where: (1) some parameters in a generalized distribution are unidentified using PSO when it is not apparent or computationally manifested using routines in R or SAS; (2) PSO can produce estimation results for the log-binomial regressions when current routines may not; (3) PSO provides flexibility in the link function for binomial regression with LASSO penalty, which is unsupported by standard packages like GLM and GENMOD in Stata and SAS, respectively, and (4) PSO provides superior MLE estimates for an EE-IW distribution compared with those from the traditional statistical methods that rely on moments.</summary></entry><entry><title type="html">Precision Mars Entry Navigation with Atmospheric Density Adaptation via Neural Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/PrecisionMarsEntryNavigationwithAtmosphericDensityAdaptationviaNeuralNetworks.html" rel="alternate" type="text/html" title="Precision Mars Entry Navigation with Atmospheric Density Adaptation via Neural Networks" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/PrecisionMarsEntryNavigationwithAtmosphericDensityAdaptationviaNeuralNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/PrecisionMarsEntryNavigationwithAtmosphericDensityAdaptationviaNeuralNetworks.html">&lt;p&gt;Spacecraft entering Mars require precise navigation algorithms capable of accurately estimating the vehicle’s position and velocity in dynamic and uncertain atmospheric environments. Discrepancies between the true Martian atmospheric density and the onboard density model can significantly impair the performance of spacecraft entry navigation filters. This work introduces a new approach to online filtering for Martian entry using a neural network to estimate atmospheric density and employing a consider analysis to account for the uncertainty in the estimate. The network is trained on an exponential atmospheric density model, and its parameters are dynamically adapted in real time to account for any mismatch between the true and estimated densities. The adaptation of the network is formulated as a maximum likelihood problem by leveraging the measurement innovations of the filter to identify optimal network parameters. Within the context of the maximum likelihood approach, incorporating a neural network enables the use of stochastic optimizers known for their efficiency in the machine learning domain. Performance comparisons are conducted against two online adaptive approaches, covariance matching and state augmentation and correction, in various realistic Martian entry navigation scenarios. The results show superior estimation accuracy compared to other approaches, and precise alignment of the estimated density with a broad selection of realistic Martian atmospheres sampled from perturbed Mars-GRAM data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.14411&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Felipe Giraldo-Grueso, Andrey A. Popov, Renato Zanetti</name></author><category term="stat.AP" /><summary type="html">Spacecraft entering Mars require precise navigation algorithms capable of accurately estimating the vehicle’s position and velocity in dynamic and uncertain atmospheric environments. Discrepancies between the true Martian atmospheric density and the onboard density model can significantly impair the performance of spacecraft entry navigation filters. This work introduces a new approach to online filtering for Martian entry using a neural network to estimate atmospheric density and employing a consider analysis to account for the uncertainty in the estimate. The network is trained on an exponential atmospheric density model, and its parameters are dynamically adapted in real time to account for any mismatch between the true and estimated densities. The adaptation of the network is formulated as a maximum likelihood problem by leveraging the measurement innovations of the filter to identify optimal network parameters. Within the context of the maximum likelihood approach, incorporating a neural network enables the use of stochastic optimizers known for their efficiency in the machine learning domain. Performance comparisons are conducted against two online adaptive approaches, covariance matching and state augmentation and correction, in various realistic Martian entry navigation scenarios. The results show superior estimation accuracy compared to other approaches, and precise alignment of the estimated density with a broad selection of realistic Martian atmospheres sampled from perturbed Mars-GRAM data.</summary></entry><entry><title type="html">Quantifying Uncertainty in Classification Performance: ROC Confidence Bands Using Conformal Prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/QuantifyingUncertaintyinClassificationPerformanceROCConfidenceBandsUsingConformalPrediction.html" rel="alternate" type="text/html" title="Quantifying Uncertainty in Classification Performance: ROC Confidence Bands Using Conformal Prediction" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/QuantifyingUncertaintyinClassificationPerformanceROCConfidenceBandsUsingConformalPrediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/QuantifyingUncertaintyinClassificationPerformanceROCConfidenceBandsUsingConformalPrediction.html">&lt;p&gt;To evaluate a classification algorithm, it is common practice to plot the ROC curve using test data. However, the inherent randomness in the test data can undermine our confidence in the conclusions drawn from the ROC curve, necessitating uncertainty quantification. In this article, we propose an algorithm to construct confidence bands for the ROC curve, quantifying the uncertainty of classification on the test data in terms of sensitivity and specificity. The algorithm is based on a procedure called conformal prediction, which constructs individualized confidence intervals for the test set and the confidence bands for the ROC curve can be obtained by combining the individualized intervals together. Furthermore, we address both scenarios where the test data are either iid or non-iid relative to the observed data set and propose distinct algorithms for each case with valid coverage probability. The proposed method is validated through both theoretical results and numerical experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12953&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zheshi Zheng, Bo Yang, Peter Song</name></author><category term="stat.ME" /><summary type="html">To evaluate a classification algorithm, it is common practice to plot the ROC curve using test data. However, the inherent randomness in the test data can undermine our confidence in the conclusions drawn from the ROC curve, necessitating uncertainty quantification. In this article, we propose an algorithm to construct confidence bands for the ROC curve, quantifying the uncertainty of classification on the test data in terms of sensitivity and specificity. The algorithm is based on a procedure called conformal prediction, which constructs individualized confidence intervals for the test set and the confidence bands for the ROC curve can be obtained by combining the individualized intervals together. Furthermore, we address both scenarios where the test data are either iid or non-iid relative to the observed data set and propose distinct algorithms for each case with valid coverage probability. The proposed method is validated through both theoretical results and numerical experiments.</summary></entry><entry><title type="html">Quantum Non-Identical Mean Estimation: Efficient Algorithms and Fundamental Limits</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/QuantumNonIdenticalMeanEstimationEfficientAlgorithmsandFundamentalLimits.html" rel="alternate" type="text/html" title="Quantum Non-Identical Mean Estimation: Efficient Algorithms and Fundamental Limits" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/QuantumNonIdenticalMeanEstimationEfficientAlgorithmsandFundamentalLimits</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/QuantumNonIdenticalMeanEstimationEfficientAlgorithmsandFundamentalLimits.html">&lt;p&gt;We systematically investigate quantum algorithms and lower bounds for mean estimation given query access to non-identically distributed samples. On the one hand, we give quantum mean estimators with quadratic quantum speed-up given samples from different bounded or sub-Gaussian random variables. On the other hand, we prove that, in general, it is impossible for any quantum algorithm to achieve quadratic speed-up over the number of classical samples needed to estimate the mean $\mu$, where the samples come from different random variables with mean close to $\mu$. Technically, our quantum algorithms reduce bounded and sub-Gaussian random variables to the Bernoulli case, and use an uncomputation trick to overcome the challenge that direct amplitude estimation does not work with non-identical query access. Our quantum query lower bounds are established by simulating non-identical oracles by parallel oracles, and also by an adversarial method with non-identical oracles. Both results pave the way for proving quantum query lower bounds with non-identical oracles in general, which may be of independent interest.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12838&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiachen Hu, Tongyang Li, Xinzhao Wang, Yecheng Xue, Chenyi Zhang, Han Zhong</name></author><category term="stat.CO" /><summary type="html">We systematically investigate quantum algorithms and lower bounds for mean estimation given query access to non-identically distributed samples. On the one hand, we give quantum mean estimators with quadratic quantum speed-up given samples from different bounded or sub-Gaussian random variables. On the other hand, we prove that, in general, it is impossible for any quantum algorithm to achieve quadratic speed-up over the number of classical samples needed to estimate the mean $\mu$, where the samples come from different random variables with mean close to $\mu$. Technically, our quantum algorithms reduce bounded and sub-Gaussian random variables to the Bernoulli case, and use an uncomputation trick to overcome the challenge that direct amplitude estimation does not work with non-identical query access. Our quantum query lower bounds are established by simulating non-identical oracles by parallel oracles, and also by an adversarial method with non-identical oracles. Both results pave the way for proving quantum query lower bounds with non-identical oracles in general, which may be of independent interest.</summary></entry><entry><title type="html">Robust Nonparametric Regression for Compositional Data: the Simplicial–Real case</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/RobustNonparametricRegressionforCompositionalDatatheSimplicialRealcase.html" rel="alternate" type="text/html" title="Robust Nonparametric Regression for Compositional Data: the Simplicial–Real case" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/RobustNonparametricRegressionforCompositionalDatatheSimplicialRealcase</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/RobustNonparametricRegressionforCompositionalDatatheSimplicialRealcase.html">&lt;p&gt;Statistical analysis on compositional data has gained a lot of attention due to their great potential of applications. A feature of these data is that they are multivariate vectors that lie in the simplex, that is, the components of each vector are positive and sum up a constant value. This fact poses a challenge to the analyst due to the internal dependency of the components which exhibit a spurious negative correlation. Since classical multivariate techniques are not appropriate in this scenario, it is necessary to endow the simplex of a suitable algebraic-geometrical structure, which is a starting point to develop adequate methodology and strategies to handle compositions. We centered our attention on regression problems with real responses and compositional covariates and we adopt a nonparametric approach due to the flexibility it provides. Aware of the potential damage that outliers may produce, we introduce a robust estimator in the framework of nonparametric regression for compositional data. The performance of the estimators is investigated by means of a numerical study where different contamination schemes are simulated. Through a real data analysis the advantages of using a robust procedure is illustrated.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12924&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ana M. Bianco, Graciela Boente, Wenceslao González--Manteiga, Francisco Gude Sampedro, Ana Pérez--González</name></author><category term="stat.ME" /><summary type="html">Statistical analysis on compositional data has gained a lot of attention due to their great potential of applications. A feature of these data is that they are multivariate vectors that lie in the simplex, that is, the components of each vector are positive and sum up a constant value. This fact poses a challenge to the analyst due to the internal dependency of the components which exhibit a spurious negative correlation. Since classical multivariate techniques are not appropriate in this scenario, it is necessary to endow the simplex of a suitable algebraic-geometrical structure, which is a starting point to develop adequate methodology and strategies to handle compositions. We centered our attention on regression problems with real responses and compositional covariates and we adopt a nonparametric approach due to the flexibility it provides. Aware of the potential damage that outliers may produce, we introduce a robust estimator in the framework of nonparametric regression for compositional data. The performance of the estimators is investigated by means of a numerical study where different contamination schemes are simulated. Through a real data analysis the advantages of using a robust procedure is illustrated.</summary></entry><entry><title type="html">Short and simple introduction to Bellman filtering and smoothing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ShortandsimpleintroductiontoBellmanfilteringandsmoothing.html" rel="alternate" type="text/html" title="Short and simple introduction to Bellman filtering and smoothing" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ShortandsimpleintroductiontoBellmanfilteringandsmoothing</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/ShortandsimpleintroductiontoBellmanfilteringandsmoothing.html">&lt;p&gt;Based on Bellman’s dynamic-programming principle, Lange (2024) presents an approximate method for filtering, smoothing and parameter estimation for possibly non-linear and/or non-Gaussian state-space models. While the approach applies more generally, this pedagogical note highlights the main results in the case where (i) the state transition remains linear and Gaussian while (ii) the observation density is log-concave and sufficiently smooth in the state variable. I demonstrate how Kalman’s (1960) filter and Rauch et al.’s (1965) smoother can be obtained as special cases within the proposed framework. The main aim is to present non-experts (and my own students) with an accessible introduction, enabling them to implement the proposed methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12668&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rutger-Jan Lange</name></author><category term="stat.ME" /><summary type="html">Based on Bellman’s dynamic-programming principle, Lange (2024) presents an approximate method for filtering, smoothing and parameter estimation for possibly non-linear and/or non-Gaussian state-space models. While the approach applies more generally, this pedagogical note highlights the main results in the case where (i) the state transition remains linear and Gaussian while (ii) the observation density is log-concave and sufficiently smooth in the state variable. I demonstrate how Kalman’s (1960) filter and Rauch et al.’s (1965) smoother can be obtained as special cases within the proposed framework. The main aim is to present non-experts (and my own students) with an accessible introduction, enabling them to implement the proposed methods.</summary></entry><entry><title type="html">Spatio-temporal modeling of co-dynamics of smallpox, measles and pertussis in pre-healthcare Finland</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/SpatiotemporalmodelingofcodynamicsofsmallpoxmeaslesandpertussisinprehealthcareFinland.html" rel="alternate" type="text/html" title="Spatio-temporal modeling of co-dynamics of smallpox, measles and pertussis in pre-healthcare Finland" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/SpatiotemporalmodelingofcodynamicsofsmallpoxmeaslesandpertussisinprehealthcareFinland</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/SpatiotemporalmodelingofcodynamicsofsmallpoxmeaslesandpertussisinprehealthcareFinland.html">&lt;p&gt;Infections are known to interact as previous infections may have an effect on risk of succumbing to a new infection. The co-dynamics can be mediated by immunosuppression or -modulation, shared environmental or climatic drivers, or competition for susceptible hosts. Research and statistical methods in epidemiology often concentrate on large pooled datasets, or high quality data from cities, leaving rural areas underrepresented in literature. Data considering rural populations are typically sparse and scarce, especially in the case of historical data sources, which may introduce considerable methodological challenges. In order to overcome many obstacles due to such data, we present a general Bayesian spatio-temporal model for disease co-dynamics. Applying the proposed model on historical (1820-1850) Finnish parish register data, we study the spread of infectious diseases in pre-healthcare Finland. We observe that measles, pertussis, and smallpox exhibit positively correlated dynamics, which could be attributed to immunosuppressive effects or, for example, the general weakening of the population due to recurring infections or poor nutritional conditions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.06538&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tiia-Maria Pasanen, Jouni Helske, Harri Högmander, Tarmo Ketola</name></author><category term="stat.AP" /><summary type="html">Infections are known to interact as previous infections may have an effect on risk of succumbing to a new infection. The co-dynamics can be mediated by immunosuppression or -modulation, shared environmental or climatic drivers, or competition for susceptible hosts. Research and statistical methods in epidemiology often concentrate on large pooled datasets, or high quality data from cities, leaving rural areas underrepresented in literature. Data considering rural populations are typically sparse and scarce, especially in the case of historical data sources, which may introduce considerable methodological challenges. In order to overcome many obstacles due to such data, we present a general Bayesian spatio-temporal model for disease co-dynamics. Applying the proposed model on historical (1820-1850) Finnish parish register data, we study the spread of infectious diseases in pre-healthcare Finland. We observe that measles, pertussis, and smallpox exhibit positively correlated dynamics, which could be attributed to immunosuppressive effects or, for example, the general weakening of the population due to recurring infections or poor nutritional conditions.</summary></entry><entry><title type="html">Spectral analysis for noisy Hawkes processes inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/SpectralanalysisfornoisyHawkesprocessesinference.html" rel="alternate" type="text/html" title="Spectral analysis for noisy Hawkes processes inference" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/SpectralanalysisfornoisyHawkesprocessesinference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/SpectralanalysisfornoisyHawkesprocessesinference.html">&lt;p&gt;Classic estimation methods for Hawkes processes rely on the assumption that observed event times are indeed a realisation of a Hawkes process, without considering any potential perturbation of the model. However, in practice, observations are often altered by some noise, the form of which depends on the context.It is then required to model the alteration mechanism in order to infer accurately such a noisy Hawkes process. While several models exist, we consider, in this work, the observations to be the indistinguishable union of event times coming from a Hawkes process and from an independent Poisson process. Since standard inference methods (such as maximum likelihood or Expectation-Maximisation) are either unworkable or numerically prohibitive in this context, we propose an estimation procedure based on the spectral analysis of second order properties of the noisy Hawkes process. Novel results include sufficient conditions for identifiability of the ensuing statistical model with exponential interaction functions for both univariate and bivariate processes. Although we mainly focus on the exponential scenario, other types of kernels are investigated and discussed. A new estimator based on maximising the spectral log-likelihood is then described, and its behaviour is numerically illustrated on synthetic data. Besides being free from knowing the source of each observed time (Hawkes or Poisson process), the proposed estimator is shown to perform accurately in estimating both processes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12581&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anna Bonnet , Felix Cheysson , Miguel Martinez Herrera , Maxime Sangnier</name></author><category term="stat.ME" /><summary type="html">Classic estimation methods for Hawkes processes rely on the assumption that observed event times are indeed a realisation of a Hawkes process, without considering any potential perturbation of the model. However, in practice, observations are often altered by some noise, the form of which depends on the context.It is then required to model the alteration mechanism in order to infer accurately such a noisy Hawkes process. While several models exist, we consider, in this work, the observations to be the indistinguishable union of event times coming from a Hawkes process and from an independent Poisson process. Since standard inference methods (such as maximum likelihood or Expectation-Maximisation) are either unworkable or numerically prohibitive in this context, we propose an estimation procedure based on the spectral analysis of second order properties of the noisy Hawkes process. Novel results include sufficient conditions for identifiability of the ensuing statistical model with exponential interaction functions for both univariate and bivariate processes. Although we mainly focus on the exponential scenario, other types of kernels are investigated and discussed. A new estimator based on maximising the spectral log-likelihood is then described, and its behaviour is numerically illustrated on synthetic data. Besides being free from knowing the source of each observed time (Hawkes or Poisson process), the proposed estimator is shown to perform accurately in estimating both processes.</summary></entry><entry><title type="html">TimeGPT-1</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/TimeGPT1.html" rel="alternate" type="text/html" title="TimeGPT-1" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/TimeGPT1</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/TimeGPT1.html">&lt;p&gt;In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.03589&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Azul Garza, Cristian Challu, Max Mergenthaler-Canseco</name></author><category term="stat.AP" /><summary type="html">In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.</summary></entry><entry><title type="html">Uncertainty quantification by block bootstrap for differentially private stochastic gradient descent</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/Uncertaintyquantificationbyblockbootstrapfordifferentiallyprivatestochasticgradientdescent.html" rel="alternate" type="text/html" title="Uncertainty quantification by block bootstrap for differentially private stochastic gradient descent" /><published>2024-05-22T00:00:00+00:00</published><updated>2024-05-22T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/Uncertaintyquantificationbyblockbootstrapfordifferentiallyprivatestochasticgradientdescent</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/Uncertaintyquantificationbyblockbootstrapfordifferentiallyprivatestochasticgradientdescent.html">&lt;p&gt;Stochastic Gradient Descent (SGD) is a widely used tool in machine learning. In the context of Differential Privacy (DP), SGD has been well studied in the last years in which the focus is mainly on convergence rates and privacy guarantees. While in the non private case, uncertainty quantification (UQ) for SGD by bootstrap has been addressed by several authors, these procedures cannot be transferred to differential privacy due to multiple queries to the private data. In this paper, we propose a novel block bootstrap for SGD under local differential privacy that is computationally tractable and does not require an adjustment of the privacy budget. The method can be easily implemented and is applicable to a broad class of estimation problems. We prove the validity of our approach and illustrate its finite sample properties by means of a simulation study. As a by-product, the new method also provides a simple alternative numerical tool for UQ for non-private SGD.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.12553&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Holger Dette, Carina Graw</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Stochastic Gradient Descent (SGD) is a widely used tool in machine learning. In the context of Differential Privacy (DP), SGD has been well studied in the last years in which the focus is mainly on convergence rates and privacy guarantees. While in the non private case, uncertainty quantification (UQ) for SGD by bootstrap has been addressed by several authors, these procedures cannot be transferred to differential privacy due to multiple queries to the private data. In this paper, we propose a novel block bootstrap for SGD under local differential privacy that is computationally tractable and does not require an adjustment of the privacy budget. The method can be easily implemented and is applicable to a broad class of estimation problems. We prove the validity of our approach and illustrate its finite sample properties by means of a simulation study. As a by-product, the new method also provides a simple alternative numerical tool for UQ for non-private SGD.</summary></entry></feed>