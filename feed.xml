<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-09T07:15:35+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Bayes Factor Framework for Unified Parameter Estimation and Hypothesis Testing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/ABayesFactorFrameworkforUnifiedParameterEstimationandHypothesisTesting.html" rel="alternate" type="text/html" title="A Bayes Factor Framework for Unified Parameter Estimation and Hypothesis Testing" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/ABayesFactorFrameworkforUnifiedParameterEstimationandHypothesisTesting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/ABayesFactorFrameworkforUnifiedParameterEstimationandHypothesisTesting.html">&lt;p&gt;The Bayes factor, the data-based updating factor of the prior to posterior odds of two hypotheses, is a natural measure of statistical evidence for one hypothesis over the other. We show how Bayes factors can also be used for parameter estimation. The key idea is to consider the Bayes factor as a function of the parameter value under the null hypothesis. This ‘Bayes factor function’ is inverted to obtain point estimates (‘maximum evidence estimates’) and interval estimates (‘support intervals’), similar to how P-value functions are inverted to obtain point estimates and confidence intervals. This provides data analysts with a unified inference framework as Bayes factors (for any tested parameter value), support intervals (at any level), and point estimates can be easily read off from a plot of the Bayes factor function. This approach shares similarities but is also distinct from conventional Bayesian and frequentist approaches: It uses the Bayesian evidence calculus, but without synthesizing data and prior, and it defines statistical evidence in terms of (integrated) likelihood ratios, but also includes a natural way for dealing with nuisance parameters. Applications to real-world examples illustrate how our framework is of practical value for making quantitative inferences.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.09350&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Samuel Pawel</name></author><category term="stat.ME" /><summary type="html">The Bayes factor, the data-based updating factor of the prior to posterior odds of two hypotheses, is a natural measure of statistical evidence for one hypothesis over the other. We show how Bayes factors can also be used for parameter estimation. The key idea is to consider the Bayes factor as a function of the parameter value under the null hypothesis. This ‘Bayes factor function’ is inverted to obtain point estimates (‘maximum evidence estimates’) and interval estimates (‘support intervals’), similar to how P-value functions are inverted to obtain point estimates and confidence intervals. This provides data analysts with a unified inference framework as Bayes factors (for any tested parameter value), support intervals (at any level), and point estimates can be easily read off from a plot of the Bayes factor function. This approach shares similarities but is also distinct from conventional Bayesian and frequentist approaches: It uses the Bayesian evidence calculus, but without synthesizing data and prior, and it defines statistical evidence in terms of (integrated) likelihood ratios, but also includes a natural way for dealing with nuisance parameters. Applications to real-world examples illustrate how our framework is of practical value for making quantitative inferences.</summary></entry><entry><title type="html">Adaptive design of experiments methodology for noise resistance with unreplicated experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Adaptivedesignofexperimentsmethodologyfornoiseresistancewithunreplicatedexperiments.html" rel="alternate" type="text/html" title="Adaptive design of experiments methodology for noise resistance with unreplicated experiments" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Adaptivedesignofexperimentsmethodologyfornoiseresistancewithunreplicatedexperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Adaptivedesignofexperimentsmethodologyfornoiseresistancewithunreplicatedexperiments.html">&lt;p&gt;A new gradient-based adaptive sampling method is proposed for design of experiments applications which balances space filling, local refinement, and error minimization objectives while reducing reliance on delicate tuning parameters. High order local maximum entropy approximants are used for metamodelling, which take advantage of boundary-corrected kernel density estimation to increase accuracy and robustness on highly clumped datasets, as well as conferring the resulting metamodel with some robustness against data noise in the common case of unreplicated experiments. Two-dimensional test cases are analyzed against full factorial and latin hypercube designs and compare favourably. The proposed method is then applied in a unique manner to the problem of adaptive spatial resolution in time-varying non-linear functions, opening up the possibility to adapt the method to solve partial differential equations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04624&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lucas Caparini, Gwynn J. Elfring, Mauricio Ponga</name></author><category term="stat.ME" /><summary type="html">A new gradient-based adaptive sampling method is proposed for design of experiments applications which balances space filling, local refinement, and error minimization objectives while reducing reliance on delicate tuning parameters. High order local maximum entropy approximants are used for metamodelling, which take advantage of boundary-corrected kernel density estimation to increase accuracy and robustness on highly clumped datasets, as well as conferring the resulting metamodel with some robustness against data noise in the common case of unreplicated experiments. Two-dimensional test cases are analyzed against full factorial and latin hypercube designs and compare favourably. The proposed method is then applied in a unique manner to the problem of adaptive spatial resolution in time-varying non-linear functions, opening up the possibility to adapt the method to solve partial differential equations.</summary></entry><entry><title type="html">A goodness-of-fit diagnostic for count data derived from half-normal plots with a simulated envelope</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Agoodnessoffitdiagnosticforcountdataderivedfromhalfnormalplotswithasimulatedenvelope.html" rel="alternate" type="text/html" title="A goodness-of-fit diagnostic for count data derived from half-normal plots with a simulated envelope" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Agoodnessoffitdiagnosticforcountdataderivedfromhalfnormalplotswithasimulatedenvelope</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Agoodnessoffitdiagnosticforcountdataderivedfromhalfnormalplotswithasimulatedenvelope.html">&lt;p&gt;Traditional methods of model diagnostics may include a plethora of graphical techniques based on residual analysis, as well as formal tests (e.g. Shapiro-Wilk test for normality and Bartlett test for homogeneity of variance). In this paper we derive a new distance metric based on the half-normal plot with a simulation envelope, a graphical model evaluation method, and investigate its properties through simulation studies. The proposed metric can help to assess the fit of a given model, and also act as a model selection criterion by being comparable across models, whether based or not on a true likelihood. More specifically, it quantitatively encompasses the model evaluation principles and removes the subjective bias when closely related models are involved. We validate the technique by means of an extensive simulation study carried out using count data, and illustrate with two case studies in ecology and fisheries research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05121&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Darshana Jayakumari, Jochen Einbeck, John Hinde, Julien Mainguy, Rafael de Andrade Moral</name></author><category term="stat.ME" /><summary type="html">Traditional methods of model diagnostics may include a plethora of graphical techniques based on residual analysis, as well as formal tests (e.g. Shapiro-Wilk test for normality and Bartlett test for homogeneity of variance). In this paper we derive a new distance metric based on the half-normal plot with a simulation envelope, a graphical model evaluation method, and investigate its properties through simulation studies. The proposed metric can help to assess the fit of a given model, and also act as a model selection criterion by being comparable across models, whether based or not on a true likelihood. More specifically, it quantitatively encompasses the model evaluation principles and removes the subjective bias when closely related models are involved. We validate the technique by means of an extensive simulation study carried out using count data, and illustrate with two case studies in ecology and fisheries research.</summary></entry><entry><title type="html">A joint model for DHS and MICS surveys: Spatial modeling with anonymized locations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/AjointmodelforDHSandMICSsurveysSpatialmodelingwithanonymizedlocations.html" rel="alternate" type="text/html" title="A joint model for DHS and MICS surveys: Spatial modeling with anonymized locations" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/AjointmodelforDHSandMICSsurveysSpatialmodelingwithanonymizedlocations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/AjointmodelforDHSandMICSsurveysSpatialmodelingwithanonymizedlocations.html">&lt;p&gt;Anonymizing the GPS locations of observations can bias a spatial model’s parameter estimates and attenuate spatial predictions when improperly accounted for, and is relevant in applications from public health to paleoseismology. In this work, we demonstrate that a newly introduced method for geostatistical modeling in the presence of anonymized point locations can be extended to account for more general kinds of positional uncertainty due to location anonymization, including both jittering (a form of random perturbations of GPS coordinates) and geomasking (reporting only the name of the area containing the true GPS coordinates). We further provide a numerical integration scheme that flexibly accounts for the positional uncertainty as well as spatial and covariate information.
  We apply the method to women’s secondary education completion data in the 2018 Nigeria demographic and health survey (NDHS) containing jittered point locations, and the 2016 Nigeria multiple indicator cluster survey (NMICS) containing geomasked locations. We show that accounting for the positional uncertainty in the surveys can improve predictions in terms of their continuous rank probability score.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04928&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>John Paige, Geir-Arne Fuglstad, Andrea Riebler</name></author><category term="stat.ME" /><summary type="html">Anonymizing the GPS locations of observations can bias a spatial model’s parameter estimates and attenuate spatial predictions when improperly accounted for, and is relevant in applications from public health to paleoseismology. In this work, we demonstrate that a newly introduced method for geostatistical modeling in the presence of anonymized point locations can be extended to account for more general kinds of positional uncertainty due to location anonymization, including both jittering (a form of random perturbations of GPS coordinates) and geomasking (reporting only the name of the area containing the true GPS coordinates). We further provide a numerical integration scheme that flexibly accounts for the positional uncertainty as well as spatial and covariate information. We apply the method to women’s secondary education completion data in the 2018 Nigeria demographic and health survey (NDHS) containing jittered point locations, and the 2016 Nigeria multiple indicator cluster survey (NMICS) containing geomasked locations. We show that accounting for the positional uncertainty in the surveys can improve predictions in terms of their continuous rank probability score.</summary></entry><entry><title type="html">Are Economically Advanced Countries More Efficient in Basic and Applied Research?</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/AreEconomicallyAdvancedCountriesMoreEfficientinBasicandAppliedResearch.html" rel="alternate" type="text/html" title="Are Economically Advanced Countries More Efficient in Basic and Applied Research?" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/AreEconomicallyAdvancedCountriesMoreEfficientinBasicandAppliedResearch</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/AreEconomicallyAdvancedCountriesMoreEfficientinBasicandAppliedResearch.html">&lt;p&gt;Research and development (R&amp;amp;D) of countries play a major role in a long-term development of the economy. We measure the R&amp;amp;D efficiency of all 28 member countries of the European Union in the years 2008–2014. Super-efficient data envelopment analysis (DEA) based on robustness of classification into efficient and inefficient units is adopted. We use the number of citations as output of basic research, the number of patents as output of applied research and R&amp;amp;D expenditures with manpower as inputs. To meet DEA assumptions and to capture R&amp;amp;D characteristics, we analyze a homogeneous sample of countries, adjust prices using purchasing power parity and consider time lag between inputs and outputs. We find that the efficiency of general R&amp;amp;D is higher for countries with higher GDP per capita. This relation also holds for specialized efficiencies of basic and applied research. However, it is much stronger for applied research suggesting its outputs are more easily distinguished and captured. Our findings are important in the evaluation of research and policy making.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05227&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vladimír Holý, Karel Šafr</name></author><category term="stat.AP" /><summary type="html">Research and development (R&amp;amp;D) of countries play a major role in a long-term development of the economy. We measure the R&amp;amp;D efficiency of all 28 member countries of the European Union in the years 2008–2014. Super-efficient data envelopment analysis (DEA) based on robustness of classification into efficient and inefficient units is adopted. We use the number of citations as output of basic research, the number of patents as output of applied research and R&amp;amp;D expenditures with manpower as inputs. To meet DEA assumptions and to capture R&amp;amp;D characteristics, we analyze a homogeneous sample of countries, adjust prices using purchasing power parity and consider time lag between inputs and outputs. We find that the efficiency of general R&amp;amp;D is higher for countries with higher GDP per capita. This relation also holds for specialized efficiencies of basic and applied research. However, it is much stronger for applied research suggesting its outputs are more easily distinguished and captured. Our findings are important in the evaluation of research and policy making.</summary></entry><entry><title type="html">Bayesian Analysis on Limiting the Student-$t$ Linear Regression Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianAnalysisonLimitingtheStudenttLinearRegressionModel.html" rel="alternate" type="text/html" title="Bayesian Analysis on Limiting the Student-$t$ Linear Regression Model" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianAnalysisonLimitingtheStudenttLinearRegressionModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianAnalysisonLimitingtheStudenttLinearRegressionModel.html">&lt;p&gt;For the outlier problem in linear regression models, the Student-$t$ linear regression model is one of the common methods for robust modeling and is widely adopted in the literature. However, most of them applies it without careful theoretical consideration. This study provides the practically useful and quite simple conditions to ensure that the Student-$t$ linear regression model is robust against an outlier in the $y$-direction using regular variation theory.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2008.04522&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yoshiko Hayashi</name></author><category term="stat.ME" /><summary type="html">For the outlier problem in linear regression models, the Student-$t$ linear regression model is one of the common methods for robust modeling and is widely adopted in the literature. However, most of them applies it without careful theoretical consideration. This study provides the practically useful and quite simple conditions to ensure that the Student-$t$ linear regression model is robust against an outlier in the $y$-direction using regular variation theory.</summary></entry><entry><title type="html">Bayesian Causal Synthesis for Meta-Inference on Heterogeneous Treatment Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianCausalSynthesisforMetaInferenceonHeterogeneousTreatmentEffects.html" rel="alternate" type="text/html" title="Bayesian Causal Synthesis for Meta-Inference on Heterogeneous Treatment Effects" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianCausalSynthesisforMetaInferenceonHeterogeneousTreatmentEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianCausalSynthesisforMetaInferenceonHeterogeneousTreatmentEffects.html">&lt;p&gt;The estimation of heterogeneous treatment effects in the potential outcome setting is biased when there exists model misspecification or unobserved confounding. As these biases are unobservable, what model to use when remains a critical open question. In this paper, we propose a novel Bayesian methodology to mitigate misspecification and improve estimation via a synthesis of multiple causal estimates, which we call Bayesian causal synthesis. Our development is built upon identifying a synthesis function that correctly specifies the heterogeneous treatment effect under no unobserved confounding, and achieves the irreducible bias under unobserved confounding. We show that our proposed method results in consistent estimates of the heterogeneous treatment effect; either with no bias or with irreducible bias. We provide a computational algorithm for fast posterior sampling. Several benchmark simulations and an empirical study highlight the efficacy of the proposed approach compared to existing methodologies, providing improved point and density estimation of the heterogeneous treatment effect, even under unobserved confounding.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.07726&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shonosuke Sugasawa, Kosaku Takanashi, Kenichiro McAlinn, Edoardo M. Airoldi</name></author><category term="stat.ME" /><summary type="html">The estimation of heterogeneous treatment effects in the potential outcome setting is biased when there exists model misspecification or unobserved confounding. As these biases are unobservable, what model to use when remains a critical open question. In this paper, we propose a novel Bayesian methodology to mitigate misspecification and improve estimation via a synthesis of multiple causal estimates, which we call Bayesian causal synthesis. Our development is built upon identifying a synthesis function that correctly specifies the heterogeneous treatment effect under no unobserved confounding, and achieves the irreducible bias under unobserved confounding. We show that our proposed method results in consistent estimates of the heterogeneous treatment effect; either with no bias or with irreducible bias. We provide a computational algorithm for fast posterior sampling. Several benchmark simulations and an empirical study highlight the efficacy of the proposed approach compared to existing methodologies, providing improved point and density estimation of the heterogeneous treatment effect, even under unobserved confounding.</summary></entry><entry><title type="html">Bayesian Estimation of Propensity Scores for Integrating Multiple Cohorts with High-Dimensional Covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianEstimationofPropensityScoresforIntegratingMultipleCohortswithHighDimensionalCovariates.html" rel="alternate" type="text/html" title="Bayesian Estimation of Propensity Scores for Integrating Multiple Cohorts with High-Dimensional Covariates" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianEstimationofPropensityScoresforIntegratingMultipleCohortswithHighDimensionalCovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianEstimationofPropensityScoresforIntegratingMultipleCohortswithHighDimensionalCovariates.html">&lt;p&gt;Comparative meta-analyses of groups of subjects by integrating multiple observational studies rely on estimated propensity scores (PSs) to mitigate covariate imbalances. However, PS estimation grapples with the theoretical and practical challenges posed by high-dimensional covariates. Motivated by an integrative analysis of breast cancer patients across seven medical centers, this paper tackles the challenges associated with integrating multiple observational datasets. The proposed inferential technique, called Bayesian Motif Submatrices for Covariates (B-MSC), addresses the curse of dimensionality by a hybrid of Bayesian and frequentist approaches. B-MSC uses nonparametric Bayesian “Chinese restaurant” processes to eliminate redundancy in the high-dimensional covariates and discover latent motifs or lower-dimensional structure. With these motifs as potential predictors, standard regression techniques can be utilized to accurately infer the PSs and facilitate covariate-balanced group comparisons. Simulations and meta-analysis of the motivating cancer investigation demonstrate the efficacy of the B-MSC approach to accurately estimate the propensity scores and efficiently address covariate imbalance when integrating observational health studies with high-dimensional covariates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.07873&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Subharup Guha, Yi Li</name></author><category term="stat.ME" /><summary type="html">Comparative meta-analyses of groups of subjects by integrating multiple observational studies rely on estimated propensity scores (PSs) to mitigate covariate imbalances. However, PS estimation grapples with the theoretical and practical challenges posed by high-dimensional covariates. Motivated by an integrative analysis of breast cancer patients across seven medical centers, this paper tackles the challenges associated with integrating multiple observational datasets. The proposed inferential technique, called Bayesian Motif Submatrices for Covariates (B-MSC), addresses the curse of dimensionality by a hybrid of Bayesian and frequentist approaches. B-MSC uses nonparametric Bayesian “Chinese restaurant” processes to eliminate redundancy in the high-dimensional covariates and discover latent motifs or lower-dimensional structure. With these motifs as potential predictors, standard regression techniques can be utilized to accurately infer the PSs and facilitate covariate-balanced group comparisons. Simulations and meta-analysis of the motivating cancer investigation demonstrate the efficacy of the B-MSC approach to accurately estimate the propensity scores and efficiently address covariate imbalance when integrating observational health studies with high-dimensional covariates.</summary></entry><entry><title type="html">Bayesian Multilevel Compositional Data Analysis: Introduction, Evaluation, and Application</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianMultilevelCompositionalDataAnalysisIntroductionEvaluationandApplication.html" rel="alternate" type="text/html" title="Bayesian Multilevel Compositional Data Analysis: Introduction, Evaluation, and Application" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianMultilevelCompositionalDataAnalysisIntroductionEvaluationandApplication</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianMultilevelCompositionalDataAnalysisIntroductionEvaluationandApplication.html">&lt;p&gt;Multilevel compositional data commonly occur in various fields, particularly in intensive, longitudinal studies using ecological momentary assessments. Examples include data repeatedly measured over time that are non-negative and sum to a constant value, such as sleep-wake movement behaviours in a 24-hour day. This article presents a novel methodology for analysing multilevel compositional data using a Bayesian inference approach. This method can be used to investigate how reallocation of time between sleep-wake movement behaviours may be associated with other phenomena (e.g., emotions, cognitions) at a daily level. We explain the theoretical details of the data and the models, and outline the steps necessary to implement this method. We introduce the R package multilevelcoda to facilitate the application of this method and illustrate using a real data example. An extensive parameter recovery simulation study verified the robust performance of the method. Across all simulation conditions investigated in the simulation study, the model had minimal convergence issues (convergence rate &amp;gt; 99%) and achieved excellent quality of parameter estimates and inference, with an average bias of 0.00 (range -0.09, 0.05) and coverage of 0.95 (range 0.93, 0.97). We conclude the article with recommendations on the use of the Bayesian compositional multilevel modelling approach, and hope to promote wider application of this method to answer robust questions using the increasingly available data from intensive, longitudinal studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03985&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Flora Le, Tyman E. Stanford, Dorothea Dumuid, Joshua F. Wiley</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Multilevel compositional data commonly occur in various fields, particularly in intensive, longitudinal studies using ecological momentary assessments. Examples include data repeatedly measured over time that are non-negative and sum to a constant value, such as sleep-wake movement behaviours in a 24-hour day. This article presents a novel methodology for analysing multilevel compositional data using a Bayesian inference approach. This method can be used to investigate how reallocation of time between sleep-wake movement behaviours may be associated with other phenomena (e.g., emotions, cognitions) at a daily level. We explain the theoretical details of the data and the models, and outline the steps necessary to implement this method. We introduce the R package multilevelcoda to facilitate the application of this method and illustrate using a real data example. An extensive parameter recovery simulation study verified the robust performance of the method. Across all simulation conditions investigated in the simulation study, the model had minimal convergence issues (convergence rate &amp;gt; 99%) and achieved excellent quality of parameter estimates and inference, with an average bias of 0.00 (range -0.09, 0.05) and coverage of 0.95 (range 0.93, 0.97). We conclude the article with recommendations on the use of the Bayesian compositional multilevel modelling approach, and hope to promote wider application of this method to answer robust questions using the increasingly available data from intensive, longitudinal studies.</summary></entry><entry><title type="html">Bayesian Penalized Transformation Models: Structured Additive Location-Scale Regression for Arbitrary Conditional Distributions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianPenalizedTransformationModelsStructuredAdditiveLocationScaleRegressionforArbitraryConditionalDistributions.html" rel="alternate" type="text/html" title="Bayesian Penalized Transformation Models: Structured Additive Location-Scale Regression for Arbitrary Conditional Distributions" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianPenalizedTransformationModelsStructuredAdditiveLocationScaleRegressionforArbitraryConditionalDistributions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BayesianPenalizedTransformationModelsStructuredAdditiveLocationScaleRegressionforArbitraryConditionalDistributions.html">&lt;p&gt;Penalized transformation models (PTMs) are a novel form of location-scale regression. In PTMs, the shape of the response’s conditional distribution is estimated directly from the data, and structured additive predictors are placed on its location and scale. The core of the model is a monotonically increasing transformation function that relates the response distribution to a reference distribution. The transformation function is equipped with a smoothness prior that regularizes how much the estimated distribution diverges from the reference distribution. These models can be seen as a bridge between conditional transformation models and generalized additive models for location, scale and shape. Markov chain Monte Carlo inference for PTMs can be conducted with the No-U-Turn sampler and offers straightforward uncertainty quantification for the conditional distribution as well as for the covariate effects. A simulation study demonstrates the effectiveness of the approach. We apply the model to data from the Fourth Dutch Growth Study and the Framingham Heart Study. A full-featured implementation is available as a Python library.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.07440&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Johannes Brachem, Paul F. V. Wiemann, Thomas Kneib</name></author><category term="stat.ME" /><summary type="html">Penalized transformation models (PTMs) are a novel form of location-scale regression. In PTMs, the shape of the response’s conditional distribution is estimated directly from the data, and structured additive predictors are placed on its location and scale. The core of the model is a monotonically increasing transformation function that relates the response distribution to a reference distribution. The transformation function is equipped with a smoothness prior that regularizes how much the estimated distribution diverges from the reference distribution. These models can be seen as a bridge between conditional transformation models and generalized additive models for location, scale and shape. Markov chain Monte Carlo inference for PTMs can be conducted with the No-U-Turn sampler and offers straightforward uncertainty quantification for the conditional distribution as well as for the covariate effects. A simulation study demonstrates the effectiveness of the approach. We apply the model to data from the Fourth Dutch Growth Study and the Framingham Heart Study. A full-featured implementation is available as a Python library.</summary></entry><entry><title type="html">Bayesian taut splines for estimating the number of modes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Bayesiantautsplinesforestimatingthenumberofmodes.html" rel="alternate" type="text/html" title="Bayesian taut splines for estimating the number of modes" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Bayesiantautsplinesforestimatingthenumberofmodes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Bayesiantautsplinesforestimatingthenumberofmodes.html">&lt;p&gt;The number of modes in a probability density function is representative of the complexity of a model and can also be viewed as the number of subpopulations. Despite its relevance, there has been limited research in this area. A novel approach to estimating the number of modes in the univariate setting is presented, focusing on prediction accuracy and inspired by some overlooked aspects of the problem: the need for structure in the solutions, the subjective and uncertain nature of modes, and the convenience of a holistic view that blends local and global density properties. The technique combines flexible kernel estimators and parsimonious compositional splines in the Bayesian inference paradigm, providing soft solutions and incorporating expert judgment. The procedure includes feature exploration, model selection, and mode testing, illustrated in a sports analytics case study showcasing multiple companion visualisation tools. A thorough simulation study also demonstrates that traditional modality-driven approaches paradoxically struggle to provide accurate results. In this context, the new method emerges as a top-tier alternative, offering innovative solutions for analysts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.05825&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>José E. Chacón, Javier Fernández Serrano</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">The number of modes in a probability density function is representative of the complexity of a model and can also be viewed as the number of subpopulations. Despite its relevance, there has been limited research in this area. A novel approach to estimating the number of modes in the univariate setting is presented, focusing on prediction accuracy and inspired by some overlooked aspects of the problem: the need for structure in the solutions, the subjective and uncertain nature of modes, and the convenience of a holistic view that blends local and global density properties. The technique combines flexible kernel estimators and parsimonious compositional splines in the Bayesian inference paradigm, providing soft solutions and incorporating expert judgment. The procedure includes feature exploration, model selection, and mode testing, illustrated in a sports analytics case study showcasing multiple companion visualisation tools. A thorough simulation study also demonstrates that traditional modality-driven approaches paradoxically struggle to provide accurate results. In this context, the new method emerges as a top-tier alternative, offering innovative solutions for analysts.</summary></entry><entry><title type="html">Bounding Causal Effects with Leaky Instruments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BoundingCausalEffectswithLeakyInstruments.html" rel="alternate" type="text/html" title="Bounding Causal Effects with Leaky Instruments" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BoundingCausalEffectswithLeakyInstruments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/BoundingCausalEffectswithLeakyInstruments.html">&lt;p&gt;Instrumental variables (IVs) are a popular and powerful tool for estimating causal effects in the presence of unobserved confounding. However, classical approaches rely on strong assumptions such as the $\textit{exclusion criterion}$, which states that instrumental effects must be entirely mediated by treatments. This assumption often fails in practice. When IV methods are improperly applied to data that do not meet the exclusion criterion, estimated causal effects may be badly biased. In this work, we propose a novel solution that provides $\textit{partial}$ identification in linear systems given a set of $\textit{leaky instruments}$, which are allowed to violate the exclusion criterion to some limited degree. We derive a convex optimization objective that provides provably sharp bounds on the average treatment effect under some common forms of information leakage, and implement inference procedures to quantify the uncertainty of resulting estimates. We demonstrate our method in a set of experiments with simulated data, where it performs favorably against the state of the art. An accompanying $\texttt{R}$ package, $\texttt{leakyIV}$, is available from $\texttt{CRAN}$.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.04446&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David S. Watson, Jordan Penn, Lee M. Gunderson, Gecia Bravo-Hermsdorff, Afsaneh Mastouri, Ricardo Silva</name></author><category term="stat.ME" /><summary type="html">Instrumental variables (IVs) are a popular and powerful tool for estimating causal effects in the presence of unobserved confounding. However, classical approaches rely on strong assumptions such as the $\textit{exclusion criterion}$, which states that instrumental effects must be entirely mediated by treatments. This assumption often fails in practice. When IV methods are improperly applied to data that do not meet the exclusion criterion, estimated causal effects may be badly biased. In this work, we propose a novel solution that provides $\textit{partial}$ identification in linear systems given a set of $\textit{leaky instruments}$, which are allowed to violate the exclusion criterion to some limited degree. We derive a convex optimization objective that provides provably sharp bounds on the average treatment effect under some common forms of information leakage, and implement inference procedures to quantify the uncertainty of resulting estimates. We demonstrate our method in a set of experiments with simulated data, where it performs favorably against the state of the art. An accompanying $\texttt{R}$ package, $\texttt{leakyIV}$, is available from $\texttt{CRAN}$.</summary></entry><entry><title type="html">Bump hunting through density curvature features</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Bumphuntingthroughdensitycurvaturefeatures.html" rel="alternate" type="text/html" title="Bump hunting through density curvature features" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Bumphuntingthroughdensitycurvaturefeatures</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Bumphuntingthroughdensitycurvaturefeatures.html">&lt;p&gt;Bump hunting deals with finding in sample spaces meaningful data subsets known as bumps. These have traditionally been conceived as modal or concave regions in the graph of the underlying density function. We define an abstract bump construct based on curvature functionals of the probability density. Then, we explore several alternative characterizations involving derivatives up to second order. In particular, a suitable implementation of Good and Gaskins’ original concave bumps is proposed in the multivariate case. Moreover, we bring to exploratory data analysis concepts like the mean curvature and the Laplacian that have produced good results in applied domains. Our methodology addresses the approximation of the curvature functional with a plug-in kernel density estimator. We provide theoretical results that assure the asymptotic consistency of bump boundaries in the Hausdorff distance with affordable convergence rates. We also present asymptotically valid and consistent confidence regions bounding curvature bumps. The theory is illustrated through several use cases in sports analytics with datasets from the NBA, MLB and NFL. We conclude that the different curvature instances effectively combine to generate insightful visualizations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.00174&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>José E. Chacón, Javier Fernández Serrano</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Bump hunting deals with finding in sample spaces meaningful data subsets known as bumps. These have traditionally been conceived as modal or concave regions in the graph of the underlying density function. We define an abstract bump construct based on curvature functionals of the probability density. Then, we explore several alternative characterizations involving derivatives up to second order. In particular, a suitable implementation of Good and Gaskins’ original concave bumps is proposed in the multivariate case. Moreover, we bring to exploratory data analysis concepts like the mean curvature and the Laplacian that have produced good results in applied domains. Our methodology addresses the approximation of the curvature functional with a plug-in kernel density estimator. We provide theoretical results that assure the asymptotic consistency of bump boundaries in the Hausdorff distance with affordable convergence rates. We also present asymptotically valid and consistent confidence regions bounding curvature bumps. The theory is illustrated through several use cases in sports analytics with datasets from the NBA, MLB and NFL. We conclude that the different curvature instances effectively combine to generate insightful visualizations.</summary></entry><entry><title type="html">Causal Flow-based Variational Auto-Encoder for Disentangled Causal Representation Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/CausalFlowbasedVariationalAutoEncoderforDisentangledCausalRepresentationLearning.html" rel="alternate" type="text/html" title="Causal Flow-based Variational Auto-Encoder for Disentangled Causal Representation Learning" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/CausalFlowbasedVariationalAutoEncoderforDisentangledCausalRepresentationLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/CausalFlowbasedVariationalAutoEncoderforDisentangledCausalRepresentationLearning.html">&lt;p&gt;Disentangled representation learning aims to learn low-dimensional representations of data, where each dimension corresponds to an underlying generative factor. Currently, Variational Auto-Encoder (VAE) are widely used for disentangled representation learning, with the majority of methods assuming independence among generative factors. However, in real-world scenarios, generative factors typically exhibit complex causal relationships. We thus design a new VAE-based framework named Disentangled Causal Variational Auto-Encoder (DCVAE), which includes a variant of autoregressive flows known as causal flows, capable of learning effective causal disentangled representations. We provide a theoretical analysis of the disentanglement identifiability of DCVAE, ensuring that our model can effectively learn causal disentangled representations. The performance of DCVAE is evaluated on both synthetic and real-world datasets, demonstrating its outstanding capability in achieving causal disentanglement and performing intervention experiments. Moreover, DCVAE exhibits remarkable performance on downstream tasks and has the potential to learn the true causal structure among factors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.09010&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Di Fan, Yannian Kou, Chuanhou Gao</name></author><category term="stat.ME" /><summary type="html">Disentangled representation learning aims to learn low-dimensional representations of data, where each dimension corresponds to an underlying generative factor. Currently, Variational Auto-Encoder (VAE) are widely used for disentangled representation learning, with the majority of methods assuming independence among generative factors. However, in real-world scenarios, generative factors typically exhibit complex causal relationships. We thus design a new VAE-based framework named Disentangled Causal Variational Auto-Encoder (DCVAE), which includes a variant of autoregressive flows known as causal flows, capable of learning effective causal disentangled representations. We provide a theoretical analysis of the disentanglement identifiability of DCVAE, ensuring that our model can effectively learn causal disentangled representations. The performance of DCVAE is evaluated on both synthetic and real-world datasets, demonstrating its outstanding capability in achieving causal disentanglement and performing intervention experiments. Moreover, DCVAE exhibits remarkable performance on downstream tasks and has the potential to learn the true causal structure among factors.</summary></entry><entry><title type="html">Causality Pursuit from Heterogeneous Environments via Neural Adversarial Invariance Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/CausalityPursuitfromHeterogeneousEnvironmentsviaNeuralAdversarialInvarianceLearning.html" rel="alternate" type="text/html" title="Causality Pursuit from Heterogeneous Environments via Neural Adversarial Invariance Learning" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/CausalityPursuitfromHeterogeneousEnvironmentsviaNeuralAdversarialInvarianceLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/CausalityPursuitfromHeterogeneousEnvironmentsviaNeuralAdversarialInvarianceLearning.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Statistics suffers from a fundamental problem, “the curse of endogeneity” – the regression function, or more broadly the prediction risk minimizer with infinite data, may not be the target we wish to pursue. This is because when complex data are collected from multiple sources, the biases deviated from the interested (causal) association inherited in individuals or sub-populations are not expected to be canceled. Traditional remedies are of hindsight and restrictive in being tailored to prior knowledge like untestable cause-effect structures, resulting in methods that risk model misspecification and lack scalable applicability. This paper seeks to offer a purely data-driven and universally applicable method that only uses the heterogeneity of the biases in the data rather than following pre-offered commandments. Such an idea is formulated as a nonparametric invariance pursuit problem, whose goal is to unveil the invariant conditional expectation $m^\star(x)\equiv \mathbb{E}[Y^{(e)}&lt;/td&gt;
      &lt;td&gt;X_{S^\star}^{(e)}=x_{S^\star}]$ with unknown important variable set $S^\star$ across heterogeneous environments $e\in \mathcal{E}$. Under the structural causal model framework, $m^\star$ can be interpreted as certain data-driven causality in general. The paper contributes to proposing a novel framework, called Focused Adversarial Invariance Regularization (FAIR), formulated as a single minimax optimization program that can solve the general invariance pursuit problem. As illustrated by the unified non-asymptotic analysis, our adversarial estimation framework can attain provable sample-efficient estimation akin to standard regression under a minimal identification condition for various tasks and models. As an application, the FAIR-NN estimator realized by two Neural Network classes is highlighted as the first approach to attain statistically efficient estimation in general nonparametric invariance learning.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04715&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yihong Gu, Cong Fang, Peter Bühlmann, Jianqing Fan</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Statistics suffers from a fundamental problem, “the curse of endogeneity” – the regression function, or more broadly the prediction risk minimizer with infinite data, may not be the target we wish to pursue. This is because when complex data are collected from multiple sources, the biases deviated from the interested (causal) association inherited in individuals or sub-populations are not expected to be canceled. Traditional remedies are of hindsight and restrictive in being tailored to prior knowledge like untestable cause-effect structures, resulting in methods that risk model misspecification and lack scalable applicability. This paper seeks to offer a purely data-driven and universally applicable method that only uses the heterogeneity of the biases in the data rather than following pre-offered commandments. Such an idea is formulated as a nonparametric invariance pursuit problem, whose goal is to unveil the invariant conditional expectation $m^\star(x)\equiv \mathbb{E}[Y^{(e)} X_{S^\star}^{(e)}=x_{S^\star}]$ with unknown important variable set $S^\star$ across heterogeneous environments $e\in \mathcal{E}$. Under the structural causal model framework, $m^\star$ can be interpreted as certain data-driven causality in general. The paper contributes to proposing a novel framework, called Focused Adversarial Invariance Regularization (FAIR), formulated as a single minimax optimization program that can solve the general invariance pursuit problem. As illustrated by the unified non-asymptotic analysis, our adversarial estimation framework can attain provable sample-efficient estimation akin to standard regression under a minimal identification condition for various tasks and models. As an application, the FAIR-NN estimator realized by two Neural Network classes is highlighted as the first approach to attain statistically efficient estimation in general nonparametric invariance learning.</summary></entry><entry><title type="html">Clustering Retail Products Based on Customer Behaviour</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/ClusteringRetailProductsBasedonCustomerBehaviour.html" rel="alternate" type="text/html" title="Clustering Retail Products Based on Customer Behaviour" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/ClusteringRetailProductsBasedonCustomerBehaviour</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/ClusteringRetailProductsBasedonCustomerBehaviour.html">&lt;p&gt;The categorization of retail products is essential for the business decision-making process. It is a common practice to classify products based on their quantitative and qualitative characteristics. In this paper we use a purely data-driven approach. Our clustering of products is based exclusively on the customer behaviour. We propose a method for clustering retail products using market basket data. Our model is formulated as an optimization problem which is solved by a genetic algorithm. It is demonstrated on simulated data how our method behaves in different settings. The application using real data from a Czech drugstore company shows that our method leads to similar results in comparison with the classification by experts. The number of clusters is a parameter of our algorithm. We demonstrate that if more clusters are allowed than the original number of categories is, the method yields additional information about the structure of the product categorization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05218&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vladimír Holý, Ondřej Sokol, Michal Černý</name></author><category term="stat.AP" /><summary type="html">The categorization of retail products is essential for the business decision-making process. It is a common practice to classify products based on their quantitative and qualitative characteristics. In this paper we use a purely data-driven approach. Our clustering of products is based exclusively on the customer behaviour. We propose a method for clustering retail products using market basket data. Our model is formulated as an optimization problem which is solved by a genetic algorithm. It is demonstrated on simulated data how our method behaves in different settings. The application using real data from a Czech drugstore company shows that our method leads to similar results in comparison with the classification by experts. The number of clusters is a parameter of our algorithm. We demonstrate that if more clusters are allowed than the original number of categories is, the method yields additional information about the structure of the product categorization.</summary></entry><entry><title type="html">Combining Rollout Designs and Clustering for Causal Inference under Low-order Interference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/CombiningRolloutDesignsandClusteringforCausalInferenceunderLoworderInterference.html" rel="alternate" type="text/html" title="Combining Rollout Designs and Clustering for Causal Inference under Low-order Interference" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/CombiningRolloutDesignsandClusteringforCausalInferenceunderLoworderInterference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/CombiningRolloutDesignsandClusteringforCausalInferenceunderLoworderInterference.html">&lt;p&gt;Estimating causal effects under interference is pertinent to many real-world settings. However, the true interference network may be unknown to the practitioner, precluding many existing techniques that leverage this information. A recent line of work with low-order potential outcomes models uses staggered rollout designs to obtain unbiased estimators that require no network information. However, their use of polynomial extrapolation can lead to prohibitively high variance. To address this, we propose a two-stage experimental design that restricts treatment rollout to a sub-population. We analyze the bias and variance of an interpolation-style estimator under this experimental design. Through numerical simulations, we explore the trade-off between the error attributable to the subsampling of our experimental design and the extrapolation of the estimator. Under low-order interactions models with degree greater than 1, the proposed design greatly reduces the error of the polynomial interpolation estimator, such that it outperforms baseline estimators, especially when the treatment probability is small.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05119&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mayleen Cortez-Rodriguez, Matthew Eichhorn, Christina Lee Yu</name></author><category term="stat.ME" /><summary type="html">Estimating causal effects under interference is pertinent to many real-world settings. However, the true interference network may be unknown to the practitioner, precluding many existing techniques that leverage this information. A recent line of work with low-order potential outcomes models uses staggered rollout designs to obtain unbiased estimators that require no network information. However, their use of polynomial extrapolation can lead to prohibitively high variance. To address this, we propose a two-stage experimental design that restricts treatment rollout to a sub-population. We analyze the bias and variance of an interpolation-style estimator under this experimental design. Through numerical simulations, we explore the trade-off between the error attributable to the subsampling of our experimental design and the extrapolation of the estimator. Under low-order interactions models with degree greater than 1, the proposed design greatly reduces the error of the polynomial interpolation estimator, such that it outperforms baseline estimators, especially when the treatment probability is small.</summary></entry><entry><title type="html">Community detection in multi-layer bipartite networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Communitydetectioninmultilayerbipartitenetworks.html" rel="alternate" type="text/html" title="Community detection in multi-layer bipartite networks" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Communitydetectioninmultilayerbipartitenetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Communitydetectioninmultilayerbipartitenetworks.html">&lt;p&gt;The problem of community detection in multi-layer undirected networks has received considerable attention in recent years. However, practical scenarios often involve multi-layer bipartite networks, where each layer consists of two distinct types of nodes. Existing community detection algorithms tailored for multi-layer undirected networks are not directly applicable to multi-layer bipartite networks. To address this challenge, this paper introduces a novel multi-layer degree-corrected stochastic co-block model specifically designed to capture the underlying community structure within multi-layer bipartite networks. Within this framework, we propose an efficient debiased spectral co-clustering algorithm for detecting nodes’ communities. We establish the consistent estimation property of our proposed algorithm and demonstrate that an increased number of layers in bipartite networks improves the accuracy of community detection. Through extensive numerical experiments, we showcase the superior performance of our algorithm compared to existing methods. Additionally, we validate our algorithm by applying it to real-world multi-layer network datasets, yielding meaningful and insightful results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04711&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Huan Qing</name></author><category term="stat.ME" /><summary type="html">The problem of community detection in multi-layer undirected networks has received considerable attention in recent years. However, practical scenarios often involve multi-layer bipartite networks, where each layer consists of two distinct types of nodes. Existing community detection algorithms tailored for multi-layer undirected networks are not directly applicable to multi-layer bipartite networks. To address this challenge, this paper introduces a novel multi-layer degree-corrected stochastic co-block model specifically designed to capture the underlying community structure within multi-layer bipartite networks. Within this framework, we propose an efficient debiased spectral co-clustering algorithm for detecting nodes’ communities. We establish the consistent estimation property of our proposed algorithm and demonstrate that an increased number of layers in bipartite networks improves the accuracy of community detection. Through extensive numerical experiments, we showcase the superior performance of our algorithm compared to existing methods. Additionally, we validate our algorithm by applying it to real-world multi-layer network datasets, yielding meaningful and insightful results.</summary></entry><entry><title type="html">Demand and Welfare Analysis in Discrete Choice Models with Social Interactions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/DemandandWelfareAnalysisinDiscreteChoiceModelswithSocialInteractions.html" rel="alternate" type="text/html" title="Demand and Welfare Analysis in Discrete Choice Models with Social Interactions" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/DemandandWelfareAnalysisinDiscreteChoiceModelswithSocialInteractions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/DemandandWelfareAnalysisinDiscreteChoiceModelswithSocialInteractions.html">&lt;p&gt;Many real-life settings of consumer-choice involve social interactions, causing targeted policies to have spillover-effects. This paper develops novel empirical tools for analyzing demand and welfare-effects of policy-interventions in binary choice settings with social interactions. Examples include subsidies for health-product adoption and vouchers for attending a high-achieving school. We establish the connection between econometrics of large games and Brock-Durlauf-type interaction models, under both I.I.D. and spatially correlated unobservables. We develop new convergence results for associated beliefs and estimates of preference-parameters under increasing-domain spatial asymptotics. Next, we show that even with fully parametric specifications and unique equilibrium, choice data, that are sufficient for counterfactual demand-prediction under interactions, are insufficient for welfare-calculations. This is because distinct underlying mechanisms producing the same interaction coefficient can imply different welfare-effects and deadweight-loss from a policy-intervention. Standard index-restrictions imply distribution-free bounds on welfare. We illustrate our results using experimental data on mosquito-net adoption in rural Kenya.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1905.04028&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Debopam Bhattacharya, Pascaline Dupas, Shin Kanaya</name></author><category term="stat.AP" /><summary type="html">Many real-life settings of consumer-choice involve social interactions, causing targeted policies to have spillover-effects. This paper develops novel empirical tools for analyzing demand and welfare-effects of policy-interventions in binary choice settings with social interactions. Examples include subsidies for health-product adoption and vouchers for attending a high-achieving school. We establish the connection between econometrics of large games and Brock-Durlauf-type interaction models, under both I.I.D. and spatially correlated unobservables. We develop new convergence results for associated beliefs and estimates of preference-parameters under increasing-domain spatial asymptotics. Next, we show that even with fully parametric specifications and unique equilibrium, choice data, that are sufficient for counterfactual demand-prediction under interactions, are insufficient for welfare-calculations. This is because distinct underlying mechanisms producing the same interaction coefficient can imply different welfare-effects and deadweight-loss from a policy-intervention. Standard index-restrictions imply distribution-free bounds on welfare. We illustrate our results using experimental data on mosquito-net adoption in rural Kenya.</summary></entry><entry><title type="html">Dependence-based fuzzy clustering of functional time series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Dependencebasedfuzzyclusteringoffunctionaltimeseries.html" rel="alternate" type="text/html" title="Dependence-based fuzzy clustering of functional time series" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Dependencebasedfuzzyclusteringoffunctionaltimeseries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Dependencebasedfuzzyclusteringoffunctionaltimeseries.html">&lt;p&gt;Time series clustering is an important data mining task with a wide variety of applications. While most methods focus on time series taking values on the real line, very few works consider functional time series. However, functional objects frequently arise in many fields, such as actuarial science, demography or finance. Functional time series are indexed collections of infinite-dimensional curves viewed as random elements taking values in a Hilbert space. In this paper, the problem of clustering functional time series is addressed. To this aim, a distance between functional time series is introduced and used to construct a clustering procedure. The metric relies on a measure of serial dependence which can be seen as a natural extension of the classical quantile autocorrelation function to the functional setting. Since the dynamics of the series may vary over time, we adopt a fuzzy approach, which enables the procedure to locate each series into several clusters with different membership degrees. The resulting algorithm can group series generated from similar stochastic processes, reaching accurate results with series coming from a broad variety of functional models and requiring minimum hyperparameter tuning. Several simulation experiments show that the method exhibits a high clustering accuracy besides being computationally efficient. Two interesting applications involving high-frequency financial time series and age-specific mortality improvement rates illustrate the potential of the proposed approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04904&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Angel Lopez-Oriona, Ying Sun, Han Lin Shang</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Time series clustering is an important data mining task with a wide variety of applications. While most methods focus on time series taking values on the real line, very few works consider functional time series. However, functional objects frequently arise in many fields, such as actuarial science, demography or finance. Functional time series are indexed collections of infinite-dimensional curves viewed as random elements taking values in a Hilbert space. In this paper, the problem of clustering functional time series is addressed. To this aim, a distance between functional time series is introduced and used to construct a clustering procedure. The metric relies on a measure of serial dependence which can be seen as a natural extension of the classical quantile autocorrelation function to the functional setting. Since the dynamics of the series may vary over time, we adopt a fuzzy approach, which enables the procedure to locate each series into several clusters with different membership degrees. The resulting algorithm can group series generated from similar stochastic processes, reaching accurate results with series coming from a broad variety of functional models and requiring minimum hyperparameter tuning. Several simulation experiments show that the method exhibits a high clustering accuracy besides being computationally efficient. Two interesting applications involving high-frequency financial time series and age-specific mortality improvement rates illustrate the potential of the proposed approach.</summary></entry><entry><title type="html">Differentially Private Linear Regression with Linked Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/DifferentiallyPrivateLinearRegressionwithLinkedData.html" rel="alternate" type="text/html" title="Differentially Private Linear Regression with Linked Data" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/DifferentiallyPrivateLinearRegressionwithLinkedData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/DifferentiallyPrivateLinearRegressionwithLinkedData.html">&lt;p&gt;There has been increasing demand for establishing privacy-preserving methodologies for modern statistics and machine learning. Differential privacy, a mathematical notion from computer science, is a rising tool offering robust privacy guarantees. Recent work focuses primarily on developing differentially private versions of individual statistical and machine learning tasks, with nontrivial upstream pre-processing typically not incorporated. An important example is when record linkage is done prior to downstream modeling. Record linkage refers to the statistical task of linking two or more data sets of the same group of entities without a unique identifier. This probabilistic procedure brings additional uncertainty to the subsequent task. In this paper, we present two differentially private algorithms for linear regression with linked data. In particular, we propose a noisy gradient method and a sufficient statistics perturbation approach for the estimation of regression coefficients. We investigate the privacy-accuracy tradeoff by providing finite-sample error bounds for the estimators, which allows us to understand the relative contributions of linkage error, estimation error, and the cost of privacy. The variances of the estimators are also discussed. We demonstrate the performance of the proposed algorithms through simulations and an application to synthetic data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.00836&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shurong Lin, Elliot Paquette, Eric D. Kolaczyk</name></author><category term="stat.ME" /><summary type="html">There has been increasing demand for establishing privacy-preserving methodologies for modern statistics and machine learning. Differential privacy, a mathematical notion from computer science, is a rising tool offering robust privacy guarantees. Recent work focuses primarily on developing differentially private versions of individual statistical and machine learning tasks, with nontrivial upstream pre-processing typically not incorporated. An important example is when record linkage is done prior to downstream modeling. Record linkage refers to the statistical task of linking two or more data sets of the same group of entities without a unique identifier. This probabilistic procedure brings additional uncertainty to the subsequent task. In this paper, we present two differentially private algorithms for linear regression with linked data. In particular, we propose a noisy gradient method and a sufficient statistics perturbation approach for the estimation of regression coefficients. We investigate the privacy-accuracy tradeoff by providing finite-sample error bounds for the estimators, which allows us to understand the relative contributions of linkage error, estimation error, and the cost of privacy. The variances of the estimators are also discussed. We demonstrate the performance of the proposed algorithms through simulations and an application to synthetic data.</summary></entry><entry><title type="html">Distributed variable screening for generalized linear models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Distributedvariablescreeningforgeneralizedlinearmodels.html" rel="alternate" type="text/html" title="Distributed variable screening for generalized linear models" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Distributedvariablescreeningforgeneralizedlinearmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Distributedvariablescreeningforgeneralizedlinearmodels.html">&lt;p&gt;In this article, we develop a distributed variable screening method for generalized linear models. This method is designed to handle situations where both the sample size and the number of covariates are large. Specifically, the proposed method selects relevant covariates by using a sparsity-restricted surrogate likelihood estimator. It takes into account the joint effects of the covariates rather than just the marginal effect, and this characteristic enhances the reliability of the screening results. We establish the sure screening property of the proposed method, which ensures that with a high probability, the true model is included in the selected model. Simulation studies are conducted to evaluate the finite sample performance of the proposed method, and an application to a real dataset showcases its practical utility.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04254&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tianbo Diao, Lianqiang Qu, Bo Li, Liuquan Sun</name></author><category term="stat.ME" /><summary type="html">In this article, we develop a distributed variable screening method for generalized linear models. This method is designed to handle situations where both the sample size and the number of covariates are large. Specifically, the proposed method selects relevant covariates by using a sparsity-restricted surrogate likelihood estimator. It takes into account the joint effects of the covariates rather than just the marginal effect, and this characteristic enhances the reliability of the screening results. We establish the sure screening property of the proposed method, which ensures that with a high probability, the true model is included in the selected model. Simulation studies are conducted to evaluate the finite sample performance of the proposed method, and an application to a real dataset showcases its practical utility.</summary></entry><entry><title type="html">Fast Computation of Leave-One-Out Cross-Validation for $k$-NN Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/FastComputationofLeaveOneOutCrossValidationforkNNRegression.html" rel="alternate" type="text/html" title="Fast Computation of Leave-One-Out Cross-Validation for $k$-NN Regression" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/FastComputationofLeaveOneOutCrossValidationforkNNRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/FastComputationofLeaveOneOutCrossValidationforkNNRegression.html">&lt;p&gt;We describe a fast computation method for leave-one-out cross-validation (LOOCV) for $k$-nearest neighbours ($k$-NN) regression. We show that, under a tie-breaking condition for nearest neighbours, the LOOCV estimate of the mean square error for $k$-NN regression is identical to the mean square error of $(k+1)$-NN regression evaluated on the training data, multiplied by the scaling factor $(k+1)^2/k^2$. Therefore, to compute the LOOCV score, one only needs to fit $(k+1)$-NN regression only once, and does not need to repeat training-validation of $k$-NN regression for the number of training data. Numerical experiments confirm the validity of the fast computation method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04919&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Motonobu Kanagawa</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">We describe a fast computation method for leave-one-out cross-validation (LOOCV) for $k$-nearest neighbours ($k$-NN) regression. We show that, under a tie-breaking condition for nearest neighbours, the LOOCV estimate of the mean square error for $k$-NN regression is identical to the mean square error of $(k+1)$-NN regression evaluated on the training data, multiplied by the scaling factor $(k+1)^2/k^2$. Therefore, to compute the LOOCV score, one only needs to fit $(k+1)$-NN regression only once, and does not need to repeat training-validation of $k$-NN regression for the number of training data. Numerical experiments confirm the validity of the fast computation method.</summary></entry><entry><title type="html">Fast Exact/Conservative Monte Carlo Confidence Intervals</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/FastExactConservativeMonteCarloConfidenceIntervals.html" rel="alternate" type="text/html" title="Fast Exact/Conservative Monte Carlo Confidence Intervals" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/FastExactConservativeMonteCarloConfidenceIntervals</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/FastExactConservativeMonteCarloConfidenceIntervals.html">&lt;p&gt;Monte Carlo tests about parameters can be “inverted” to form confidence sets: the confidence set comprises all hypothesized values of the parameter that are not rejected at level $\alpha$. When the tests are exact or conservative – as some families of such tests are – so are the confidence sets. Because the validity of confidence sets depends only on the significance level of the test of the true null, every null can be tested using the same Monte Carlo sample, substantially reducing the computational burden of constructing confidence sets: the computation count is $O(n)$, where $n$ is the number of data. The Monte Carlo sample can be arbitrarily small, although the highest nontrivial attainable confidence level generally increases as the number of Monte Carlo replicates increases. When the parameter is real-valued and the $P$-value is quasiconcave in that parameter, it is straightforward to find the endpoints of the confidence interval using bisection in a conservative way. For some test statistics, values for different simulations and parameter values have a simple relationship that make more savings possible. An open-source Python implementation of the approach for the one-sample and two-sample problems is available.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05238&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Amanda K. Glazer, Philip B. Stark</name></author><category term="stat.CO" /><summary type="html">Monte Carlo tests about parameters can be “inverted” to form confidence sets: the confidence set comprises all hypothesized values of the parameter that are not rejected at level $\alpha$. When the tests are exact or conservative – as some families of such tests are – so are the confidence sets. Because the validity of confidence sets depends only on the significance level of the test of the true null, every null can be tested using the same Monte Carlo sample, substantially reducing the computational burden of constructing confidence sets: the computation count is $O(n)$, where $n$ is the number of data. The Monte Carlo sample can be arbitrarily small, although the highest nontrivial attainable confidence level generally increases as the number of Monte Carlo replicates increases. When the parameter is real-valued and the $P$-value is quasiconcave in that parameter, it is straightforward to find the endpoints of the confidence interval using bisection in a conservative way. For some test statistics, values for different simulations and parameter values have a simple relationship that make more savings possible. An open-source Python implementation of the approach for the one-sample and two-sample problems is available.</summary></entry><entry><title type="html">Fitted value shrinkage</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Fittedvalueshrinkage.html" rel="alternate" type="text/html" title="Fitted value shrinkage" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Fittedvalueshrinkage</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Fittedvalueshrinkage.html">&lt;p&gt;We propose a penalized least-squares method to fit the linear regression model with fitted values that are invariant to invertible linear transformations of the design matrix. This invariance is important, for example, when practitioners have categorical predictors and interactions. Our method has the same computational cost as ridge-penalized least squares, which lacks this invariance. We derive the expected squared distance between the vector of population fitted values and its shrinkage estimator as well as the tuning parameter value that minimizes this expectation. In addition to using cross validation, we construct two estimators of this optimal tuning parameter value and study their asymptotic properties. Our numerical experiments and data examples show that our method performs similarly to ridge-penalized least-squares.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.03317&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Daeyoung Ham, Adam J. Rothman</name></author><category term="stat.ME" /><summary type="html">We propose a penalized least-squares method to fit the linear regression model with fitted values that are invariant to invertible linear transformations of the design matrix. This invariance is important, for example, when practitioners have categorical predictors and interactions. Our method has the same computational cost as ridge-penalized least squares, which lacks this invariance. We derive the expected squared distance between the vector of population fitted values and its shrinkage estimator as well as the tuning parameter value that minimizes this expectation. In addition to using cross validation, we construct two estimators of this optimal tuning parameter value and study their asymptotic properties. Our numerical experiments and data examples show that our method performs similarly to ridge-penalized least-squares.</summary></entry><entry><title type="html">Guiding adaptive shrinkage by co-data to improve regression-based prediction and feature selection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Guidingadaptiveshrinkagebycodatatoimproveregressionbasedpredictionandfeatureselection.html" rel="alternate" type="text/html" title="Guiding adaptive shrinkage by co-data to improve regression-based prediction and feature selection" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Guidingadaptiveshrinkagebycodatatoimproveregressionbasedpredictionandfeatureselection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Guidingadaptiveshrinkagebycodatatoimproveregressionbasedpredictionandfeatureselection.html">&lt;p&gt;The high dimensional nature of genomics data complicates feature selection, in particular in low sample size studies - not uncommon in clinical prediction settings. It is widely recognized that complementary data on the features, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;co-data&apos;, may improve results. Examples are prior feature groups or p-values from a related study. Such co-data are ubiquitous in genomics settings due to the availability of public repositories. Yet, the uptake of learning methods that structurally use such co-data is limited. We review guided adaptive shrinkage methods: a class of regression-based learners that use co-data to adapt the shrinkage parameters, crucial for the performance of those learners. We discuss technical aspects, but also the applicability in terms of types of co-data that can be handled. This class of methods is contrasted with several others. In particular, group-adaptive shrinkage is compared with the better-known sparse group-lasso by evaluating feature selection. Finally, we demonstrate the versatility of the guided shrinkage methodology by showing how to &lt;/code&gt;do-it-yourself’: we integrate implementations of a co-data learner and the spike-and-slab prior for the purpose of improving feature selection in genetics studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04917&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mark A. van de Wiel, Wessel N. van Wieringen</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">The high dimensional nature of genomics data complicates feature selection, in particular in low sample size studies - not uncommon in clinical prediction settings. It is widely recognized that complementary data on the features, co-data&apos;, may improve results. Examples are prior feature groups or p-values from a related study. Such co-data are ubiquitous in genomics settings due to the availability of public repositories. Yet, the uptake of learning methods that structurally use such co-data is limited. We review guided adaptive shrinkage methods: a class of regression-based learners that use co-data to adapt the shrinkage parameters, crucial for the performance of those learners. We discuss technical aspects, but also the applicability in terms of types of co-data that can be handled. This class of methods is contrasted with several others. In particular, group-adaptive shrinkage is compared with the better-known sparse group-lasso by evaluating feature selection. Finally, we demonstrate the versatility of the guided shrinkage methodology by showing how to do-it-yourself’: we integrate implementations of a co-data learner and the spike-and-slab prior for the purpose of improving feature selection in genetics studies.</summary></entry><entry><title type="html">Inference With Combining Rules From Multiple Differentially Private Synthetic Datasets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/InferenceWithCombiningRulesFromMultipleDifferentiallyPrivateSyntheticDatasets.html" rel="alternate" type="text/html" title="Inference With Combining Rules From Multiple Differentially Private Synthetic Datasets" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/InferenceWithCombiningRulesFromMultipleDifferentiallyPrivateSyntheticDatasets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/InferenceWithCombiningRulesFromMultipleDifferentiallyPrivateSyntheticDatasets.html">&lt;p&gt;Differential privacy (DP) has been accepted as a rigorous criterion for measuring the privacy protection offered by random mechanisms used to obtain statistics or, as we will study here, synthetic datasets from confidential data. Methods to generate such datasets are increasingly numerous, using varied tools including Bayesian models, deep neural networks and copulas. However, little is still known about how to properly perform statistical inference with these differentially private synthetic (DIPS) datasets. The challenge is for the analyses to take into account the variability from the synthetic data generation in addition to the usual sampling variability. A similar challenge also occurs when missing data is imputed before analysis, and statisticians have developed appropriate inference procedures for this case, which we tend extended to the case of synthetic datasets for privacy. In this work, we study the applicability of these procedures, based on combining rules, to the analysis of DIPS datasets. Our empirical experiments show that the proposed combining rules may offer accurate inference in certain contexts, but not in all cases.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04769&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Leila Nombo, Anne-Sophie Charest</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Differential privacy (DP) has been accepted as a rigorous criterion for measuring the privacy protection offered by random mechanisms used to obtain statistics or, as we will study here, synthetic datasets from confidential data. Methods to generate such datasets are increasingly numerous, using varied tools including Bayesian models, deep neural networks and copulas. However, little is still known about how to properly perform statistical inference with these differentially private synthetic (DIPS) datasets. The challenge is for the analyses to take into account the variability from the synthetic data generation in addition to the usual sampling variability. A similar challenge also occurs when missing data is imputed before analysis, and statisticians have developed appropriate inference procedures for this case, which we tend extended to the case of synthetic datasets for privacy. In this work, we study the applicability of these procedures, based on combining rules, to the analysis of DIPS datasets. Our empirical experiments show that the proposed combining rules may offer accurate inference in certain contexts, but not in all cases.</summary></entry><entry><title type="html">Multivariate group sequential tests for global summary statistics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Multivariategroupsequentialtestsforglobalsummarystatistics.html" rel="alternate" type="text/html" title="Multivariate group sequential tests for global summary statistics" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Multivariategroupsequentialtestsforglobalsummarystatistics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Multivariategroupsequentialtestsforglobalsummarystatistics.html">&lt;p&gt;We describe group sequential tests which efficiently incorporate information from multiple endpoints allowing for early stopping at pre-planned interim analyses. We formulate a testing procedure where several outcomes are examined, and interim decisions are based on a global summary statistic. An error spending approach to this problem is defined which allows for unpredictable group sizes and nuisance parameters such as the correlation between endpoints. We present and compare three methods for implementation of the testing procedure including numerical integration, the Delta approximation and Monte Carlo simulation. In our evaluation, numerical integration techniques performed best for implementation with error rate calculations accurate to five decimal places. Our proposed testing method is flexible and accommodates summary statistics derived from general, non-linear functions of endpoints informed by the statistical model. Type 1 error rates are controlled, and sample size calculations can easily be performed to satisfy power requirements.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05139&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Abigail J. Burdon, Thomas Jaki</name></author><category term="stat.ME" /><summary type="html">We describe group sequential tests which efficiently incorporate information from multiple endpoints allowing for early stopping at pre-planned interim analyses. We formulate a testing procedure where several outcomes are examined, and interim decisions are based on a global summary statistic. An error spending approach to this problem is defined which allows for unpredictable group sizes and nuisance parameters such as the correlation between endpoints. We present and compare three methods for implementation of the testing procedure including numerical integration, the Delta approximation and Monte Carlo simulation. In our evaluation, numerical integration techniques performed best for implementation with error rate calculations accurate to five decimal places. Our proposed testing method is flexible and accommodates summary statistics derived from general, non-linear functions of endpoints informed by the statistical model. Type 1 error rates are controlled, and sample size calculations can easily be performed to satisfy power requirements.</summary></entry><entry><title type="html">Non-locality and Spillover Effects of Residential Flood Damage on Community Recovery: Insights from High-resolution Flood Claim and Mobility Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/NonlocalityandSpilloverEffectsofResidentialFloodDamageonCommunityRecoveryInsightsfromHighresolutionFloodClaimandMobilityData.html" rel="alternate" type="text/html" title="Non-locality and Spillover Effects of Residential Flood Damage on Community Recovery: Insights from High-resolution Flood Claim and Mobility Data" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/NonlocalityandSpilloverEffectsofResidentialFloodDamageonCommunityRecoveryInsightsfromHighresolutionFloodClaimandMobilityData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/NonlocalityandSpilloverEffectsofResidentialFloodDamageonCommunityRecoveryInsightsfromHighresolutionFloodClaimandMobilityData.html">&lt;p&gt;Examining the relationship between vulnerability of the built environment and community recovery is crucial for understanding disaster resilience. Yet, this relationship is rather neglected in the existing literature due to previous limitations in the availability of empirical datasets needed for such analysis. In this study, we combine fine-resolution flood damage claims data (composed of both insured and uninsured losses) and human mobility data (composed of millions of movement trajectories) during the 2017 Hurricane Harvey in Harris County, Texas, to specify the extent to which vulnerability of the built environment (i.e., flood property damage) affects community recovery (based on the speed of human mobility recovery) locally and regionally. We examine this relationship using a spatial lag, spatial reach, and spatial decay models to measure the extent of spillover effects of residential damage on community recovery. The findings show that: first, the severity of residential damage significantly affects the speed of community recovery. A greater extent of residential damage suppresses community recovery not only locally but also in the surrounding areas. Second, the spatial spillover effect of residential damage on community recovery speed decays with distance from the highly damaged areas. Third, spatial areas display heterogeneous spatial decay coefficients, which are associated with urban structure features such as the density of points-of-interest facilities and roads. These findings provide a novel data-driven characterization of the spatial diffusion of residential flood damage effects on community recovery and move us closer to a better understanding of complex spatial processes that shape community resilience to hazards. This study also provides valuable insights for emergency managers and public officials seeking to mitigate the non-local effects of residential damage.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.03874&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Junwei Ma, Russell Blessing, Samuel Brody, Ali Mostafavi</name></author><category term="stat.AP" /><summary type="html">Examining the relationship between vulnerability of the built environment and community recovery is crucial for understanding disaster resilience. Yet, this relationship is rather neglected in the existing literature due to previous limitations in the availability of empirical datasets needed for such analysis. In this study, we combine fine-resolution flood damage claims data (composed of both insured and uninsured losses) and human mobility data (composed of millions of movement trajectories) during the 2017 Hurricane Harvey in Harris County, Texas, to specify the extent to which vulnerability of the built environment (i.e., flood property damage) affects community recovery (based on the speed of human mobility recovery) locally and regionally. We examine this relationship using a spatial lag, spatial reach, and spatial decay models to measure the extent of spillover effects of residential damage on community recovery. The findings show that: first, the severity of residential damage significantly affects the speed of community recovery. A greater extent of residential damage suppresses community recovery not only locally but also in the surrounding areas. Second, the spatial spillover effect of residential damage on community recovery speed decays with distance from the highly damaged areas. Third, spatial areas display heterogeneous spatial decay coefficients, which are associated with urban structure features such as the density of points-of-interest facilities and roads. These findings provide a novel data-driven characterization of the spatial diffusion of residential flood damage effects on community recovery and move us closer to a better understanding of complex spatial processes that shape community resilience to hazards. This study also provides valuable insights for emergency managers and public officials seeking to mitigate the non-local effects of residential damage.</summary></entry><entry><title type="html">Occam Factor for Random Graphs: Erdös-Rényi, Independent Edge, and Rank-1 Stochastic Blockmodel</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/OccamFactorforRandomGraphsErd%C3%B6sR%C3%A9nyiIndependentEdgeandRank1StochasticBlockmodel.html" rel="alternate" type="text/html" title="Occam Factor for Random Graphs: Erdös-Rényi, Independent Edge, and Rank-1 Stochastic Blockmodel" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/OccamFactorforRandomGraphsErd%C3%B6sR%C3%A9nyiIndependentEdgeandRank1StochasticBlockmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/OccamFactorforRandomGraphsErd%C3%B6sR%C3%A9nyiIndependentEdgeandRank1StochasticBlockmodel.html">&lt;p&gt;We investigate the evidence/flexibility (i.e., “Occam”) paradigm and demonstrate the theoretical and empirical consistency of Bayesian evidence for the task of determining an appropriate generative model for network data. This model selection framework involves determining a collection of candidate models, equipping each of these models’ parameters with prior distributions derived via the encompassing priors method, and computing or approximating each models’ evidence. We demonstrate how such a criterion may be used to select the most suitable model among the Erd&quot;{o}s-R&apos;{e}nyi (ER) model, independent edge (IE) model, and rank-1 stochastic blockmodel (SBM). The Erd&quot;{o}s-R&apos;{e}nyi may be considered as being linearly nested within IE, a fact which permits exponential family results. The rank-1 SBM is not so ideal, so we propose a numerical method to approximate its evidence. We apply this paradigm to brain connectome data. Future work necessitates deriving and equipping additional candidate random graph models with appropriate priors so they may be included in the paradigm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.06465&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tianyu Wang, Zachary M. Pisano, Carey E. Priebe</name></author><category term="stat.ME" /><summary type="html">We investigate the evidence/flexibility (i.e., “Occam”) paradigm and demonstrate the theoretical and empirical consistency of Bayesian evidence for the task of determining an appropriate generative model for network data. This model selection framework involves determining a collection of candidate models, equipping each of these models’ parameters with prior distributions derived via the encompassing priors method, and computing or approximating each models’ evidence. We demonstrate how such a criterion may be used to select the most suitable model among the Erd&quot;{o}s-R&apos;{e}nyi (ER) model, independent edge (IE) model, and rank-1 stochastic blockmodel (SBM). The Erd&quot;{o}s-R&apos;{e}nyi may be considered as being linearly nested within IE, a fact which permits exponential family results. The rank-1 SBM is not so ideal, so we propose a numerical method to approximate its evidence. We apply this paradigm to brain connectome data. Future work necessitates deriving and equipping additional candidate random graph models with appropriate priors so they may be included in the paradigm.</summary></entry><entry><title type="html">On Correlation and Prediction Interval Reduction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/OnCorrelationandPredictionIntervalReduction.html" rel="alternate" type="text/html" title="On Correlation and Prediction Interval Reduction" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/OnCorrelationandPredictionIntervalReduction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/OnCorrelationandPredictionIntervalReduction.html">&lt;p&gt;Pearson’s correlation coefficient is a popular statistical measure to summarize the strength of association between two continuous variables. It is usually interpreted via its square as percentage of variance of one variable predicted by the other in a linear regression model. It can be generalized for multiple regression via the coefficient of determination, which is not straightforward to interpret in terms of prediction accuracy. In this paper, we propose to assess the prediction accuracy of a linear model via the prediction interval reduction (PIR) by comparing the width of the prediction interval derived from this model with the width of the prediction interval obtained without this model. At the population level, PIR is one-to-one related to the correlation and the coefficient of determination. In particular, a correlation of 0.5 corresponds to a PIR of only 13%. It is also the one’s complement of the coefficient of alienation introduced at the beginning of last century. We argue that PIR is easily interpretable and useful to keep in mind how difficult it is to make accurate individual predictions, an important message in the era of precision medicine and artificial intelligence. Different estimates of PIR are compared in the context of a linear model and an extension of the PIR concept to non-linear models is outlined.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04895&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Romain Piaget-Rossel, Valentin Rousson</name></author><category term="stat.ME" /><summary type="html">Pearson’s correlation coefficient is a popular statistical measure to summarize the strength of association between two continuous variables. It is usually interpreted via its square as percentage of variance of one variable predicted by the other in a linear regression model. It can be generalized for multiple regression via the coefficient of determination, which is not straightforward to interpret in terms of prediction accuracy. In this paper, we propose to assess the prediction accuracy of a linear model via the prediction interval reduction (PIR) by comparing the width of the prediction interval derived from this model with the width of the prediction interval obtained without this model. At the population level, PIR is one-to-one related to the correlation and the coefficient of determination. In particular, a correlation of 0.5 corresponds to a PIR of only 13%. It is also the one’s complement of the coefficient of alienation introduced at the beginning of last century. We argue that PIR is easily interpretable and useful to keep in mind how difficult it is to make accurate individual predictions, an important message in the era of precision medicine and artificial intelligence. Different estimates of PIR are compared in the context of a linear model and an extension of the PIR concept to non-linear models is outlined.</summary></entry><entry><title type="html">Ranking-Based Second Stage in Data Envelopment Analysis: An Application to Research Efficiency in Higher Education</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/RankingBasedSecondStageinDataEnvelopmentAnalysisAnApplicationtoResearchEfficiencyinHigherEducation.html" rel="alternate" type="text/html" title="Ranking-Based Second Stage in Data Envelopment Analysis: An Application to Research Efficiency in Higher Education" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/RankingBasedSecondStageinDataEnvelopmentAnalysisAnApplicationtoResearchEfficiencyinHigherEducation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/RankingBasedSecondStageinDataEnvelopmentAnalysisAnApplicationtoResearchEfficiencyinHigherEducation.html">&lt;p&gt;An alternative approach for the panel second stage of data envelopment analysis (DEA) is presented in this paper. Instead of efficiency scores, we propose to model rankings in the second stage using a dynamic ranking model in the score-driven framework. We argue that this approach is suitable to complement traditional panel regression as a robustness check. To demonstrate the proposed approach, we determine research efficiency of higher education systems at country level by examining scientific publications and analyze its relation to good governance. The proposed approach confirms positive relation to the Voice and Accountability indicator, as found by the standard panel linear regression, while suggesting caution regarding the Government Effectiveness indicator.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.01869&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vladimír Holý</name></author><category term="stat.AP" /><summary type="html">An alternative approach for the panel second stage of data envelopment analysis (DEA) is presented in this paper. Instead of efficiency scores, we propose to model rankings in the second stage using a dynamic ranking model in the score-driven framework. We argue that this approach is suitable to complement traditional panel regression as a robustness check. To demonstrate the proposed approach, we determine research efficiency of higher education systems at country level by examining scientific publications and analyze its relation to good governance. The proposed approach confirms positive relation to the Voice and Accountability indicator, as found by the standard panel linear regression, while suggesting caution regarding the Government Effectiveness indicator.</summary></entry><entry><title type="html">Rethinking recidivism through a causal lens</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Rethinkingrecidivismthroughacausallens.html" rel="alternate" type="text/html" title="Rethinking recidivism through a causal lens" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Rethinkingrecidivismthroughacausallens</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Rethinkingrecidivismthroughacausallens.html">&lt;p&gt;Predictive modeling of criminal recidivism, or whether people will re-offend in the future, has a long and contentious history. Modern causal inference methods allow us to move beyond prediction and target the “treatment effect” of a specific intervention on an outcome in an observational dataset. In this paper, we look specifically at the effect of incarceration (prison time) on recidivism, using a well-known dataset from North Carolina. Two popular causal methods for addressing confounding bias are explained and demonstrated: directed acyclic graph (DAG) adjustment and double machine learning (DML), including a sensitivity analysis for unobserved confounders. We find that incarceration has a detrimental effect on recidivism, i.e., longer prison sentences make it more likely that individuals will re-offend after release, although this conclusion should not be generalized beyond the scope of our data. We hope that this case study can inform future applications of causal inference to criminal justice analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2011.11483&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vik Shirvaikar, Choudur Lakshminarayan</name></author><category term="stat.AP" /><summary type="html">Predictive modeling of criminal recidivism, or whether people will re-offend in the future, has a long and contentious history. Modern causal inference methods allow us to move beyond prediction and target the “treatment effect” of a specific intervention on an outcome in an observational dataset. In this paper, we look specifically at the effect of incarceration (prison time) on recidivism, using a well-known dataset from North Carolina. Two popular causal methods for addressing confounding bias are explained and demonstrated: directed acyclic graph (DAG) adjustment and double machine learning (DML), including a sensitivity analysis for unobserved confounders. We find that incarceration has a detrimental effect on recidivism, i.e., longer prison sentences make it more likely that individuals will re-offend after release, although this conclusion should not be generalized beyond the scope of our data. We hope that this case study can inform future applications of causal inference to criminal justice analysis.</summary></entry><entry><title type="html">Robust Bayesian Modeling of Counts with Zero inflation and Outliers: Theoretical Robustness and Efficient Computation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/RobustBayesianModelingofCountswithZeroinflationandOutliersTheoreticalRobustnessandEfficientComputation.html" rel="alternate" type="text/html" title="Robust Bayesian Modeling of Counts with Zero inflation and Outliers: Theoretical Robustness and Efficient Computation" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/RobustBayesianModelingofCountswithZeroinflationandOutliersTheoreticalRobustnessandEfficientComputation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/RobustBayesianModelingofCountswithZeroinflationandOutliersTheoreticalRobustnessandEfficientComputation.html">&lt;p&gt;Count data with zero inflation and large outliers are ubiquitous in many scientific applications. However, posterior analysis under a standard statistical model, such as Poisson or negative binomial distribution, is sensitive to such contamination. This study introduces a novel framework for Bayesian modeling of counts that is robust to both zero inflation and large outliers. In doing so, we introduce rescaled beta distribution and adopt it to absorb undesirable effects from zero and outlying counts. The proposed approach has two appealing features: the efficiency of the posterior computation via a custom Gibbs sampling algorithm and a theoretically guaranteed posterior robustness, where extreme outliers are automatically removed from the posterior distribution. We demonstrate the usefulness of the proposed method by applying it to trend filtering and spatial modeling using predictive Gaussian processes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.10503&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa</name></author><category term="stat.ME" /><summary type="html">Count data with zero inflation and large outliers are ubiquitous in many scientific applications. However, posterior analysis under a standard statistical model, such as Poisson or negative binomial distribution, is sensitive to such contamination. This study introduces a novel framework for Bayesian modeling of counts that is robust to both zero inflation and large outliers. In doing so, we introduce rescaled beta distribution and adopt it to absorb undesirable effects from zero and outlying counts. The proposed approach has two appealing features: the efficiency of the posterior computation via a custom Gibbs sampling algorithm and a theoretically guaranteed posterior robustness, where extreme outliers are automatically removed from the posterior distribution. We demonstrate the usefulness of the proposed method by applying it to trend filtering and spatial modeling using predictive Gaussian processes.</summary></entry><entry><title type="html">Sensitivity-Aware Amortized Bayesian Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/SensitivityAwareAmortizedBayesianInference.html" rel="alternate" type="text/html" title="Sensitivity-Aware Amortized Bayesian Inference" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/SensitivityAwareAmortizedBayesianInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/SensitivityAwareAmortizedBayesianInference.html">&lt;p&gt;Sensitivity analyses reveal the influence of various modeling choices on the outcomes of statistical analyses. While theoretically appealing, they are overwhelmingly inefficient for complex Bayesian models. In this work, we propose sensitivity-aware amortized Bayesian inference (SA-ABI), a multifaceted approach to efficiently integrate sensitivity analyses into simulation-based inference with neural networks. First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to data perturbations and preprocessing steps. In contrast to most other Bayesian approaches, both steps circumvent the costly bottleneck of refitting the model for each choice of likelihood, prior, or data set. Finally, we propose to use deep ensembles to detect sensitivity arising from unreliable approximation (e.g., due to model misspecification). We demonstrate the effectiveness of our method in applied modeling problems, ranging from disease outbreak dynamics and global warming thresholds to human decision-making. Our results support sensitivity-aware inference as a default choice for amortized Bayesian workflows, automatically providing modelers with insights into otherwise hidden dimensions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.11122&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lasse Elsemüller, Hans Olischläger, Marvin Schmitt, Paul-Christian Bürkner, Ullrich Köthe, Stefan T. Radev</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Sensitivity analyses reveal the influence of various modeling choices on the outcomes of statistical analyses. While theoretically appealing, they are overwhelmingly inefficient for complex Bayesian models. In this work, we propose sensitivity-aware amortized Bayesian inference (SA-ABI), a multifaceted approach to efficiently integrate sensitivity analyses into simulation-based inference with neural networks. First, we utilize weight sharing to encode the structural similarities between alternative likelihood and prior specifications in the training process with minimal computational overhead. Second, we leverage the rapid inference of neural networks to assess sensitivity to data perturbations and preprocessing steps. In contrast to most other Bayesian approaches, both steps circumvent the costly bottleneck of refitting the model for each choice of likelihood, prior, or data set. Finally, we propose to use deep ensembles to detect sensitivity arising from unreliable approximation (e.g., due to model misspecification). We demonstrate the effectiveness of our method in applied modeling problems, ranging from disease outbreak dynamics and global warming thresholds to human decision-making. Our results support sensitivity-aware inference as a default choice for amortized Bayesian workflows, automatically providing modelers with insights into otherwise hidden dimensions.</summary></entry><entry><title type="html">Sequential model confidence sets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Sequentialmodelconfidencesets.html" rel="alternate" type="text/html" title="Sequential model confidence sets" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Sequentialmodelconfidencesets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/Sequentialmodelconfidencesets.html">&lt;p&gt;In most prediction and estimation situations, scientists consider various statistical models for the same problem, and naturally want to select amongst the best. Hansen et al. (2011) provide a powerful solution to this problem by the so-called model confidence set, a subset of the original set of available models that contains the best models with a given level of confidence. Importantly, model confidence sets respect the underlying selection uncertainty by being flexible in size. However, they presuppose a fixed sample size which stands in contrast to the fact that model selection and forecast evaluation are inherently sequential tasks where we successively collect new data and where the decision to continue or conclude a study may depend on the previous outcomes. In this article, we extend model confidence sets sequentially over time by relying on sequential testing methods. Recently, e-processes and confidence sequences have been introduced as new, safe methods for assessing statistical evidence. Sequential model confidence sets allow to continuously monitor the models’ performances and come with time-uniform, nonasymptotic coverage guarantees.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18678&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sebastian Arnold, Georgios Gavrilopoulos, Benedikt Schulz, Johanna Ziegel</name></author><category term="stat.ME" /><summary type="html">In most prediction and estimation situations, scientists consider various statistical models for the same problem, and naturally want to select amongst the best. Hansen et al. (2011) provide a powerful solution to this problem by the so-called model confidence set, a subset of the original set of available models that contains the best models with a given level of confidence. Importantly, model confidence sets respect the underlying selection uncertainty by being flexible in size. However, they presuppose a fixed sample size which stands in contrast to the fact that model selection and forecast evaluation are inherently sequential tasks where we successively collect new data and where the decision to continue or conclude a study may depend on the previous outcomes. In this article, we extend model confidence sets sequentially over time by relying on sequential testing methods. Recently, e-processes and confidence sequences have been introduced as new, safe methods for assessing statistical evidence. Sequential model confidence sets allow to continuously monitor the models’ performances and come with time-uniform, nonasymptotic coverage guarantees.</summary></entry><entry><title type="html">Statistical Response of ENSO Complexity to Initial Condition and Model Parameter Perturbations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/StatisticalResponseofENSOComplexitytoInitialConditionandModelParameterPerturbations.html" rel="alternate" type="text/html" title="Statistical Response of ENSO Complexity to Initial Condition and Model Parameter Perturbations" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/StatisticalResponseofENSOComplexitytoInitialConditionandModelParameterPerturbations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/StatisticalResponseofENSOComplexitytoInitialConditionandModelParameterPerturbations.html">&lt;p&gt;Studying the response of a climate system to perturbations has practical significance. Standard methods in computing the trajectory-wise deviation caused by perturbations may suffer from the chaotic nature that makes the model error dominate the true response after a short lead time. Statistical response, which computes the return described by the statistics, provides a systematic way of reaching robust outcomes with an appropriate quantification of the uncertainty and extreme events. In this paper, information theory is applied to compute the statistical response and find the most sensitive perturbation direction of different El Ni~no-Southern Oscillation (ENSO) events to initial value and model parameter perturbations. Depending on the initial phase and the time horizon, different state variables contribute to the most sensitive perturbation direction. While initial perturbations in sea surface temperature (SST) and thermocline depth usually lead to the most significant response of SST at short- and long-range, respectively, initial adjustment of the zonal advection can be crucial to trigger strong statistical responses at medium-range around 5 to 7 months, especially at the transient phases between El Ni~no and La Ni~na. It is also shown that the response in the variance triggered by external random forcing perturbations, such as the wind bursts, often dominates the mean response, making the resulting most sensitive direction very different from the trajectory-wise methods. Finally, despite the strong non-Gaussian climatology distributions, using Gaussian approximations in the information theory is efficient and accurate for computing the statistical response, allowing the method to be applied to sophisticated operational systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.03281&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marios Andreou, Nan Chen</name></author><category term="stat.AP" /><summary type="html">Studying the response of a climate system to perturbations has practical significance. Standard methods in computing the trajectory-wise deviation caused by perturbations may suffer from the chaotic nature that makes the model error dominate the true response after a short lead time. Statistical response, which computes the return described by the statistics, provides a systematic way of reaching robust outcomes with an appropriate quantification of the uncertainty and extreme events. In this paper, information theory is applied to compute the statistical response and find the most sensitive perturbation direction of different El Ni~no-Southern Oscillation (ENSO) events to initial value and model parameter perturbations. Depending on the initial phase and the time horizon, different state variables contribute to the most sensitive perturbation direction. While initial perturbations in sea surface temperature (SST) and thermocline depth usually lead to the most significant response of SST at short- and long-range, respectively, initial adjustment of the zonal advection can be crucial to trigger strong statistical responses at medium-range around 5 to 7 months, especially at the transient phases between El Ni~no and La Ni~na. It is also shown that the response in the variance triggered by external random forcing perturbations, such as the wind bursts, often dominates the mean response, making the resulting most sensitive direction very different from the trajectory-wise methods. Finally, despite the strong non-Gaussian climatology distributions, using Gaussian approximations in the information theory is efficient and accurate for computing the statistical response, allowing the method to be applied to sophisticated operational systems.</summary></entry><entry><title type="html">StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/StepMixAPythonPackageforPseudoLikelihoodEstimationofGeneralizedMixtureModelswithExternalVariables.html" rel="alternate" type="text/html" title="StepMix: A Python Package for Pseudo-Likelihood Estimation of Generalized Mixture Models with External Variables" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/StepMixAPythonPackageforPseudoLikelihoodEstimationofGeneralizedMixtureModelswithExternalVariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/StepMixAPythonPackageforPseudoLikelihoodEstimationofGeneralizedMixtureModelswithExternalVariables.html">&lt;p&gt;StepMix is an open-source Python package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estimation methods from the literature, including the bias-adjusted three-step methods with Bolk-Croon-Hagenaars and maximum likelihood corrections and the more recent two-step approach. These pseudo-likelihood estimators are presented in this paper under a unified framework as specific expectation-maximization subroutines. To facilitate and promote their adoption among the data science community, StepMix follows the object-oriented design of the scikit-learn library and provides an additional R wrapper.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.03853&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sacha Morin, Robin Legault, Félix Laliberté, Zsuzsa Bakk, Charles-Édouard Giguère, Roxane de la Sablonnière, Éric Lacourse</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">StepMix is an open-source Python package for the pseudo-likelihood estimation (one-, two- and three-step approaches) of generalized finite mixture models (latent profile and latent class analysis) with external variables (covariates and distal outcomes). In many applications in social sciences, the main objective is not only to cluster individuals into latent classes, but also to use these classes to develop more complex statistical models. These models generally divide into a measurement model that relates the latent classes to observed indicators, and a structural model that relates covariates and outcome variables to the latent classes. The measurement and structural models can be estimated jointly using the so-called one-step approach or sequentially using stepwise methods, which present significant advantages for practitioners regarding the interpretability of the estimated latent classes. In addition to the one-step approach, StepMix implements the most important stepwise estimation methods from the literature, including the bias-adjusted three-step methods with Bolk-Croon-Hagenaars and maximum likelihood corrections and the more recent two-step approach. These pseudo-likelihood estimators are presented in this paper under a unified framework as specific expectation-maximization subroutines. To facilitate and promote their adoption among the data science community, StepMix follows the object-oriented design of the scikit-learn library and provides an additional R wrapper.</summary></entry><entry><title type="html">Testing the Fairness-Improvability of Algorithms</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/TestingtheFairnessImprovabilityofAlgorithms.html" rel="alternate" type="text/html" title="Testing the Fairness-Improvability of Algorithms" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/TestingtheFairnessImprovabilityofAlgorithms</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/TestingtheFairnessImprovabilityofAlgorithms.html">&lt;p&gt;Many algorithms have a disparate impact in that their benefits or harms fall disproportionately on certain social groups. Addressing an algorithm’s disparate impact can be challenging, however, because it is not always clear whether there exists an alternative more-fair algorithm that does not compromise on other key objectives such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy maker can formally substantiate, or refute, this necessity defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can incorporate any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and demonstrate its use empirically by evaluating a healthcare algorithm originally considered by Obermeyer et al. (2019). In this demonstration, we find strong statistically significant evidence that it is possible to reduce the algorithm’s disparate impact without compromising on the accuracy of its predictions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04816&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Eric Auerbach, Annie Liang, Max Tabord-Meehan, Kyohei Okumura</name></author><category term="stat.AP" /><summary type="html">Many algorithms have a disparate impact in that their benefits or harms fall disproportionately on certain social groups. Addressing an algorithm’s disparate impact can be challenging, however, because it is not always clear whether there exists an alternative more-fair algorithm that does not compromise on other key objectives such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy maker can formally substantiate, or refute, this necessity defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can incorporate any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and demonstrate its use empirically by evaluating a healthcare algorithm originally considered by Obermeyer et al. (2019). In this demonstration, we find strong statistically significant evidence that it is possible to reduce the algorithm’s disparate impact without compromising on the accuracy of its predictions.</summary></entry><entry><title type="html">The Impact of TV Advertising on Website Traffic</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/TheImpactofTVAdvertisingonWebsiteTraffic.html" rel="alternate" type="text/html" title="The Impact of TV Advertising on Website Traffic" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/TheImpactofTVAdvertisingonWebsiteTraffic</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/TheImpactofTVAdvertisingonWebsiteTraffic.html">&lt;p&gt;We propose a modeling procedure for estimating immediate responses to TV ads and evaluating the factors influencing their size. First, we capture diurnal and seasonal patterns of website visits using the kernel smoothing method. Second, we estimate a gradual increase in website visits after an ad using the maximum likelihood method. Third, we analyze the non-linear dependence of the estimated increase in website visits on characteristics of the ads using the random forest method. The proposed methodology is applied to a dataset containing minute-by-minute organic website visits and detailed characteristics of TV ads for an e-commerce company in 2019. The results show that people are indeed willing to switch between screens and multitask. Moreover, the time of the day, the TV channel, and the advertising motive play a great role in the impact of the ads.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.08530&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lukáš Veverka, Vladimír Holý</name></author><category term="stat.AP" /><summary type="html">We propose a modeling procedure for estimating immediate responses to TV ads and evaluating the factors influencing their size. First, we capture diurnal and seasonal patterns of website visits using the kernel smoothing method. Second, we estimate a gradual increase in website visits after an ad using the maximum likelihood method. Third, we analyze the non-linear dependence of the estimated increase in website visits on characteristics of the ads using the random forest method. The proposed methodology is applied to a dataset containing minute-by-minute organic website visits and detailed characteristics of TV ads for an e-commerce company in 2019. The results show that people are indeed willing to switch between screens and multitask. Moreover, the time of the day, the TV channel, and the advertising motive play a great role in the impact of the ads.</summary></entry><entry><title type="html">Time-Varying Identification of Monetary Policy Shocks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/TimeVaryingIdentificationofMonetaryPolicyShocks.html" rel="alternate" type="text/html" title="Time-Varying Identification of Monetary Policy Shocks" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/TimeVaryingIdentificationofMonetaryPolicyShocks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/TimeVaryingIdentificationofMonetaryPolicyShocks.html">&lt;p&gt;We propose a new Bayesian heteroskedastic Markov-switching structural vector autoregression with data-driven time-varying identification. The model selects alternative exclusion restrictions over time and, as a condition for the search, allows to verify identification through heteroskedasticity within each regime. Based on four alternative monetary policy rules, we show that a monthly six-variable system supports time variation in US monetary policy shock identification. In the sample-dominating first regime, systematic monetary policy follows a Taylor rule extended by the term spread, effectively curbing inflation. In the second regime, occurring after 2000 and gaining more persistence after the global financial and COVID crises, it is characterized by a money-augmented Taylor rule. This regime’s unconventional monetary policy provides economic stimulus, features the liquidity effect, and is complemented by a pure term spread shock. Absent the specific monetary policy of the second regime, inflation would be over one percentage point higher on average after 2008.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.05883&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Annika Camehl , Tomasz Woźniak</name></author><category term="stat.AP" /><summary type="html">We propose a new Bayesian heteroskedastic Markov-switching structural vector autoregression with data-driven time-varying identification. The model selects alternative exclusion restrictions over time and, as a condition for the search, allows to verify identification through heteroskedasticity within each regime. Based on four alternative monetary policy rules, we show that a monthly six-variable system supports time variation in US monetary policy shock identification. In the sample-dominating first regime, systematic monetary policy follows a Taylor rule extended by the term spread, effectively curbing inflation. In the second regime, occurring after 2000 and gaining more persistence after the global financial and COVID crises, it is characterized by a money-augmented Taylor rule. This regime’s unconventional monetary policy provides economic stimulus, features the liquidity effect, and is complemented by a pure term spread shock. Absent the specific monetary policy of the second regime, inflation would be over one percentage point higher on average after 2008.</summary></entry><entry><title type="html">Uncertainty-Aware Bayes’ Rule and Its Applications</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/UncertaintyAwareBayesRuleandItsApplications.html" rel="alternate" type="text/html" title="Uncertainty-Aware Bayes’ Rule and Its Applications" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/UncertaintyAwareBayesRuleandItsApplications</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/UncertaintyAwareBayesRuleandItsApplications.html">&lt;p&gt;Bayes’ rule has enabled innumerable powerful algorithms of statistical signal processing and statistical machine learning. However, when there exist model misspecifications in prior distributions and/or data distributions, the direct application of Bayes’ rule is questionable. Philosophically, the key is to balance the relative importance of prior and data distributions when calculating posterior distributions: if prior (resp. data) distributions are overly conservative, we should upweight the prior belief (resp. data evidence); if prior (resp. data) distributions are overly opportunistic, we should downweight the prior belief (resp. data evidence). This paper derives a generalized Bayes’ rule, called uncertainty-aware Bayes’ rule, to technically realize the above philosophy, i.e., to combat the model uncertainties in prior distributions and/or data distributions. Simulated and real-world experiments on classification and estimation showcase the superiority of the presented uncertainty-aware Bayes’ rule over the conventional Bayes’ rule: In particular, the uncertainty-aware Bayes classifier, the uncertainty-aware Kalman filter, the uncertainty-aware particle filter, and the uncertainty-aware interactive-multiple-model filter are suggested and validated.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.05532&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shixiong Wang</name></author><category term="stat.ME" /><summary type="html">Bayes’ rule has enabled innumerable powerful algorithms of statistical signal processing and statistical machine learning. However, when there exist model misspecifications in prior distributions and/or data distributions, the direct application of Bayes’ rule is questionable. Philosophically, the key is to balance the relative importance of prior and data distributions when calculating posterior distributions: if prior (resp. data) distributions are overly conservative, we should upweight the prior belief (resp. data evidence); if prior (resp. data) distributions are overly opportunistic, we should downweight the prior belief (resp. data evidence). This paper derives a generalized Bayes’ rule, called uncertainty-aware Bayes’ rule, to technically realize the above philosophy, i.e., to combat the model uncertainties in prior distributions and/or data distributions. Simulated and real-world experiments on classification and estimation showcase the superiority of the presented uncertainty-aware Bayes’ rule over the conventional Bayes’ rule: In particular, the uncertainty-aware Bayes classifier, the uncertainty-aware Kalman filter, the uncertainty-aware particle filter, and the uncertainty-aware interactive-multiple-model filter are suggested and validated.</summary></entry><entry><title type="html">Weighted Particle-Based Optimization for Efficient Generalized Posterior Calibration</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/WeightedParticleBasedOptimizationforEfficientGeneralizedPosteriorCalibration.html" rel="alternate" type="text/html" title="Weighted Particle-Based Optimization for Efficient Generalized Posterior Calibration" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/WeightedParticleBasedOptimizationforEfficientGeneralizedPosteriorCalibration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/WeightedParticleBasedOptimizationforEfficientGeneralizedPosteriorCalibration.html">&lt;p&gt;In the realm of statistical learning, the increasing volume of accessible data and increasing model complexity necessitate robust methodologies. This paper explores two branches of robust Bayesian methods in response to this trend. The first is generalized Bayesian inference, which introduces a learning rate parameter to enhance robustness against model misspecifications. The second is Gibbs posterior inference, which formulates inferential problems using generic loss functions rather than probabilistic models. In such approaches, it is necessary to calibrate the spread of the posterior distribution by selecting a learning rate parameter. The study aims to enhance the generalized posterior calibration (GPC) algorithm proposed by Syring and Martin (2019) [Biometrika, Volume 106, Issue 2, pp. 479-486]. Their algorithm chooses the learning rate to achieve the nominal frequentist coverage probability, but it is computationally intensive because it requires repeated posterior simulations for bootstrap samples. We propose a more efficient version of the GPC inspired by sequential Monte Carlo (SMC) samplers. A target distribution with a different learning rate is evaluated without posterior simulation as in the reweighting step in SMC sampling. Thus, the proposed algorithm can reach the desired value within a few iterations. This improvement substantially reduces the computational cost of the GPC. Its efficacy is demonstrated through synthetic and real data applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04845&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Masahiro Tanaka</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">In the realm of statistical learning, the increasing volume of accessible data and increasing model complexity necessitate robust methodologies. This paper explores two branches of robust Bayesian methods in response to this trend. The first is generalized Bayesian inference, which introduces a learning rate parameter to enhance robustness against model misspecifications. The second is Gibbs posterior inference, which formulates inferential problems using generic loss functions rather than probabilistic models. In such approaches, it is necessary to calibrate the spread of the posterior distribution by selecting a learning rate parameter. The study aims to enhance the generalized posterior calibration (GPC) algorithm proposed by Syring and Martin (2019) [Biometrika, Volume 106, Issue 2, pp. 479-486]. Their algorithm chooses the learning rate to achieve the nominal frequentist coverage probability, but it is computationally intensive because it requires repeated posterior simulations for bootstrap samples. We propose a more efficient version of the GPC inspired by sequential Monte Carlo (SMC) samplers. A target distribution with a different learning rate is evaluated without posterior simulation as in the reweighting step in SMC sampling. Thus, the proposed algorithm can reach the desired value within a few iterations. This improvement substantially reduces the computational cost of the GPC. Its efficacy is demonstrated through synthetic and real data applications.</summary></entry><entry><title type="html">Zero-Inflated Autoregressive Conditional Duration Model for Discrete Trade Durations with Excessive Zeros</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/ZeroInflatedAutoregressiveConditionalDurationModelforDiscreteTradeDurationswithExcessiveZeros.html" rel="alternate" type="text/html" title="Zero-Inflated Autoregressive Conditional Duration Model for Discrete Trade Durations with Excessive Zeros" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/ZeroInflatedAutoregressiveConditionalDurationModelforDiscreteTradeDurationswithExcessiveZeros</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/ZeroInflatedAutoregressiveConditionalDurationModelforDiscreteTradeDurationswithExcessiveZeros.html">&lt;p&gt;In finance, durations between successive transactions are usually modeled by the autoregressive conditional duration model based on a continuous distribution omitting zero values. Zero or close-to-zero durations can be caused by either split transactions or independent transactions. We propose a discrete model allowing for excessive zero values based on the zero-inflated negative binomial distribution with score dynamics. This model allows to distinguish between the processes generating split and standard transactions. We use the existing theory on score models to establish the invertibility of the score filter and verify that sufficient conditions hold for the consistency and asymptotic normality of the maximum likelihood of the model parameters. In an empirical study, we find that split transactions cause between 92 and 98 percent of zero and close-to-zero values. Furthermore, the loss of decimal places in the proposed approach is less severe than the incorrect treatment of zero values in continuous models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1812.07318&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Francisco Blasques, Vladimír Holý, Petra Tomanová</name></author><category term="stat.ME" /><summary type="html">In finance, durations between successive transactions are usually modeled by the autoregressive conditional duration model based on a continuous distribution omitting zero values. Zero or close-to-zero durations can be caused by either split transactions or independent transactions. We propose a discrete model allowing for excessive zero values based on the zero-inflated negative binomial distribution with score dynamics. This model allows to distinguish between the processes generating split and standard transactions. We use the existing theory on score models to establish the invertibility of the score filter and verify that sufficient conditions hold for the consistency and asymptotic normality of the maximum likelihood of the model parameters. In an empirical study, we find that split transactions cause between 92 and 98 percent of zero and close-to-zero values. Furthermore, the loss of decimal places in the proposed approach is less severe than the incorrect treatment of zero values in continuous models.</summary></entry><entry><title type="html">gasmodel: An R Package for Generalized Autoregressive Score Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/gasmodelAnRPackageforGeneralizedAutoregressiveScoreModels.html" rel="alternate" type="text/html" title="gasmodel: An R Package for Generalized Autoregressive Score Models" /><published>2024-05-09T00:00:00+00:00</published><updated>2024-05-09T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/gasmodelAnRPackageforGeneralizedAutoregressiveScoreModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/09/gasmodelAnRPackageforGeneralizedAutoregressiveScoreModels.html">&lt;p&gt;Generalized autoregressive score (GAS) models are a class of observation-driven time series models that employ the score to dynamically update time-varying parameters of the underlying probability distribution. GAS models have been extensively studied and numerous variants have been proposed in the literature to accommodate diverse data types and probability distributions. This paper introduces the gasmodel package, which has been designed to facilitate the estimation, forecasting, and simulation of a wide range of GAS models. The package provides a rich selection of distributions, offers flexible options for specifying dynamics, and allows to incorporate exogenous variables. Model estimation utilizes the maximum likelihood method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05073&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vladimír Holý</name></author><category term="stat.CO" /><summary type="html">Generalized autoregressive score (GAS) models are a class of observation-driven time series models that employ the score to dynamically update time-varying parameters of the underlying probability distribution. GAS models have been extensively studied and numerous variants have been proposed in the literature to accommodate diverse data types and probability distributions. This paper introduces the gasmodel package, which has been designed to facilitate the estimation, forecasting, and simulation of a wide range of GAS models. The package provides a rich selection of distributions, offers flexible options for specifying dynamics, and allows to incorporate exogenous variables. Model estimation utilizes the maximum likelihood method.</summary></entry></feed>