<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-11T07:15:04+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html"></title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/2024-06-11-MinusOneDataPredictionGeneratesSyntheticCensusDatawithGoodCrosstabulationFidelity.html" rel="alternate" type="text/html" title="" /><published>2024-06-11T07:15:04+00:00</published><updated>2024-06-11T07:15:04+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/2024-06-11-MinusOneDataPredictionGeneratesSyntheticCensusDatawithGoodCrosstabulationFidelity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/2024-06-11-MinusOneDataPredictionGeneratesSyntheticCensusDatawithGoodCrosstabulationFidelity.html">&lt;p&gt;We propose to capture relevant statistical associations in a dataset of categorical survey responses by a method, here termed MODP, that “learns” a probabilistic prediction function L. Specifically, L predicts each question’s response based on the same respondent’s answers to all the other questions. Draws from the resulting probability distribution become synthetic responses. Applying this methodology to the PUMS subset of Census ACS data, and with a learned L akin to multiple parallel logistic regression, we generate synthetic responses whose crosstabulations (two-point conditionals) are found to have a median accuracy of ~5% across all crosstabulation cells, with cell counts ranging over four orders of magnitude. We investigate and attempt to quantify the degree to which the privacy of the original data is protected.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05264&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Aliverti</name></author></entry><entry><title type="html">A Multiscale Perspective on Maximum Marginal Likelihood Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AMultiscalePerspectiveonMaximumMarginalLikelihoodEstimation.html" rel="alternate" type="text/html" title="A Multiscale Perspective on Maximum Marginal Likelihood Estimation" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AMultiscalePerspectiveonMaximumMarginalLikelihoodEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AMultiscalePerspectiveonMaximumMarginalLikelihoodEstimation.html">&lt;p&gt;In this paper, we provide a multiscale perspective on the problem of maximum marginal likelihood estimation. We consider and analyse a diffusion-based maximum marginal likelihood estimation scheme using ideas from multiscale dynamics. Our perspective is based on stochastic averaging; we make an explicit connection between ideas in applied probability and parameter inference in computational statistics. In particular, we consider a general class of coupled Langevin diffusions for joint inference of latent variables and parameters in statistical models, where the latent variables are sampled from a fast Langevin process (which acts as a sampler), and the parameters are updated using a slow Langevin process (which acts as an optimiser). We show that the resulting system of stochastic differential equations (SDEs) can be viewed as a two-time scale system. To demonstrate the utility of such a perspective, we show that the averaged parameter dynamics obtained in the limit of scale separation can be used to estimate the optimal parameter, within the strongly convex setting. We do this by using recent uniform-in-time non-asymptotic averaging bounds. Finally, we conclude by showing that the slow-fast algorithm we consider here, termed Slow-Fast Langevin Algorithm, performs on par with state-of-the-art methods on a variety of examples. We believe that the stochastic averaging approach we provide in this paper enables us to look at these algorithms from a fresh angle, as well as unlocking the path to develop and analyse new methods using well-established averaging principles.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.04187&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>O. Deniz Akyildiz, Michela Ottobre, Iain Souttar</name></author><category term="stat.CO" /><summary type="html">In this paper, we provide a multiscale perspective on the problem of maximum marginal likelihood estimation. We consider and analyse a diffusion-based maximum marginal likelihood estimation scheme using ideas from multiscale dynamics. Our perspective is based on stochastic averaging; we make an explicit connection between ideas in applied probability and parameter inference in computational statistics. In particular, we consider a general class of coupled Langevin diffusions for joint inference of latent variables and parameters in statistical models, where the latent variables are sampled from a fast Langevin process (which acts as a sampler), and the parameters are updated using a slow Langevin process (which acts as an optimiser). We show that the resulting system of stochastic differential equations (SDEs) can be viewed as a two-time scale system. To demonstrate the utility of such a perspective, we show that the averaged parameter dynamics obtained in the limit of scale separation can be used to estimate the optimal parameter, within the strongly convex setting. We do this by using recent uniform-in-time non-asymptotic averaging bounds. Finally, we conclude by showing that the slow-fast algorithm we consider here, termed Slow-Fast Langevin Algorithm, performs on par with state-of-the-art methods on a variety of examples. We believe that the stochastic averaging approach we provide in this paper enables us to look at these algorithms from a fresh angle, as well as unlocking the path to develop and analyse new methods using well-established averaging principles.</summary></entry><entry><title type="html">A Note on the Prediction-Powered Bootstrap</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ANoteonthePredictionPoweredBootstrap.html" rel="alternate" type="text/html" title="A Note on the Prediction-Powered Bootstrap" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ANoteonthePredictionPoweredBootstrap</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ANoteonthePredictionPoweredBootstrap.html">&lt;p&gt;We introduce PPBoot: a bootstrap-based method for prediction-powered inference. PPBoot is applicable to arbitrary estimation problems and is very simple to implement, essentially only requiring one application of the bootstrap. Through a series of examples, we demonstrate that PPBoot often performs nearly identically to (and sometimes better than) the earlier PPI(++) method based on asymptotic normality$\unicode{x2013}$when the latter is applicable$\unicode{x2013}$without requiring any asymptotic characterizations. Given its versatility, PPBoot could simplify and expand the scope of application of prediction-powered inference to problems where central limit theorems are hard to prove.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18379&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tijana Zrnic</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">We introduce PPBoot: a bootstrap-based method for prediction-powered inference. PPBoot is applicable to arbitrary estimation problems and is very simple to implement, essentially only requiring one application of the bootstrap. Through a series of examples, we demonstrate that PPBoot often performs nearly identically to (and sometimes better than) the earlier PPI(++) method based on asymptotic normality$\unicode{x2013}$when the latter is applicable$\unicode{x2013}$without requiring any asymptotic characterizations. Given its versatility, PPBoot could simplify and expand the scope of application of prediction-powered inference to problems where central limit theorems are hard to prove.</summary></entry><entry><title type="html">A Statistical Theory of Regularization-Based Continual Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AStatisticalTheoryofRegularizationBasedContinualLearning.html" rel="alternate" type="text/html" title="A Statistical Theory of Regularization-Based Continual Learning" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AStatisticalTheoryofRegularizationBasedContinualLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AStatisticalTheoryofRegularizationBasedContinualLearning.html">&lt;p&gt;We provide a statistical analysis of regularization-based continual learning on a sequence of linear regression tasks, with emphasis on how different regularization terms affect the model performance. We first derive the convergence rate for the oracle estimator obtained as if all data were available simultaneously. Next, we consider a family of generalized $\ell_2$-regularization algorithms indexed by matrix-valued hyperparameters, which includes the minimum norm estimator and continual ridge regression as special cases. As more tasks are introduced, we derive an iterative update formula for the estimation error of generalized $\ell_2$-regularized estimators, from which we determine the hyperparameters resulting in the optimal algorithm. Interestingly, the choice of hyperparameters can effectively balance the trade-off between forward and backward knowledge transfer and adjust for data heterogeneity. Moreover, the estimation error of the optimal algorithm is derived explicitly, which is of the same order as that of the oracle estimator. In contrast, our lower bounds for the minimum norm estimator and continual ridge regression show their suboptimality. A byproduct of our theoretical analysis is the equivalence between early stopping and generalized $\ell_2$-regularization in continual learning, which may be of independent interest. Finally, we conduct experiments to complement our theory.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06213&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xuyang Zhao, Huiyuan Wang, Weiran Huang, Wei Lin</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">We provide a statistical analysis of regularization-based continual learning on a sequence of linear regression tasks, with emphasis on how different regularization terms affect the model performance. We first derive the convergence rate for the oracle estimator obtained as if all data were available simultaneously. Next, we consider a family of generalized $\ell_2$-regularization algorithms indexed by matrix-valued hyperparameters, which includes the minimum norm estimator and continual ridge regression as special cases. As more tasks are introduced, we derive an iterative update formula for the estimation error of generalized $\ell_2$-regularized estimators, from which we determine the hyperparameters resulting in the optimal algorithm. Interestingly, the choice of hyperparameters can effectively balance the trade-off between forward and backward knowledge transfer and adjust for data heterogeneity. Moreover, the estimation error of the optimal algorithm is derived explicitly, which is of the same order as that of the oracle estimator. In contrast, our lower bounds for the minimum norm estimator and continual ridge regression show their suboptimality. A byproduct of our theoretical analysis is the equivalence between early stopping and generalized $\ell_2$-regularization in continual learning, which may be of independent interest. Finally, we conduct experiments to complement our theory.</summary></entry><entry><title type="html">A Three-groups Non-local Model for Combining Heterogeneous Data Sources to Identify Genes Associated with Parkinson’s Disease</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AThreegroupsNonlocalModelforCombiningHeterogeneousDataSourcestoIdentifyGenesAssociatedwithParkinsonsDisease.html" rel="alternate" type="text/html" title="A Three-groups Non-local Model for Combining Heterogeneous Data Sources to Identify Genes Associated with Parkinson’s Disease" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AThreegroupsNonlocalModelforCombiningHeterogeneousDataSourcestoIdentifyGenesAssociatedwithParkinsonsDisease</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AThreegroupsNonlocalModelforCombiningHeterogeneousDataSourcestoIdentifyGenesAssociatedwithParkinsonsDisease.html">&lt;p&gt;We seek to identify genes involved in Parkinson’s Disease (PD) by combining information across different experiment types. Each experiment, taken individually, may contain too little information to distinguish some important genes from incidental ones. However, when experiments are combined using the proposed statistical framework, additional power emerges. The fundamental building block of the family of statistical models that we propose is a hierarchical three-group mixture of distributions. Each gene is modeled probabilistically as belonging to either a null group that is unassociated with PD, a deleterious group, or a beneficial group. This three-group formalism has two key features. By apportioning prior probability of group assignments with a Dirichlet distribution, the resultant posterior group probabilities automatically account for the multiplicity inherent in analyzing many genes simultaneously. By building models for experimental outcomes conditionally on the group labels, any number of data modalities may be combined in a single coherent probability model, allowing information sharing across experiment types. These two features result in parsimonious inference with few false positives, while simultaneously enhancing power to detect signals. Simulations show that our three-groups approach performs at least as well as commonly-used tools for GWAS and RNA-seq, and in some cases it performs better. We apply our proposed approach to publicly-available GWAS and RNA-seq datasets, discovering novel genes that are potential therapeutic targets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05262&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Troy P. Wixson, Benjamin A. Shaby, Daisy L. Philtron, International Parkinson Disease Genomics Consortium, Leandro A. Lima, Stacia K. Wyman, Julia A. Kaye, Steven Finkbeiner</name></author><category term="stat.AP" /><summary type="html">We seek to identify genes involved in Parkinson’s Disease (PD) by combining information across different experiment types. Each experiment, taken individually, may contain too little information to distinguish some important genes from incidental ones. However, when experiments are combined using the proposed statistical framework, additional power emerges. The fundamental building block of the family of statistical models that we propose is a hierarchical three-group mixture of distributions. Each gene is modeled probabilistically as belonging to either a null group that is unassociated with PD, a deleterious group, or a beneficial group. This three-group formalism has two key features. By apportioning prior probability of group assignments with a Dirichlet distribution, the resultant posterior group probabilities automatically account for the multiplicity inherent in analyzing many genes simultaneously. By building models for experimental outcomes conditionally on the group labels, any number of data modalities may be combined in a single coherent probability model, allowing information sharing across experiment types. These two features result in parsimonious inference with few false positives, while simultaneously enhancing power to detect signals. Simulations show that our three-groups approach performs at least as well as commonly-used tools for GWAS and RNA-seq, and in some cases it performs better. We apply our proposed approach to publicly-available GWAS and RNA-seq datasets, discovering novel genes that are potential therapeutic targets.</summary></entry><entry><title type="html">A critical appraisal of water table depth estimation: Challenges and opportunities within machine learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AcriticalappraisalofwatertabledepthestimationChallengesandopportunitieswithinmachinelearning.html" rel="alternate" type="text/html" title="A critical appraisal of water table depth estimation: Challenges and opportunities within machine learning" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AcriticalappraisalofwatertabledepthestimationChallengesandopportunitieswithinmachinelearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AcriticalappraisalofwatertabledepthestimationChallengesandopportunitieswithinmachinelearning.html">&lt;p&gt;Fine-resolution spatial patterns of water table depth (WTD) play a crucial role in shaping ecological resilience, hydrological connectivity, and anthropocentric objectives. Generally, a large-scale (e.g., continental or global) spatial map of static WTD can be simulated using either physically-based (PB) or machine learning-based (ML) models. We construct three fine-resolution (500 m) ML simulations of WTD, using the XGBoost algorithm and more than 20 million real and proxy observations of WTD, across the United States and Canada. The three ML models were constrained using known physical relations between WTD’s drivers and WTD and were trained by sequentially adding real and proxy observations of WTD. We interpret the black box of our physically constrained ML models and compare it against available literature in groundwater hydrology. Through an extensive (pixel-by-pixel) evaluation, we demonstrate that our models can more accurately predict unseen real and proxy observations of WTD across most of North America’s ecoregions compared to three available PB simulations of WTD. However, we still argue that large-scale WTD estimation is far from being a solved problem. We reason that due to biased observational data mainly collected from low-elevation floodplains, the misspecification of equations within physically-based models, and the over-flexibility of machine learning models, verifiably accurate simulations of WTD do not yet exist. Ultimately, we thoroughly discuss future directions that may help hydrogeologists decide how to proceed with WTD estimations, with a particular focus on the application of machine learning and the use of proxy satellite data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04579&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joseph Janssen, Ardalan Tootchi, Ali A. Ameli</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Fine-resolution spatial patterns of water table depth (WTD) play a crucial role in shaping ecological resilience, hydrological connectivity, and anthropocentric objectives. Generally, a large-scale (e.g., continental or global) spatial map of static WTD can be simulated using either physically-based (PB) or machine learning-based (ML) models. We construct three fine-resolution (500 m) ML simulations of WTD, using the XGBoost algorithm and more than 20 million real and proxy observations of WTD, across the United States and Canada. The three ML models were constrained using known physical relations between WTD’s drivers and WTD and were trained by sequentially adding real and proxy observations of WTD. We interpret the black box of our physically constrained ML models and compare it against available literature in groundwater hydrology. Through an extensive (pixel-by-pixel) evaluation, we demonstrate that our models can more accurately predict unseen real and proxy observations of WTD across most of North America’s ecoregions compared to three available PB simulations of WTD. However, we still argue that large-scale WTD estimation is far from being a solved problem. We reason that due to biased observational data mainly collected from low-elevation floodplains, the misspecification of equations within physically-based models, and the over-flexibility of machine learning models, verifiably accurate simulations of WTD do not yet exist. Ultimately, we thoroughly discuss future directions that may help hydrogeologists decide how to proceed with WTD estimations, with a particular focus on the application of machine learning and the use of proxy satellite data.</summary></entry><entry><title type="html">Active sampling: A machine-learning-assisted framework for finite population inference with optimal subsamples</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ActivesamplingAmachinelearningassistedframeworkforfinitepopulationinferencewithoptimalsubsamples.html" rel="alternate" type="text/html" title="Active sampling: A machine-learning-assisted framework for finite population inference with optimal subsamples" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ActivesamplingAmachinelearningassistedframeworkforfinitepopulationinferencewithoptimalsubsamples</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ActivesamplingAmachinelearningassistedframeworkforfinitepopulationinferencewithoptimalsubsamples.html">&lt;p&gt;Data subsampling has become widely recognized as a tool to overcome computational and economic bottlenecks in analyzing massive datasets. We contribute to the development of adaptive design for estimation of finite population characteristics, using active learning and adaptive importance sampling. We propose an active sampling strategy that iterates between estimation and data collection with optimal subsamples, guided by machine learning predictions on yet unseen data. The method is illustrated on virtual simulation-based safety assessment of advanced driver assistance systems. Substantial performance improvements are demonstrated compared to traditional sampling methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.10024&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Henrik Imberg, Xiaomi Yang, Carol Flannagan, Jonas Bärgman</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Data subsampling has become widely recognized as a tool to overcome computational and economic bottlenecks in analyzing massive datasets. We contribute to the development of adaptive design for estimation of finite population characteristics, using active learning and adaptive importance sampling. We propose an active sampling strategy that iterates between estimation and data collection with optimal subsamples, guided by machine learning predictions on yet unseen data. The method is illustrated on virtual simulation-based safety assessment of advanced driver assistance systems. Substantial performance improvements are demonstrated compared to traditional sampling methods.</summary></entry><entry><title type="html">Alternative ranking measures to predict international football results</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Alternativerankingmeasurestopredictinternationalfootballresults.html" rel="alternate" type="text/html" title="Alternative ranking measures to predict international football results" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Alternativerankingmeasurestopredictinternationalfootballresults</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Alternativerankingmeasurestopredictinternationalfootballresults.html">&lt;p&gt;Over the last few years, there has been a growing interest in the prediction and modelling of competitive sports outcomes, with particular emphasis placed on this area by the Bayesian statistics and machine learning communities. In this paper, we have carried out a comparative evaluation of statistical and machine learning models to assess their predictive performance for the 2022 FIFA World Cup and for the 2023 CAF Africa Cup of Nations by evaluating alternative summaries of past performances related to the involved teams. More specifically, we consider the Bayesian Bradley-Terry-Davidson model, which is a widely used statistical framework for ranking items based on paired comparisons that have been applied successfully in various domains, including football. The analysis was performed including in some canonical goal-based models both the Bradley-Terry-Davidson derived ranking and the widely recognized Coca-Cola FIFA ranking commonly adopted by football fans and amateurs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10247&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roberto Macrì Demartino, Leonardo Egidi, Nicola Torelli</name></author><category term="stat.AP" /><summary type="html">Over the last few years, there has been a growing interest in the prediction and modelling of competitive sports outcomes, with particular emphasis placed on this area by the Bayesian statistics and machine learning communities. In this paper, we have carried out a comparative evaluation of statistical and machine learning models to assess their predictive performance for the 2022 FIFA World Cup and for the 2023 CAF Africa Cup of Nations by evaluating alternative summaries of past performances related to the involved teams. More specifically, we consider the Bayesian Bradley-Terry-Davidson model, which is a widely used statistical framework for ranking items based on paired comparisons that have been applied successfully in various domains, including football. The analysis was performed including in some canonical goal-based models both the Bradley-Terry-Davidson derived ranking and the widely recognized Coca-Cola FIFA ranking commonly adopted by football fans and amateurs.</summary></entry><entry><title type="html">Analyzing the factors that are involved in length of inpatient stay at the hospital for diabetes patients</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Analyzingthefactorsthatareinvolvedinlengthofinpatientstayatthehospitalfordiabetespatients.html" rel="alternate" type="text/html" title="Analyzing the factors that are involved in length of inpatient stay at the hospital for diabetes patients" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Analyzingthefactorsthatareinvolvedinlengthofinpatientstayatthehospitalfordiabetespatients</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Analyzingthefactorsthatareinvolvedinlengthofinpatientstayatthehospitalfordiabetespatients.html">&lt;p&gt;The paper investigates the escalating concerns surrounding the surge in diabetes cases, exacerbated by the COVID-19 pandemic, and the subsequent strain on medical resources. The research aims to construct a predictive model quantifying factors influencing inpatient hospital stay durations for diabetes patients, offering insights to hospital administrators for improved patient management strategies. The literature review highlights the increasing prevalence of diabetes, emphasizing the need for continued attention and analysis of urban-rural disparities in healthcare access. International studies underscore the financial implications and healthcare burden associated with diabetes-related hospitalizations and complications, emphasizing the significance of effective management strategies. The methodology involves a quantitative approach, utilizing a dataset comprising 10,000 observations of diabetic inpatient encounters in U.S. hospitals from 1999 to 2008. Predictive modeling techniques, particularly Generalized Linear Models (GLM), are employed to develop a model predicting hospital stay durations based on patient demographics, admission types, medical history, and treatment regimen. The results highlight the influence of age, medical history, and treatment regimen on hospital stay durations for diabetes patients. Despite model limitations, such as heteroscedasticity and deviations from normality in residual analysis, the findings offer valuable insights for hospital administrators in patient management. The paper concludes with recommendations for future research to address model limitations and explore the implications of predictive models on healthcare management strategies, ensuring equitable patient care and resource allocation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05189&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jorden Lam, Kunpeng Xu</name></author><category term="stat.AP" /><summary type="html">The paper investigates the escalating concerns surrounding the surge in diabetes cases, exacerbated by the COVID-19 pandemic, and the subsequent strain on medical resources. The research aims to construct a predictive model quantifying factors influencing inpatient hospital stay durations for diabetes patients, offering insights to hospital administrators for improved patient management strategies. The literature review highlights the increasing prevalence of diabetes, emphasizing the need for continued attention and analysis of urban-rural disparities in healthcare access. International studies underscore the financial implications and healthcare burden associated with diabetes-related hospitalizations and complications, emphasizing the significance of effective management strategies. The methodology involves a quantitative approach, utilizing a dataset comprising 10,000 observations of diabetic inpatient encounters in U.S. hospitals from 1999 to 2008. Predictive modeling techniques, particularly Generalized Linear Models (GLM), are employed to develop a model predicting hospital stay durations based on patient demographics, admission types, medical history, and treatment regimen. The results highlight the influence of age, medical history, and treatment regimen on hospital stay durations for diabetes patients. Despite model limitations, such as heteroscedasticity and deviations from normality in residual analysis, the findings offer valuable insights for hospital administrators in patient management. The paper concludes with recommendations for future research to address model limitations and explore the implications of predictive models on healthcare management strategies, ensuring equitable patient care and resource allocation.</summary></entry><entry><title type="html">Assessing small area estimates via bootstrap-weighted k-Nearest-Neighbor artificial populations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AssessingsmallareaestimatesviabootstrapweightedkNearestNeighborartificialpopulations.html" rel="alternate" type="text/html" title="Assessing small area estimates via bootstrap-weighted k-Nearest-Neighbor artificial populations" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AssessingsmallareaestimatesviabootstrapweightedkNearestNeighborartificialpopulations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/AssessingsmallareaestimatesviabootstrapweightedkNearestNeighborartificialpopulations.html">&lt;p&gt;Comparing and evaluating small area estimation (SAE) models for a given application is inherently difficult. Typically, many areas lack enough data to check unit-level modeling assumptions or to assess unit-level predictions empirically; and no ground truth is available for checking area-level estimates. Design-based simulation from artificial populations can help with each of these issues, but only if the artificial populations realistically represent the application at hand and are not built using assumptions that inherently favor one SAE model over another. In this paper, we borrow ideas from random hot deck, approximate Bayesian bootstrap (ABB), and k Nearest Neighbor (kNN) imputation methods to propose a kNN-based approximation to ABB (KBAABB), for generating an artificial population when rich unit-level auxiliary data is available. We introduce diagnostic checks on the process of building the artificial population, and we demonstrate how to use such an artificial population for design-based simulation studies to compare and evaluate SAE models, using real data from the Forest Inventory and Analysis (FIA) program of the US Forest Service.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.15607&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Grayson W. White, Jerzy A. Wieczorek, Zachariah W. Cody, Emily X. Tan, Jacqueline O. Chistolini, Kelly S. McConville, Tracey S. Frescino, Gretchen G. Moisen</name></author><category term="stat.ME" /><summary type="html">Comparing and evaluating small area estimation (SAE) models for a given application is inherently difficult. Typically, many areas lack enough data to check unit-level modeling assumptions or to assess unit-level predictions empirically; and no ground truth is available for checking area-level estimates. Design-based simulation from artificial populations can help with each of these issues, but only if the artificial populations realistically represent the application at hand and are not built using assumptions that inherently favor one SAE model over another. In this paper, we borrow ideas from random hot deck, approximate Bayesian bootstrap (ABB), and k Nearest Neighbor (kNN) imputation methods to propose a kNN-based approximation to ABB (KBAABB), for generating an artificial population when rich unit-level auxiliary data is available. We introduce diagnostic checks on the process of building the artificial population, and we demonstrate how to use such an artificial population for design-based simulation studies to compare and evaluate SAE models, using real data from the Forest Inventory and Analysis (FIA) program of the US Forest Service.</summary></entry><entry><title type="html">Bayesian Parametric Methods for Deriving Distribution of Restricted Mean Survival Time</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/BayesianParametricMethodsforDerivingDistributionofRestrictedMeanSurvivalTime.html" rel="alternate" type="text/html" title="Bayesian Parametric Methods for Deriving Distribution of Restricted Mean Survival Time" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/BayesianParametricMethodsforDerivingDistributionofRestrictedMeanSurvivalTime</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/BayesianParametricMethodsforDerivingDistributionofRestrictedMeanSurvivalTime.html">&lt;p&gt;We propose a Bayesian method for deriving the distribution of restricted mean survival time (RMST) using posterior samples, which accounts for covariates and heterogeneity among clusters based on a parametric model for survival time. We derive an explicit RMST equation by devising an integral of the survival function, allowing for the calculation of not only the mean and credible interval but also the mode, median, and probability of exceeding a certain value. Additionally, We propose two methods: one using random effects to account for heterogeneity among clusters and another utilizing frailty. We developed custom Stan code for the exponential, Weibull, log-normal frailty, and log-logistic models, as they cannot be processed using the brm functions in R. We evaluate our proposed methods through computer simulations and analyze real data from the eight Empowered Action Group states in India to confirm consistent results across states after adjusting for cluster differences. In conclusion, we derived explicit RMST formulas for parametric models and their distributions, enabling the calculation of the mean, median, mode, and credible interval. Our simulations confirmed the robustness of the proposed methods, and using the shrinkage effect allowed for more accurate results for each cluster. This manuscript has not been published elsewhere. The manuscript is not under consideration in whole or in part by another journal.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06071&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Keisuke Hanada, Masahiro Kojima</name></author><category term="stat.ME" /><summary type="html">We propose a Bayesian method for deriving the distribution of restricted mean survival time (RMST) using posterior samples, which accounts for covariates and heterogeneity among clusters based on a parametric model for survival time. We derive an explicit RMST equation by devising an integral of the survival function, allowing for the calculation of not only the mean and credible interval but also the mode, median, and probability of exceeding a certain value. Additionally, We propose two methods: one using random effects to account for heterogeneity among clusters and another utilizing frailty. We developed custom Stan code for the exponential, Weibull, log-normal frailty, and log-logistic models, as they cannot be processed using the brm functions in R. We evaluate our proposed methods through computer simulations and analyze real data from the eight Empowered Action Group states in India to confirm consistent results across states after adjusting for cluster differences. In conclusion, we derived explicit RMST formulas for parametric models and their distributions, enabling the calculation of the mean, median, mode, and credible interval. Our simulations confirmed the robustness of the proposed methods, and using the shrinkage effect allowed for more accurate results for each cluster. This manuscript has not been published elsewhere. The manuscript is not under consideration in whole or in part by another journal.</summary></entry><entry><title type="html">Bayesian multi-exposure image fusion for robust high dynamic range ptychography</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Bayesianmultiexposureimagefusionforrobusthighdynamicrangeptychography.html" rel="alternate" type="text/html" title="Bayesian multi-exposure image fusion for robust high dynamic range ptychography" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Bayesianmultiexposureimagefusionforrobusthighdynamicrangeptychography</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Bayesianmultiexposureimagefusionforrobusthighdynamicrangeptychography.html">&lt;p&gt;The limited dynamic range of the detector can impede coherent diffractive imaging (CDI) schemes from achieving diffraction-limited resolution. To overcome this limitation, a straightforward approach is to utilize high dynamic range (HDR) imaging through multi-exposure image fusion (MEF). This method involves capturing measurements at different exposure times, spanning from under to overexposure and fusing them into a single HDR image. The conventional MEF technique in ptychography typically involves subtracting the background noise, ignoring the saturated pixels and then merging the acquisitions. However, this approach is inadequate under conditions of low signal-to-noise ratio (SNR). Additionally, variations in illumination intensity significantly affect the phase retrieval process. To address these issues, we propose a Bayesian MEF modeling approach based on a modified Poisson distribution that takes the background and saturation into account. To infer the model parameters, the expectation-maximization (EM) algorithm is employed. As demonstrated with synthetic and experimental data, our approach outperforms the conventional MEF method, offering superior phase retrieval under challenging experimental conditions. This work underscores the significance of robust multi-exposure image fusion for ptychography, particularly in imaging shot-noise-dominated weakly scattering specimens or in cases where access to HDR detectors with high SNR is limited. Furthermore, the applicability of the Bayesian MEF approach extends beyond CDI to any imaging scheme that requires HDR treatment. Given this versatility, we provide the implementation of our algorithm as a Python package.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.11344&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shantanu Kodgirwar, Lars Loetgering, Chang Liu, Aleena Joseph, Leona Licht, Daniel S. Penagos Molina, Wilhelm Eschen, Jan Rothhardt, Michael Habeck</name></author><category term="stat.AP" /><summary type="html">The limited dynamic range of the detector can impede coherent diffractive imaging (CDI) schemes from achieving diffraction-limited resolution. To overcome this limitation, a straightforward approach is to utilize high dynamic range (HDR) imaging through multi-exposure image fusion (MEF). This method involves capturing measurements at different exposure times, spanning from under to overexposure and fusing them into a single HDR image. The conventional MEF technique in ptychography typically involves subtracting the background noise, ignoring the saturated pixels and then merging the acquisitions. However, this approach is inadequate under conditions of low signal-to-noise ratio (SNR). Additionally, variations in illumination intensity significantly affect the phase retrieval process. To address these issues, we propose a Bayesian MEF modeling approach based on a modified Poisson distribution that takes the background and saturation into account. To infer the model parameters, the expectation-maximization (EM) algorithm is employed. As demonstrated with synthetic and experimental data, our approach outperforms the conventional MEF method, offering superior phase retrieval under challenging experimental conditions. This work underscores the significance of robust multi-exposure image fusion for ptychography, particularly in imaging shot-noise-dominated weakly scattering specimens or in cases where access to HDR detectors with high SNR is limited. Furthermore, the applicability of the Bayesian MEF approach extends beyond CDI to any imaging scheme that requires HDR treatment. Given this versatility, we provide the implementation of our algorithm as a Python package.</summary></entry><entry><title type="html">Bayesian size-and-shape regression modelling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Bayesiansizeandshaperegressionmodelling.html" rel="alternate" type="text/html" title="Bayesian size-and-shape regression modelling" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Bayesiansizeandshaperegressionmodelling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Bayesiansizeandshaperegressionmodelling.html">&lt;p&gt;Building on Dryden et al. (2021), this note presents the Bayesian estimation of a regression model for size-and-shape response variables with Gaussian landmarks. Our proposal fits into the framework of Bayesian latent variable models and allows a highly flexible modelling framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.06661&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Antonio Di Noia, Gianluca Mastrantonio, Giovanna Jona Lasinio</name></author><category term="stat.ME" /><summary type="html">Building on Dryden et al. (2021), this note presents the Bayesian estimation of a regression model for size-and-shape response variables with Gaussian landmarks. Our proposal fits into the framework of Bayesian latent variable models and allows a highly flexible modelling framework.</summary></entry><entry><title type="html">Biomarker-Guided Adaptive Enrichment Design with Threshold Detection for Clinical Trials with Time-to-Event Outcome</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/BiomarkerGuidedAdaptiveEnrichmentDesignwithThresholdDetectionforClinicalTrialswithTimetoEventOutcome.html" rel="alternate" type="text/html" title="Biomarker-Guided Adaptive Enrichment Design with Threshold Detection for Clinical Trials with Time-to-Event Outcome" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/BiomarkerGuidedAdaptiveEnrichmentDesignwithThresholdDetectionforClinicalTrialswithTimetoEventOutcome</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/BiomarkerGuidedAdaptiveEnrichmentDesignwithThresholdDetectionforClinicalTrialswithTimetoEventOutcome.html">&lt;p&gt;Biomarker-guided designs are increasingly used to evaluate personalized treatments based on patients’ biomarker status in Phase II and III clinical trials. With adaptive enrichment, these designs can improve the efficiency of evaluating the treatment effect in biomarker-positive patients by increasing their proportion in the randomized trial. While time-to-event outcomes are often used as the primary endpoint to measure treatment effects for a new therapy in severe diseases like cancer and cardiovascular diseases, there is limited research on biomarker-guided adaptive enrichment trials in this context. Such trials almost always adopt hazard ratio methods for statistical measurement of treatment effects. In contrast, restricted mean survival time (RMST) has gained popularity for analyzing time-to-event outcomes because it offers more straightforward interpretations of treatment effects and does not require the proportional hazard assumption. This paper proposes a two-stage biomarker-guided adaptive RMST design with threshold detection and patient enrichment. We develop sophisticated methods for identifying the optimal biomarker threshold, treatment effect estimators in the biomarker-positive subgroup, and approaches for type I error rate, power analysis, and sample size calculation. We present a numerical example of re-designing an oncology trial. An extensive simulation study is conducted to evaluate the performance of the proposed design.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06426&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kaiyuan Hua, Hwanhee Hong, Xiaofei Wang</name></author><category term="stat.ME" /><summary type="html">Biomarker-guided designs are increasingly used to evaluate personalized treatments based on patients’ biomarker status in Phase II and III clinical trials. With adaptive enrichment, these designs can improve the efficiency of evaluating the treatment effect in biomarker-positive patients by increasing their proportion in the randomized trial. While time-to-event outcomes are often used as the primary endpoint to measure treatment effects for a new therapy in severe diseases like cancer and cardiovascular diseases, there is limited research on biomarker-guided adaptive enrichment trials in this context. Such trials almost always adopt hazard ratio methods for statistical measurement of treatment effects. In contrast, restricted mean survival time (RMST) has gained popularity for analyzing time-to-event outcomes because it offers more straightforward interpretations of treatment effects and does not require the proportional hazard assumption. This paper proposes a two-stage biomarker-guided adaptive RMST design with threshold detection and patient enrichment. We develop sophisticated methods for identifying the optimal biomarker threshold, treatment effect estimators in the biomarker-positive subgroup, and approaches for type I error rate, power analysis, and sample size calculation. We present a numerical example of re-designing an oncology trial. An extensive simulation study is conducted to evaluate the performance of the proposed design.</summary></entry><entry><title type="html">Calibrated sensitivity models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Calibratedsensitivitymodels.html" rel="alternate" type="text/html" title="Calibrated sensitivity models" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Calibratedsensitivitymodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Calibratedsensitivitymodels.html">&lt;p&gt;In causal inference, sensitivity models assess how unmeasured confounders could alter causal analyses, but the sensitivity parameter – which quantifies the degree of unmeasured confounding – is often difficult to interpret. For this reason, researchers sometimes compare the sensitivity parameter to an estimate for measured confounding. This is known as calibration. Although calibration can aid interpretation, it is typically conducted post hoc, and uncertainty in the point estimate for measured confounding is rarely accounted for. To address these limitations, we propose novel calibrated sensitivity models, which directly bound the degree of unmeasured confounding by a multiple of measured confounding. The calibrated sensitivity parameter is interpretable as an intuitive unit-less ratio of unmeasured to measured confounding, and uncertainty due to estimating measured confounding can be incorporated. Incorporating this uncertainty shows causal analyses can be less or more robust to unmeasured confounding than would have been suggested by standard approaches. We develop efficient estimators and inferential methods for bounds on the average treatment effect with three calibrated sensitivity models, establishing parametric efficiency and asymptotic normality under doubly robust style nonparametric conditions. We illustrate our methods with a data analysis of the effect of mothers’ smoking on infant birthweight.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08738&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alec McClean, Zach Branson, Edward H. Kennedy</name></author><category term="stat.ME" /><summary type="html">In causal inference, sensitivity models assess how unmeasured confounders could alter causal analyses, but the sensitivity parameter – which quantifies the degree of unmeasured confounding – is often difficult to interpret. For this reason, researchers sometimes compare the sensitivity parameter to an estimate for measured confounding. This is known as calibration. Although calibration can aid interpretation, it is typically conducted post hoc, and uncertainty in the point estimate for measured confounding is rarely accounted for. To address these limitations, we propose novel calibrated sensitivity models, which directly bound the degree of unmeasured confounding by a multiple of measured confounding. The calibrated sensitivity parameter is interpretable as an intuitive unit-less ratio of unmeasured to measured confounding, and uncertainty due to estimating measured confounding can be incorporated. Incorporating this uncertainty shows causal analyses can be less or more robust to unmeasured confounding than would have been suggested by standard approaches. We develop efficient estimators and inferential methods for bounds on the average treatment effect with three calibrated sensitivity models, establishing parametric efficiency and asymptotic normality under doubly robust style nonparametric conditions. We illustrate our methods with a data analysis of the effect of mothers’ smoking on infant birthweight.</summary></entry><entry><title type="html">Capturing the Macroscopic Behaviour of Molecular Dynamics with Membership Functions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CapturingtheMacroscopicBehaviourofMolecularDynamicswithMembershipFunctions.html" rel="alternate" type="text/html" title="Capturing the Macroscopic Behaviour of Molecular Dynamics with Membership Functions" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CapturingtheMacroscopicBehaviourofMolecularDynamicswithMembershipFunctions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CapturingtheMacroscopicBehaviourofMolecularDynamicswithMembershipFunctions.html">&lt;p&gt;Markov processes serve as foundational models in many scientific disciplines, such as molecular dynamics, and their simulation forms a common basis for analysis. While simulations produce useful trajectories, obtaining macroscopic information directly from microstate data presents significant challenges. This paper addresses this gap by introducing the concept of membership functions being the macrostates themselves. We derive equations for the holding times of these macrostates and demonstrate their consistency with the classical definition. Furthermore, we discuss the application of the ISOKANN method for learning these quantities from simulation data. In addition, we present a novel method for extracting transition paths from simulations based on the ISOKANN results and demonstrate its efficacy by applying it to simulations of the {\mu}-opioid receptor. With this approach we provide a new perspective on the analysis of macroscopic behaviour of Markov systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.10523&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexander Sikorski, Robert Julian Rabben, Surahit Chewle, Marcus Weber</name></author><category term="stat.ME" /><summary type="html">Markov processes serve as foundational models in many scientific disciplines, such as molecular dynamics, and their simulation forms a common basis for analysis. While simulations produce useful trajectories, obtaining macroscopic information directly from microstate data presents significant challenges. This paper addresses this gap by introducing the concept of membership functions being the macrostates themselves. We derive equations for the holding times of these macrostates and demonstrate their consistency with the classical definition. Furthermore, we discuss the application of the ISOKANN method for learning these quantities from simulation data. In addition, we present a novel method for extracting transition paths from simulations based on the ISOKANN results and demonstrate its efficacy by applying it to simulations of the {\mu}-opioid receptor. With this approach we provide a new perspective on the analysis of macroscopic behaviour of Markov systems.</summary></entry><entry><title type="html">Causal Discovery over High-Dimensional Structured Hypothesis Spaces with Causal Graph Partitioning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CausalDiscoveryoverHighDimensionalStructuredHypothesisSpaceswithCausalGraphPartitioning.html" rel="alternate" type="text/html" title="Causal Discovery over High-Dimensional Structured Hypothesis Spaces with Causal Graph Partitioning" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CausalDiscoveryoverHighDimensionalStructuredHypothesisSpaceswithCausalGraphPartitioning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CausalDiscoveryoverHighDimensionalStructuredHypothesisSpaceswithCausalGraphPartitioning.html">&lt;p&gt;The aim in many sciences is to understand the mechanisms that underlie the observed distribution of variables, starting from a set of initial hypotheses. Causal discovery allows us to infer mechanisms as sets of cause and effect relationships in a generalized way – without necessarily tailoring to a specific domain. Causal discovery algorithms search over a structured hypothesis space, defined by the set of directed acyclic graphs, to find the graph that best explains the data. For high-dimensional problems, however, this search becomes intractable and scalable algorithms for causal discovery are needed to bridge the gap. In this paper, we define a novel causal graph partition that allows for divide-and-conquer causal discovery with theoretical guarantees. We leverage the idea of a superstructure – a set of learned or existing candidate hypotheses – to partition the search space. We prove under certain assumptions that learning with a causal graph partition always yields the Markov Equivalence Class of the true causal graph. We show our algorithm achieves comparable accuracy and a faster time to solution for biologically-tuned synthetic networks and networks up to ${10^4}$ variables. This makes our method applicable to gene regulatory network inference and other domains with high-dimensional structured hypothesis spaces.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06348&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ashka Shah, Adela DePavia, Nathaniel Hudson, Ian Foster, Rick Stevens</name></author><category term="stat.ME" /><summary type="html">The aim in many sciences is to understand the mechanisms that underlie the observed distribution of variables, starting from a set of initial hypotheses. Causal discovery allows us to infer mechanisms as sets of cause and effect relationships in a generalized way – without necessarily tailoring to a specific domain. Causal discovery algorithms search over a structured hypothesis space, defined by the set of directed acyclic graphs, to find the graph that best explains the data. For high-dimensional problems, however, this search becomes intractable and scalable algorithms for causal discovery are needed to bridge the gap. In this paper, we define a novel causal graph partition that allows for divide-and-conquer causal discovery with theoretical guarantees. We leverage the idea of a superstructure – a set of learned or existing candidate hypotheses – to partition the search space. We prove under certain assumptions that learning with a causal graph partition always yields the Markov Equivalence Class of the true causal graph. We show our algorithm achieves comparable accuracy and a faster time to solution for biologically-tuned synthetic networks and networks up to ${10^4}$ variables. This makes our method applicable to gene regulatory network inference and other domains with high-dimensional structured hypothesis spaces.</summary></entry><entry><title type="html">Causal Inference with Cocycles</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CausalInferencewithCocycles.html" rel="alternate" type="text/html" title="Causal Inference with Cocycles" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CausalInferencewithCocycles</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CausalInferencewithCocycles.html">&lt;p&gt;Many interventions in causal inference can be represented as transformations. We identify a local symmetry property satisfied by a large class of causal models under such interventions. Where present, this symmetry can be characterized by a type of map called a cocycle, an object that is central to dynamical systems theory. We show that such cocycles exist under general conditions and are sufficient to identify interventional and counterfactual distributions. We use these results to derive cocycle-based estimators for causal estimands and show they achieve semiparametric efficiency under typical conditions. Since (infinitely) many distributions can share the same cocycle, these estimators make causal inference robust to mis-specification by sidestepping superfluous modelling assumptions. We demonstrate both robustness and state-of-the-art performance in several simulations, and apply our method to estimate the effects of 401(k) pension plan eligibility on asset accumulation using a real dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13844&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hugh Dance, Benjamin Bloem-Reddy</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Many interventions in causal inference can be represented as transformations. We identify a local symmetry property satisfied by a large class of causal models under such interventions. Where present, this symmetry can be characterized by a type of map called a cocycle, an object that is central to dynamical systems theory. We show that such cocycles exist under general conditions and are sufficient to identify interventional and counterfactual distributions. We use these results to derive cocycle-based estimators for causal estimands and show they achieve semiparametric efficiency under typical conditions. Since (infinitely) many distributions can share the same cocycle, these estimators make causal inference robust to mis-specification by sidestepping superfluous modelling assumptions. We demonstrate both robustness and state-of-the-art performance in several simulations, and apply our method to estimate the effects of 401(k) pension plan eligibility on asset accumulation using a real dataset.</summary></entry><entry><title type="html">Causal Interpretation of Regressions With Ranks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CausalInterpretationofRegressionsWithRanks.html" rel="alternate" type="text/html" title="Causal Interpretation of Regressions With Ranks" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CausalInterpretationofRegressionsWithRanks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CausalInterpretationofRegressionsWithRanks.html">&lt;p&gt;In studies of educational production functions or intergenerational mobility, it is common to transform the key variables into percentile ranks. Yet, it remains unclear what the regression coefficient estimates with ranks of the outcome or the treatment. In this paper, we derive effective causal estimands for a broad class of commonly-used regression methods, including the ordinary least squares (OLS), two-stage least squares (2SLS), difference-in-differences (DiD), and regression discontinuity designs (RDD). Specifically, we introduce a novel primitive causal estimand, the Rank Average Treatment Effect (rank-ATE), and prove that it serves as the building block of the effective estimands of all the aforementioned econometrics methods. For 2SLS, DiD, and RDD, we show that direct applications to outcome ranks identify parameters that are difficult to interpret. To address this issue, we develop alternative methods to identify more interpretable causal parameters.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05548&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lihua Lei</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In studies of educational production functions or intergenerational mobility, it is common to transform the key variables into percentile ranks. Yet, it remains unclear what the regression coefficient estimates with ranks of the outcome or the treatment. In this paper, we derive effective causal estimands for a broad class of commonly-used regression methods, including the ordinary least squares (OLS), two-stage least squares (2SLS), difference-in-differences (DiD), and regression discontinuity designs (RDD). Specifically, we introduce a novel primitive causal estimand, the Rank Average Treatment Effect (rank-ATE), and prove that it serves as the building block of the effective estimands of all the aforementioned econometrics methods. For 2SLS, DiD, and RDD, we show that direct applications to outcome ranks identify parameters that are difficult to interpret. To address this issue, we develop alternative methods to identify more interpretable causal parameters.</summary></entry><entry><title type="html">Classification of multivariate functional data on different domains with Partial Least Squares approaches</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ClassificationofmultivariatefunctionaldataondifferentdomainswithPartialLeastSquaresapproaches.html" rel="alternate" type="text/html" title="Classification of multivariate functional data on different domains with Partial Least Squares approaches" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ClassificationofmultivariatefunctionaldataondifferentdomainswithPartialLeastSquaresapproaches</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ClassificationofmultivariatefunctionaldataondifferentdomainswithPartialLeastSquaresapproaches.html">&lt;p&gt;Classification (supervised-learning) of multivariate functional data is considered when the elements of the random functional vector of interest are defined on different domains. In this setting, PLS classification and tree PLS-based methods for multivariate functional data are presented. From a computational point of view, we show that the PLS components of the regression with multivariate functional data can be obtained using only the PLS methodology with univariate functional data. This offers an alternative way to present the PLS algorithm for multivariate functional data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.09145&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Issam-Ali Moindjie, Sophie Dabo-Niang, Cristian Preda</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Classification (supervised-learning) of multivariate functional data is considered when the elements of the random functional vector of interest are defined on different domains. In this setting, PLS classification and tree PLS-based methods for multivariate functional data are presented. From a computational point of view, we show that the PLS components of the regression with multivariate functional data can be obtained using only the PLS methodology with univariate functional data. This offers an alternative way to present the PLS algorithm for multivariate functional data.</summary></entry><entry><title type="html">Computationally efficient permutation tests for the multivariate two-sample problem based on energy distance or maximum mean discrepancy statistics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Computationallyefficientpermutationtestsforthemultivariatetwosampleproblembasedonenergydistanceormaximummeandiscrepancystatistics.html" rel="alternate" type="text/html" title="Computationally efficient permutation tests for the multivariate two-sample problem based on energy distance or maximum mean discrepancy statistics" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Computationallyefficientpermutationtestsforthemultivariatetwosampleproblembasedonenergydistanceormaximummeandiscrepancystatistics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Computationallyefficientpermutationtestsforthemultivariatetwosampleproblembasedonenergydistanceormaximummeandiscrepancystatistics.html">&lt;p&gt;Non-parametric two-sample tests based on energy distance or maximum mean discrepancy are widely used statistical tests for comparing multivariate data from two populations. While these tests enjoy desirable statistical properties, their test statistics can be expensive to compute as they require the computation of 3 distinct Euclidean distance (or kernel) matrices between samples, where the time complexity of each of these computations (namely, $O(n_{x}^2 p)$, $O(n_{y}^2 p)$, and $O(n_{x} n_{y} p)$) scales quadratically with the number of samples ($n_x$, $n_y$) and linearly with the number of variables ($p$). Since the standard permutation test requires repeated re-computations of these expensive statistics it’s application to large datasets can become unfeasible. While several statistical approaches have been proposed to mitigate this issue, they all sacrifice desirable statistical properties to decrease the computational cost (e.g., trade computation speed by a decrease in statistical power). A better computational strategy is to first pre-compute the Euclidean distance (kernel) matrix of the concatenated data, and then permute indexes and retrieve the corresponding elements to compute the re-sampled statistics. While this strategy can reduce the computation cost relative to the standard permutation test, it relies on the computation of a larger Euclidean distance (kernel) matrix with complexity $O((n_x + n_y)^2 p)$. In this paper, we present a novel computationally efficient permutation algorithm which only requires the pre-computation of the 3 smaller matrices and achieves large computational speedups without sacrificing finite-sample validity or statistical power. We illustrate its computational gains in a series of experiments and compare its statistical power to the current state-of-the-art approach for balancing computational cost and statistical performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06488&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elias Chaibub Neto</name></author><category term="stat.CO" /><summary type="html">Non-parametric two-sample tests based on energy distance or maximum mean discrepancy are widely used statistical tests for comparing multivariate data from two populations. While these tests enjoy desirable statistical properties, their test statistics can be expensive to compute as they require the computation of 3 distinct Euclidean distance (or kernel) matrices between samples, where the time complexity of each of these computations (namely, $O(n_{x}^2 p)$, $O(n_{y}^2 p)$, and $O(n_{x} n_{y} p)$) scales quadratically with the number of samples ($n_x$, $n_y$) and linearly with the number of variables ($p$). Since the standard permutation test requires repeated re-computations of these expensive statistics it’s application to large datasets can become unfeasible. While several statistical approaches have been proposed to mitigate this issue, they all sacrifice desirable statistical properties to decrease the computational cost (e.g., trade computation speed by a decrease in statistical power). A better computational strategy is to first pre-compute the Euclidean distance (kernel) matrix of the concatenated data, and then permute indexes and retrieve the corresponding elements to compute the re-sampled statistics. While this strategy can reduce the computation cost relative to the standard permutation test, it relies on the computation of a larger Euclidean distance (kernel) matrix with complexity $O((n_x + n_y)^2 p)$. In this paper, we present a novel computationally efficient permutation algorithm which only requires the pre-computation of the 3 smaller matrices and achieves large computational speedups without sacrificing finite-sample validity or statistical power. We illustrate its computational gains in a series of experiments and compare its statistical power to the current state-of-the-art approach for balancing computational cost and statistical performance.</summary></entry><entry><title type="html">Confidence Intervals for the F1 Score: A Comparison of Four Methods</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ConfidenceIntervalsfortheF1ScoreAComparisonofFourMethods.html" rel="alternate" type="text/html" title="Confidence Intervals for the F1 Score: A Comparison of Four Methods" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ConfidenceIntervalsfortheF1ScoreAComparisonofFourMethods</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ConfidenceIntervalsfortheF1ScoreAComparisonofFourMethods.html">&lt;p&gt;In Natural Language Processing (NLP), binary classification algorithms are often evaluated using the F1 score. Because the sample F1 score is an estimate of the population F1 score, it is not sufficient to report the sample F1 score without an indication of how accurate it is. Confidence intervals are an indication of how accurate the sample F1 score is. However, most studies either do not report them or report them using methods that demonstrate poor statistical properties. In the present study, I review current analytical methods (i.e., Clopper-Pearson method and Wald method) to construct confidence intervals for the population F1 score, propose two new analytical methods (i.e., Wilson direct method and Wilson indirect method) to do so, and compare these methods based on their coverage probabilities and interval lengths, as well as whether these methods suffer from overshoot and degeneracy. Theoretical results demonstrate that both proposed methods do not suffer from overshoot and degeneracy. Experimental results suggest that both proposed methods perform better, as compared to current methods, in terms of coverage probabilities and interval lengths. I illustrate both current and proposed methods on two suggestion mining tasks. I discuss the practical implications of these results, and suggest areas for future research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.14621&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kevin Fu Yuan Lam, Vikneswaran Gopal, Jiang Qian</name></author><category term="stat.ME" /><summary type="html">In Natural Language Processing (NLP), binary classification algorithms are often evaluated using the F1 score. Because the sample F1 score is an estimate of the population F1 score, it is not sufficient to report the sample F1 score without an indication of how accurate it is. Confidence intervals are an indication of how accurate the sample F1 score is. However, most studies either do not report them or report them using methods that demonstrate poor statistical properties. In the present study, I review current analytical methods (i.e., Clopper-Pearson method and Wald method) to construct confidence intervals for the population F1 score, propose two new analytical methods (i.e., Wilson direct method and Wilson indirect method) to do so, and compare these methods based on their coverage probabilities and interval lengths, as well as whether these methods suffer from overshoot and degeneracy. Theoretical results demonstrate that both proposed methods do not suffer from overshoot and degeneracy. Experimental results suggest that both proposed methods perform better, as compared to current methods, in terms of coverage probabilities and interval lengths. I illustrate both current and proposed methods on two suggestion mining tasks. I discuss the practical implications of these results, and suggest areas for future research.</summary></entry><entry><title type="html">Constrained Design of a Binary Instrument in a Partially Linear Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ConstrainedDesignofaBinaryInstrumentinaPartiallyLinearModel.html" rel="alternate" type="text/html" title="Constrained Design of a Binary Instrument in a Partially Linear Model" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ConstrainedDesignofaBinaryInstrumentinaPartiallyLinearModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ConstrainedDesignofaBinaryInstrumentinaPartiallyLinearModel.html">&lt;p&gt;We study the question of how best to assign an encouragement in a randomized encouragement study. In our setting, units arrive with covariates, receive a nudge toward treatment or control, acquire one of those statuses in a way that need not align with the nudge, and finally have a response observed. The nudge can be seen as a binary instrument that affects the response only via the treatment status. Our goal is to assign the nudge as a function of covariates in a way that best estimates the local average treatment effect (LATE). We assume a partially linear model, wherein the baseline model is non-parametric and the treatment term is linear in the covariates. Under this model, we outline a two-stage procedure to consistently estimate the LATE. Though the variance of the LATE is intractable, we derive a finite sample approximation and thus a design criterion to minimize. This criterion is convex, allowing for constraints that might arise for budgetary or ethical reasons. We prove conditions under which our solution asymptotically recovers the lowest true variance among all possible nudge propensities. We apply our method to a semi-synthetic example involving triage in an emergency department and find significant gains relative to a regression discontinuity design.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05592&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tim Morrison, Minh Nguyen, Michael Baiocchi, Art B. Owen</name></author><category term="stat.ME" /><summary type="html">We study the question of how best to assign an encouragement in a randomized encouragement study. In our setting, units arrive with covariates, receive a nudge toward treatment or control, acquire one of those statuses in a way that need not align with the nudge, and finally have a response observed. The nudge can be seen as a binary instrument that affects the response only via the treatment status. Our goal is to assign the nudge as a function of covariates in a way that best estimates the local average treatment effect (LATE). We assume a partially linear model, wherein the baseline model is non-parametric and the treatment term is linear in the covariates. Under this model, we outline a two-stage procedure to consistently estimate the LATE. Though the variance of the LATE is intractable, we derive a finite sample approximation and thus a design criterion to minimize. This criterion is convex, allowing for constraints that might arise for budgetary or ethical reasons. We prove conditions under which our solution asymptotically recovers the lowest true variance among all possible nudge propensities. We apply our method to a semi-synthetic example involving triage in an emergency department and find significant gains relative to a regression discontinuity design.</summary></entry><entry><title type="html">Cross-sectional shape analysis for risk assessment and prognosis of patients with true lumen narrowing after type-A aortic dissection surgery</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CrosssectionalshapeanalysisforriskassessmentandprognosisofpatientswithtruelumennarrowingaftertypeAaorticdissectionsurgery.html" rel="alternate" type="text/html" title="Cross-sectional shape analysis for risk assessment and prognosis of patients with true lumen narrowing after type-A aortic dissection surgery" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CrosssectionalshapeanalysisforriskassessmentandprognosisofpatientswithtruelumennarrowingaftertypeAaorticdissectionsurgery</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/CrosssectionalshapeanalysisforriskassessmentandprognosisofpatientswithtruelumennarrowingaftertypeAaorticdissectionsurgery.html">&lt;p&gt;Background: For acute type-A aortic dissection (ATAAD) surgery, early post-surgery assessment is crucially important for effective treatment plans, underscoring the need for a framework to identify the risk level of aortic dissection cases. We examined true-lumen narrowing during follow-up examinations, collected morphological data 14 days (early stages) after surgery, and assessed patient risk levels over 2.8 years.
  Purpose: To establish an implementable framework supported by mathematical techniques to predict the risk of aortic dissection patients experiencing true-lumen narrowing after ATAAD surgery.
  Materials and Methods: This retrospective study analyzed CT data from 21 ATAAD patients. Forty uniformly distributed cross-sectional shapes (CSSs) are derived from each lumen to account for gradual changes in shape. We introduced the form factor (FF) to assess CSS morphology. Linear discriminant analysis (LDA) is used for the risk classification of aortic dissection patients. Leave-one-patient-out cross-validation (LOPO-CV) is used for risk prediction.
  Results: For this investigation, we examined data of 21 ATAAD patients categorized into high-risk, medium-risk, and low-risk cases based on clinical observations of the range of true-lumen narrowing. Our risk classification machine-learning (ML) model preserving the model’s generalizability. The model’s predictions reliably identified low-risk patients, thereby potentially reducing hospital visits. It also demonstrated proficiency in accurately predicting the risk for all high-risk patients.
  Conclusion: The suggested method anticipates the risk linked to aortic enlargement in patients with a narrowing true lumen in the early stage following ATAAD surgery, thereby aiding follow-up doctors in enhancing patient care.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05173&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>J V Ramana Reddy , Toshitaka Watanabe , Taro Hayashi , Hiroshi Suito</name></author><category term="stat.AP" /><summary type="html">Background: For acute type-A aortic dissection (ATAAD) surgery, early post-surgery assessment is crucially important for effective treatment plans, underscoring the need for a framework to identify the risk level of aortic dissection cases. We examined true-lumen narrowing during follow-up examinations, collected morphological data 14 days (early stages) after surgery, and assessed patient risk levels over 2.8 years. Purpose: To establish an implementable framework supported by mathematical techniques to predict the risk of aortic dissection patients experiencing true-lumen narrowing after ATAAD surgery. Materials and Methods: This retrospective study analyzed CT data from 21 ATAAD patients. Forty uniformly distributed cross-sectional shapes (CSSs) are derived from each lumen to account for gradual changes in shape. We introduced the form factor (FF) to assess CSS morphology. Linear discriminant analysis (LDA) is used for the risk classification of aortic dissection patients. Leave-one-patient-out cross-validation (LOPO-CV) is used for risk prediction. Results: For this investigation, we examined data of 21 ATAAD patients categorized into high-risk, medium-risk, and low-risk cases based on clinical observations of the range of true-lumen narrowing. Our risk classification machine-learning (ML) model preserving the model’s generalizability. The model’s predictions reliably identified low-risk patients, thereby potentially reducing hospital visits. It also demonstrated proficiency in accurately predicting the risk for all high-risk patients. Conclusion: The suggested method anticipates the risk linked to aortic enlargement in patients with a narrowing true lumen in the early stage following ATAAD surgery, thereby aiding follow-up doctors in enhancing patient care.</summary></entry><entry><title type="html">Distribution-Free Predictive Inference under Unknown Temporal Drift</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/DistributionFreePredictiveInferenceunderUnknownTemporalDrift.html" rel="alternate" type="text/html" title="Distribution-Free Predictive Inference under Unknown Temporal Drift" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/DistributionFreePredictiveInferenceunderUnknownTemporalDrift</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/DistributionFreePredictiveInferenceunderUnknownTemporalDrift.html">&lt;p&gt;Distribution-free prediction sets play a pivotal role in uncertainty quantification for complex statistical models. Their validity hinges on reliable calibration data, which may not be readily available as real-world environments often undergo unknown changes over time. In this paper, we propose a strategy for choosing an adaptive window and use the data therein to construct prediction sets. The window is selected by optimizing an estimated bias-variance tradeoff. We provide sharp coverage guarantees for our method, showing its adaptivity to the underlying temporal drift. We also illustrate its efficacy through numerical experiments on synthetic and real data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06516&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elise Han, Chengpiao Huang, Kaizheng Wang</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Distribution-free prediction sets play a pivotal role in uncertainty quantification for complex statistical models. Their validity hinges on reliable calibration data, which may not be readily available as real-world environments often undergo unknown changes over time. In this paper, we propose a strategy for choosing an adaptive window and use the data therein to construct prediction sets. The window is selected by optimizing an estimated bias-variance tradeoff. We provide sharp coverage guarantees for our method, showing its adaptivity to the underlying temporal drift. We also illustrate its efficacy through numerical experiments on synthetic and real data.</summary></entry><entry><title type="html">Dynamic Contextual Pricing with Doubly Non-Parametric Random Utility Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/DynamicContextualPricingwithDoublyNonParametricRandomUtilityModels.html" rel="alternate" type="text/html" title="Dynamic Contextual Pricing with Doubly Non-Parametric Random Utility Models" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/DynamicContextualPricingwithDoublyNonParametricRandomUtilityModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/DynamicContextualPricingwithDoublyNonParametricRandomUtilityModels.html">&lt;p&gt;In the evolving landscape of digital commerce, adaptive dynamic pricing strategies are essential for gaining a competitive edge. This paper introduces novel {\em doubly nonparametric random utility models} that eschew traditional parametric assumptions used in estimating consumer demand’s mean utility function and noise distribution. Existing nonparametric methods like multi-scale {\em Distributional Nearest Neighbors (DNN and TDNN)}, initially designed for offline regression, face challenges in dynamic online pricing due to design limitations, such as the indirect observability of utility-related variables and the absence of uniform convergence guarantees. We address these challenges with innovative population equations that facilitate nonparametric estimation within decision-making frameworks and establish new analytical results on the uniform convergence rates of DNN and TDNN, enhancing their applicability in dynamic environments.
  Our theoretical analysis confirms that the statistical learning rates for the mean utility function and noise distribution are minimax optimal. We also derive a regret bound that illustrates the critical interaction between model dimensionality and noise distribution smoothness, deepening our understanding of dynamic pricing under varied market conditions. These contributions offer substantial theoretical insights and practical tools for implementing effective, data-driven pricing strategies, advancing the theoretical framework of pricing models and providing robust methodologies for navigating the complexities of modern markets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06866&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elynn Chen, Xi Chen, Lan Gao, Jiayu Li</name></author><category term="stat.ME" /><summary type="html">In the evolving landscape of digital commerce, adaptive dynamic pricing strategies are essential for gaining a competitive edge. This paper introduces novel {\em doubly nonparametric random utility models} that eschew traditional parametric assumptions used in estimating consumer demand’s mean utility function and noise distribution. Existing nonparametric methods like multi-scale {\em Distributional Nearest Neighbors (DNN and TDNN)}, initially designed for offline regression, face challenges in dynamic online pricing due to design limitations, such as the indirect observability of utility-related variables and the absence of uniform convergence guarantees. We address these challenges with innovative population equations that facilitate nonparametric estimation within decision-making frameworks and establish new analytical results on the uniform convergence rates of DNN and TDNN, enhancing their applicability in dynamic environments. Our theoretical analysis confirms that the statistical learning rates for the mean utility function and noise distribution are minimax optimal. We also derive a regret bound that illustrates the critical interaction between model dimensionality and noise distribution smoothness, deepening our understanding of dynamic pricing under varied market conditions. These contributions offer substantial theoretical insights and practical tools for implementing effective, data-driven pricing strategies, advancing the theoretical framework of pricing models and providing robust methodologies for navigating the complexities of modern markets.</summary></entry><entry><title type="html">Early School Leaving in Spain: a longitudinal analysis by gender</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EarlySchoolLeavinginSpainalongitudinalanalysisbygender.html" rel="alternate" type="text/html" title="Early School Leaving in Spain: a longitudinal analysis by gender" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EarlySchoolLeavinginSpainalongitudinalanalysisbygender</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EarlySchoolLeavinginSpainalongitudinalanalysisbygender.html">&lt;p&gt;Spain is one of the eight EU-27 countries that failed to reduce early school leaving (ESL) below 10% in 2020, and now faces the challenge of achieving a rate below 9% by 2030. The determinants of this phenomenon are usually studied using cross-sectional data at the micro-level and without differentiation by gender. In this study, we analyse it for the first time for Spain using panel data (between 2002-2020), taking into account the high regional inequalities at the macroeconomic level and the masculinisation of the phenomenon. The results show a positive relationship between ESL and socioeconomic variables such as the adolescent fertility rate, immigration, unemployment or the weight of the industrial and construction sectors in the regional economy, with significant gender differences that invite us to discuss educational policies. Surprisingly, youth unemployment has only small but significant impact on female ESL.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05172&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Martín Martín-González, Sara M. González-Betancor, Carmen Pérez-Esparrells</name></author><category term="stat.AP" /><summary type="html">Spain is one of the eight EU-27 countries that failed to reduce early school leaving (ESL) below 10% in 2020, and now faces the challenge of achieving a rate below 9% by 2030. The determinants of this phenomenon are usually studied using cross-sectional data at the micro-level and without differentiation by gender. In this study, we analyse it for the first time for Spain using panel data (between 2002-2020), taking into account the high regional inequalities at the macroeconomic level and the masculinisation of the phenomenon. The results show a positive relationship between ESL and socioeconomic variables such as the adolescent fertility rate, immigration, unemployment or the weight of the industrial and construction sectors in the regional economy, with significant gender differences that invite us to discuss educational policies. Surprisingly, youth unemployment has only small but significant impact on female ESL.</summary></entry><entry><title type="html">Effective Causal Discovery under Identifiable Heteroscedastic Noise Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EffectiveCausalDiscoveryunderIdentifiableHeteroscedasticNoiseModel.html" rel="alternate" type="text/html" title="Effective Causal Discovery under Identifiable Heteroscedastic Noise Model" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EffectiveCausalDiscoveryunderIdentifiableHeteroscedasticNoiseModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EffectiveCausalDiscoveryunderIdentifiableHeteroscedasticNoiseModel.html">&lt;p&gt;Capturing the underlying structural causal relations represented by Directed Acyclic Graphs (DAGs) has been a fundamental task in various AI disciplines. Causal DAG learning via the continuous optimization framework has recently achieved promising performance in terms of both accuracy and efficiency. However, most methods make strong assumptions of homoscedastic noise, i.e., exogenous noises have equal variances across variables, observations, or even both. The noises in real data usually violate both assumptions due to the biases introduced by different data collection processes. To address the issue of heteroscedastic noise, we introduce relaxed and implementable sufficient conditions, proving the identifiability of a general class of SEM subject to these conditions. Based on the identifiable general SEM, we propose a novel formulation for DAG learning that accounts for the variation in noise variance across variables and observations. We then propose an effective two-phase iterative DAG learning algorithm to address the increasing optimization difficulties and to learn a causal DAG from data with heteroscedastic variable noise under varying variance. We show significant empirical gains of the proposed approaches over state-of-the-art methods on both synthetic data and real data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.12844&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Naiyu Yin, Tian Gao, Yue Yu, Qiang Ji</name></author><category term="stat.ME" /><summary type="html">Capturing the underlying structural causal relations represented by Directed Acyclic Graphs (DAGs) has been a fundamental task in various AI disciplines. Causal DAG learning via the continuous optimization framework has recently achieved promising performance in terms of both accuracy and efficiency. However, most methods make strong assumptions of homoscedastic noise, i.e., exogenous noises have equal variances across variables, observations, or even both. The noises in real data usually violate both assumptions due to the biases introduced by different data collection processes. To address the issue of heteroscedastic noise, we introduce relaxed and implementable sufficient conditions, proving the identifiability of a general class of SEM subject to these conditions. Based on the identifiable general SEM, we propose a novel formulation for DAG learning that accounts for the variation in noise variance across variables and observations. We then propose an effective two-phase iterative DAG learning algorithm to address the increasing optimization difficulties and to learn a causal DAG from data with heteroscedastic variable noise under varying variance. We show significant empirical gains of the proposed approaches over state-of-the-art methods on both synthetic data and real data.</summary></entry><entry><title type="html">Efficient algorithms for the sensitivities of the Pearson correlation coefficient and its statistical significance to online data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EfficientalgorithmsforthesensitivitiesofthePearsoncorrelationcoefficientanditsstatisticalsignificancetoonlinedata.html" rel="alternate" type="text/html" title="Efficient algorithms for the sensitivities of the Pearson correlation coefficient and its statistical significance to online data" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EfficientalgorithmsforthesensitivitiesofthePearsoncorrelationcoefficientanditsstatisticalsignificancetoonlinedata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EfficientalgorithmsforthesensitivitiesofthePearsoncorrelationcoefficientanditsstatisticalsignificancetoonlinedata.html">&lt;p&gt;Reliably measuring the collinearity of bivariate data is crucial in statistics, particularly for time-series analysis or ongoing studies in which incoming observations can significantly impact current collinearity estimates. Leveraging identities from Welford’s online algorithm for sample variance, we develop a rigorous theoretical framework for analyzing the maximal change to the Pearson correlation coefficient and its p-value that can be induced by additional data. Further, we show that the resulting optimization problems yield elegant closed-form solutions that can be accurately computed by linear- and constant-time algorithms. Our work not only creates new theoretical avenues for robust correlation measures, but also has broad practical implications for disciplines that span econometrics, operations research, clinical trials, climatology, differential privacy, and bioinformatics. Software implementations of our algorithms in Cython-wrapped C are made available at https://github.com/marc-harary/sensitivity for reproducibility, practical deployment, and future theoretical development.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14686&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marc Harary</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Reliably measuring the collinearity of bivariate data is crucial in statistics, particularly for time-series analysis or ongoing studies in which incoming observations can significantly impact current collinearity estimates. Leveraging identities from Welford’s online algorithm for sample variance, we develop a rigorous theoretical framework for analyzing the maximal change to the Pearson correlation coefficient and its p-value that can be induced by additional data. Further, we show that the resulting optimization problems yield elegant closed-form solutions that can be accurately computed by linear- and constant-time algorithms. Our work not only creates new theoretical avenues for robust correlation measures, but also has broad practical implications for disciplines that span econometrics, operations research, clinical trials, climatology, differential privacy, and bioinformatics. Software implementations of our algorithms in Cython-wrapped C are made available at https://github.com/marc-harary/sensitivity for reproducibility, practical deployment, and future theoretical development.</summary></entry><entry><title type="html">Efficient and Multiply Robust Risk Estimation under General Forms of Dataset Shift</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EfficientandMultiplyRobustRiskEstimationunderGeneralFormsofDatasetShift.html" rel="alternate" type="text/html" title="Efficient and Multiply Robust Risk Estimation under General Forms of Dataset Shift" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EfficientandMultiplyRobustRiskEstimationunderGeneralFormsofDatasetShift</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EfficientandMultiplyRobustRiskEstimationunderGeneralFormsofDatasetShift.html">&lt;p&gt;Statistical machine learning methods often face the challenge of limited data available from the population of interest. One remedy is to leverage data from auxiliary source populations, which share some conditional distributions or are linked in other ways with the target domain. Techniques leveraging such \emph{dataset shift} conditions are known as \emph{domain adaptation} or \emph{transfer learning}. Despite extensive literature on dataset shift, limited works address how to efficiently use the auxiliary populations to improve the accuracy of risk evaluation for a given machine learning task in the target population.
  In this paper, we study the general problem of efficiently estimating target population risk under various dataset shift conditions, leveraging semiparametric efficiency theory. We consider a general class of dataset shift conditions, which includes three popular conditions – covariate, label and concept shift – as special cases. We allow for partially non-overlapping support between the source and target populations. We develop efficient and multiply robust estimators along with a straightforward specification test of these dataset shift conditions. We also derive efficiency bounds for two other dataset shift conditions, posterior drift and location-scale shift. Simulation studies support the efficiency gains due to leveraging plausible dataset shift conditions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.16406&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hongxiang Qiu, Eric Tchetgen Tchetgen, Edgar Dobriban</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Statistical machine learning methods often face the challenge of limited data available from the population of interest. One remedy is to leverage data from auxiliary source populations, which share some conditional distributions or are linked in other ways with the target domain. Techniques leveraging such \emph{dataset shift} conditions are known as \emph{domain adaptation} or \emph{transfer learning}. Despite extensive literature on dataset shift, limited works address how to efficiently use the auxiliary populations to improve the accuracy of risk evaluation for a given machine learning task in the target population. In this paper, we study the general problem of efficiently estimating target population risk under various dataset shift conditions, leveraging semiparametric efficiency theory. We consider a general class of dataset shift conditions, which includes three popular conditions – covariate, label and concept shift – as special cases. We allow for partially non-overlapping support between the source and target populations. We develop efficient and multiply robust estimators along with a straightforward specification test of these dataset shift conditions. We also derive efficiency bounds for two other dataset shift conditions, posterior drift and location-scale shift. Simulation studies support the efficiency gains due to leveraging plausible dataset shift conditions.</summary></entry><entry><title type="html">Embedding Network Autoregression for time series analysis and causal peer effect inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EmbeddingNetworkAutoregressionfortimeseriesanalysisandcausalpeereffectinference.html" rel="alternate" type="text/html" title="Embedding Network Autoregression for time series analysis and causal peer effect inference" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EmbeddingNetworkAutoregressionfortimeseriesanalysisandcausalpeereffectinference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EmbeddingNetworkAutoregressionfortimeseriesanalysisandcausalpeereffectinference.html">&lt;p&gt;We propose an Embedding Network Autoregressive Model (ENAR) for multivariate networked longitudinal data. We assume the network is generated from a latent variable model, and these unobserved variables are included in a structural peer effect model or a time series network autoregressive model as additive effects. This approach takes a unified view of two related problems, (1) modeling and predicting multivariate time series data and (2) causal peer influence estimation in the presence of homophily from finite time longitudinal data. Our estimation strategy comprises estimating latent factors from the observed network adjacency matrix either through spectral embedding or maximum likelihood estimation, followed by least squares estimation of the network autoregressive model. We show that the estimated momentum and peer effect parameters are consistent and asymptotically normal in asymptotic setups with a growing number of network vertices N while including a growing number of time points T and finite T cases. We allow the number of latent vectors K to grow at appropriate rates, which improves upon existing rates when such results are available for related models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05944&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jae Ho Chang, Subhadeep Paul</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We propose an Embedding Network Autoregressive Model (ENAR) for multivariate networked longitudinal data. We assume the network is generated from a latent variable model, and these unobserved variables are included in a structural peer effect model or a time series network autoregressive model as additive effects. This approach takes a unified view of two related problems, (1) modeling and predicting multivariate time series data and (2) causal peer influence estimation in the presence of homophily from finite time longitudinal data. Our estimation strategy comprises estimating latent factors from the observed network adjacency matrix either through spectral embedding or maximum likelihood estimation, followed by least squares estimation of the network autoregressive model. We show that the estimated momentum and peer effect parameters are consistent and asymptotically normal in asymptotic setups with a growing number of network vertices N while including a growing number of time points T and finite T cases. We allow the number of latent vectors K to grow at appropriate rates, which improves upon existing rates when such results are available for related models.</summary></entry><entry><title type="html">Estimating Heterogeneous Exposure Effects in the Case-Crossover Design using BART</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EstimatingHeterogeneousExposureEffectsintheCaseCrossoverDesignusingBART.html" rel="alternate" type="text/html" title="Estimating Heterogeneous Exposure Effects in the Case-Crossover Design using BART" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EstimatingHeterogeneousExposureEffectsintheCaseCrossoverDesignusingBART</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EstimatingHeterogeneousExposureEffectsintheCaseCrossoverDesignusingBART.html">&lt;p&gt;Epidemiological approaches for examining human health responses to environmental exposures in observational studies often control for confounding by implementing clever matching schemes and using statistical methods based on conditional likelihood. Nonparametric regression models have surged in popularity in recent years as a tool for estimating individual-level heterogeneous effects, which provide a more detailed picture of the exposure-response relationship but can also be aggregated to obtain improved marginal estimates at the population level. In this work we incorporate Bayesian additive regression trees (BART) into the conditional logistic regression model to identify heterogeneous exposure effects in a case-crossover design. Conditional logistic BART (CL-BART) utilizes reversible jump Markov chain Monte Carlo to bypass the conditional conjugacy requirement of the original BART algorithm. Our work is motivated by the growing interest in identifying subpopulations more vulnerable to environmental exposures. We apply CL-BART to a study of the impact of heat waves on people with Alzheimer’s disease in California and effect modification by other chronic conditions. Through this application, we also describe strategies to examine heterogeneous odds ratios through variable importance, partial dependence, and lower-dimensional summaries.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.12016&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jacob Englert, Stefanie Ebelt, Howard Chang</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Epidemiological approaches for examining human health responses to environmental exposures in observational studies often control for confounding by implementing clever matching schemes and using statistical methods based on conditional likelihood. Nonparametric regression models have surged in popularity in recent years as a tool for estimating individual-level heterogeneous effects, which provide a more detailed picture of the exposure-response relationship but can also be aggregated to obtain improved marginal estimates at the population level. In this work we incorporate Bayesian additive regression trees (BART) into the conditional logistic regression model to identify heterogeneous exposure effects in a case-crossover design. Conditional logistic BART (CL-BART) utilizes reversible jump Markov chain Monte Carlo to bypass the conditional conjugacy requirement of the original BART algorithm. Our work is motivated by the growing interest in identifying subpopulations more vulnerable to environmental exposures. We apply CL-BART to a study of the impact of heat waves on people with Alzheimer’s disease in California and effect modification by other chronic conditions. Through this application, we also describe strategies to examine heterogeneous odds ratios through variable importance, partial dependence, and lower-dimensional summaries.</summary></entry><entry><title type="html">Estimating Heterogeneous Treatment Effects by Combining Weak Instruments and Observational Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EstimatingHeterogeneousTreatmentEffectsbyCombiningWeakInstrumentsandObservationalData.html" rel="alternate" type="text/html" title="Estimating Heterogeneous Treatment Effects by Combining Weak Instruments and Observational Data" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EstimatingHeterogeneousTreatmentEffectsbyCombiningWeakInstrumentsandObservationalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EstimatingHeterogeneousTreatmentEffectsbyCombiningWeakInstrumentsandObservationalData.html">&lt;p&gt;Accurately predicting conditional average treatment effects (CATEs) is crucial in personalized medicine and digital platform analytics. Since often the treatments of interest cannot be directly randomized, observational data is leveraged to learn CATEs, but this approach can incur significant bias from unobserved confounding. One strategy to overcome these limitations is to seek latent quasi-experiments in instrumental variables (IVs) for the treatment, for example, a randomized intent to treat or a randomized product recommendation. This approach, on the other hand, can suffer from low compliance, i.e., IV weakness. Some subgroups may even exhibit zero compliance meaning we cannot instrument for their CATEs at all. In this paper we develop a novel approach to combine IV and observational data to enable reliable CATE estimation in the presence of unobserved confounding in the observational data and low compliance in the IV data, including no compliance for some subgroups. We propose a two-stage framework that first learns biased CATEs from the observational data, and then applies a compliance-weighted correction using IV data, effectively leveraging IV strength variability across covariates. We characterize the convergence rates of our method and validate its effectiveness through a simulation study. Additionally, we demonstrate its utility with real data by analyzing the heterogeneous effects of 401(k) plan participation on wealth.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06452&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Miruna Oprescu, Nathan Kallus</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Accurately predicting conditional average treatment effects (CATEs) is crucial in personalized medicine and digital platform analytics. Since often the treatments of interest cannot be directly randomized, observational data is leveraged to learn CATEs, but this approach can incur significant bias from unobserved confounding. One strategy to overcome these limitations is to seek latent quasi-experiments in instrumental variables (IVs) for the treatment, for example, a randomized intent to treat or a randomized product recommendation. This approach, on the other hand, can suffer from low compliance, i.e., IV weakness. Some subgroups may even exhibit zero compliance meaning we cannot instrument for their CATEs at all. In this paper we develop a novel approach to combine IV and observational data to enable reliable CATE estimation in the presence of unobserved confounding in the observational data and low compliance in the IV data, including no compliance for some subgroups. We propose a two-stage framework that first learns biased CATEs from the observational data, and then applies a compliance-weighted correction using IV data, effectively leveraging IV strength variability across covariates. We characterize the convergence rates of our method and validate its effectiveness through a simulation study. Additionally, we demonstrate its utility with real data by analyzing the heterogeneous effects of 401(k) plan participation on wealth.</summary></entry><entry><title type="html">Estimating Unknown Population Sizes Using the Hypergeometric Distribution</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EstimatingUnknownPopulationSizesUsingtheHypergeometricDistribution.html" rel="alternate" type="text/html" title="Estimating Unknown Population Sizes Using the Hypergeometric Distribution" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EstimatingUnknownPopulationSizesUsingtheHypergeometricDistribution</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EstimatingUnknownPopulationSizesUsingtheHypergeometricDistribution.html">&lt;p&gt;The multivariate hypergeometric distribution describes sampling without replacement from a discrete population of elements divided into multiple categories. Addressing a gap in the literature, we tackle the challenge of estimating discrete distributions when both the total population size and the sizes of its constituent categories are unknown. Here, we propose a novel solution using the hypergeometric likelihood to solve this estimation challenge, even in the presence of severe under-sampling. We develop our approach to account for a data generating process where the ground-truth is a mixture of distributions conditional on a continuous latent variable, such as with collaborative filtering, using the variational autoencoder framework. Empirical data simulation demonstrates that our method outperforms other likelihood functions used to model count data, both in terms of accuracy of population size estimate and in its ability to learn an informative latent space. We demonstrate our method’s versatility through applications in NLP, by inferring and estimating the complexity of latent vocabularies in text excerpts, and in biology, by accurately recovering the true number of gene transcripts from sparse single-cell genomics data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.14220&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Liam Hodgson, Danilo Bzdok</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">The multivariate hypergeometric distribution describes sampling without replacement from a discrete population of elements divided into multiple categories. Addressing a gap in the literature, we tackle the challenge of estimating discrete distributions when both the total population size and the sizes of its constituent categories are unknown. Here, we propose a novel solution using the hypergeometric likelihood to solve this estimation challenge, even in the presence of severe under-sampling. We develop our approach to account for a data generating process where the ground-truth is a mixture of distributions conditional on a continuous latent variable, such as with collaborative filtering, using the variational autoencoder framework. Empirical data simulation demonstrates that our method outperforms other likelihood functions used to model count data, both in terms of accuracy of population size estimate and in its ability to learn an informative latent space. We demonstrate our method’s versatility through applications in NLP, by inferring and estimating the complexity of latent vocabularies in text excerpts, and in biology, by accurately recovering the true number of gene transcripts from sparse single-cell genomics data.</summary></entry><entry><title type="html">Estimation of uncertainties in the density driven flow in fractured porous media using MLMC</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EstimationofuncertaintiesinthedensitydrivenflowinfracturedporousmediausingMLMC.html" rel="alternate" type="text/html" title="Estimation of uncertainties in the density driven flow in fractured porous media using MLMC" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EstimationofuncertaintiesinthedensitydrivenflowinfracturedporousmediausingMLMC</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/EstimationofuncertaintiesinthedensitydrivenflowinfracturedporousmediausingMLMC.html">&lt;p&gt;We use the Multi Level Monte Carlo method to estimate uncertainties in a Henry-like salt water intrusion problem with a fracture. The flow is induced by the variation of the density of the fluid phase, which depends on the mass fraction of salt. We assume that the fracture has a known fixed location but an uncertain aperture. Other input uncertainties are the porosity and permeability fields and the recharge. In our setting, porosity and permeability vary spatially and recharge is time-dependent. For each realisation of these uncertain parameters, the evolution of the mass fraction and pressure fields is modelled by a system of non-linear and time-dependent PDEs with a jump of the solution at the fracture. The uncertainties propagate into the distribution of the salt concentration, which is an important characteristic of the quality of water resources. We show that the multilevel Monte Carlo (MLMC) method is able to reduce the overall computational cost compared to classical Monte Carlo methods. This is achieved by balancing discretisation and statistical errors. Multiple scenarios are evaluated at different spatial and temporal mesh levels. The deterministic solver ug4 is run in parallel to calculate all stochastic scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18003&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Dmitry Logashenko, Alexander Litvinenko, Raul Tempone, Gabriel Wittum</name></author><category term="stat.CO" /><summary type="html">We use the Multi Level Monte Carlo method to estimate uncertainties in a Henry-like salt water intrusion problem with a fracture. The flow is induced by the variation of the density of the fluid phase, which depends on the mass fraction of salt. We assume that the fracture has a known fixed location but an uncertain aperture. Other input uncertainties are the porosity and permeability fields and the recharge. In our setting, porosity and permeability vary spatially and recharge is time-dependent. For each realisation of these uncertain parameters, the evolution of the mass fraction and pressure fields is modelled by a system of non-linear and time-dependent PDEs with a jump of the solution at the fracture. The uncertainties propagate into the distribution of the salt concentration, which is an important characteristic of the quality of water resources. We show that the multilevel Monte Carlo (MLMC) method is able to reduce the overall computational cost compared to classical Monte Carlo methods. This is achieved by balancing discretisation and statistical errors. Multiple scenarios are evaluated at different spatial and temporal mesh levels. The deterministic solver ug4 is run in parallel to calculate all stochastic scenarios.</summary></entry><entry><title type="html">Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/GeneralizedIndependentNoiseConditionforEstimatingCausalStructurewithLatentVariables.html" rel="alternate" type="text/html" title="Generalized Independent Noise Condition for Estimating Causal Structure with Latent Variables" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/GeneralizedIndependentNoiseConditionforEstimatingCausalStructurewithLatentVariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/GeneralizedIndependentNoiseConditionforEstimatingCausalStructurewithLatentVariables.html">&lt;p&gt;We investigate the task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To this end, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic models. Roughly speaking, GIN implies the existence of a set $\mathcal{S}$ such that $\mathcal{S}$ is causally earlier (w.r.t. the causal ordering) than $\mathbf{Y}$, and that every active (collider-free) path between $\mathbf{Y}$ and $\mathbf{Z}$ must contain a node from $\mathcal{S}$. Interestingly, we find that the independent noise condition (i.e., if there is no confounder, causes are independent of the residual derived from regressing the effect on the causes) can be seen as a special case of GIN. With such a connection between GIN and latent causal structures, we further leverage the proposed GIN condition, together with a well-designed search procedure, to efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs), where latent confounders may also be causally related and may even follow a hierarchical structure. We show that the causal structure of a LiNGLaH is identifiable in light of GIN conditions. Experimental results show the effectiveness of the proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.06718&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Feng Xie, Biwei Huang, Zhengming Chen, Ruichu Cai, Clark Glymour, Zhi Geng, Kun Zhang</name></author><category term="stat.ME" /><summary type="html">We investigate the task of learning causal structure in the presence of latent variables, including locating latent variables and determining their quantity, and identifying causal relationships among both latent and observed variables. To this end, we propose a Generalized Independent Noise (GIN) condition for linear non-Gaussian acyclic causal models that incorporate latent variables, which establishes the independence between a linear combination of certain measured variables and some other measured variables. Specifically, for two observed random vectors $\bf{Y}$ and $\bf{Z}$, GIN holds if and only if $\omega^{\intercal}\mathbf{Y}$ and $\mathbf{Z}$ are independent, where $\omega$ is a non-zero parameter vector determined by the cross-covariance between $\mathbf{Y}$ and $\mathbf{Z}$. We then give necessary and sufficient graphical criteria of the GIN condition in linear non-Gaussian acyclic models. Roughly speaking, GIN implies the existence of a set $\mathcal{S}$ such that $\mathcal{S}$ is causally earlier (w.r.t. the causal ordering) than $\mathbf{Y}$, and that every active (collider-free) path between $\mathbf{Y}$ and $\mathbf{Z}$ must contain a node from $\mathcal{S}$. Interestingly, we find that the independent noise condition (i.e., if there is no confounder, causes are independent of the residual derived from regressing the effect on the causes) can be seen as a special case of GIN. With such a connection between GIN and latent causal structures, we further leverage the proposed GIN condition, together with a well-designed search procedure, to efficiently estimate Linear, Non-Gaussian Latent Hierarchical Models (LiNGLaHs), where latent confounders may also be causally related and may even follow a hierarchical structure. We show that the causal structure of a LiNGLaH is identifiable in light of GIN conditions. Experimental results show the effectiveness of the proposed method.</summary></entry><entry><title type="html">HAL-based Plugin Estimation of the Causal Dose-Response Curve</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/HALbasedPluginEstimationoftheCausalDoseResponseCurve.html" rel="alternate" type="text/html" title="HAL-based Plugin Estimation of the Causal Dose-Response Curve" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/HALbasedPluginEstimationoftheCausalDoseResponseCurve</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/HALbasedPluginEstimationoftheCausalDoseResponseCurve.html">&lt;p&gt;Estimating the marginally adjusted dose-response curve for continuous treatments is a longstanding statistical challenge critical across multiple fields. In the context of parametric models, mis-specification may result in substantial bias, hindering the accurate discernment of the true data generating distribution and the associated dose-response curve. In contrast, non-parametric models face difficulties as the dose-response curve isn’t pathwise differentiable, and then there is no $\sqrt{n}$-consistent estimator. The emergence of the Highly Adaptive Lasso (HAL) MLE by van der Laan [2015] and van der Laan [2017] and the subsequent theoretical evidence by van der Laan [2023] regarding its pointwise asymptotic normality and uniform convergence rates, have highlighted the asymptotic efficacy of the HAL-based plug-in estimator for this intricate problem. This paper delves into the HAL-based plug-in estimators, including those with cross-validation and undersmoothing selectors, and introduces the undersmoothed smoothness-adaptive HAL-based plug-in estimator. We assess these estimators through extensive simulations, employing detailed evaluation metrics. Building upon the theoretical proofs in van der Laan [2023], our empirical findings underscore the asymptotic effectiveness of the undersmoothed smoothness-adaptive HAL-based plug-in estimator in estimating the marginally adjusted dose-response curve.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05607&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Junming ,  Shi, Wenxin Zhang, Alan E. Hubbard, Mar van der Laan</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Estimating the marginally adjusted dose-response curve for continuous treatments is a longstanding statistical challenge critical across multiple fields. In the context of parametric models, mis-specification may result in substantial bias, hindering the accurate discernment of the true data generating distribution and the associated dose-response curve. In contrast, non-parametric models face difficulties as the dose-response curve isn’t pathwise differentiable, and then there is no $\sqrt{n}$-consistent estimator. The emergence of the Highly Adaptive Lasso (HAL) MLE by van der Laan [2015] and van der Laan [2017] and the subsequent theoretical evidence by van der Laan [2023] regarding its pointwise asymptotic normality and uniform convergence rates, have highlighted the asymptotic efficacy of the HAL-based plug-in estimator for this intricate problem. This paper delves into the HAL-based plug-in estimators, including those with cross-validation and undersmoothing selectors, and introduces the undersmoothed smoothness-adaptive HAL-based plug-in estimator. We assess these estimators through extensive simulations, employing detailed evaluation metrics. Building upon the theoretical proofs in van der Laan [2023], our empirical findings underscore the asymptotic effectiveness of the undersmoothed smoothness-adaptive HAL-based plug-in estimator in estimating the marginally adjusted dose-response curve.</summary></entry><entry><title type="html">Identifying Peer Influence in Therapeutic Communities Adjusting for Latent Homophily</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/IdentifyingPeerInfluenceinTherapeuticCommunitiesAdjustingforLatentHomophily.html" rel="alternate" type="text/html" title="Identifying Peer Influence in Therapeutic Communities Adjusting for Latent Homophily" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/IdentifyingPeerInfluenceinTherapeuticCommunitiesAdjustingforLatentHomophily</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/IdentifyingPeerInfluenceinTherapeuticCommunitiesAdjustingforLatentHomophily.html">&lt;p&gt;We investigate peer role model influence on successful graduation from Therapeutic Communities (TCs) for substance abuse and criminal behavior. We use data from 3 TCs that kept records of exchanges of affirmations among residents and their precise entry and exit dates, allowing us to form peer networks and define a causal effect of interest. The role model effect measures the difference in the expected outcome of a resident (ego) who can observe one of their peers graduate before the ego’s exit vs not graduating. To identify peer influence in the presence of unobserved homophily in observational data, we model the network with a latent variable model. We show that our peer influence estimator is asymptotically unbiased when the unobserved latent positions are estimated from the observed network. We additionally propose a measurement error bias correction method to further reduce bias due to estimating latent positions. Our simulations show the proposed latent homophily adjustment and bias correction perform well in finite samples. We also extend the methodology to the case of binary response with a probit model. Our results indicate a positive effect of peers’ graduation on residents’ graduation and that it differs based on gender, race, and the definition of the role model effect. A counterfactual exercise quantifies the potential benefits of an intervention directly on the treated resident and indirectly on their peers through network propagation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.14223&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shanjukta Nath, Keith Warren, Subhadeep Paul</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We investigate peer role model influence on successful graduation from Therapeutic Communities (TCs) for substance abuse and criminal behavior. We use data from 3 TCs that kept records of exchanges of affirmations among residents and their precise entry and exit dates, allowing us to form peer networks and define a causal effect of interest. The role model effect measures the difference in the expected outcome of a resident (ego) who can observe one of their peers graduate before the ego’s exit vs not graduating. To identify peer influence in the presence of unobserved homophily in observational data, we model the network with a latent variable model. We show that our peer influence estimator is asymptotically unbiased when the unobserved latent positions are estimated from the observed network. We additionally propose a measurement error bias correction method to further reduce bias due to estimating latent positions. Our simulations show the proposed latent homophily adjustment and bias correction perform well in finite samples. We also extend the methodology to the case of binary response with a probit model. Our results indicate a positive effect of peers’ graduation on residents’ graduation and that it differs based on gender, race, and the definition of the role model effect. A counterfactual exercise quantifies the potential benefits of an intervention directly on the treated resident and indirectly on their peers through network propagation.</summary></entry><entry><title type="html">Inestability presented in the estimating of the Nelson-Siegel-Svensson model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/InestabilitypresentedintheestimatingoftheNelsonSiegelSvenssonmodel.html" rel="alternate" type="text/html" title="Inestability presented in the estimating of the Nelson-Siegel-Svensson model" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/InestabilitypresentedintheestimatingoftheNelsonSiegelSvenssonmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/InestabilitypresentedintheestimatingoftheNelsonSiegelSvenssonmodel.html">&lt;p&gt;The literature shows the possible existence of a problem called collinearity in both Nelson-Siegel and Nelson-Siegel-Svensson models due to the relationship between the slope and curvature components. The presence of this problem and the estimation of both models by Ordinary Least Squares would lead to coefficients estimates that may be unstable among other consequences. However, these estimates are used to make monetary policy decisions. For this reason, it is important to try mitigating this collinearity problem. Consequently, some authors propose traditional procedures for the treatment of collinearity such as: non-linear optimisation, to fix the shape parameter or ridge regression. Nevertheless, all these processes have their disadvantages. Alternatively, a new method with good properties called raise regression is proposed in this paper. Finally, the methodologies are illustrated with an empirical comparison on Euribor Overnight Index Swap and Euribor Interest Rates Swap data between 2011 and 2021.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06177&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ainara Rodríguez-Sánchez</name></author><category term="stat.AP" /><summary type="html">The literature shows the possible existence of a problem called collinearity in both Nelson-Siegel and Nelson-Siegel-Svensson models due to the relationship between the slope and curvature components. The presence of this problem and the estimation of both models by Ordinary Least Squares would lead to coefficients estimates that may be unstable among other consequences. However, these estimates are used to make monetary policy decisions. For this reason, it is important to try mitigating this collinearity problem. Consequently, some authors propose traditional procedures for the treatment of collinearity such as: non-linear optimisation, to fix the shape parameter or ridge regression. Nevertheless, all these processes have their disadvantages. Alternatively, a new method with good properties called raise regression is proposed in this paper. Finally, the methodologies are illustrated with an empirical comparison on Euribor Overnight Index Swap and Euribor Interest Rates Swap data between 2011 and 2021.</summary></entry><entry><title type="html">LaLonde (1986) after Nearly Four Decades: Lessons Learned</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/LaLonde1986afterNearlyFourDecadesLessonsLearned.html" rel="alternate" type="text/html" title="LaLonde (1986) after Nearly Four Decades: Lessons Learned" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/LaLonde1986afterNearlyFourDecadesLessonsLearned</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/LaLonde1986afterNearlyFourDecadesLessonsLearned.html">&lt;p&gt;In 1986, Robert LaLonde published an article that compared nonexperimental estimates to experimental benchmarks (LaLonde 1986). He concluded that the nonexperimental methods at the time could not systematically replicate experimental benchmarks, casting doubt on the credibility of these methods. Following LaLonde’s critical assessment, there have been significant methodological advances and practical changes, including (i) an emphasis on estimators based on unconfoundedness, (ii) a focus on the importance of overlap in covariate distributions, (iii) the introduction of propensity score-based methods leading to doubly robust estimators, (iv) a greater emphasis on validation exercises to bolster research credibility, and (v) methods for estimating and exploiting treatment effect heterogeneity. To demonstrate the practical lessons from these advances, we reexamine the LaLonde data and the Imbens-Rubin-Sacerdote lottery data. We show that modern methods, when applied in contexts with sufficient covariate overlap, yield robust estimates for the adjusted differences between the treatment and control groups. However, this does not mean that these estimates are valid. To assess their credibility, validation exercises (such as placebo tests) are essential, whereas goodness of fit tests alone are inadequate. Our findings highlight the importance of closely examining the assignment process, carefully inspecting overlap, and conducting validation exercises when analyzing causal effects with nonexperimental data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00827&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Guido Imbens, Yiqing Xu</name></author><category term="stat.ME" /><summary type="html">In 1986, Robert LaLonde published an article that compared nonexperimental estimates to experimental benchmarks (LaLonde 1986). He concluded that the nonexperimental methods at the time could not systematically replicate experimental benchmarks, casting doubt on the credibility of these methods. Following LaLonde’s critical assessment, there have been significant methodological advances and practical changes, including (i) an emphasis on estimators based on unconfoundedness, (ii) a focus on the importance of overlap in covariate distributions, (iii) the introduction of propensity score-based methods leading to doubly robust estimators, (iv) a greater emphasis on validation exercises to bolster research credibility, and (v) methods for estimating and exploiting treatment effect heterogeneity. To demonstrate the practical lessons from these advances, we reexamine the LaLonde data and the Imbens-Rubin-Sacerdote lottery data. We show that modern methods, when applied in contexts with sufficient covariate overlap, yield robust estimates for the adjusted differences between the treatment and control groups. However, this does not mean that these estimates are valid. To assess their credibility, validation exercises (such as placebo tests) are essential, whereas goodness of fit tests alone are inadequate. Our findings highlight the importance of closely examining the assignment process, carefully inspecting overlap, and conducting validation exercises when analyzing causal effects with nonexperimental data.</summary></entry><entry><title type="html">Liouville Flow Importance Sampler</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/LiouvilleFlowImportanceSampler.html" rel="alternate" type="text/html" title="Liouville Flow Importance Sampler" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/LiouvilleFlowImportanceSampler</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/LiouvilleFlowImportanceSampler.html">&lt;p&gt;We present the Liouville Flow Importance Sampler (LFIS), an innovative flow-based model for generating samples from unnormalized density functions. LFIS learns a time-dependent velocity field that deterministically transports samples from a simple initial distribution to a complex target distribution, guided by a prescribed path of annealed distributions. The training of LFIS utilizes a unique method that enforces the structure of a derived partial differential equation to neural networks modeling velocity fields. By considering the neural velocity field as an importance sampler, sample weights can be computed through accumulating errors along the sample trajectories driven by neural velocity fields, ensuring unbiased and consistent estimation of statistical quantities. We demonstrate the effectiveness of LFIS through its application to a range of benchmark problems, on many of which LFIS achieved state-of-the-art performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06672&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yifeng Tian, Nishant Panda, Yen Ting Lin</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">We present the Liouville Flow Importance Sampler (LFIS), an innovative flow-based model for generating samples from unnormalized density functions. LFIS learns a time-dependent velocity field that deterministically transports samples from a simple initial distribution to a complex target distribution, guided by a prescribed path of annealed distributions. The training of LFIS utilizes a unique method that enforces the structure of a derived partial differential equation to neural networks modeling velocity fields. By considering the neural velocity field as an importance sampler, sample weights can be computed through accumulating errors along the sample trajectories driven by neural velocity fields, ensuring unbiased and consistent estimation of statistical quantities. We demonstrate the effectiveness of LFIS through its application to a range of benchmark problems, on many of which LFIS achieved state-of-the-art performance.</summary></entry><entry><title type="html">Markov chain Monte Carlo without evaluating the target: an auxiliary variable approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/MarkovchainMonteCarlowithoutevaluatingthetargetanauxiliaryvariableapproach.html" rel="alternate" type="text/html" title="Markov chain Monte Carlo without evaluating the target: an auxiliary variable approach" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/MarkovchainMonteCarlowithoutevaluatingthetargetanauxiliaryvariableapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/MarkovchainMonteCarlowithoutevaluatingthetargetanauxiliaryvariableapproach.html">&lt;p&gt;In sampling tasks, it is common for target distributions to be known up to a normalizing constant. However, in many situations, evaluating even the unnormalized distribution can be costly or infeasible. This issue arises in scenarios such as sampling from the Bayesian posterior for tall datasets and the `doubly-intractable’ distributions. In this paper, we begin by observing that seemingly different Markov chain Monte Carlo (MCMC) algorithms, such as the exchange algorithm, PoissonMH, and TunaMH, can be unified under a simple common procedure. We then extend this procedure into a novel framework that allows the use of auxiliary variables in both the proposal and acceptance-rejection steps. We develop the theory of the new framework, applying it to existing algorithms to simplify and extend their results. Several new algorithms emerge from this framework, with improved performance demonstrated on both synthetic and real datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05242&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wei Yuan, Guanyang Wang</name></author><category term="stat.CO," /><category term="stat.ME," /><category term="stat.ML" /><summary type="html">In sampling tasks, it is common for target distributions to be known up to a normalizing constant. However, in many situations, evaluating even the unnormalized distribution can be costly or infeasible. This issue arises in scenarios such as sampling from the Bayesian posterior for tall datasets and the `doubly-intractable’ distributions. In this paper, we begin by observing that seemingly different Markov chain Monte Carlo (MCMC) algorithms, such as the exchange algorithm, PoissonMH, and TunaMH, can be unified under a simple common procedure. We then extend this procedure into a novel framework that allows the use of auxiliary variables in both the proposal and acceptance-rejection steps. We develop the theory of the new framework, applying it to existing algorithms to simplify and extend their results. Several new algorithms emerge from this framework, with improved performance demonstrated on both synthetic and real datasets.</summary></entry><entry><title type="html">Metrics sonification: The introduction of new ways to present bibliometric data using publication data of Loet Leydesdorff as an example</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/MetricssonificationTheintroductionofnewwaystopresentbibliometricdatausingpublicationdataofLoetLeydesdorffasanexample.html" rel="alternate" type="text/html" title="Metrics sonification: The introduction of new ways to present bibliometric data using publication data of Loet Leydesdorff as an example" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/MetricssonificationTheintroductionofnewwaystopresentbibliometricdatausingpublicationdataofLoetLeydesdorffasanexample</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/MetricssonificationTheintroductionofnewwaystopresentbibliometricdatausingpublicationdataofLoetLeydesdorffasanexample.html">&lt;p&gt;The visualization of publication and citation data is popular in bibliometrics. Although less common, the representation of empirical data as sound is an alternative form of presentation (in other fields than bibliometrics). In this representation, the data are mapped into sound and listened to by an audience. Approaches for the sonification of data have been developed in many fields since decades. Since sonification has several advantages for the presentation of data, this study is intended to introduce sonification to bibliometrics named as ‘metrics sonification’. Metrics sonification is defined as the sonification of bibliometric information (measurements, data or results) for their empirical analysis and/or presentation. In this study, we used metadata of publications by Loet Leydesdorff (named as Loet in the following) to sonify their properties. Loet was a giant in the field of scientometrics, who passed away in 2023. The track based on Loet’s publications can be listened to on SoundCloud using the following link: https://on.soundcloud.com/oxBTA32x4EgwvKVz5. The track has been composed in F minor; this key was chosen to express the sad occasion. The quantitative part of the track includes a parameter mapping (a sonification) of three properties of his publications: (1) publication output, (2) open access publication, and (3) citation impact of publications. The qualitative part (spoken audio) focuses on explanations of the parameter mapping and descriptions of the mapped papers (based on their titles and abstracts). The sonification of Loet’s publications presented in this study is only one possible type of metrics sonification application. As the great number of projects from other disciplines have demonstrated, many other types of applications are possible in bibliometrics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05679&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lutz Bornmann, Rouven Lazlo Haegner</name></author><category term="stat.AP" /><summary type="html">The visualization of publication and citation data is popular in bibliometrics. Although less common, the representation of empirical data as sound is an alternative form of presentation (in other fields than bibliometrics). In this representation, the data are mapped into sound and listened to by an audience. Approaches for the sonification of data have been developed in many fields since decades. Since sonification has several advantages for the presentation of data, this study is intended to introduce sonification to bibliometrics named as ‘metrics sonification’. Metrics sonification is defined as the sonification of bibliometric information (measurements, data or results) for their empirical analysis and/or presentation. In this study, we used metadata of publications by Loet Leydesdorff (named as Loet in the following) to sonify their properties. Loet was a giant in the field of scientometrics, who passed away in 2023. The track based on Loet’s publications can be listened to on SoundCloud using the following link: https://on.soundcloud.com/oxBTA32x4EgwvKVz5. The track has been composed in F minor; this key was chosen to express the sad occasion. The quantitative part of the track includes a parameter mapping (a sonification) of three properties of his publications: (1) publication output, (2) open access publication, and (3) citation impact of publications. The qualitative part (spoken audio) focuses on explanations of the parameter mapping and descriptions of the mapped papers (based on their titles and abstracts). The sonification of Loet’s publications presented in this study is only one possible type of metrics sonification application. As the great number of projects from other disciplines have demonstrated, many other types of applications are possible in bibliometrics.</summary></entry><entry><title type="html">Model-Free Error Assessment for Breadth-First Studies, with Applications to Cell-Perturbation Experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ModelFreeErrorAssessmentforBreadthFirstStudieswithApplicationstoCellPerturbationExperiments.html" rel="alternate" type="text/html" title="Model-Free Error Assessment for Breadth-First Studies, with Applications to Cell-Perturbation Experiments" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ModelFreeErrorAssessmentforBreadthFirstStudieswithApplicationstoCellPerturbationExperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ModelFreeErrorAssessmentforBreadthFirstStudieswithApplicationstoCellPerturbationExperiments.html">&lt;p&gt;With the advent of high-throughput screenings, it has become increasingly common for studies to devote limited resources to estimating many parameters imprecisely rather than to estimating a few parameters well. In these studies, only two or three independent replicates measure each parameter, and therefore it is challenging to assess the variance of these measurements. One solution is to pool variance estimates across different parameters using a parametric model of estimator error. However, such models are difficult to specify correctly, especially in the presence of ``batch effects.’’ In this paper, we propose new model-free methods for assessing and controlling estimator error. Our focus is on type S error, which is of particular importance in many settings. To produce tight confidence intervals without making unrealistic assumptions, we improve on Hoeffding’s bounds for sums of bounded random variables and obtain the tightest possible Chernoff-Cram&apos;er bound. Our methods compare favorably with existing practice for high-throughput screenings, such as methods based on the Irreproducible Discovery Rate (IDR) and the Benjamini-Hochberg procedure. Existing practices fail to control error at the nominal level in some cases and are needlessly conservative in others.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.01745&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jackson Loper, Jeffrey Regier</name></author><category term="stat.AP" /><summary type="html">With the advent of high-throughput screenings, it has become increasingly common for studies to devote limited resources to estimating many parameters imprecisely rather than to estimating a few parameters well. In these studies, only two or three independent replicates measure each parameter, and therefore it is challenging to assess the variance of these measurements. One solution is to pool variance estimates across different parameters using a parametric model of estimator error. However, such models are difficult to specify correctly, especially in the presence of ``batch effects.’’ In this paper, we propose new model-free methods for assessing and controlling estimator error. Our focus is on type S error, which is of particular importance in many settings. To produce tight confidence intervals without making unrealistic assumptions, we improve on Hoeffding’s bounds for sums of bounded random variables and obtain the tightest possible Chernoff-Cram&apos;er bound. Our methods compare favorably with existing practice for high-throughput screenings, such as methods based on the Irreproducible Discovery Rate (IDR) and the Benjamini-Hochberg procedure. Existing practices fail to control error at the nominal level in some cases and are needlessly conservative in others.</summary></entry><entry><title type="html">Multi-Task Learning for Sparsity Pattern Heterogeneity: Statistical and Computational Perspectives</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/MultiTaskLearningforSparsityPatternHeterogeneityStatisticalandComputationalPerspectives.html" rel="alternate" type="text/html" title="Multi-Task Learning for Sparsity Pattern Heterogeneity: Statistical and Computational Perspectives" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/MultiTaskLearningforSparsityPatternHeterogeneityStatisticalandComputationalPerspectives</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/MultiTaskLearningforSparsityPatternHeterogeneityStatisticalandComputationalPerspectives.html">&lt;p&gt;We consider a problem in Multi-Task Learning (MTL) where multiple linear models are jointly trained on a collection of datasets (“tasks”). A key novelty of our framework is that it allows the sparsity pattern of regression coefficients and the values of non-zero coefficients to differ across tasks while still leveraging partially shared structure. Our methods encourage models to share information across tasks through separately encouraging 1) coefficient supports, and/or 2) nonzero coefficient values to be similar. This allows models to borrow strength during variable selection even when non-zero coefficient values differ across tasks. We propose a novel mixed-integer programming formulation for our estimator. We develop custom scalable algorithms based on block coordinate descent and combinatorial local search to obtain high-quality (approximate) solutions for our estimator. Additionally, we propose a novel exact optimization algorithm to obtain globally optimal solutions. We investigate the theoretical properties of our estimators. We formally show how our estimators leverage the shared support information across tasks to achieve better variable selection performance. We evaluate the performance of our methods in simulations and two biomedical applications. Our proposed approaches appear to outperform other sparse MTL methods in variable selection and prediction accuracy. We provide the sMTL package on CRAN.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.08697&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kayhan Behdin, Gabriel Loewinger, Kenneth T. Kishida, Giovanni Parmigiani, Rahul Mazumder</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We consider a problem in Multi-Task Learning (MTL) where multiple linear models are jointly trained on a collection of datasets (“tasks”). A key novelty of our framework is that it allows the sparsity pattern of regression coefficients and the values of non-zero coefficients to differ across tasks while still leveraging partially shared structure. Our methods encourage models to share information across tasks through separately encouraging 1) coefficient supports, and/or 2) nonzero coefficient values to be similar. This allows models to borrow strength during variable selection even when non-zero coefficient values differ across tasks. We propose a novel mixed-integer programming formulation for our estimator. We develop custom scalable algorithms based on block coordinate descent and combinatorial local search to obtain high-quality (approximate) solutions for our estimator. Additionally, we propose a novel exact optimization algorithm to obtain globally optimal solutions. We investigate the theoretical properties of our estimators. We formally show how our estimators leverage the shared support information across tasks to achieve better variable selection performance. We evaluate the performance of our methods in simulations and two biomedical applications. Our proposed approaches appear to outperform other sparse MTL methods in variable selection and prediction accuracy. We provide the sMTL package on CRAN.</summary></entry><entry><title type="html">Multilayer random dot product graphs: Estimation and online change point detection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/MultilayerrandomdotproductgraphsEstimationandonlinechangepointdetection.html" rel="alternate" type="text/html" title="Multilayer random dot product graphs: Estimation and online change point detection" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/MultilayerrandomdotproductgraphsEstimationandonlinechangepointdetection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/MultilayerrandomdotproductgraphsEstimationandonlinechangepointdetection.html">&lt;p&gt;We study the multilayer random dot product graph (MRDPG) model, an extension of the random dot product graph to multilayer networks. To estimate the edge probabilities, we deploy a tensor-based methodology and demonstrate its superiority over existing approaches. Moving to dynamic MRDPGs, we formulate and analyse an online change point detection framework. At every time point, we observe a realization from an MRDPG. Across layers, we assume fixed shared common node sets and latent positions but allow for different connectivity matrices. We propose efficient tensor algorithms under both fixed and random latent position cases to minimize the detection delay while controlling false alarms. Notably, in the random latent position case, we devise a novel nonparametric change point detection algorithm based on density kernel estimation that is applicable to a wide range of scenarios, including stochastic block models as special cases. Our theoretical findings are supported by extensive numerical experiments, with the code available online https://github.com/MountLee/MRDPG.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.15286&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fan Wang, Wanshan Li, Oscar Hernan Madrid Padilla, Yi Yu, Alessandro Rinaldo</name></author><category term="stat.ME" /><summary type="html">We study the multilayer random dot product graph (MRDPG) model, an extension of the random dot product graph to multilayer networks. To estimate the edge probabilities, we deploy a tensor-based methodology and demonstrate its superiority over existing approaches. Moving to dynamic MRDPGs, we formulate and analyse an online change point detection framework. At every time point, we observe a realization from an MRDPG. Across layers, we assume fixed shared common node sets and latent positions but allow for different connectivity matrices. We propose efficient tensor algorithms under both fixed and random latent position cases to minimize the detection delay while controlling false alarms. Notably, in the random latent position case, we devise a novel nonparametric change point detection algorithm based on density kernel estimation that is applicable to a wide range of scenarios, including stochastic block models as special cases. Our theoretical findings are supported by extensive numerical experiments, with the code available online https://github.com/MountLee/MRDPG.</summary></entry><entry><title type="html">Network two-sample test for block models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Networktwosampletestforblockmodels.html" rel="alternate" type="text/html" title="Network two-sample test for block models" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Networktwosampletestforblockmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Networktwosampletestforblockmodels.html">&lt;p&gt;We consider the two-sample testing problem for networks, where the goal is to determine whether two sets of networks originated from the same stochastic model. Assuming no vertex correspondence and allowing for different numbers of nodes, we address a fundamental network testing problem that goes beyond simple adjacency matrix comparisons. We adopt the stochastic block model (SBM) for network distributions, due to their interpretability and the potential to approximate more general models. The lack of meaningful node labels and vertex correspondence translate to a graph matching challenge when developing a test for SBMs. We introduce an efficient algorithm to match estimated network parameters, allowing us to properly combine and contrast information within and across samples, leading to a powerful test. We show that the matching algorithm, and the overall test are consistent, under mild conditions on the sparsity of the networks and the sample sizes, and derive a chi-squared asymptotic null distribution for the test. Through a mixture of theoretical insights and empirical validations, including experiments with both synthetic and real-world data, this study advances robust statistical inference for complex network data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06014&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chung Kyong Nguen, Oscar Hernan Madrid Padilla, Arash A. Amini</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">We consider the two-sample testing problem for networks, where the goal is to determine whether two sets of networks originated from the same stochastic model. Assuming no vertex correspondence and allowing for different numbers of nodes, we address a fundamental network testing problem that goes beyond simple adjacency matrix comparisons. We adopt the stochastic block model (SBM) for network distributions, due to their interpretability and the potential to approximate more general models. The lack of meaningful node labels and vertex correspondence translate to a graph matching challenge when developing a test for SBMs. We introduce an efficient algorithm to match estimated network parameters, allowing us to properly combine and contrast information within and across samples, leading to a powerful test. We show that the matching algorithm, and the overall test are consistent, under mild conditions on the sparsity of the networks and the sample sizes, and derive a chi-squared asymptotic null distribution for the test. Through a mixture of theoretical insights and empirical validations, including experiments with both synthetic and real-world data, this study advances robust statistical inference for complex network data.</summary></entry><entry><title type="html">Neural Methods for Amortised Parameter Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/NeuralMethodsforAmortisedParameterInference.html" rel="alternate" type="text/html" title="Neural Methods for Amortised Parameter Inference" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/NeuralMethodsforAmortisedParameterInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/NeuralMethodsforAmortisedParameterInference.html">&lt;p&gt;Simulation-based methods for making statistical inference have evolved dramatically over the past 50 years, keeping pace with technological advancements. The field is undergoing a new revolution as it embraces the representational capacity of neural networks, optimisation libraries and graphics processing units for learning complex mappings between data and inferential targets. The resulting tools are amortised, in the sense that they allow inference to be made quickly through fast feedforward operations. In this article we review recent progress made in the context of point estimation, approximate Bayesian inference, summary-statistic construction, and likelihood approximation. The review also covers available software, and includes a simple illustration to showcase the wide array of tools available for amortised inference and the benefits they offer over state-of-the-art Markov chain Monte Carlo methods. The article concludes with an overview of relevant topics and an outlook on future research directions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.12484&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrew Zammit-Mangion, Matthew Sainsbury-Dale, Raphaël Huser</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Simulation-based methods for making statistical inference have evolved dramatically over the past 50 years, keeping pace with technological advancements. The field is undergoing a new revolution as it embraces the representational capacity of neural networks, optimisation libraries and graphics processing units for learning complex mappings between data and inferential targets. The resulting tools are amortised, in the sense that they allow inference to be made quickly through fast feedforward operations. In this article we review recent progress made in the context of point estimation, approximate Bayesian inference, summary-statistic construction, and likelihood approximation. The review also covers available software, and includes a simple illustration to showcase the wide array of tools available for amortised inference and the benefits they offer over state-of-the-art Markov chain Monte Carlo methods. The article concludes with an overview of relevant topics and an outlook on future research directions.</summary></entry><entry><title type="html">Non-Asymptotic State-Space Identification of Closed-Loop Stochastic Linear Systems using Instrumental Variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/NonAsymptoticStateSpaceIdentificationofClosedLoopStochasticLinearSystemsusingInstrumentalVariables.html" rel="alternate" type="text/html" title="Non-Asymptotic State-Space Identification of Closed-Loop Stochastic Linear Systems using Instrumental Variables" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/NonAsymptoticStateSpaceIdentificationofClosedLoopStochasticLinearSystemsusingInstrumentalVariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/NonAsymptoticStateSpaceIdentificationofClosedLoopStochasticLinearSystemsusingInstrumentalVariables.html">&lt;p&gt;The paper suggests a generalization of the Sign-Perturbed Sums (SPS) finite sample system identification method for the identification of closed-loop observable stochastic linear systems in state-space form. The solution builds on the theory of matrix-variate regression and instrumental variable methods to construct distribution-free confidence regions for the state-space matrices. Both direct and indirect identification are studied, and the exactness as well as the strong consistency of the construction are proved. Furthermore, a new, computationally efficient ellipsoidal outer-approximation algorithm for the confidence regions is proposed. The new construction results in a semidefinite optimization problem which has an order-of-magnitude smaller number of constraints, as if one applied the ellipsoidal outer-approximation after vectorization. The effectiveness of the approach is also demonstrated empirically via a series of numerical experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.12537&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Szabolcs Szentpéteri, Balázs Csanád Csáji</name></author><category term="stat.ME" /><summary type="html">The paper suggests a generalization of the Sign-Perturbed Sums (SPS) finite sample system identification method for the identification of closed-loop observable stochastic linear systems in state-space form. The solution builds on the theory of matrix-variate regression and instrumental variable methods to construct distribution-free confidence regions for the state-space matrices. Both direct and indirect identification are studied, and the exactness as well as the strong consistency of the construction are proved. Furthermore, a new, computationally efficient ellipsoidal outer-approximation algorithm for the confidence regions is proposed. The new construction results in a semidefinite optimization problem which has an order-of-magnitude smaller number of constraints, as if one applied the ellipsoidal outer-approximation after vectorization. The effectiveness of the approach is also demonstrated empirically via a series of numerical experiments.</summary></entry><entry><title type="html">Numerically robust square root implementations of statistical linear regression filters and smoothers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Numericallyrobustsquarerootimplementationsofstatisticallinearregressionfiltersandsmoothers.html" rel="alternate" type="text/html" title="Numerically robust square root implementations of statistical linear regression filters and smoothers" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Numericallyrobustsquarerootimplementationsofstatisticallinearregressionfiltersandsmoothers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Numericallyrobustsquarerootimplementationsofstatisticallinearregressionfiltersandsmoothers.html">&lt;p&gt;In this article, square-root formulations of the statistical linear regression filter and smoother are developed. Crucially, the method uses QR decompositions rather than Cholesky downdates. This makes the method inherently more numerically robust than the downdate based methods, which may fail in the face of rounding errors. This increased robustness is demonstrated in an ill-conditioned problem, where it is compared against a reference implementation in both double and single precision arithmetic. The new implementation is found to be more robust, when implemented in lower precision arithmetic as compared to the alternative.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05188&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Filip Tronarp</name></author><category term="stat.ME" /><summary type="html">In this article, square-root formulations of the statistical linear regression filter and smoother are developed. Crucially, the method uses QR decompositions rather than Cholesky downdates. This makes the method inherently more numerically robust than the downdate based methods, which may fail in the face of rounding errors. This increased robustness is demonstrated in an ill-conditioned problem, where it is compared against a reference implementation in both double and single precision arithmetic. The new implementation is found to be more robust, when implemented in lower precision arithmetic as compared to the alternative.</summary></entry><entry><title type="html">Planning for Gold: Sample Splitting for Valid Powerful Design of Observational Studies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/PlanningforGoldSampleSplittingforValidPowerfulDesignofObservationalStudies.html" rel="alternate" type="text/html" title="Planning for Gold: Sample Splitting for Valid Powerful Design of Observational Studies" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/PlanningforGoldSampleSplittingforValidPowerfulDesignofObservationalStudies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/PlanningforGoldSampleSplittingforValidPowerfulDesignofObservationalStudies.html">&lt;p&gt;Observational studies are valuable tools for inferring causal effects in the absence of controlled experiments. However, these studies may be biased due to the presence of some relevant, unmeasured set of covariates. The design of an observational study has a prominent effect on its sensitivity to hidden biases, and the best design may not be apparent without examining the data. One approach to facilitate a data-inspired design is to split the sample into a planning sample for choosing the design and an analysis sample for making inferences. We devise a powerful and flexible method for selecting outcomes in the planning sample when an unknown number of outcomes are affected by the treatment. We investigate the theoretical properties of our method and conduct extensive simulations that demonstrate pronounced benefits, especially at higher levels of allowance for unmeasured confounding. Finally, we demonstrate our method in an observational study of the multi-dimensional impacts of a devastating flood in Bangladesh.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00866&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>William Bekerman, Abhinandan Dalal, Carlo del Ninno, Dylan S. Small</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.TH" /><summary type="html">Observational studies are valuable tools for inferring causal effects in the absence of controlled experiments. However, these studies may be biased due to the presence of some relevant, unmeasured set of covariates. The design of an observational study has a prominent effect on its sensitivity to hidden biases, and the best design may not be apparent without examining the data. One approach to facilitate a data-inspired design is to split the sample into a planning sample for choosing the design and an analysis sample for making inferences. We devise a powerful and flexible method for selecting outcomes in the planning sample when an unknown number of outcomes are affected by the treatment. We investigate the theoretical properties of our method and conduct extensive simulations that demonstrate pronounced benefits, especially at higher levels of allowance for unmeasured confounding. Finally, we demonstrate our method in an observational study of the multi-dimensional impacts of a devastating flood in Bangladesh.</summary></entry><entry><title type="html">Polytomous Explanatory Item Response Models for Item Discrimination: Assessing Negative-Framing Effects in Social-Emotional Learning Surveys</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/PolytomousExplanatoryItemResponseModelsforItemDiscriminationAssessingNegativeFramingEffectsinSocialEmotionalLearningSurveys.html" rel="alternate" type="text/html" title="Polytomous Explanatory Item Response Models for Item Discrimination: Assessing Negative-Framing Effects in Social-Emotional Learning Surveys" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/PolytomousExplanatoryItemResponseModelsforItemDiscriminationAssessingNegativeFramingEffectsinSocialEmotionalLearningSurveys</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/PolytomousExplanatoryItemResponseModelsforItemDiscriminationAssessingNegativeFramingEffectsinSocialEmotionalLearningSurveys.html">&lt;p&gt;Modeling item parameters as a function of item characteristics has a long history but has generally focused on models for item location. Explanatory item response models for item discrimination are available but rarely used. In this study, we extend existing approaches for modeling item discrimination from dichotomous to polytomous item responses. We illustrate our proposed approach with an application to four social-emotional learning surveys of preschool children to investigate how item discrimination depends on whether an item is positively or negatively framed. Negative framing predicts significantly lower item discrimination on two of the four surveys, and a plausibly causal estimate from a regression discontinuity analysis shows that negative framing reduces discrimination by about 30\% on one survey. We conclude with a discussion of potential applications of explanatory models for item discrimination.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05304&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joshua B. Gilbert, Lijin Zhang, Esther Ulitzsch, Benjamin W. Domingue</name></author><category term="stat.ME" /><summary type="html">Modeling item parameters as a function of item characteristics has a long history but has generally focused on models for item location. Explanatory item response models for item discrimination are available but rarely used. In this study, we extend existing approaches for modeling item discrimination from dichotomous to polytomous item responses. We illustrate our proposed approach with an application to four social-emotional learning surveys of preschool children to investigate how item discrimination depends on whether an item is positively or negatively framed. Negative framing predicts significantly lower item discrimination on two of the four surveys, and a plausibly causal estimate from a regression discontinuity analysis shows that negative framing reduces discrimination by about 30\% on one survey. We conclude with a discussion of potential applications of explanatory models for item discrimination.</summary></entry><entry><title type="html">Position: Insights from Survey Methodology can Improve Training Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/PositionInsightsfromSurveyMethodologycanImproveTrainingData.html" rel="alternate" type="text/html" title="Position: Insights from Survey Methodology can Improve Training Data" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/PositionInsightsfromSurveyMethodologycanImproveTrainingData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/PositionInsightsfromSurveyMethodologycanImproveTrainingData.html">&lt;p&gt;Whether future AI models are fair, trustworthy, and aligned with the public’s interests rests in part on our ability to collect accurate data about what we want the models to do. However, collecting high-quality data is difficult, and few AI/ML researchers are trained in data collection methods. Recent research in data-centric AI has show that higher quality training data leads to better performing models, making this the right moment to introduce AI/ML researchers to the field of survey methodology, the science of data collection. We summarize insights from the survey methodology literature and discuss how they can improve the quality of training and feedback data. We also suggest collaborative research ideas into how biases in data collection can be mitigated, making models more accurate and human-centric.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.01208&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Stephanie Eckman, Barbara Plank, Frauke Kreuter</name></author><category term="stat.ME" /><summary type="html">Whether future AI models are fair, trustworthy, and aligned with the public’s interests rests in part on our ability to collect accurate data about what we want the models to do. However, collecting high-quality data is difficult, and few AI/ML researchers are trained in data collection methods. Recent research in data-centric AI has show that higher quality training data leads to better performing models, making this the right moment to introduce AI/ML researchers to the field of survey methodology, the science of data collection. We summarize insights from the survey methodology literature and discuss how they can improve the quality of training and feedback data. We also suggest collaborative research ideas into how biases in data collection can be mitigated, making models more accurate and human-centric.</summary></entry><entry><title type="html">Probabilistic Approach to Black-Box Binary Optimization with Budget Constraints: Application to Sensor Placement</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ProbabilisticApproachtoBlackBoxBinaryOptimizationwithBudgetConstraintsApplicationtoSensorPlacement.html" rel="alternate" type="text/html" title="Probabilistic Approach to Black-Box Binary Optimization with Budget Constraints: Application to Sensor Placement" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ProbabilisticApproachtoBlackBoxBinaryOptimizationwithBudgetConstraintsApplicationtoSensorPlacement</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ProbabilisticApproachtoBlackBoxBinaryOptimizationwithBudgetConstraintsApplicationtoSensorPlacement.html">&lt;p&gt;We present a fully probabilistic approach for solving binary optimization problems with black-box objective functions and with budget constraints. In the probabilistic approach, the optimization variable is viewed as a random variable and is associated with a parametric probability distribution. The original optimization problem is replaced with an optimization over the expected value of the original objective, which is then optimized over the probability distribution parameters. The resulting optimal parameter (optimal policy) is used to sample the binary space to produce estimates of the optimal solution(s) of the original binary optimization problem. The probability distribution is chosen from the family of Bernoulli models because the optimization variable is binary. The optimization constraints generally restrict the feasibility region. This can be achieved by modeling the random variable with a conditional distribution given satisfiability of the constraints. Thus, in this work we develop conditional Bernoulli distributions to model the random variable conditioned by the total number of nonzero entries, that is, the budget constraint. This approach (a) is generally applicable to binary optimization problems with nonstochastic black-box objective functions and budget constraints; (b) accounts for budget constraints by employing conditional probabilities that sample only the feasible region and thus considerably reduces the computational cost compared with employing soft constraints; and (c) does not employ soft constraints and thus does not require tuning of a regularization parameter, for example to promote sparsity, which is challenging in sensor placement optimization problems. The proposed approach is verified numerically by using an idealized bilinear binary optimization problem and is validated by using a sensor placement experiment in a parameter identification setup.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05830&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ahmed Attia</name></author><category term="stat.AP" /><summary type="html">We present a fully probabilistic approach for solving binary optimization problems with black-box objective functions and with budget constraints. In the probabilistic approach, the optimization variable is viewed as a random variable and is associated with a parametric probability distribution. The original optimization problem is replaced with an optimization over the expected value of the original objective, which is then optimized over the probability distribution parameters. The resulting optimal parameter (optimal policy) is used to sample the binary space to produce estimates of the optimal solution(s) of the original binary optimization problem. The probability distribution is chosen from the family of Bernoulli models because the optimization variable is binary. The optimization constraints generally restrict the feasibility region. This can be achieved by modeling the random variable with a conditional distribution given satisfiability of the constraints. Thus, in this work we develop conditional Bernoulli distributions to model the random variable conditioned by the total number of nonzero entries, that is, the budget constraint. This approach (a) is generally applicable to binary optimization problems with nonstochastic black-box objective functions and budget constraints; (b) accounts for budget constraints by employing conditional probabilities that sample only the feasible region and thus considerably reduces the computational cost compared with employing soft constraints; and (c) does not employ soft constraints and thus does not require tuning of a regularization parameter, for example to promote sparsity, which is challenging in sensor placement optimization problems. The proposed approach is verified numerically by using an idealized bilinear binary optimization problem and is validated by using a sensor placement experiment in a parameter identification setup.</summary></entry><entry><title type="html">Probabilistic Clustering using Shared Latent Variable Model for Assessing Alzheimers Disease Biomarkers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ProbabilisticClusteringusingSharedLatentVariableModelforAssessingAlzheimersDiseaseBiomarkers.html" rel="alternate" type="text/html" title="Probabilistic Clustering using Shared Latent Variable Model for Assessing Alzheimers Disease Biomarkers" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ProbabilisticClusteringusingSharedLatentVariableModelforAssessingAlzheimersDiseaseBiomarkers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/ProbabilisticClusteringusingSharedLatentVariableModelforAssessingAlzheimersDiseaseBiomarkers.html">&lt;p&gt;The preclinical stage of many neurodegenerative diseases can span decades before symptoms become apparent. Understanding the sequence of preclinical biomarker changes provides a critical opportunity for early diagnosis and effective intervention prior to significant loss of patients’ brain functions. The main challenge to early detection lies in the absence of direct observation of the disease state and the considerable variability in both biomarkers and disease dynamics among individuals. Recent research hypothesized the existence of subgroups with distinct biomarker patterns due to co-morbidities and degrees of brain resilience. Our ability to early diagnose and intervene during the preclinical stage of neurodegenerative diseases will be enhanced by further insights into heterogeneity in the biomarker-disease relationship. In this paper, we focus on Alzheimer’s disease (AD) and attempt to identify the systematic patterns within the heterogeneous AD biomarker-disease cascade. Specifically, we quantify the disease progression using a dynamic latent variable whose mixture distribution represents patient subgroups. Model estimation uses Hamiltonian Monte Carlo with the number of clusters determined by the Bayesian Information Criterion (BIC). We report simulation studies that investigate the performance of the proposed model in finite sample settings that are similar to our motivating application. We apply the proposed model to the BIOCARD data, a longitudinal study that was conducted over two decades among individuals who were initially cognitively normal. Our application yields evidence consistent with the hypothetical model of biomarker dynamics presented in Jack et al. (2013). In addition, our analysis identified two subgroups with distinct disease-onset patterns. Finally, we develop a dynamic prediction approach to improve the precision of prognoses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05193&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yizhen Xu, Scott Zeger, Zheyu Wang</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">The preclinical stage of many neurodegenerative diseases can span decades before symptoms become apparent. Understanding the sequence of preclinical biomarker changes provides a critical opportunity for early diagnosis and effective intervention prior to significant loss of patients’ brain functions. The main challenge to early detection lies in the absence of direct observation of the disease state and the considerable variability in both biomarkers and disease dynamics among individuals. Recent research hypothesized the existence of subgroups with distinct biomarker patterns due to co-morbidities and degrees of brain resilience. Our ability to early diagnose and intervene during the preclinical stage of neurodegenerative diseases will be enhanced by further insights into heterogeneity in the biomarker-disease relationship. In this paper, we focus on Alzheimer’s disease (AD) and attempt to identify the systematic patterns within the heterogeneous AD biomarker-disease cascade. Specifically, we quantify the disease progression using a dynamic latent variable whose mixture distribution represents patient subgroups. Model estimation uses Hamiltonian Monte Carlo with the number of clusters determined by the Bayesian Information Criterion (BIC). We report simulation studies that investigate the performance of the proposed model in finite sample settings that are similar to our motivating application. We apply the proposed model to the BIOCARD data, a longitudinal study that was conducted over two decades among individuals who were initially cognitively normal. Our application yields evidence consistent with the hypothetical model of biomarker dynamics presented in Jack et al. (2013). In addition, our analysis identified two subgroups with distinct disease-onset patterns. Finally, we develop a dynamic prediction approach to improve the precision of prognoses.</summary></entry><entry><title type="html">Projection predictive variable selection for discrete response families with finite support</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Projectionpredictivevariableselectionfordiscreteresponsefamilieswithfinitesupport.html" rel="alternate" type="text/html" title="Projection predictive variable selection for discrete response families with finite support" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Projectionpredictivevariableselectionfordiscreteresponsefamilieswithfinitesupport</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Projectionpredictivevariableselectionfordiscreteresponsefamilieswithfinitesupport.html">&lt;p&gt;The projection predictive variable selection is a decision-theoretically justified Bayesian variable selection approach achieving an outstanding trade-off between predictive performance and sparsity. Its projection problem is not easy to solve in general because it is based on the Kullback-Leibler divergence from a restricted posterior predictive distribution of the so-called reference model to the parameter-conditional predictive distribution of a candidate model. Previous work showed how this projection problem can be solved for response families employed in generalized linear models and how an approximate latent-space approach can be used for many other response families. Here, we present an exact projection method for all response families with discrete and finite support, called the augmented-data projection. A simulation study for an ordinal response family shows that the proposed method performs better than or similarly to the previously proposed approximate latent-space projection. The cost of the slightly better performance of the augmented-data projection is a substantial increase in runtime. Thus, in such cases, we recommend the latent projection in the early phase of a model-building workflow and the augmented-data projection for final results. The ordinal response family from our simulation study is supported by both projection methods, but we also include a real-world cancer subtyping example with a nominal response family, a case that is not supported by the latent projection.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.01660&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Frank Weber, Änne Glass, Aki Vehtari</name></author><category term="stat.ME" /><summary type="html">The projection predictive variable selection is a decision-theoretically justified Bayesian variable selection approach achieving an outstanding trade-off between predictive performance and sparsity. Its projection problem is not easy to solve in general because it is based on the Kullback-Leibler divergence from a restricted posterior predictive distribution of the so-called reference model to the parameter-conditional predictive distribution of a candidate model. Previous work showed how this projection problem can be solved for response families employed in generalized linear models and how an approximate latent-space approach can be used for many other response families. Here, we present an exact projection method for all response families with discrete and finite support, called the augmented-data projection. A simulation study for an ordinal response family shows that the proposed method performs better than or similarly to the previously proposed approximate latent-space projection. The cost of the slightly better performance of the augmented-data projection is a substantial increase in runtime. Thus, in such cases, we recommend the latent projection in the early phase of a model-building workflow and the augmented-data projection for final results. The ordinal response family from our simulation study is supported by both projection methods, but we also include a real-world cancer subtyping example with a nominal response family, a case that is not supported by the latent projection.</summary></entry><entry><title type="html">Random Effect Restricted Mean Survival Time Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/RandomEffectRestrictedMeanSurvivalTimeModel.html" rel="alternate" type="text/html" title="Random Effect Restricted Mean Survival Time Model" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/RandomEffectRestrictedMeanSurvivalTimeModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/RandomEffectRestrictedMeanSurvivalTimeModel.html">&lt;p&gt;The restricted mean survival time (RMST) model has been garnering attention as a way to provide a clinically intuitive measure: the mean survival time. RMST models, which use methods based on pseudo time-to-event values and inverse probability censoring weighting, can adjust covariates. However, no approach has yet been introduced that considers random effects for clusters. In this paper, we propose a new random-effect RMST. We present two methods of analysis that consider variable effects by i) using a generalized mixed model with pseudo-values and ii) integrating the estimated results from the inverse probability censoring weighting estimating equations for each cluster. We evaluate our proposed methods through computer simulations. In addition, we analyze the effect of a mother’s age at birth on under-five deaths in India using states as clusters.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.02048&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Keisuke Hanada, Masahiro Kojima</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The restricted mean survival time (RMST) model has been garnering attention as a way to provide a clinically intuitive measure: the mean survival time. RMST models, which use methods based on pseudo time-to-event values and inverse probability censoring weighting, can adjust covariates. However, no approach has yet been introduced that considers random effects for clusters. In this paper, we propose a new random-effect RMST. We present two methods of analysis that consider variable effects by i) using a generalized mixed model with pseudo-values and ii) integrating the estimated results from the inverse probability censoring weighting estimating equations for each cluster. We evaluate our proposed methods through computer simulations. In addition, we analyze the effect of a mother’s age at birth on under-five deaths in India using states as clusters.</summary></entry><entry><title type="html">Selecting the Number of Communities for Weighted Degree-Corrected Stochastic Block Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/SelectingtheNumberofCommunitiesforWeightedDegreeCorrectedStochasticBlockModels.html" rel="alternate" type="text/html" title="Selecting the Number of Communities for Weighted Degree-Corrected Stochastic Block Models" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/SelectingtheNumberofCommunitiesforWeightedDegreeCorrectedStochasticBlockModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/SelectingtheNumberofCommunitiesforWeightedDegreeCorrectedStochasticBlockModels.html">&lt;p&gt;We investigate how to select the number of communities for weighted networks without a full likelihood modeling. First, we propose a novel weighted degree-corrected stochastic block model (DCSBM), in which the mean adjacency matrix is modeled as the same as in standard DCSBM, while the variance profile matrix is assumed to be related to the mean adjacency matrix through a given variance function. Our method of selection the number of communities is based on a sequential testing framework, in each step the weighed DCSBM is fitted via some spectral clustering method. A key step is to carry out matrix scaling on the estimated variance profile matrix. The resulting scaling factors can be used to normalize the adjacency matrix, from which the testing statistic is obtained. Under mild conditions on the weighted DCSBM, our proposed procedure is shown to be consistent in estimating the true number of communities. Numerical experiments on both simulated and real network data also demonstrate the desirable empirical properties of our method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05340&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yucheng Liu, Xiaodong Li</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We investigate how to select the number of communities for weighted networks without a full likelihood modeling. First, we propose a novel weighted degree-corrected stochastic block model (DCSBM), in which the mean adjacency matrix is modeled as the same as in standard DCSBM, while the variance profile matrix is assumed to be related to the mean adjacency matrix through a given variance function. Our method of selection the number of communities is based on a sequential testing framework, in each step the weighed DCSBM is fitted via some spectral clustering method. A key step is to carry out matrix scaling on the estimated variance profile matrix. The resulting scaling factors can be used to normalize the adjacency matrix, from which the testing statistic is obtained. Under mild conditions on the weighted DCSBM, our proposed procedure is shown to be consistent in estimating the true number of communities. Numerical experiments on both simulated and real network data also demonstrate the desirable empirical properties of our method.</summary></entry><entry><title type="html">Smiles2Dock: an open large-scale multi-task dataset for ML-based molecular docking</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Smiles2DockanopenlargescalemultitaskdatasetforMLbasedmoleculardocking.html" rel="alternate" type="text/html" title="Smiles2Dock: an open large-scale multi-task dataset for ML-based molecular docking" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Smiles2DockanopenlargescalemultitaskdatasetforMLbasedmoleculardocking</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Smiles2DockanopenlargescalemultitaskdatasetforMLbasedmoleculardocking.html">&lt;p&gt;Docking is a crucial component in drug discovery aimed at predicting the binding conformation and affinity between small molecules and target proteins. ML-based docking has recently emerged as a prominent approach, outpacing traditional methods like DOCK and AutoDock Vina in handling the growing scale and complexity of molecular libraries. However, the availability of comprehensive and user-friendly datasets for training and benchmarking ML-based docking algorithms remains limited. We introduce Smiles2Dock, an open large-scale multi-task dataset for molecular docking. We created a framework combining P2Rank and AutoDock Vina to dock 1.7 million ligands from the ChEMBL database against 15 AlphaFold proteins, giving us more than 25 million protein-ligand binding scores. The dataset leverages a wide range of high-accuracy AlphaFold protein models, encompasses a diverse set of biologically relevant compounds and enables researchers to benchmark all major approaches for ML-based docking such as Graph, Transformer and CNN-based methods. We also introduce a novel Transformer-based architecture for docking scores prediction and set it as an initial benchmark for our dataset. Our dataset and code are publicly available to support the development of novel ML-based methods for molecular docking to advance scientific research in this field.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05738&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thomas Le Menestrel, Manuel Rivas</name></author><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Docking is a crucial component in drug discovery aimed at predicting the binding conformation and affinity between small molecules and target proteins. ML-based docking has recently emerged as a prominent approach, outpacing traditional methods like DOCK and AutoDock Vina in handling the growing scale and complexity of molecular libraries. However, the availability of comprehensive and user-friendly datasets for training and benchmarking ML-based docking algorithms remains limited. We introduce Smiles2Dock, an open large-scale multi-task dataset for molecular docking. We created a framework combining P2Rank and AutoDock Vina to dock 1.7 million ligands from the ChEMBL database against 15 AlphaFold proteins, giving us more than 25 million protein-ligand binding scores. The dataset leverages a wide range of high-accuracy AlphaFold protein models, encompasses a diverse set of biologically relevant compounds and enables researchers to benchmark all major approaches for ML-based docking such as Graph, Transformer and CNN-based methods. We also introduce a novel Transformer-based architecture for docking scores prediction and set it as an initial benchmark for our dataset. Our dataset and code are publicly available to support the development of novel ML-based methods for molecular docking to advance scientific research in this field.</summary></entry><entry><title type="html">Sparse estimation in ordinary kriging for functional data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Sparseestimationinordinarykrigingforfunctionaldata.html" rel="alternate" type="text/html" title="Sparse estimation in ordinary kriging for functional data" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Sparseestimationinordinarykrigingforfunctionaldata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Sparseestimationinordinarykrigingforfunctionaldata.html">&lt;p&gt;We introduce a sparse estimation in the ordinary kriging for functional data. The functional kriging predicts a feature given as a function at a location where the data are not observed by a linear combination of data observed at other locations. To estimate the weights of the linear combination, we apply the lasso-type regularization in minimizing the expected squared error. We derive an algorithm to derive the estimator using the augmented Lagrange method. Tuning parameters included in the estimation procedure are selected by cross-validation. Since the proposed method can shrink some of the weights of the linear combination toward zeros exactly, we can investigate which locations are necessary or unnecessary to predict the feature. Simulation and real data analysis show that the proposed method appropriately provides reasonable results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.15537&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hidetoshi Matsui, Yuya Yamakawa</name></author><category term="stat.ME" /><summary type="html">We introduce a sparse estimation in the ordinary kriging for functional data. The functional kriging predicts a feature given as a function at a location where the data are not observed by a linear combination of data observed at other locations. To estimate the weights of the linear combination, we apply the lasso-type regularization in minimizing the expected squared error. We derive an algorithm to derive the estimator using the augmented Lagrange method. Tuning parameters included in the estimation procedure are selected by cross-validation. Since the proposed method can shrink some of the weights of the linear combination toward zeros exactly, we can investigate which locations are necessary or unnecessary to predict the feature. Simulation and real data analysis show that the proposed method appropriately provides reasonable results.</summary></entry><entry><title type="html">Statistical Inference for Privatized Data with Unknown Sample Size</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/StatisticalInferenceforPrivatizedDatawithUnknownSampleSize.html" rel="alternate" type="text/html" title="Statistical Inference for Privatized Data with Unknown Sample Size" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/StatisticalInferenceforPrivatizedDatawithUnknownSampleSize</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/StatisticalInferenceforPrivatizedDatawithUnknownSampleSize.html">&lt;p&gt;We develop both theory and algorithms to analyze privatized data in the unbounded differential privacy(DP), where even the sample size is considered a sensitive quantity that requires privacy protection. We show that the distance between the sampling distributions under unbounded DP and bounded DP goes to zero as the sample size $n$ goes to infinity, provided that the noise used to privatize $n$ is at an appropriate rate; we also establish that ABC-type posterior distributions converge under similar assumptions. We further give asymptotic results in the regime where the privacy budget for $n$ goes to zero, establishing similarity of sampling distributions as well as showing that the MLE in the unbounded setting converges to the bounded-DP MLE. In order to facilitate valid, finite-sample Bayesian inference on privatized data in the unbounded DP setting, we propose a reversible jump MCMC algorithm which extends the data augmentation MCMC of Ju et al. (2022). We also propose a Monte Carlo EM algorithm to compute the MLE from privatized data in both bounded and unbounded DP. We apply our methodology to analyze a linear regression model as well as a 2019 American Time Use Survey Microdata File which we model using a Dirichlet distribution.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06231&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jordan Awan, Andres Felipe Barrientos, Nianqiao Ju</name></author><category term="stat.CO," /><category term="stat.TH" /><summary type="html">We develop both theory and algorithms to analyze privatized data in the unbounded differential privacy(DP), where even the sample size is considered a sensitive quantity that requires privacy protection. We show that the distance between the sampling distributions under unbounded DP and bounded DP goes to zero as the sample size $n$ goes to infinity, provided that the noise used to privatize $n$ is at an appropriate rate; we also establish that ABC-type posterior distributions converge under similar assumptions. We further give asymptotic results in the regime where the privacy budget for $n$ goes to zero, establishing similarity of sampling distributions as well as showing that the MLE in the unbounded setting converges to the bounded-DP MLE. In order to facilitate valid, finite-sample Bayesian inference on privatized data in the unbounded DP setting, we propose a reversible jump MCMC algorithm which extends the data augmentation MCMC of Ju et al. (2022). We also propose a Monte Carlo EM algorithm to compute the MLE from privatized data in both bounded and unbounded DP. We apply our methodology to analyze a linear regression model as well as a 2019 American Time Use Survey Microdata File which we model using a Dirichlet distribution.</summary></entry><entry><title type="html">Stochastic ordering of series and parallel systems lifetime in Archimedean copula under random shock</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/StochasticorderingofseriesandparallelsystemslifetimeinArchimedeancopulaunderrandomshock.html" rel="alternate" type="text/html" title="Stochastic ordering of series and parallel systems lifetime in Archimedean copula under random shock" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/StochasticorderingofseriesandparallelsystemslifetimeinArchimedeancopulaunderrandomshock</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/StochasticorderingofseriesandparallelsystemslifetimeinArchimedeancopulaunderrandomshock.html">&lt;p&gt;In this manuscript, we studied the stochastic ordering behavior of series as well as parallel systems lifetimes comprising dependent and heterogeneous components, experiencing random shocks, and exhibiting distinct dependency structures. We establish certain conditions for the lifetime of individual components, the dependency among components defined by Archimedean copulas, and the impact of random shocks on the overall system lifetime to reach the conclusion. We consider components whose survival functions are either increasing log-concave or decreasing log-convex functions of the parameters involved and systems exhibit different dependency structures. These conditions make it possible to compare the lifetimes of two systems using the usual stochastic order framework. Additionally, we provide examples and graphical representations to elucidate our theoretical findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05834&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sarikul Islam, Nitin Gupta</name></author><category term="stat.AP," /><category term="stat.TH" /><summary type="html">In this manuscript, we studied the stochastic ordering behavior of series as well as parallel systems lifetimes comprising dependent and heterogeneous components, experiencing random shocks, and exhibiting distinct dependency structures. We establish certain conditions for the lifetime of individual components, the dependency among components defined by Archimedean copulas, and the impact of random shocks on the overall system lifetime to reach the conclusion. We consider components whose survival functions are either increasing log-concave or decreasing log-convex functions of the parameters involved and systems exhibit different dependency structures. These conditions make it possible to compare the lifetimes of two systems using the usual stochastic order framework. Additionally, we provide examples and graphical representations to elucidate our theoretical findings.</summary></entry><entry><title type="html">The Multi-Range Theory of Translation Quality Measurement: MQM scoring models and Statistical Quality Control</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/TheMultiRangeTheoryofTranslationQualityMeasurementMQMscoringmodelsandStatisticalQualityControl.html" rel="alternate" type="text/html" title="The Multi-Range Theory of Translation Quality Measurement: MQM scoring models and Statistical Quality Control" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/TheMultiRangeTheoryofTranslationQualityMeasurementMQMscoringmodelsandStatisticalQualityControl</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/TheMultiRangeTheoryofTranslationQualityMeasurementMQMscoringmodelsandStatisticalQualityControl.html">&lt;p&gt;The year 2024 marks the 10th anniversary of the Multidimensional Quality Metrics (MQM) framework for analytic translation quality evaluation. The MQM error typology has been widely used by practitioners in the translation and localization industry and has served as the basis for many derivative projects. The annual Conference on Machine Translation (WMT) shared tasks on both human and automatic translation quality evaluations used the MQM error typology.
  The metric stands on two pillars: error typology and the scoring model. The scoring model calculates the quality score from annotation data, detailing how to convert error type and severity counts into numeric scores to determine if the content meets specifications. Previously, only the raw scoring model had been published. This April, the MQM Council published the Linear Calibrated Scoring Model, officially presented herein, along with the Non-Linear Scoring Model, which had not been published before.
  This paper details the latest MQM developments and presents a universal approach to translation quality measurement across three sample size ranges. It also explains why Statistical Quality Control should be used for very small sample sizes, starting from a single sentence.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16969&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arle Lommel, Serge Gladkoff, Alan Melby, Sue Ellen Wright, Ingemar Strandvik, Katerina Gasova, Angelika Vaasa, Andy Benzo, Romina Marazzato Sparano, Monica Foresi, Johani Innis, Lifeng Han, Goran Nenadic</name></author><category term="stat.AP" /><summary type="html">The year 2024 marks the 10th anniversary of the Multidimensional Quality Metrics (MQM) framework for analytic translation quality evaluation. The MQM error typology has been widely used by practitioners in the translation and localization industry and has served as the basis for many derivative projects. The annual Conference on Machine Translation (WMT) shared tasks on both human and automatic translation quality evaluations used the MQM error typology. The metric stands on two pillars: error typology and the scoring model. The scoring model calculates the quality score from annotation data, detailing how to convert error type and severity counts into numeric scores to determine if the content meets specifications. Previously, only the raw scoring model had been published. This April, the MQM Council published the Linear Calibrated Scoring Model, officially presented herein, along with the Non-Linear Scoring Model, which had not been published before. This paper details the latest MQM developments and presents a universal approach to translation quality measurement across three sample size ranges. It also explains why Statistical Quality Control should be used for very small sample sizes, starting from a single sentence.</summary></entry><entry><title type="html">The oracle property of the generalized outcome adaptive lasso</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Theoraclepropertyofthegeneralizedoutcomeadaptivelasso.html" rel="alternate" type="text/html" title="The oracle property of the generalized outcome adaptive lasso" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Theoraclepropertyofthegeneralizedoutcomeadaptivelasso</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Theoraclepropertyofthegeneralizedoutcomeadaptivelasso.html">&lt;p&gt;The generalized outcome-adaptive lasso (GOAL) is a variable selection for high-dimensional causal inference proposed by Bald&apos;e et al. [2023, {\em Biometrics} {\bfseries 79(1)}, 514–520]. When the dimension is high, it is now well established that an ideal variable selection method should have the oracle property to ensure the optimal large sample performance. However, the oracle property of GOAL has not been proven. In this paper, we show that the GOAL estimator enjoys the oracle property. Our simulation shows that the GOAL method deals with the collinearity problem better than the oracle-like method, the outcome-adaptive lasso (OAL).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.00250&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ismaila Baldé</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">The generalized outcome-adaptive lasso (GOAL) is a variable selection for high-dimensional causal inference proposed by Bald&apos;e et al. [2023, {\em Biometrics} {\bfseries 79(1)}, 514–520]. When the dimension is high, it is now well established that an ideal variable selection method should have the oracle property to ensure the optimal large sample performance. However, the oracle property of GOAL has not been proven. In this paper, we show that the GOAL estimator enjoys the oracle property. Our simulation shows that the GOAL method deals with the collinearity problem better than the oracle-like method, the outcome-adaptive lasso (OAL).</summary></entry><entry><title type="html">Toward identifiability of total effects in summary causal graphs with latent confounders: an extension of the front-door criterion</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Towardidentifiabilityoftotaleffectsinsummarycausalgraphswithlatentconfoundersanextensionofthefrontdoorcriterion.html" rel="alternate" type="text/html" title="Toward identifiability of total effects in summary causal graphs with latent confounders: an extension of the front-door criterion" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Towardidentifiabilityoftotaleffectsinsummarycausalgraphswithlatentconfoundersanextensionofthefrontdoorcriterion</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Towardidentifiabilityoftotaleffectsinsummarycausalgraphswithlatentconfoundersanextensionofthefrontdoorcriterion.html">&lt;p&gt;Conducting experiments to estimate total effects can be challenging due to cost, ethical concerns, or practical limitations. As an alternative, researchers often rely on causal graphs to determine if it is possible to identify these effects from observational data. Identifying total effects in fully specified non-temporal causal graphs has garnered considerable attention, with Pearl’s front-door criterion enabling the identification of total effects in the presence of latent confounding even when no variable set is sufficient for adjustment. However, specifying a complete causal graph is challenging in many domains. Extending these identifiability results to partially specified graphs is crucial, particularly in dynamic systems where causal relationships evolve over time. This paper addresses the challenge of identifying total effects using a specific and well-known partially specified graph in dynamic systems called a summary causal graph, which does not specify the temporal lag between causal relations and can contain cycles. In particular, this paper presents sufficient graphical conditions for identifying total effects from observational data, even in the presence of hidden confounding and when no variable set is sufficient for adjustment, contributing to the ongoing effort to understand and estimate causal effects from observational data using summary causal graphs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05805&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Charles K. Assaad</name></author><category term="stat.ME" /><summary type="html">Conducting experiments to estimate total effects can be challenging due to cost, ethical concerns, or practical limitations. As an alternative, researchers often rely on causal graphs to determine if it is possible to identify these effects from observational data. Identifying total effects in fully specified non-temporal causal graphs has garnered considerable attention, with Pearl’s front-door criterion enabling the identification of total effects in the presence of latent confounding even when no variable set is sufficient for adjustment. However, specifying a complete causal graph is challenging in many domains. Extending these identifiability results to partially specified graphs is crucial, particularly in dynamic systems where causal relationships evolve over time. This paper addresses the challenge of identifying total effects using a specific and well-known partially specified graph in dynamic systems called a summary causal graph, which does not specify the temporal lag between causal relations and can contain cycles. In particular, this paper presents sufficient graphical conditions for identifying total effects from observational data, even in the presence of hidden confounding and when no variable set is sufficient for adjustment, contributing to the ongoing effort to understand and estimate causal effects from observational data using summary causal graphs.</summary></entry><entry><title type="html">Tree balance in phylogenetic models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Treebalanceinphylogeneticmodels.html" rel="alternate" type="text/html" title="Tree balance in phylogenetic models" /><published>2024-06-11T00:00:00+00:00</published><updated>2024-06-11T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Treebalanceinphylogeneticmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/11/Treebalanceinphylogeneticmodels.html">&lt;p&gt;Tree shape statistics, particularly measures of tree (im)balance, play an important role in the analysis of the shape of phylogenetic trees. With applications ranging from testing evolutionary models to studying the impact of fertility inheritance and selection, or tumor development and language evolution, the assessment of tree balance is crucial. Currently, a multitude of at least 30 (im)balance indices can be found in the literature, alongside numerous other tree shape statistics.
  This diversity prompts essential questions: How can we minimize the selection of indices to mitigate the challenges of multiple testing? Is there a preeminent balance index tailored to specific tasks? Previous studies comparing the statistical power of indices in detecting trees deviating from the Yule model have been limited in scope, utilizing only a subset of indices and alternative tree models.
  This research expands upon the examination of index power, encompassing all established indices and a broader array of alternative models. Our investigation reveals distinct groups of balance indices better suited for different tree models, suggesting that decisions on balance index selection can be enhanced with prior knowledge. Furthermore, we present the \textsf{R} software package \textsf{poweRbal} which allows the inclusion of new indices and models, thus facilitating future research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05185&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sophie J. Kersting, Kristina Wicke, Mareike Fischer</name></author><category term="stat.AP" /><summary type="html">Tree shape statistics, particularly measures of tree (im)balance, play an important role in the analysis of the shape of phylogenetic trees. With applications ranging from testing evolutionary models to studying the impact of fertility inheritance and selection, or tumor development and language evolution, the assessment of tree balance is crucial. Currently, a multitude of at least 30 (im)balance indices can be found in the literature, alongside numerous other tree shape statistics. This diversity prompts essential questions: How can we minimize the selection of indices to mitigate the challenges of multiple testing? Is there a preeminent balance index tailored to specific tasks? Previous studies comparing the statistical power of indices in detecting trees deviating from the Yule model have been limited in scope, utilizing only a subset of indices and alternative tree models. This research expands upon the examination of index power, encompassing all established indices and a broader array of alternative models. Our investigation reveals distinct groups of balance indices better suited for different tree models, suggesting that decisions on balance index selection can be enhanced with prior knowledge. Furthermore, we present the \textsf{R} software package \textsf{poweRbal} which allows the inclusion of new indices and models, thus facilitating future research.</summary></entry></feed>