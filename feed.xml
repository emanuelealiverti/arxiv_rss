<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-10T07:15:12+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A critical appraisal of water table depth estimation: Challenges and opportunities within machine learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/AcriticalappraisalofwatertabledepthestimationChallengesandopportunitieswithinmachinelearning.html" rel="alternate" type="text/html" title="A critical appraisal of water table depth estimation: Challenges and opportunities within machine learning" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/AcriticalappraisalofwatertabledepthestimationChallengesandopportunitieswithinmachinelearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/AcriticalappraisalofwatertabledepthestimationChallengesandopportunitieswithinmachinelearning.html">&lt;p&gt;Fine-resolution spatial patterns of water table depth (WTD) can inform the dynamics of groundwater-dependent systems, including ecological, hydrological, and anthropogenic systems. Generally, a large-scale (e.g., continental or global) spatial map of static WTD can be simulated using either physically-based (PB) or machine learning-based (ML) models. We construct three fine-resolution (500 m) ML simulations of WTD, using the XGBoost algorithm and more than 20 million real and proxy observations of WTD, across the United States and Canada. The three ML models were constrained using known physical relations between WTD’s drivers and WTD and were trained by sequentially adding real and proxy observations of WTD. We interpret the black box of our physically constrained ML models and compare it against available literature in groundwater hydrology. Through an extensive (pixel-by-pixel) evaluation, we demonstrate that our models can more accurately predict unseen real and proxy observations of WTD across most of North America’s ecoregions compared to three available PB simulations of WTD. However, we still argue that large-scale WTD estimation is far from being a solved problem. We reason that due to biased and untrustworthy observational data, the misspecification of physically-based equations, and the over-flexibility of machine learning models, our community’s confidence in ML or PB simulations of WTD is far too high and verifiably accurate simulations of WTD do not yet exist in the literature, particularly in arid high-elevation landscapes. Ultimately, we thoroughly discuss future directions that may help hydrogeologists decide how to proceed with WTD estimations, with a particular focus on the application of machine learning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04579&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joseph Janssen, Ardalan Tootchi, Ali A. Ameli</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Fine-resolution spatial patterns of water table depth (WTD) can inform the dynamics of groundwater-dependent systems, including ecological, hydrological, and anthropogenic systems. Generally, a large-scale (e.g., continental or global) spatial map of static WTD can be simulated using either physically-based (PB) or machine learning-based (ML) models. We construct three fine-resolution (500 m) ML simulations of WTD, using the XGBoost algorithm and more than 20 million real and proxy observations of WTD, across the United States and Canada. The three ML models were constrained using known physical relations between WTD’s drivers and WTD and were trained by sequentially adding real and proxy observations of WTD. We interpret the black box of our physically constrained ML models and compare it against available literature in groundwater hydrology. Through an extensive (pixel-by-pixel) evaluation, we demonstrate that our models can more accurately predict unseen real and proxy observations of WTD across most of North America’s ecoregions compared to three available PB simulations of WTD. However, we still argue that large-scale WTD estimation is far from being a solved problem. We reason that due to biased and untrustworthy observational data, the misspecification of physically-based equations, and the over-flexibility of machine learning models, our community’s confidence in ML or PB simulations of WTD is far too high and verifiably accurate simulations of WTD do not yet exist in the literature, particularly in arid high-elevation landscapes. Ultimately, we thoroughly discuss future directions that may help hydrogeologists decide how to proceed with WTD estimations, with a particular focus on the application of machine learning.</summary></entry><entry><title type="html">Advancing Distribution Decomposition Methods Beyond Common Supports: Applications to Racial Wealth Disparities</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/AdvancingDistributionDecompositionMethodsBeyondCommonSupportsApplicationstoRacialWealthDisparities.html" rel="alternate" type="text/html" title="Advancing Distribution Decomposition Methods Beyond Common Supports: Applications to Racial Wealth Disparities" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/AdvancingDistributionDecompositionMethodsBeyondCommonSupportsApplicationstoRacialWealthDisparities</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/AdvancingDistributionDecompositionMethodsBeyondCommonSupportsApplicationstoRacialWealthDisparities.html">&lt;p&gt;I generalize state-of-the-art approaches that decompose differences in the distribution of a variable of interest between two groups into a portion explained by covariates and a residual portion. The method that I propose relaxes the overlapping supports assumption, allowing the groups being compared to not necessarily share exactly the same covariate support. I illustrate my method revisiting the black-white wealth gap in the U.S. as a function of labor income and other variables. Traditionally used decomposition methods would trim (or assign zero weight to) observations that lie outside the common covariate support region. On the other hand, by allowing all observations to contribute to the existing wealth gap, I find that otherwise trimmed observations contribute from 3% to 19% to the overall wealth gap, at different portions of the wealth distribution.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05759&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bernardo Modenesi</name></author><category term="stat.ME" /><summary type="html">I generalize state-of-the-art approaches that decompose differences in the distribution of a variable of interest between two groups into a portion explained by covariates and a residual portion. The method that I propose relaxes the overlapping supports assumption, allowing the groups being compared to not necessarily share exactly the same covariate support. I illustrate my method revisiting the black-white wealth gap in the U.S. as a function of labor income and other variables. Traditionally used decomposition methods would trim (or assign zero weight to) observations that lie outside the common covariate support region. On the other hand, by allowing all observations to contribute to the existing wealth gap, I find that otherwise trimmed observations contribute from 3% to 19% to the overall wealth gap, at different portions of the wealth distribution.</summary></entry><entry><title type="html">A fast and accurate inferential method for complex parametric models: the implicit bootstrap</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Afastandaccurateinferentialmethodforcomplexparametricmodelstheimplicitbootstrap.html" rel="alternate" type="text/html" title="A fast and accurate inferential method for complex parametric models: the implicit bootstrap" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Afastandaccurateinferentialmethodforcomplexparametricmodelstheimplicitbootstrap</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Afastandaccurateinferentialmethodforcomplexparametricmodelstheimplicitbootstrap.html">&lt;p&gt;Performing inference such a computing confidence intervals is traditionally done, in the parametric case, by first fitting a model and then using the estimates to compute quantities derived at the asymptotic level or by means of simulations such as the ones from the family of bootstrap methods. These methods require the derivation and computation of a consistent estimator that can be very challenging to obtain when the models are complex as is the case for example when the data exhibit some types of features such as censoring, missclassification errors or contain outliers. In this paper, we propose a simulation based inferential method, the implicit bootstrap, that bypasses the need to compute a consistent estimator and can therefore be easily implemented. While being transformation respecting, we show that under similar conditions as for the studentized bootstrap, without the need of a consistent estimator, the implicit bootstrap is first and second order accurate. Using simulation studies, we also show the coverage accuracy of the method with data settings for which traditional methods are computationally very involving and also lead to poor coverage, especially when the sample size is relatively small. Based on these empirical results, we also explore theoretically the case of exact inference.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05403&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Samuel Orso, Mucyo Karemera, Maria-Pia Victoria-Feser, Stéphane Guerrier</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Performing inference such a computing confidence intervals is traditionally done, in the parametric case, by first fitting a model and then using the estimates to compute quantities derived at the asymptotic level or by means of simulations such as the ones from the family of bootstrap methods. These methods require the derivation and computation of a consistent estimator that can be very challenging to obtain when the models are complex as is the case for example when the data exhibit some types of features such as censoring, missclassification errors or contain outliers. In this paper, we propose a simulation based inferential method, the implicit bootstrap, that bypasses the need to compute a consistent estimator and can therefore be easily implemented. While being transformation respecting, we show that under similar conditions as for the studentized bootstrap, without the need of a consistent estimator, the implicit bootstrap is first and second order accurate. Using simulation studies, we also show the coverage accuracy of the method with data settings for which traditional methods are computationally very involving and also lead to poor coverage, especially when the sample size is relatively small. Based on these empirical results, we also explore theoretically the case of exact inference.</summary></entry><entry><title type="html">An Efficient Finite Difference Approximation via a Double Sample-Recycling Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/AnEfficientFiniteDifferenceApproximationviaaDoubleSampleRecyclingApproach.html" rel="alternate" type="text/html" title="An Efficient Finite Difference Approximation via a Double Sample-Recycling Approach" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/AnEfficientFiniteDifferenceApproximationviaaDoubleSampleRecyclingApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/AnEfficientFiniteDifferenceApproximationviaaDoubleSampleRecyclingApproach.html">&lt;p&gt;Estimating stochastic gradients is pivotal in fields like service systems within operations research. The classical method for this estimation is the finite difference approximation, which entails generating samples at perturbed inputs. Nonetheless, practical challenges persist in determining the perturbation and obtaining an optimal finite difference estimator in the sense of possessing the smallest mean squared error (MSE). To tackle this problem, we propose a double sample-recycling approach in this paper. Firstly, pilot samples are recycled to estimate the optimal perturbation. Secondly, recycling these pilot samples again and generating new samples at the estimated perturbation, lead to an efficient finite difference estimator. We analyze its bias, variance and MSE. Our analyses demonstrate a reduction in asymptotic variance, and in some cases, a decrease in asymptotic bias, compared to the optimal finite difference estimator. Therefore, our proposed estimator consistently coincides with, or even outperforms the optimal finite difference estimator. In numerical experiments, we apply the estimator in several examples, and numerical results demonstrate its robustness, as well as coincidence with the theory presented, especially in the case of small sample sizes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05638&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Guo Liang, Guangwu Liu, Kun Zhang</name></author><category term="stat.ME" /><summary type="html">Estimating stochastic gradients is pivotal in fields like service systems within operations research. The classical method for this estimation is the finite difference approximation, which entails generating samples at perturbed inputs. Nonetheless, practical challenges persist in determining the perturbation and obtaining an optimal finite difference estimator in the sense of possessing the smallest mean squared error (MSE). To tackle this problem, we propose a double sample-recycling approach in this paper. Firstly, pilot samples are recycled to estimate the optimal perturbation. Secondly, recycling these pilot samples again and generating new samples at the estimated perturbation, lead to an efficient finite difference estimator. We analyze its bias, variance and MSE. Our analyses demonstrate a reduction in asymptotic variance, and in some cases, a decrease in asymptotic bias, compared to the optimal finite difference estimator. Therefore, our proposed estimator consistently coincides with, or even outperforms the optimal finite difference estimator. In numerical experiments, we apply the estimator in several examples, and numerical results demonstrate its robustness, as well as coincidence with the theory presented, especially in the case of small sample sizes.</summary></entry><entry><title type="html">Cancer mortality projection: disparities, COVID-19, and late diagnosis impact</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/CancermortalityprojectiondisparitiesCOVID19andlatediagnosisimpact.html" rel="alternate" type="text/html" title="Cancer mortality projection: disparities, COVID-19, and late diagnosis impact" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/CancermortalityprojectiondisparitiesCOVID19andlatediagnosisimpact</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/CancermortalityprojectiondisparitiesCOVID19andlatediagnosisimpact.html">&lt;p&gt;This paper investigates projection of two major causes of cancer mortality, breast cancer and lung cancer, by using a Bayesian modelling framework. We investigate patterns in 2001-2018 (as baseline) in cause-specific cancer mortality and project these by year of death and various risk factors: age, gender, regions of England, income deprivation quintile, average age-at-diagnosis, and non-smoker prevalence rates. We then assess excess cancer mortality during the COVID-19 pandemic years, and we examine the impact of diagnosis delays on lung cancer mortality across various scenarios. Our findings indicate that socio-economic disparities in lung cancer mortality will persist in the future. Additionally, we observe slight variations in breast cancer mortality across different regions up to 2036. Furthermore, marginal increases in excess deaths from lung and breast cancer are estimated in specific regions of England throughout the pandemic year (2020-2022), contrasting with the national trend. However, the excess lung cancer deaths markedly differ by age, region and deprivation as a result of delays in cancer diagnosis. Specifically, we find a notably higher number of excess deaths in the northern regions of England compared to the southern regions, as well as among individuals living in the most deprived areas compared to those in the least deprived areas.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05643&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>A. Arik, A. J. G. Cairns, G. Streftaris</name></author><category term="stat.AP" /><summary type="html">This paper investigates projection of two major causes of cancer mortality, breast cancer and lung cancer, by using a Bayesian modelling framework. We investigate patterns in 2001-2018 (as baseline) in cause-specific cancer mortality and project these by year of death and various risk factors: age, gender, regions of England, income deprivation quintile, average age-at-diagnosis, and non-smoker prevalence rates. We then assess excess cancer mortality during the COVID-19 pandemic years, and we examine the impact of diagnosis delays on lung cancer mortality across various scenarios. Our findings indicate that socio-economic disparities in lung cancer mortality will persist in the future. Additionally, we observe slight variations in breast cancer mortality across different regions up to 2036. Furthermore, marginal increases in excess deaths from lung and breast cancer are estimated in specific regions of England throughout the pandemic year (2020-2022), contrasting with the national trend. However, the excess lung cancer deaths markedly differ by age, region and deprivation as a result of delays in cancer diagnosis. Specifically, we find a notably higher number of excess deaths in the northern regions of England compared to the southern regions, as well as among individuals living in the most deprived areas compared to those in the least deprived areas.</summary></entry><entry><title type="html">Causal Diffusion Autoencoders: Toward Counterfactual Generation via Diffusion Probabilistic Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/CausalDiffusionAutoencodersTowardCounterfactualGenerationviaDiffusionProbabilisticModels.html" rel="alternate" type="text/html" title="Causal Diffusion Autoencoders: Toward Counterfactual Generation via Diffusion Probabilistic Models" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/CausalDiffusionAutoencodersTowardCounterfactualGenerationviaDiffusionProbabilisticModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/CausalDiffusionAutoencodersTowardCounterfactualGenerationviaDiffusionProbabilisticModels.html">&lt;p&gt;Diffusion probabilistic models (DPMs) have become the state-of-the-art in high-quality image generation. However, DPMs have an arbitrary noisy latent space with no interpretable or controllable semantics. Although there has been significant research effort to improve image sample quality, there is little work on representation-controlled generation using diffusion models. Specifically, causal modeling and controllable counterfactual generation using DPMs is an underexplored area. In this work, we propose CausalDiffAE, a diffusion-based causal representation learning framework to enable counterfactual generation according to a specified causal model. Our key idea is to use an encoder to extract high-level semantically meaningful causal variables from high-dimensional data and model stochastic variation using reverse diffusion. We propose a causal encoding mechanism that maps high-dimensional data to causally related latent factors and parameterize the causal mechanisms among latent factors using neural networks. To enforce the disentanglement of causal variables, we formulate a variational objective and leverage auxiliary label information in a prior to regularize the latent space. We propose a DDIM-based counterfactual generation procedure subject to do-interventions. Finally, to address the limited label supervision scenario, we also study the application of CausalDiffAE when a part of the training data is unlabeled, which also enables granular control over the strength of interventions in generating counterfactuals during inference. We empirically show that CausalDiffAE learns a disentangled latent space and is capable of generating high-quality counterfactual images.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17735&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aneesh Komanduri, Chen Zhao, Feng Chen, Xintao Wu</name></author><category term="stat.ME" /><summary type="html">Diffusion probabilistic models (DPMs) have become the state-of-the-art in high-quality image generation. However, DPMs have an arbitrary noisy latent space with no interpretable or controllable semantics. Although there has been significant research effort to improve image sample quality, there is little work on representation-controlled generation using diffusion models. Specifically, causal modeling and controllable counterfactual generation using DPMs is an underexplored area. In this work, we propose CausalDiffAE, a diffusion-based causal representation learning framework to enable counterfactual generation according to a specified causal model. Our key idea is to use an encoder to extract high-level semantically meaningful causal variables from high-dimensional data and model stochastic variation using reverse diffusion. We propose a causal encoding mechanism that maps high-dimensional data to causally related latent factors and parameterize the causal mechanisms among latent factors using neural networks. To enforce the disentanglement of causal variables, we formulate a variational objective and leverage auxiliary label information in a prior to regularize the latent space. We propose a DDIM-based counterfactual generation procedure subject to do-interventions. Finally, to address the limited label supervision scenario, we also study the application of CausalDiffAE when a part of the training data is unlabeled, which also enables granular control over the strength of interventions in generating counterfactuals during inference. We empirically show that CausalDiffAE learns a disentangled latent space and is capable of generating high-quality counterfactual images.</summary></entry><entry><title type="html">Change point localisation and inference in fragmented functional data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Changepointlocalisationandinferenceinfragmentedfunctionaldata.html" rel="alternate" type="text/html" title="Change point localisation and inference in fragmented functional data" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Changepointlocalisationandinferenceinfragmentedfunctionaldata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Changepointlocalisationandinferenceinfragmentedfunctionaldata.html">&lt;p&gt;We study the problem of change point localisation and inference for sequentially collected fragmented functional data, where each curve is observed only over discrete grids randomly sampled over a short fragment. The sequence of underlying covariance functions is assumed to be piecewise constant, with changes happening at unknown time points. To localise the change points, we propose a computationally efficient fragmented functional dynamic programming (FFDP) algorithm with consistent change point localisation rates. With an extra step of local refinement, we derive the limiting distributions for the refined change point estimators in two different regimes where the minimal jump size vanishes and where it remains constant as the sample size diverges. Such results are the first time seen in the fragmented functional data literature. As a byproduct of independent interest, we also present a non-asymptotic result on the estimation error of the covariance function estimators over intervals with change points inspired by Lin et al. (2021). Our result accounts for the effects of the sampling grid size within each fragment under novel identifiability conditions. Extensive numerical studies are also provided to support our theoretical results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05730&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Gengyu Xue, Haotian Xu, Yi Yu</name></author><category term="stat.ME" /><summary type="html">We study the problem of change point localisation and inference for sequentially collected fragmented functional data, where each curve is observed only over discrete grids randomly sampled over a short fragment. The sequence of underlying covariance functions is assumed to be piecewise constant, with changes happening at unknown time points. To localise the change points, we propose a computationally efficient fragmented functional dynamic programming (FFDP) algorithm with consistent change point localisation rates. With an extra step of local refinement, we derive the limiting distributions for the refined change point estimators in two different regimes where the minimal jump size vanishes and where it remains constant as the sample size diverges. Such results are the first time seen in the fragmented functional data literature. As a byproduct of independent interest, we also present a non-asymptotic result on the estimation error of the covariance function estimators over intervals with change points inspired by Lin et al. (2021). Our result accounts for the effects of the sampling grid size within each fragment under novel identifiability conditions. Extensive numerical studies are also provided to support our theoretical results.</summary></entry><entry><title type="html">Consistent Empirical Bayes estimation of the mean of a mixing distribution without identifiability assumption. With applications to treatment of non-response</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/ConsistentEmpiricalBayesestimationofthemeanofamixingdistributionwithoutidentifiabilityassumptionWithapplicationstotreatmentofnonresponse.html" rel="alternate" type="text/html" title="Consistent Empirical Bayes estimation of the mean of a mixing distribution without identifiability assumption. With applications to treatment of non-response" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/ConsistentEmpiricalBayesestimationofthemeanofamixingdistributionwithoutidentifiabilityassumptionWithapplicationstotreatmentofnonresponse</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/ConsistentEmpiricalBayesestimationofthemeanofamixingdistributionwithoutidentifiabilityassumptionWithapplicationstotreatmentofnonresponse.html">&lt;p&gt;{\bf Abstract}
  Consider a Non-Parametric Empirical Bayes (NPEB) setup. We observe $Y_i, \sim f(y|\theta_i)$, $\theta_i \in \Theta$ independent, where $\theta_i \sim G$ are independent $i=1,…,n$. The mixing distribution $G$ is unknown $G \in {G}$ with no parametric assumptions about the class ${G }$. The common NPEB task is to estimate $\theta_i, \; i=1,…,n$. Conditions that imply ‘optimality’ of such NPEB estimators typically require identifiability of $G$ based on $Y_1,…,Y_n$. We consider the task of estimating $E_G \theta$. We show that `often’ consistent estimation of $E_G \theta$ is implied without identifiability.
  We motivate the later task, especially in setups with non-response and missing data. We demonstrate consistency in simulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05656&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Eitan Greenshtein</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">{\bf Abstract} Consider a Non-Parametric Empirical Bayes (NPEB) setup. We observe $Y_i, \sim f(y|\theta_i)$, $\theta_i \in \Theta$ independent, where $\theta_i \sim G$ are independent $i=1,…,n$. The mixing distribution $G$ is unknown $G \in {G}$ with no parametric assumptions about the class ${G }$. The common NPEB task is to estimate $\theta_i, \; i=1,…,n$. Conditions that imply ‘optimality’ of such NPEB estimators typically require identifiability of $G$ based on $Y_1,…,Y_n$. We consider the task of estimating $E_G \theta$. We show that `often’ consistent estimation of $E_G \theta$ is implied without identifiability. We motivate the later task, especially in setups with non-response and missing data. We demonstrate consistency in simulations.</summary></entry><entry><title type="html">Decompounding Under General Mixing Distributions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/DecompoundingUnderGeneralMixingDistributions.html" rel="alternate" type="text/html" title="Decompounding Under General Mixing Distributions" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/DecompoundingUnderGeneralMixingDistributions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/DecompoundingUnderGeneralMixingDistributions.html">&lt;p&gt;This study focuses on statistical inference for compound models of the form $X=\xi_1+\ldots+\xi_N$, where $N$ is a random variable denoting the count of summands, which are independent and identically distributed (i.i.d.) random variables $\xi_1, \xi_2, \ldots$. The paper addresses the problem of reconstructing the distribution of $\xi$ from observed samples of $X$’s distribution, a process referred to as decompounding, with the assumption that $N$’s distribution is known. This work diverges from the conventional scope by not limiting $N$’s distribution to the Poisson type, thus embracing a broader context. We propose a nonparametric estimate for the density of $\xi$, derive its rates of convergence and prove that these rates are minimax optimal for suitable classes of distributions for $\xi$ and $N$. Finally, we illustrate the numerical performance of the algorithm on simulated examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05419&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Denis Belomestny, Ekaterina Morozova, Vladimir Panov</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This study focuses on statistical inference for compound models of the form $X=\xi_1+\ldots+\xi_N$, where $N$ is a random variable denoting the count of summands, which are independent and identically distributed (i.i.d.) random variables $\xi_1, \xi_2, \ldots$. The paper addresses the problem of reconstructing the distribution of $\xi$ from observed samples of $X$’s distribution, a process referred to as decompounding, with the assumption that $N$’s distribution is known. This work diverges from the conventional scope by not limiting $N$’s distribution to the Poisson type, thus embracing a broader context. We propose a nonparametric estimate for the density of $\xi$, derive its rates of convergence and prove that these rates are minimax optimal for suitable classes of distributions for $\xi$ and $N$. Finally, we illustrate the numerical performance of the algorithm on simulated examples.</summary></entry><entry><title type="html">Democratizing Uncertainty Quantification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/DemocratizingUncertaintyQuantification.html" rel="alternate" type="text/html" title="Democratizing Uncertainty Quantification" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/DemocratizingUncertaintyQuantification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/DemocratizingUncertaintyQuantification.html">&lt;p&gt;Uncertainty Quantification (UQ) is vital to safety-critical model-based analyses, but the widespread adoption of sophisticated UQ methods is limited by technical complexity. In this paper, we introduce UM-Bridge (the UQ and Modeling Bridge), a high-level abstraction and software protocol that facilitates universal interoperability of UQ software with simulation codes. It breaks down the technical complexity of advanced UQ applications and enables separation of concerns between experts. UM-Bridge democratizes UQ by allowing effective interdisciplinary collaboration, accelerating the development of advanced UQ methods, and making it easy to perform UQ analyses from prototype to High Performance Computing (HPC) scale.
  In addition, we present a library of ready-to-run UQ benchmark problems, all easily accessible through UM-Bridge. These benchmarks support UQ methodology research, enabling reproducible performance comparisons. We demonstrate UM-Bridge with several scientific applications, harnessing HPC resources even using UQ codes not designed with HPC support.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.13768&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Linus Seelinger, Anne Reinarz, Mikkel B. Lykkegaard, Robert Akers, Amal M. A. Alghamdi, David Aristoff, Wolfgang Bangerth, Jean Bénézech, Matteo Diez, Kurt Frey, John D. Jakeman, Jakob S. J{\o}rgensen, Ki-Tae Kim, Massimiliano Martinelli, Matthew Parno, Riccardo Pellegrini, Noemi Petra, Nicolai A. B. Riis, Katherine Rosenfeld, Andrea Serani, Lorenzo Tamellini, Umberto Villa, Tim J. Dodwell, Robert Scheichl</name></author><category term="stat.AP" /><summary type="html">Uncertainty Quantification (UQ) is vital to safety-critical model-based analyses, but the widespread adoption of sophisticated UQ methods is limited by technical complexity. In this paper, we introduce UM-Bridge (the UQ and Modeling Bridge), a high-level abstraction and software protocol that facilitates universal interoperability of UQ software with simulation codes. It breaks down the technical complexity of advanced UQ applications and enables separation of concerns between experts. UM-Bridge democratizes UQ by allowing effective interdisciplinary collaboration, accelerating the development of advanced UQ methods, and making it easy to perform UQ analyses from prototype to High Performance Computing (HPC) scale. In addition, we present a library of ready-to-run UQ benchmark problems, all easily accessible through UM-Bridge. These benchmarks support UQ methodology research, enabling reproducible performance comparisons. We demonstrate UM-Bridge with several scientific applications, harnessing HPC resources even using UQ codes not designed with HPC support.</summary></entry><entry><title type="html">Derivative based global sensitivity analysis and its entropic link</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Derivativebasedglobalsensitivityanalysisanditsentropiclink.html" rel="alternate" type="text/html" title="Derivative based global sensitivity analysis and its entropic link" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Derivativebasedglobalsensitivityanalysisanditsentropiclink</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Derivativebasedglobalsensitivityanalysisanditsentropiclink.html">&lt;p&gt;Variance-based Sobol’ sensitivity is one of the most well known measures in global sensitivity analysis (GSA). However, uncertainties with certain distributions, such as highly skewed distributions or those with a heavy tail, cannot be adequately characterised using the second central moment only. Entropy-based GSA can consider the entire probability density function, but its application has been limited because it is difficult to estimate. Here we present a novel derivative-based upper bound for conditional entropies, to efficiently rank uncertain variables and to work as a proxy for entropy-based total effect indices. To overcome the non-desirable issue of negativity for differential entropies as sensitivity indices, we discuss an exponentiation of the total effect entropy and its proxy. We found that the proposed new entropy proxy is equivalent to the proxy for variance-based GSA for linear functions with Gaussian inputs, but outperforms the latter for a river flood physics model with 8 inputs of different distributions. We expect the new entropy proxy to increase the variable screening power of derivative-based GSA and to complement Sobol’-index proxy for a more diverse type of distributions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.00551&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiannan Yang</name></author><category term="stat.CO" /><summary type="html">Variance-based Sobol’ sensitivity is one of the most well known measures in global sensitivity analysis (GSA). However, uncertainties with certain distributions, such as highly skewed distributions or those with a heavy tail, cannot be adequately characterised using the second central moment only. Entropy-based GSA can consider the entire probability density function, but its application has been limited because it is difficult to estimate. Here we present a novel derivative-based upper bound for conditional entropies, to efficiently rank uncertain variables and to work as a proxy for entropy-based total effect indices. To overcome the non-desirable issue of negativity for differential entropies as sensitivity indices, we discuss an exponentiation of the total effect entropy and its proxy. We found that the proposed new entropy proxy is equivalent to the proxy for variance-based GSA for linear functions with Gaussian inputs, but outperforms the latter for a river flood physics model with 8 inputs of different distributions. We expect the new entropy proxy to increase the variable screening power of derivative-based GSA and to complement Sobol’-index proxy for a more diverse type of distributions.</summary></entry><entry><title type="html">Estimation and Inference for Change Points in Functional Regression Time Series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/EstimationandInferenceforChangePointsinFunctionalRegressionTimeSeries.html" rel="alternate" type="text/html" title="Estimation and Inference for Change Points in Functional Regression Time Series" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/EstimationandInferenceforChangePointsinFunctionalRegressionTimeSeries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/EstimationandInferenceforChangePointsinFunctionalRegressionTimeSeries.html">&lt;p&gt;In this paper, we study the estimation and inference of change points under a functional linear regression model with changes in the slope function. We present a novel Functional Regression Binary Segmentation (FRBS) algorithm which is computationally efficient as well as achieving consistency in multiple change point detection. This algorithm utilizes the predictive power of piece-wise constant functional linear regression models in the reproducing kernel Hilbert space framework. We further propose a refinement step that improves the localization rate of the initial estimator output by FRBS, and derive asymptotic distributions of the refined estimators for two different regimes determined by the magnitude of a change. To facilitate the construction of confidence intervals for underlying change points based on the limiting distribution, we propose a consistent block-type long-run variance estimator. Our theoretical justifications for the proposed approach accommodate temporal dependence and heavy-tailedness in both the functional covariates and the measurement errors. Empirical effectiveness of our methodology is demonstrated through extensive simulation studies and an application to the Standard and Poor’s 500 index dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05459&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shivam Kumar, Haotian Xu, Haeran Cho, Daren Wang</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In this paper, we study the estimation and inference of change points under a functional linear regression model with changes in the slope function. We present a novel Functional Regression Binary Segmentation (FRBS) algorithm which is computationally efficient as well as achieving consistency in multiple change point detection. This algorithm utilizes the predictive power of piece-wise constant functional linear regression models in the reproducing kernel Hilbert space framework. We further propose a refinement step that improves the localization rate of the initial estimator output by FRBS, and derive asymptotic distributions of the refined estimators for two different regimes determined by the magnitude of a change. To facilitate the construction of confidence intervals for underlying change points based on the limiting distribution, we propose a consistent block-type long-run variance estimator. Our theoretical justifications for the proposed approach accommodate temporal dependence and heavy-tailedness in both the functional covariates and the measurement errors. Empirical effectiveness of our methodology is demonstrated through extensive simulation studies and an application to the Standard and Poor’s 500 index dataset.</summary></entry><entry><title type="html">How Generalizable Is My Behavior Cloning Policy? A Statistical Approach to Trustworthy Performance Evaluation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/HowGeneralizableIsMyBehaviorCloningPolicyAStatisticalApproachtoTrustworthyPerformanceEvaluation.html" rel="alternate" type="text/html" title="How Generalizable Is My Behavior Cloning Policy? A Statistical Approach to Trustworthy Performance Evaluation" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/HowGeneralizableIsMyBehaviorCloningPolicyAStatisticalApproachtoTrustworthyPerformanceEvaluation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/HowGeneralizableIsMyBehaviorCloningPolicyAStatisticalApproachtoTrustworthyPerformanceEvaluation.html">&lt;p&gt;With the rise of stochastic generative models in robot policy learning, end-to-end visuomotor policies are increasingly successful at solving complex tasks by learning from human demonstrations. Nevertheless, since real-world evaluation costs afford users only a small number of policy rollouts, it remains a challenge to accurately gauge the performance of such policies. This is exacerbated by distribution shifts causing unpredictable changes in performance during deployment. To rigorously evaluate behavior cloning policies, we present a framework that provides a tight lower-bound on robot performance in an arbitrary environment, using a minimal number of experimental policy rollouts. Notably, by applying the standard stochastic ordering to robot performance distributions, we provide a worst-case bound on the entire distribution of performance (via bounds on the cumulative distribution function) for a given task. We build upon established statistical results to ensure that the bounds hold with a user-specified confidence level and tightness, and are constructed from as few policy rollouts as possible. In experiments we evaluate policies for visuomotor manipulation in both simulation and hardware. Specifically, we (i) empirically validate the guarantees of the bounds in simulated manipulation settings, (ii) find the degree to which a learned policy deployed on hardware generalizes to new real-world environments, and (iii) rigorously compare two policies tested in out-of-distribution settings. Our experimental data, code, and implementation of confidence bounds are open-source.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05439&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joseph A. Vincent, Haruki Nishimura, Masha Itkina, Paarth Shah, Mac Schwager, Thomas Kollar</name></author><category term="stat.AP" /><summary type="html">With the rise of stochastic generative models in robot policy learning, end-to-end visuomotor policies are increasingly successful at solving complex tasks by learning from human demonstrations. Nevertheless, since real-world evaluation costs afford users only a small number of policy rollouts, it remains a challenge to accurately gauge the performance of such policies. This is exacerbated by distribution shifts causing unpredictable changes in performance during deployment. To rigorously evaluate behavior cloning policies, we present a framework that provides a tight lower-bound on robot performance in an arbitrary environment, using a minimal number of experimental policy rollouts. Notably, by applying the standard stochastic ordering to robot performance distributions, we provide a worst-case bound on the entire distribution of performance (via bounds on the cumulative distribution function) for a given task. We build upon established statistical results to ensure that the bounds hold with a user-specified confidence level and tightness, and are constructed from as few policy rollouts as possible. In experiments we evaluate policies for visuomotor manipulation in both simulation and hardware. Specifically, we (i) empirically validate the guarantees of the bounds in simulated manipulation settings, (ii) find the degree to which a learned policy deployed on hardware generalizes to new real-world environments, and (iii) rigorously compare two policies tested in out-of-distribution settings. Our experimental data, code, and implementation of confidence bounds are open-source.</summary></entry><entry><title type="html">How Inverse Conditional Flows Can Serve as a Substitute for Distributional Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/HowInverseConditionalFlowsCanServeasaSubstituteforDistributionalRegression.html" rel="alternate" type="text/html" title="How Inverse Conditional Flows Can Serve as a Substitute for Distributional Regression" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/HowInverseConditionalFlowsCanServeasaSubstituteforDistributionalRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/HowInverseConditionalFlowsCanServeasaSubstituteforDistributionalRegression.html">&lt;p&gt;Neural network representations of simple models, such as linear regression, are being studied increasingly to better understand the underlying principles of deep learning algorithms. However, neural representations of distributional regression models, such as the Cox model, have received little attention so far. We close this gap by proposing a framework for distributional regression using inverse flow transformations (DRIFT), which includes neural representations of the aforementioned models. We empirically demonstrate that the neural representations of models in DRIFT can serve as a substitute for their classical statistical counterparts in several applications involving continuous, ordered, time-series, and survival outcomes. We confirm that models in DRIFT empirically match the performance of several statistical methods in terms of estimation of partial effects, prediction, and aleatoric uncertainty quantification. DRIFT covers both interpretable statistical models and flexible neural networks opening up new avenues in both statistical modeling and deep learning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05429&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lucas Kook, Chris Kolb, Philipp Schiele, Daniel Dold, Marcel Arpogaus, Cornelius Fritz, Philipp F. Baumann, Philipp Kopper, Tobias Pielok, Emilio Dorigatti, David Rügamer</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">Neural network representations of simple models, such as linear regression, are being studied increasingly to better understand the underlying principles of deep learning algorithms. However, neural representations of distributional regression models, such as the Cox model, have received little attention so far. We close this gap by proposing a framework for distributional regression using inverse flow transformations (DRIFT), which includes neural representations of the aforementioned models. We empirically demonstrate that the neural representations of models in DRIFT can serve as a substitute for their classical statistical counterparts in several applications involving continuous, ordered, time-series, and survival outcomes. We confirm that models in DRIFT empirically match the performance of several statistical methods in terms of estimation of partial effects, prediction, and aleatoric uncertainty quantification. DRIFT covers both interpretable statistical models and flexible neural networks opening up new avenues in both statistical modeling and deep learning.</summary></entry><entry><title type="html">Learned harmonic mean estimation of the Bayesian evidence with normalizing flows</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/LearnedharmonicmeanestimationoftheBayesianevidencewithnormalizingflows.html" rel="alternate" type="text/html" title="Learned harmonic mean estimation of the Bayesian evidence with normalizing flows" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/LearnedharmonicmeanestimationoftheBayesianevidencewithnormalizingflows</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/LearnedharmonicmeanestimationoftheBayesianevidencewithnormalizingflows.html">&lt;p&gt;We present the learned harmonic mean estimator with normalizing flows - a robust, scalable and flexible estimator of the Bayesian evidence for model comparison. Since the estimator is agnostic to sampling strategy and simply requires posterior samples, it can be applied to compute the evidence using any Markov chain Monte Carlo (MCMC) sampling technique, including saved down MCMC chains, or any variational inference approach. The learned harmonic mean estimator was recently introduced, where machine learning techniques were developed to learn a suitable internal importance sampling target distribution to solve the issue of exploding variance of the original harmonic mean estimator. In this article we present the use of normalizing flows as the internal machine learning technique within the learned harmonic mean estimator. Normalizing flows can be elegantly coupled with the learned harmonic mean to provide an approach that is more robust, flexible and scalable than the machine learning models considered previously. We perform a series of numerical experiments, applying our method to benchmark problems and to a cosmological example in up to 21 dimensions. We find the learned harmonic mean estimator is in agreement with ground truth values and nested sampling estimates. The open-source harmonic Python package implementing the learned harmonic mean, now with normalizing flows included, is publicly available.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05969&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alicja Polanska, Matthew A. Price, Davide Piras, Alessio Spurio Mancini, Jason D. McEwen</name></author><category term="stat.ME" /><summary type="html">We present the learned harmonic mean estimator with normalizing flows - a robust, scalable and flexible estimator of the Bayesian evidence for model comparison. Since the estimator is agnostic to sampling strategy and simply requires posterior samples, it can be applied to compute the evidence using any Markov chain Monte Carlo (MCMC) sampling technique, including saved down MCMC chains, or any variational inference approach. The learned harmonic mean estimator was recently introduced, where machine learning techniques were developed to learn a suitable internal importance sampling target distribution to solve the issue of exploding variance of the original harmonic mean estimator. In this article we present the use of normalizing flows as the internal machine learning technique within the learned harmonic mean estimator. Normalizing flows can be elegantly coupled with the learned harmonic mean to provide an approach that is more robust, flexible and scalable than the machine learning models considered previously. We perform a series of numerical experiments, applying our method to benchmark problems and to a cosmological example in up to 21 dimensions. We find the learned harmonic mean estimator is in agreement with ground truth values and nested sampling estimates. The open-source harmonic Python package implementing the learned harmonic mean, now with normalizing flows included, is publicly available.</summary></entry><entry><title type="html">Measuring Strategization in Recommendation: Users Adapt Their Behavior to Shape Future Content</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/MeasuringStrategizationinRecommendationUsersAdaptTheirBehaviortoShapeFutureContent.html" rel="alternate" type="text/html" title="Measuring Strategization in Recommendation: Users Adapt Their Behavior to Shape Future Content" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/MeasuringStrategizationinRecommendationUsersAdaptTheirBehaviortoShapeFutureContent</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/MeasuringStrategizationinRecommendationUsersAdaptTheirBehaviortoShapeFutureContent.html">&lt;p&gt;Most modern recommendation algorithms are data-driven: they generate personalized recommendations by observing users’ past behaviors. A common assumption in recommendation is that how a user interacts with a piece of content (e.g., whether they choose to “like” it) is a reflection of the content, but not of the algorithm that generated it. Although this assumption is convenient, it fails to capture user strategization: that users may attempt to shape their future recommendations by adapting their behavior to the recommendation algorithm. In this work, we test for user strategization by conducting a lab experiment and survey. To capture strategization, we adopt a model in which strategic users select their engagement behavior based not only on the content, but also on how their behavior affects downstream recommendations. Using a custom music player that we built, we study how users respond to different information about their recommendation algorithm as well as to different incentives about how their actions affect downstream outcomes. We find strong evidence of strategization across outcome metrics, including participants’ dwell time and use of “likes.” For example, participants who are told that the algorithm mainly pays attention to “likes” and “dislikes” use those functions 1.9x more than participants told that the algorithm mainly pays attention to dwell time. A close analysis of participant behavior (e.g., in response to our incentive conditions) rules out experimenter demand as the main driver of these trends. Further, in our post-experiment survey, nearly half of participants self-report strategizing “in the wild,” with some stating that they ignore content they actually like to avoid over-recommendation of that content in the future. Together, our findings suggest that user strategization is common and that platforms cannot ignore the effect of their algorithms on user behavior.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05596&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sarah H. Cen, Andrew Ilyas, Jennifer Allen, Hannah Li, Aleksander Madry</name></author><category term="stat.ME" /><summary type="html">Most modern recommendation algorithms are data-driven: they generate personalized recommendations by observing users’ past behaviors. A common assumption in recommendation is that how a user interacts with a piece of content (e.g., whether they choose to “like” it) is a reflection of the content, but not of the algorithm that generated it. Although this assumption is convenient, it fails to capture user strategization: that users may attempt to shape their future recommendations by adapting their behavior to the recommendation algorithm. In this work, we test for user strategization by conducting a lab experiment and survey. To capture strategization, we adopt a model in which strategic users select their engagement behavior based not only on the content, but also on how their behavior affects downstream recommendations. Using a custom music player that we built, we study how users respond to different information about their recommendation algorithm as well as to different incentives about how their actions affect downstream outcomes. We find strong evidence of strategization across outcome metrics, including participants’ dwell time and use of “likes.” For example, participants who are told that the algorithm mainly pays attention to “likes” and “dislikes” use those functions 1.9x more than participants told that the algorithm mainly pays attention to dwell time. A close analysis of participant behavior (e.g., in response to our incentive conditions) rules out experimenter demand as the main driver of these trends. Further, in our post-experiment survey, nearly half of participants self-report strategizing “in the wild,” with some stating that they ignore content they actually like to avoid over-recommendation of that content in the future. Together, our findings suggest that user strategization is common and that platforms cannot ignore the effect of their algorithms on user behavior.</summary></entry><entry><title type="html">Multilayer Network Regression with Eigenvector Centrality and Community Structure</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/MultilayerNetworkRegressionwithEigenvectorCentralityandCommunityStructure.html" rel="alternate" type="text/html" title="Multilayer Network Regression with Eigenvector Centrality and Community Structure" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/MultilayerNetworkRegressionwithEigenvectorCentralityandCommunityStructure</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/MultilayerNetworkRegressionwithEigenvectorCentralityandCommunityStructure.html">&lt;p&gt;In the analysis of complex networks, centrality measures and community structures are two important aspects. For multilayer networks, one crucial task is to integrate information across different layers, especially taking the dependence structure within and between layers into consideration. In this study, we introduce a novel two-stage regression model (CC-MNetR) that leverages the eigenvector centrality and network community structure of fourth-order tensor-like multilayer networks. In particular, we construct community-based centrality measures, which are then incorporated into the regression model. In addition, considering the noise of network data, we analyze the centrality measure with and without measurement errors respectively, and establish the consistent properties of the least squares estimates in the regression. Our proposed method is then applied to the World Input-Output Database (WIOD) dataset to explore how input-output network data between different countries and different industries affect the Gross Output of each industry.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.06204&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhuoye Han, Tiandong Wang</name></author><category term="stat.ME" /><summary type="html">In the analysis of complex networks, centrality measures and community structures are two important aspects. For multilayer networks, one crucial task is to integrate information across different layers, especially taking the dependence structure within and between layers into consideration. In this study, we introduce a novel two-stage regression model (CC-MNetR) that leverages the eigenvector centrality and network community structure of fourth-order tensor-like multilayer networks. In particular, we construct community-based centrality measures, which are then incorporated into the regression model. In addition, considering the noise of network data, we analyze the centrality measure with and without measurement errors respectively, and establish the consistent properties of the least squares estimates in the regression. Our proposed method is then applied to the World Input-Output Database (WIOD) dataset to explore how input-output network data between different countries and different industries affect the Gross Output of each industry.</summary></entry><entry><title type="html">Multilevel Regression and Poststratification Interface: Application to Track Community-level COVID-19 Viral Transmission</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/MultilevelRegressionandPoststratificationInterfaceApplicationtoTrackCommunitylevelCOVID19ViralTransmission.html" rel="alternate" type="text/html" title="Multilevel Regression and Poststratification Interface: Application to Track Community-level COVID-19 Viral Transmission" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/MultilevelRegressionandPoststratificationInterfaceApplicationtoTrackCommunitylevelCOVID19ViralTransmission</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/MultilevelRegressionandPoststratificationInterfaceApplicationtoTrackCommunitylevelCOVID19ViralTransmission.html">&lt;p&gt;In the absence of comprehensive or random testing throughout the COVID-19 pandemic, we have developed a proxy method for synthetic random sampling to estimate the actual viral incidence in the community, based on viral RNA testing of asymptomatic patients who present for elective procedures within a hospital system. The approach collects routine testing data on SARS-CoV-2 exposure among outpatients and performs statistical adjustments of sample representation using multilevel regression and poststratification (MRP). MRP adjusts for selection bias and yields stable small area estimates. We have developed an open-source, user-friendly MRP interface for public implementation of the statistical workflow. We illustrate the MRP interface with an application to track community-level COVID-19 viral transmission in the state of Michigan.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05909&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yajuan Si, Toan Tran, Jonah Gabry, Mitzi Morris, Andrew Gelman</name></author><category term="stat.AP" /><summary type="html">In the absence of comprehensive or random testing throughout the COVID-19 pandemic, we have developed a proxy method for synthetic random sampling to estimate the actual viral incidence in the community, based on viral RNA testing of asymptomatic patients who present for elective procedures within a hospital system. The approach collects routine testing data on SARS-CoV-2 exposure among outpatients and performs statistical adjustments of sample representation using multilevel regression and poststratification (MRP). MRP adjusts for selection bias and yields stable small area estimates. We have developed an open-source, user-friendly MRP interface for public implementation of the statistical workflow. We illustrate the MRP interface with an application to track community-level COVID-19 viral transmission in the state of Michigan.</summary></entry><entry><title type="html">Multiple testing under negative dependence</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Multipletestingundernegativedependence.html" rel="alternate" type="text/html" title="Multiple testing under negative dependence" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Multipletestingundernegativedependence</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Multipletestingundernegativedependence.html">&lt;p&gt;The multiple testing literature has primarily dealt with three types of dependence assumptions between p-values: independence, positive regression dependence, and arbitrary dependence. In this paper, we provide what we believe are the first theoretical results under various notions of negative dependence (negative Gaussian dependence, negative regression dependence, negative association, negative orthant dependence and weak negative dependence). These include the Simes global null test and the Benjamini-Hochberg procedure, which are known experimentally to be anti-conservative under negative dependence. The anti-conservativeness of these procedures is bounded by factors smaller than that under arbitrary dependence (in particular, by factors independent of the number of hypotheses). We also provide new results about negatively dependent e-values, and provide several examples as to when negative dependence may arise. Our proofs are elementary and short, thus amenable to extensions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.09706&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ziyu Chi, Aaditya Ramdas, Ruodu Wang</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">The multiple testing literature has primarily dealt with three types of dependence assumptions between p-values: independence, positive regression dependence, and arbitrary dependence. In this paper, we provide what we believe are the first theoretical results under various notions of negative dependence (negative Gaussian dependence, negative regression dependence, negative association, negative orthant dependence and weak negative dependence). These include the Simes global null test and the Benjamini-Hochberg procedure, which are known experimentally to be anti-conservative under negative dependence. The anti-conservativeness of these procedures is bounded by factors smaller than that under arbitrary dependence (in particular, by factors independent of the number of hypotheses). We also provide new results about negatively dependent e-values, and provide several examples as to when negative dependence may arise. Our proofs are elementary and short, thus amenable to extensions.</summary></entry><entry><title type="html">Negative Control Falsification Tests for Instrumental Variable Designs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/NegativeControlFalsificationTestsforInstrumentalVariableDesigns.html" rel="alternate" type="text/html" title="Negative Control Falsification Tests for Instrumental Variable Designs" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/NegativeControlFalsificationTestsforInstrumentalVariableDesigns</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/NegativeControlFalsificationTestsforInstrumentalVariableDesigns.html">&lt;p&gt;We develop theoretical foundations for widely used falsification tests for instrumental variable (IV) designs. We characterize these tests as conditional independence tests between negative control variables - proxies for potential threats - and either the IV or the outcome. We find that conventional applications of these falsification tests would flag problems in exogenous IV designs, and propose simple solutions to avoid this. We also propose new falsification tests that incorporate new types of negative control variables or alternative statistical tests. Finally, we illustrate that under stronger assumptions, negative control variables can also be used for bias correction.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.15624&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Oren Danieli, Daniel Nevo, Itai Walk, Bar Weinstein, Dan Zeltzer</name></author><category term="stat.ME" /><summary type="html">We develop theoretical foundations for widely used falsification tests for instrumental variable (IV) designs. We characterize these tests as conditional independence tests between negative control variables - proxies for potential threats - and either the IV or the outcome. We find that conventional applications of these falsification tests would flag problems in exogenous IV designs, and propose simple solutions to avoid this. We also propose new falsification tests that incorporate new types of negative control variables or alternative statistical tests. Finally, we illustrate that under stronger assumptions, negative control variables can also be used for bias correction.</summary></entry><entry><title type="html">Non-asymptotic estimates for accelerated high order Langevin Monte Carlo algorithms</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/NonasymptoticestimatesforacceleratedhighorderLangevinMonteCarloalgorithms.html" rel="alternate" type="text/html" title="Non-asymptotic estimates for accelerated high order Langevin Monte Carlo algorithms" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/NonasymptoticestimatesforacceleratedhighorderLangevinMonteCarloalgorithms</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/NonasymptoticestimatesforacceleratedhighorderLangevinMonteCarloalgorithms.html">&lt;p&gt;In this paper, we propose two new algorithms, namely aHOLA and aHOLLA, to sample from high-dimensional target distributions with possibly super-linearly growing potentials. We establish non-asymptotic convergence bounds for aHOLA in Wasserstein-1 and Wasserstein-2 distances with rates of convergence equal to $1+q/2$ and $1/2+q/4$, respectively, under a local H&quot;{o}lder condition with exponent $q\in(0,1]$ and a convexity at infinity condition on the potential of the target distribution. Similar results are obtained for aHOLLA under certain global continuity conditions and a dissipativity condition. Crucially, we achieve state-of-the-art rates of convergence of the proposed algorithms in the non-convex setting which are higher than those of the existing algorithms. Numerical experiments are conducted to sample from several distributions and the results support our main findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05679&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ariel Neufeld, Ying Zhang</name></author><category term="stat.CO," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">In this paper, we propose two new algorithms, namely aHOLA and aHOLLA, to sample from high-dimensional target distributions with possibly super-linearly growing potentials. We establish non-asymptotic convergence bounds for aHOLA in Wasserstein-1 and Wasserstein-2 distances with rates of convergence equal to $1+q/2$ and $1/2+q/4$, respectively, under a local H&quot;{o}lder condition with exponent $q\in(0,1]$ and a convexity at infinity condition on the potential of the target distribution. Similar results are obtained for aHOLLA under certain global continuity conditions and a dissipativity condition. Crucially, we achieve state-of-the-art rates of convergence of the proposed algorithms in the non-convex setting which are higher than those of the existing algorithms. Numerical experiments are conducted to sample from several distributions and the results support our main findings.</summary></entry><entry><title type="html">Nonparametric estimation of a future entry time distribution given the knowledge of a past state occupation in a progressive multistate model with current status data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Nonparametricestimationofafutureentrytimedistributiongiventheknowledgeofapaststateoccupationinaprogressivemultistatemodelwithcurrentstatusdata.html" rel="alternate" type="text/html" title="Nonparametric estimation of a future entry time distribution given the knowledge of a past state occupation in a progressive multistate model with current status data" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Nonparametricestimationofafutureentrytimedistributiongiventheknowledgeofapaststateoccupationinaprogressivemultistatemodelwithcurrentstatusdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Nonparametricestimationofafutureentrytimedistributiongiventheknowledgeofapaststateoccupationinaprogressivemultistatemodelwithcurrentstatusdata.html">&lt;p&gt;Case-I interval-censored (current status) data from multistate systems are often encountered in cancer and other epidemiological studies. In this article, we focus on the problem of estimating state entry distribution and occupation probabilities, contingent on a preceding state occupation. This endeavor is particularly complex owing to the inherent challenge of the unavailability of directly observed counts of individuals at risk of transitioning from a state, due to the cross-sectional nature of the data. We propose two nonparametric approaches, one using the fractional at-risk set approach recently adopted in the right-censoring framework and the other a new estimator based on the ratio of marginal state occupation probabilities. Both estimation approaches utilize innovative applications of concepts from the competing risks paradigm. The finite-sample behavior of the proposed estimators is studied via extensive simulation studies where we show that the estimators based on severely censored current status data have good performance when compared with those based on complete data. We demonstrate the application of the two methods to analyze data from patients diagnosed with breast cancer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05781&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Samuel Anyaso-Samuel, Somnath Datta</name></author><category term="stat.ME" /><summary type="html">Case-I interval-censored (current status) data from multistate systems are often encountered in cancer and other epidemiological studies. In this article, we focus on the problem of estimating state entry distribution and occupation probabilities, contingent on a preceding state occupation. This endeavor is particularly complex owing to the inherent challenge of the unavailability of directly observed counts of individuals at risk of transitioning from a state, due to the cross-sectional nature of the data. We propose two nonparametric approaches, one using the fractional at-risk set approach recently adopted in the right-censoring framework and the other a new estimator based on the ratio of marginal state occupation probabilities. Both estimation approaches utilize innovative applications of concepts from the competing risks paradigm. The finite-sample behavior of the proposed estimators is studied via extensive simulation studies where we show that the estimators based on severely censored current status data have good performance when compared with those based on complete data. We demonstrate the application of the two methods to analyze data from patients diagnosed with breast cancer.</summary></entry><entry><title type="html">On foundation of generative statistics with F-entropy: a gradient-based approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach.html" rel="alternate" type="text/html" title="On foundation of generative statistics with F-entropy: a gradient-based approach" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach.html">&lt;p&gt;This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can {\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g. missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisher’s divergence. (The F-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for Possible future projects include discrete data and Bayesian variational inference.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05389&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bing Cheng, Howell Tong</name></author><category term="stat.ME" /><summary type="html">This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can {\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g. missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisher’s divergence. (The F-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for Possible future projects include discrete data and Bayesian variational inference.</summary></entry><entry><title type="html">Parametric Analysis of Bivariate Current Status data with Competing risks using Frailty model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/ParametricAnalysisofBivariateCurrentStatusdatawithCompetingrisksusingFrailtymodel.html" rel="alternate" type="text/html" title="Parametric Analysis of Bivariate Current Status data with Competing risks using Frailty model" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/ParametricAnalysisofBivariateCurrentStatusdatawithCompetingrisksusingFrailtymodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/ParametricAnalysisofBivariateCurrentStatusdatawithCompetingrisksusingFrailtymodel.html">&lt;p&gt;Shared and correlated Gamma frailty models are widely used in the literature to model the association in multivariate current status data. In this paper, we have proposed two other new Gamma frailty models, namely shared cause-specific and correlated cause-specific Gamma frailty to capture association in bivariate current status data with competing risks. We have investigated the identifiability of the bivariate models with competing risks for each of the four frailty variables. We have considered maximum likelihood estimation of the model parameters. Thorough simulation studies have been performed to study the finite sample behaviour of the estimated parameters. Also, we have analyzed a real data set on hearing loss in two ears using Exponential type and Weibull type cause-specific baseline hazard functions with the four different Gamma frailty variables and compare the fits using AIC.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05773&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Biswadeep Ghosh, Anup Dewanji, Sudipta Das</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Shared and correlated Gamma frailty models are widely used in the literature to model the association in multivariate current status data. In this paper, we have proposed two other new Gamma frailty models, namely shared cause-specific and correlated cause-specific Gamma frailty to capture association in bivariate current status data with competing risks. We have investigated the identifiability of the bivariate models with competing risks for each of the four frailty variables. We have considered maximum likelihood estimation of the model parameters. Thorough simulation studies have been performed to study the finite sample behaviour of the estimated parameters. Also, we have analyzed a real data set on hearing loss in two ears using Exponential type and Weibull type cause-specific baseline hazard functions with the four different Gamma frailty variables and compare the fits using AIC.</summary></entry><entry><title type="html">Regularized Stein Variational Gradient Flow</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/RegularizedSteinVariationalGradientFlow.html" rel="alternate" type="text/html" title="Regularized Stein Variational Gradient Flow" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/RegularizedSteinVariationalGradientFlow</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/RegularizedSteinVariationalGradientFlow.html">&lt;p&gt;The Stein Variational Gradient Descent (SVGD) algorithm is a deterministic particle method for sampling. However, a mean-field analysis reveals that the gradient flow corresponding to the SVGD algorithm (i.e., the Stein Variational Gradient Flow) only provides a constant-order approximation to the Wasserstein Gradient Flow corresponding to the KL-divergence minimization. In this work, we propose the Regularized Stein Variational Gradient Flow, which interpolates between the Stein Variational Gradient Flow and the Wasserstein Gradient Flow. We establish various theoretical properties of the Regularized Stein Variational Gradient Flow (and its time-discretization) including convergence to equilibrium, existence and uniqueness of weak solutions, and stability of the solutions. We provide preliminary numerical evidence of the improved performance offered by the regularization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.07861&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ye He, Krishnakumar Balasubramanian, Bharath K. Sriperumbudur, Jianfeng Lu</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">The Stein Variational Gradient Descent (SVGD) algorithm is a deterministic particle method for sampling. However, a mean-field analysis reveals that the gradient flow corresponding to the SVGD algorithm (i.e., the Stein Variational Gradient Flow) only provides a constant-order approximation to the Wasserstein Gradient Flow corresponding to the KL-divergence minimization. In this work, we propose the Regularized Stein Variational Gradient Flow, which interpolates between the Stein Variational Gradient Flow and the Wasserstein Gradient Flow. We establish various theoretical properties of the Regularized Stein Variational Gradient Flow (and its time-discretization) including convergence to equilibrium, existence and uniqueness of weak solutions, and stability of the solutions. We provide preliminary numerical evidence of the improved performance offered by the regularization.</summary></entry><entry><title type="html">Reliability analysis of arbitrary systems based on active learning and global sensitivity analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Reliabilityanalysisofarbitrarysystemsbasedonactivelearningandglobalsensitivityanalysis.html" rel="alternate" type="text/html" title="Reliability analysis of arbitrary systems based on active learning and global sensitivity analysis" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Reliabilityanalysisofarbitrarysystemsbasedonactivelearningandglobalsensitivityanalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Reliabilityanalysisofarbitrarysystemsbasedonactivelearningandglobalsensitivityanalysis.html">&lt;p&gt;System reliability analysis aims at computing the probability of failure of an engineering system given a set of uncertain inputs and limit state functions. Active-learning solution schemes have been shown to be a viable tool but as of yet they are not as efficient as in the context of component reliability analysis. This is due to some peculiarities of system problems, such as the presence of multiple failure modes and their uneven contribution to failure, or the dependence on the system configuration (e.g., series or parallel). In this work, we propose a novel active learning strategy designed for solving general system reliability problems. This algorithm combines subset simulation and Kriging/PC-Kriging, and relies on an enrichment scheme tailored to specifically address the weaknesses of this class of methods. More specifically, it relies on three components: (i) a new learning function that does not require the specification of the system configuration, (ii) a density-based clustering technique that allows one to automatically detect the different failure modes, and (iii) sensitivity analysis to estimate the contribution of each limit state to system failure so as to select only the most relevant ones for enrichment. The proposed method is validated on two analytical examples and compared against results gathered in the literature. Finally, a complex engineering problem related to power transmission is solved, thereby showcasing the efficiency of the proposed method in a real-case scenario.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.19885&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maliki Moustapha, Pietro Parisi, Stefano Marelli, Bruno Sudret</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">System reliability analysis aims at computing the probability of failure of an engineering system given a set of uncertain inputs and limit state functions. Active-learning solution schemes have been shown to be a viable tool but as of yet they are not as efficient as in the context of component reliability analysis. This is due to some peculiarities of system problems, such as the presence of multiple failure modes and their uneven contribution to failure, or the dependence on the system configuration (e.g., series or parallel). In this work, we propose a novel active learning strategy designed for solving general system reliability problems. This algorithm combines subset simulation and Kriging/PC-Kriging, and relies on an enrichment scheme tailored to specifically address the weaknesses of this class of methods. More specifically, it relies on three components: (i) a new learning function that does not require the specification of the system configuration, (ii) a density-based clustering technique that allows one to automatically detect the different failure modes, and (iii) sensitivity analysis to estimate the contribution of each limit state to system failure so as to select only the most relevant ones for enrichment. The proposed method is validated on two analytical examples and compared against results gathered in the literature. Finally, a complex engineering problem related to power transmission is solved, thereby showcasing the efficiency of the proposed method in a real-case scenario.</summary></entry><entry><title type="html">Robust covariance estimation and explainable outlier detection for matrix-valued data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Robustcovarianceestimationandexplainableoutlierdetectionformatrixvalueddata.html" rel="alternate" type="text/html" title="Robust covariance estimation and explainable outlier detection for matrix-valued data" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Robustcovarianceestimationandexplainableoutlierdetectionformatrixvalueddata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/Robustcovarianceestimationandexplainableoutlierdetectionformatrixvalueddata.html">&lt;p&gt;This work introduces the Matrix Minimum Covariance Determinant (MMCD) method, a novel robust location and covariance estimation procedure designed for data that are naturally represented in the form of a matrix. Unlike standard robust multivariate estimators, which would only be applicable after a vectorization of the matrix-variate samples leading to high-dimensional datasets, the MMCD estimators account for the matrix-variate data structure and consistently estimate the mean matrix, as well as the rowwise and columnwise covariance matrices in the class of matrix-variate elliptical distributions. Additionally, we show that the MMCD estimators are matrix affine equivariant and achieve a higher breakdown point than the maximal achievable one by any multivariate, affine equivariant location/covariance estimator when applied to the vectorized data. An efficient algorithm with convergence guarantees is proposed and implemented. As a result, robust Mahalanobis distances based on MMCD estimators offer a reliable tool for outlier detection. Additionally, we extend the concept of Shapley values for outlier explanation to the matrix-variate setting, enabling the decomposition of the squared Mahalanobis distances into contributions of the rows, columns, or individual cells of matrix-valued observations. Notably, both the theoretical guarantees and simulations show that the MMCD estimators outperform robust estimators based on vectorized observations, offering better computational efficiency and improved robustness. Moreover, real-world data examples demonstrate the practical relevance of the MMCD estimators and the resulting robust Shapley values.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.03975&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marcus Mayrhofer, Una Radojičić, Peter Filzmoser</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">This work introduces the Matrix Minimum Covariance Determinant (MMCD) method, a novel robust location and covariance estimation procedure designed for data that are naturally represented in the form of a matrix. Unlike standard robust multivariate estimators, which would only be applicable after a vectorization of the matrix-variate samples leading to high-dimensional datasets, the MMCD estimators account for the matrix-variate data structure and consistently estimate the mean matrix, as well as the rowwise and columnwise covariance matrices in the class of matrix-variate elliptical distributions. Additionally, we show that the MMCD estimators are matrix affine equivariant and achieve a higher breakdown point than the maximal achievable one by any multivariate, affine equivariant location/covariance estimator when applied to the vectorized data. An efficient algorithm with convergence guarantees is proposed and implemented. As a result, robust Mahalanobis distances based on MMCD estimators offer a reliable tool for outlier detection. Additionally, we extend the concept of Shapley values for outlier explanation to the matrix-variate setting, enabling the decomposition of the squared Mahalanobis distances into contributions of the rows, columns, or individual cells of matrix-valued observations. Notably, both the theoretical guarantees and simulations show that the MMCD estimators outperform robust estimators based on vectorized observations, offering better computational efficiency and improved robustness. Moreover, real-world data examples demonstrate the practical relevance of the MMCD estimators and the resulting robust Shapley values.</summary></entry><entry><title type="html">Trustworthy Dimensionality Reduction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/TrustworthyDimensionalityReduction.html" rel="alternate" type="text/html" title="Trustworthy Dimensionality Reduction" /><published>2024-05-10T00:00:00+00:00</published><updated>2024-05-10T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/TrustworthyDimensionalityReduction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/10/TrustworthyDimensionalityReduction.html">&lt;p&gt;Different unsupervised models for dimensionality reduction like PCA, LLE, Shannon’s mapping, tSNE, UMAP, etc. work on different principles, hence, they are difficult to compare on the same ground. Although they are usually good for visualisation purposes, they can produce spurious patterns that are not present in the original data, losing its trustability (or credibility). On the other hand, information about some response variable (or knowledge of class labels) allows us to do supervised dimensionality reduction such as SIR, SAVE, etc. which work to reduce the data dimension without hampering its ability to explain the particular response at hand. Therefore, the reduced dataset cannot be used to further analyze its relationship with some other kind of responses, i.e., it loses its generalizability. To make a better dimensionality reduction algorithm with a better balance between these two, we shall formally describe the mathematical model used by dimensionality reduction algorithms and provide two indices to measure these intuitive concepts such as trustability and generalizability. Then, we propose a Localized Skeletonization and Dimensionality Reduction (LSDR) algorithm which approximately achieves optimality in both these indices to some extent. The proposed algorithm has been compared with state-of-the-art algorithms such as tSNE and UMAP and is found to be better overall in preserving global structure while retaining useful local information as well. We also propose some of the possible extensions of LSDR which could make this algorithm universally applicable for various types of data similar to tSNE and UMAP.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05868&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Subhrajyoty Roy</name></author><category term="stat.ME" /><summary type="html">Different unsupervised models for dimensionality reduction like PCA, LLE, Shannon’s mapping, tSNE, UMAP, etc. work on different principles, hence, they are difficult to compare on the same ground. Although they are usually good for visualisation purposes, they can produce spurious patterns that are not present in the original data, losing its trustability (or credibility). On the other hand, information about some response variable (or knowledge of class labels) allows us to do supervised dimensionality reduction such as SIR, SAVE, etc. which work to reduce the data dimension without hampering its ability to explain the particular response at hand. Therefore, the reduced dataset cannot be used to further analyze its relationship with some other kind of responses, i.e., it loses its generalizability. To make a better dimensionality reduction algorithm with a better balance between these two, we shall formally describe the mathematical model used by dimensionality reduction algorithms and provide two indices to measure these intuitive concepts such as trustability and generalizability. Then, we propose a Localized Skeletonization and Dimensionality Reduction (LSDR) algorithm which approximately achieves optimality in both these indices to some extent. The proposed algorithm has been compared with state-of-the-art algorithms such as tSNE and UMAP and is found to be better overall in preserving global structure while retaining useful local information as well. We also propose some of the possible extensions of LSDR which could make this algorithm universally applicable for various types of data similar to tSNE and UMAP.</summary></entry></feed>