<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-14T07:14:39+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Short Note on a Flexible Cholesky Parameterization of Correlation Matrices</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/AShortNoteonaFlexibleCholeskyParameterizationofCorrelationMatrices.html" rel="alternate" type="text/html" title="A Short Note on a Flexible Cholesky Parameterization of Correlation Matrices" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/AShortNoteonaFlexibleCholeskyParameterizationofCorrelationMatrices</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/AShortNoteonaFlexibleCholeskyParameterizationofCorrelationMatrices.html">&lt;p&gt;We propose a Cholesky factor parameterization of correlation matrices that facilitates a priori restrictions on the correlation matrix. It is a smooth and differentiable transform that allows additional boundary constraints on the correlation values. Our particular motivation is random sampling under positivity constraints on the space of correlation matrices using MCMC methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07286&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sean Pinkney</name></author><category term="stat.CO" /><summary type="html">We propose a Cholesky factor parameterization of correlation matrices that facilitates a priori restrictions on the correlation matrix. It is a smooth and differentiable transform that allows additional boundary constraints on the correlation values. Our particular motivation is random sampling under positivity constraints on the space of correlation matrices using MCMC methods.</summary></entry><entry><title type="html">A Tidy Framework and Infrastructure to Systematically Assemble Spatio-temporal Indexes from Multivariate Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ATidyFrameworkandInfrastructuretoSystematicallyAssembleSpatiotemporalIndexesfromMultivariateData.html" rel="alternate" type="text/html" title="A Tidy Framework and Infrastructure to Systematically Assemble Spatio-temporal Indexes from Multivariate Data" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ATidyFrameworkandInfrastructuretoSystematicallyAssembleSpatiotemporalIndexesfromMultivariateData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ATidyFrameworkandInfrastructuretoSystematicallyAssembleSpatiotemporalIndexesfromMultivariateData.html">&lt;p&gt;Indexes are useful for summarizing multivariate information into single metrics for monitoring, communicating, and decision-making. While most work has focused on defining new indexes for specific purposes, more attention needs to be directed towards making it possible to understand index behavior in different data conditions, and to determine how their structure affects their values and variation in values. Here we discuss a modular data pipeline recommendation to assemble indexes. It is universally applicable to index computation and allows investigation of index behavior as part of the development procedure. One can compute indexes with different parameter choices, adjust steps in the index definition by adding, removing, and swapping them to experiment with various index designs, calculate uncertainty measures, and assess indexes robustness. The paper presents three examples to illustrate the pipeline framework usage: comparison of two different indexes designed to monitor the spatio-temporal distribution of drought in Queensland, Australia; the effect of dimension reduction choices on the Global Gender Gap Index (GGGI) on countries ranking; and how to calculate bootstrap confidence intervals for the Standardized Precipitation Index (SPI). The methods are supported by a new R package, called tidyindex.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.05812&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>H. Sherry Zhang, Dianne Cook, Ursula Laa, Nicolas Langrené, Patricia Menéndez</name></author><category term="stat.CO" /><summary type="html">Indexes are useful for summarizing multivariate information into single metrics for monitoring, communicating, and decision-making. While most work has focused on defining new indexes for specific purposes, more attention needs to be directed towards making it possible to understand index behavior in different data conditions, and to determine how their structure affects their values and variation in values. Here we discuss a modular data pipeline recommendation to assemble indexes. It is universally applicable to index computation and allows investigation of index behavior as part of the development procedure. One can compute indexes with different parameter choices, adjust steps in the index definition by adding, removing, and swapping them to experiment with various index designs, calculate uncertainty measures, and assess indexes robustness. The paper presents three examples to illustrate the pipeline framework usage: comparison of two different indexes designed to monitor the spatio-temporal distribution of drought in Queensland, Australia; the effect of dimension reduction choices on the Global Gender Gap Index (GGGI) on countries ranking; and how to calculate bootstrap confidence intervals for the Standardized Precipitation Index (SPI). The methods are supported by a new R package, called tidyindex.</summary></entry><entry><title type="html">A Unification of Exchangeability and Continuous Exposure and Confounder Measurement Errors: Probabilistic Exchangeability</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/AUnificationofExchangeabilityandContinuousExposureandConfounderMeasurementErrorsProbabilisticExchangeability.html" rel="alternate" type="text/html" title="A Unification of Exchangeability and Continuous Exposure and Confounder Measurement Errors: Probabilistic Exchangeability" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/AUnificationofExchangeabilityandContinuousExposureandConfounderMeasurementErrorsProbabilisticExchangeability</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/AUnificationofExchangeabilityandContinuousExposureandConfounderMeasurementErrorsProbabilisticExchangeability.html">&lt;p&gt;Exchangeability concerning a continuous exposure, X, implies no confounding bias when identifying average exposure effects of X, AEE(X). When X is measured with error (Xep), two challenges arise in identifying AEE(X). Firstly, exchangeability regarding Xep does not equal exchangeability regarding X. Secondly, the necessity of the non-differential error assumption (NDEA), overly stringent in practice, remains uncertain. To address them, this article proposes unifying exchangeability and exposure and confounder measurement errors with three novel concepts. The first, Probabilistic Exchangeability (PE), states that the outcomes of those with Xep=e are probabilistically exchangeable with the outcomes of those truly exposed to X=eT. The relationship between AEE(Xep) and AEE(X) in risk difference and ratio scales is mathematically expressed as a probabilistic certainty, termed exchangeability probability (Pe). Squared Pe (Pe.sq) quantifies the extent to which AEE(Xep) differs from AEE(X) due to exposure measurement error not akin to confounding mechanisms. In realistic settings, the coefficient of determination (R.sq) in the regression of X against Xep may be sufficient to measure Pe.sq. The second concept, Emergent Pseudo Confounding (EPC), describes the bias introduced by exposure measurement error, akin to confounding mechanisms. PE can hold when EPC is controlled for, which is weaker than NDEA. The third, Emergent Confounding, describes when bias due to confounder measurement error arises. Adjustment for E(P)C can be performed like confounding adjustment to ensure PE. This paper provides justifies for using AEE(Xep) and maximum insight into potential divergence of AEE(Xep) from AEE(X) and its measurement. Differential errors do not necessarily compromise causal inference.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07910&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Honghyok Kim</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Exchangeability concerning a continuous exposure, X, implies no confounding bias when identifying average exposure effects of X, AEE(X). When X is measured with error (Xep), two challenges arise in identifying AEE(X). Firstly, exchangeability regarding Xep does not equal exchangeability regarding X. Secondly, the necessity of the non-differential error assumption (NDEA), overly stringent in practice, remains uncertain. To address them, this article proposes unifying exchangeability and exposure and confounder measurement errors with three novel concepts. The first, Probabilistic Exchangeability (PE), states that the outcomes of those with Xep=e are probabilistically exchangeable with the outcomes of those truly exposed to X=eT. The relationship between AEE(Xep) and AEE(X) in risk difference and ratio scales is mathematically expressed as a probabilistic certainty, termed exchangeability probability (Pe). Squared Pe (Pe.sq) quantifies the extent to which AEE(Xep) differs from AEE(X) due to exposure measurement error not akin to confounding mechanisms. In realistic settings, the coefficient of determination (R.sq) in the regression of X against Xep may be sufficient to measure Pe.sq. The second concept, Emergent Pseudo Confounding (EPC), describes the bias introduced by exposure measurement error, akin to confounding mechanisms. PE can hold when EPC is controlled for, which is weaker than NDEA. The third, Emergent Confounding, describes when bias due to confounder measurement error arises. Adjustment for E(P)C can be performed like confounding adjustment to ensure PE. This paper provides justifies for using AEE(Xep) and maximum insight into potential divergence of AEE(Xep) from AEE(X) and its measurement. Differential errors do not necessarily compromise causal inference.</summary></entry><entry><title type="html">Adaptive-TMLE for the Average Treatment Effect based on Randomized Controlled Trial Augmented with Real-World Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/AdaptiveTMLEfortheAverageTreatmentEffectbasedonRandomizedControlledTrialAugmentedwithRealWorldData.html" rel="alternate" type="text/html" title="Adaptive-TMLE for the Average Treatment Effect based on Randomized Controlled Trial Augmented with Real-World Data" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/AdaptiveTMLEfortheAverageTreatmentEffectbasedonRandomizedControlledTrialAugmentedwithRealWorldData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/AdaptiveTMLEfortheAverageTreatmentEffectbasedonRandomizedControlledTrialAugmentedwithRealWorldData.html">&lt;p&gt;We consider the problem of estimating the average treatment effect (ATE) when both randomized control trial (RCT) data and real-world data (RWD) are available. We decompose the ATE estimand as the difference between a pooled-ATE estimand that integrates RCT and RWD and a bias estimand that captures the conditional effect of RCT enrollment on the outcome. We introduce an adaptive targeted minimum loss-based estimation (A-TMLE) framework to estimate them. We prove that the A-TMLE estimator is root-n-consistent and asymptotically normal. Moreover, in finite sample, it achieves the super-efficiency one would obtain had one known the oracle model for the conditional effect of the RCT enrollment on the outcome. Consequently, the smaller the working model of the bias induced by the RWD is, the greater our estimator’s efficiency, while our estimator will always be at least as efficient as an efficient estimator that uses the RCT data only. A-TMLE outperforms existing methods in simulations by having smaller mean-squared-error and 95% confidence intervals. A-TMLE could help utilize RWD to improve the efficiency of randomized trial results without biasing the estimates of intervention effects. This approach could allow for smaller, faster trials, decreasing the time until patients can receive effective treatments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07186&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mark van der Laan, Sky Qiu, Lars van der Laan</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We consider the problem of estimating the average treatment effect (ATE) when both randomized control trial (RCT) data and real-world data (RWD) are available. We decompose the ATE estimand as the difference between a pooled-ATE estimand that integrates RCT and RWD and a bias estimand that captures the conditional effect of RCT enrollment on the outcome. We introduce an adaptive targeted minimum loss-based estimation (A-TMLE) framework to estimate them. We prove that the A-TMLE estimator is root-n-consistent and asymptotically normal. Moreover, in finite sample, it achieves the super-efficiency one would obtain had one known the oracle model for the conditional effect of the RCT enrollment on the outcome. Consequently, the smaller the working model of the bias induced by the RWD is, the greater our estimator’s efficiency, while our estimator will always be at least as efficient as an efficient estimator that uses the RCT data only. A-TMLE outperforms existing methods in simulations by having smaller mean-squared-error and 95% confidence intervals. A-TMLE could help utilize RWD to improve the efficiency of randomized trial results without biasing the estimates of intervention effects. This approach could allow for smaller, faster trials, decreasing the time until patients can receive effective treatments.</summary></entry><entry><title type="html">An Analysis of Sea Level Spatial Variability by Topological Indicators and $k$-means Clustering Algorithm</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/AnAnalysisofSeaLevelSpatialVariabilitybyTopologicalIndicatorsandkmeansClusteringAlgorithm.html" rel="alternate" type="text/html" title="An Analysis of Sea Level Spatial Variability by Topological Indicators and $k$-means Clustering Algorithm" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/AnAnalysisofSeaLevelSpatialVariabilitybyTopologicalIndicatorsandkmeansClusteringAlgorithm</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/AnAnalysisofSeaLevelSpatialVariabilitybyTopologicalIndicatorsandkmeansClusteringAlgorithm.html">&lt;p&gt;The time-series data of sea level rise and fall contains crucial information on the variability of sea level patterns. Traditional $k$-means clustering is commonly used for categorizing regional variability of sea level, however, its results are not robust against a number of factors. This study analyzed fourteen datasets of monthly sea level in fourteen shoreline regions of Peninsular Malaysia. We applied a hybridization of clustering technique to analyze data categorization and topological data analysis method to enhance the performance of our clustering analysis. Specifically, our approach utilized the persistent homology and $k$-means/$k$-means++ clustering. The fourteen data sets from fourteen tide gauge stations were categorized in classes based on a prior categorization that was determined by topological information, and the probability of data points that belong to certain groups that is yielded by $k$-means/$k$-means++ clustering. Our results demonstrated that our method significantly improves the performance of traditional clustering techniques.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04269&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zixin Lin, Nur Fariha Syaqina Zulkepli, Mohd Shareduwan Mohd Kasihmuddin, R. U. Gobithaasan</name></author><category term="stat.AP" /><summary type="html">The time-series data of sea level rise and fall contains crucial information on the variability of sea level patterns. Traditional $k$-means clustering is commonly used for categorizing regional variability of sea level, however, its results are not robust against a number of factors. This study analyzed fourteen datasets of monthly sea level in fourteen shoreline regions of Peninsular Malaysia. We applied a hybridization of clustering technique to analyze data categorization and topological data analysis method to enhance the performance of our clustering analysis. Specifically, our approach utilized the persistent homology and $k$-means/$k$-means++ clustering. The fourteen data sets from fourteen tide gauge stations were categorized in classes based on a prior categorization that was determined by topological information, and the probability of data points that belong to certain groups that is yielded by $k$-means/$k$-means++ clustering. Our results demonstrated that our method significantly improves the performance of traditional clustering techniques.</summary></entry><entry><title type="html">A note on distance variance for categorical variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Anoteondistancevarianceforcategoricalvariables.html" rel="alternate" type="text/html" title="A note on distance variance for categorical variables" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Anoteondistancevarianceforcategoricalvariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Anoteondistancevarianceforcategoricalvariables.html">&lt;p&gt;This study investigates the extension of distance variance, a validated spread metric for continuous and binary variables [Edelmann et al., 2020, Ann. Stat., 48(6)], to quantify the spread of general categorical variables. We provide both geometric and algebraic characterizations of distance variance, revealing its connections to some commonly used entropy measures, and the variance-covariance matrix of the one-hot encoded representation. However, we demonstrate that distance variance fails to satisfy the Schur-concavity axiom for categorical variables with more than two categories, leading to counterintuitive results. This limitation hinders its applicability as a universal measure of spread.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06813&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Qingyang Zhang</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This study investigates the extension of distance variance, a validated spread metric for continuous and binary variables [Edelmann et al., 2020, Ann. Stat., 48(6)], to quantify the spread of general categorical variables. We provide both geometric and algebraic characterizations of distance variance, revealing its connections to some commonly used entropy measures, and the variance-covariance matrix of the one-hot encoded representation. However, we demonstrate that distance variance fails to satisfy the Schur-concavity axiom for categorical variables with more than two categories, leading to counterintuitive results. This limitation hinders its applicability as a universal measure of spread.</summary></entry><entry><title type="html">Bayesian Projection of Refugee and Asylum Seeker Populations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/BayesianProjectionofRefugeeandAsylumSeekerPopulations.html" rel="alternate" type="text/html" title="Bayesian Projection of Refugee and Asylum Seeker Populations" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/BayesianProjectionofRefugeeandAsylumSeekerPopulations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/BayesianProjectionofRefugeeandAsylumSeekerPopulations.html">&lt;p&gt;Estimates of future migration patterns are a crucial input to world population projections. Forced migration, including refugee and asylum seekers, plays an important role in overall migration patterns, but is notoriously difficult to forecast. We propose a modeling pipeline based on Bayesian hierarchical time-series modeling for projecting combined refugee and asylum seeker populations by country of origin using data from the United Nations High Commissioner for Human Rights (UNHCR). Our approach is based on a conceptual model of refugee crises following growth and decline phases, separated by a peak. The growth and decline phases are modeled by logistic growth and decline through an interrupted logistic process model. We evaluate our method through a set of validation exercises that show it has good performance for forecasts at 1, 5, and 10 year horizons, and we present projections for 35 countries with ongoing refugee crises.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06857&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Herbert Susmann, Adrian E. Raftery</name></author><category term="stat.AP" /><summary type="html">Estimates of future migration patterns are a crucial input to world population projections. Forced migration, including refugee and asylum seekers, plays an important role in overall migration patterns, but is notoriously difficult to forecast. We propose a modeling pipeline based on Bayesian hierarchical time-series modeling for projecting combined refugee and asylum seeker populations by country of origin using data from the United Nations High Commissioner for Human Rights (UNHCR). Our approach is based on a conceptual model of refugee crises following growth and decline phases, separated by a peak. The growth and decline phases are modeled by logistic growth and decline through an interrupted logistic process model. We evaluate our method through a set of validation exercises that show it has good performance for forecasts at 1, 5, and 10 year horizons, and we present projections for 35 countries with ongoing refugee crises.</summary></entry><entry><title type="html">Bayesian Spatially Clustered Compositional Regression: Linking intersectoral GDP contributions to Gini Coefficients</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/BayesianSpatiallyClusteredCompositionalRegressionLinkingintersectoralGDPcontributionstoGiniCoefficients.html" rel="alternate" type="text/html" title="Bayesian Spatially Clustered Compositional Regression: Linking intersectoral GDP contributions to Gini Coefficients" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/BayesianSpatiallyClusteredCompositionalRegressionLinkingintersectoralGDPcontributionstoGiniCoefficients</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/BayesianSpatiallyClusteredCompositionalRegressionLinkingintersectoralGDPcontributionstoGiniCoefficients.html">&lt;p&gt;The Gini coefficient is an universally used measurement of income inequality. Intersectoral GDP contributions reveal the economic development of different sectors of the national economy. Linking intersectoral GDP contributions to Gini coefficients will provide better understandings of how the Gini coefficient is influenced by different industries. In this paper, a compositional regression with spatially clustered coefficients is proposed to explore heterogeneous effects over spatial locations under nonparametric Bayesian framework. Specifically, a Markov random field constraint mixture of finite mixtures prior is designed for Bayesian log contrast regression with compostional covariates, which allows for both spatially contiguous clusters and discontinous clusters. In addition, an efficient Markov chain Monte Carlo algorithm for posterior sampling that enables simultaneous inference on both cluster configurations and cluster-wise parameters is designed. The compelling empirical performance of the proposed method is demonstrated via extensive simulation studies and an application to 51 states of United States from 2019 Bureau of Economic Analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07408&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jingcheng Meng, Yimeng Ren, Xuening Zhu, Guanyu Hu</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The Gini coefficient is an universally used measurement of income inequality. Intersectoral GDP contributions reveal the economic development of different sectors of the national economy. Linking intersectoral GDP contributions to Gini coefficients will provide better understandings of how the Gini coefficient is influenced by different industries. In this paper, a compositional regression with spatially clustered coefficients is proposed to explore heterogeneous effects over spatial locations under nonparametric Bayesian framework. Specifically, a Markov random field constraint mixture of finite mixtures prior is designed for Bayesian log contrast regression with compostional covariates, which allows for both spatially contiguous clusters and discontinous clusters. In addition, an efficient Markov chain Monte Carlo algorithm for posterior sampling that enables simultaneous inference on both cluster configurations and cluster-wise parameters is designed. The compelling empirical performance of the proposed method is demonstrated via extensive simulation studies and an application to 51 states of United States from 2019 Bureau of Economic Analysis.</summary></entry><entry><title type="html">Bridging Binarization: Causal Inference with Dichotomized Continuous Treatments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/BridgingBinarizationCausalInferencewithDichotomizedContinuousTreatments.html" rel="alternate" type="text/html" title="Bridging Binarization: Causal Inference with Dichotomized Continuous Treatments" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/BridgingBinarizationCausalInferencewithDichotomizedContinuousTreatments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/BridgingBinarizationCausalInferencewithDichotomizedContinuousTreatments.html">&lt;p&gt;The average treatment effect (ATE) is a common parameter estimated in causal inference literature, but it is only defined for binary treatments. Thus, despite concerns raised by some researchers, many studies seeking to estimate the causal effect of a continuous treatment create a new binary treatment variable by dichotomizing the continuous values into two categories. In this paper, we affirm binarization as a statistically valid method for answering causal questions about continuous treatments by showing the equivalence between the binarized ATE and the difference in the average outcomes of two specific modified treatment policies. These policies impose cut-offs corresponding to the binarized treatment variable and assume preservation of relative self-selection. Relative self-selection is the ratio of the probability density of an individual having an exposure equal to one value of the continuous treatment variable versus another. The policies assume that, for any two values of the treatment variable with non-zero probability density after the cut-off, this ratio will remain unchanged. Through this equivalence, we clarify the assumptions underlying binarization and discuss how to properly interpret the resulting estimator. Additionally, we introduce a new target parameter that can be computed after binarization that considers the status-quo world. We argue that this parameter addresses more relevant causal questions than the traditional binarized ATE parameter. Finally, we present a simulation study to illustrate the implications of these assumptions when analyzing data and to demonstrate how to correctly implement estimators of the parameters discussed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07109&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kaitlyn J. Lee, Alan Hubbard, Alejandro Schuler</name></author><category term="stat.ME" /><summary type="html">The average treatment effect (ATE) is a common parameter estimated in causal inference literature, but it is only defined for binary treatments. Thus, despite concerns raised by some researchers, many studies seeking to estimate the causal effect of a continuous treatment create a new binary treatment variable by dichotomizing the continuous values into two categories. In this paper, we affirm binarization as a statistically valid method for answering causal questions about continuous treatments by showing the equivalence between the binarized ATE and the difference in the average outcomes of two specific modified treatment policies. These policies impose cut-offs corresponding to the binarized treatment variable and assume preservation of relative self-selection. Relative self-selection is the ratio of the probability density of an individual having an exposure equal to one value of the continuous treatment variable versus another. The policies assume that, for any two values of the treatment variable with non-zero probability density after the cut-off, this ratio will remain unchanged. Through this equivalence, we clarify the assumptions underlying binarization and discuss how to properly interpret the resulting estimator. Additionally, we introduce a new target parameter that can be computed after binarization that considers the status-quo world. We argue that this parameter addresses more relevant causal questions than the traditional binarized ATE parameter. Finally, we present a simulation study to illustrate the implications of these assumptions when analyzing data and to demonstrate how to correctly implement estimators of the parameters discussed.</summary></entry><entry><title type="html">Calibrating dimension reduction hyperparameters in the presence of noise</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Calibratingdimensionreductionhyperparametersinthepresenceofnoise.html" rel="alternate" type="text/html" title="Calibrating dimension reduction hyperparameters in the presence of noise" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Calibratingdimensionreductionhyperparametersinthepresenceofnoise</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Calibratingdimensionreductionhyperparametersinthepresenceofnoise.html">&lt;p&gt;The goal of dimension reduction tools is to construct a low-dimensional representation of high-dimensional data. These tools are employed for a variety of reasons such as noise reduction, visualization, and to lower computational costs. However, there is a fundamental issue that is discussed in other modeling problems that is often overlooked in dimension reduction – overfitting. In the context of other modeling problems, techniques such as feature-selection, cross-validation, and regularization are employed to combat overfitting, but rarely are such precautions taken when applying dimension reduction. Prior applications of the two most popular non-linear dimension reduction methods, t-SNE and UMAP, fail to acknowledge data as a combination of signal and noise when assessing performance. These methods are typically calibrated to capture the entirety of the data, not just the signal. In this paper, we demonstrate the importance of acknowledging noise when calibrating hyperparameters and present a framework that enables users to do so. We use this framework to explore the role hyperparameter calibration plays in overfitting the data when applying t-SNE and UMAP. More specifically, we show previously recommended values for perplexity and n_neighbors are too small and overfit the noise. We also provide a workflow others may use to calibrate hyperparameters in the presence of noise.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.02946&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Justin Lin, Julia Fukuyama</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">The goal of dimension reduction tools is to construct a low-dimensional representation of high-dimensional data. These tools are employed for a variety of reasons such as noise reduction, visualization, and to lower computational costs. However, there is a fundamental issue that is discussed in other modeling problems that is often overlooked in dimension reduction – overfitting. In the context of other modeling problems, techniques such as feature-selection, cross-validation, and regularization are employed to combat overfitting, but rarely are such precautions taken when applying dimension reduction. Prior applications of the two most popular non-linear dimension reduction methods, t-SNE and UMAP, fail to acknowledge data as a combination of signal and noise when assessing performance. These methods are typically calibrated to capture the entirety of the data, not just the signal. In this paper, we demonstrate the importance of acknowledging noise when calibrating hyperparameters and present a framework that enables users to do so. We use this framework to explore the role hyperparameter calibration plays in overfitting the data when applying t-SNE and UMAP. More specifically, we show previously recommended values for perplexity and n_neighbors are too small and overfit the noise. We also provide a workflow others may use to calibrate hyperparameters in the presence of noise.</summary></entry><entry><title type="html">Comparing statistical likelihoods with diagnostic probabilities based on directly observed proportions to help understand the replication crisis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Comparingstatisticallikelihoodswithdiagnosticprobabilitiesbasedondirectlyobservedproportionstohelpunderstandthereplicationcrisis.html" rel="alternate" type="text/html" title="Comparing statistical likelihoods with diagnostic probabilities based on directly observed proportions to help understand the replication crisis" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Comparingstatisticallikelihoodswithdiagnosticprobabilitiesbasedondirectlyobservedproportionstohelpunderstandthereplicationcrisis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Comparingstatisticallikelihoodswithdiagnosticprobabilitiesbasedondirectlyobservedproportionstohelpunderstandthereplicationcrisis.html">&lt;p&gt;Diagnosticians use an observed proportion as a direct estimate of the posterior probability of a diagnosis. Therefore, a diagnostician might regard a continuous Gaussian probability distribution of possible numerical outcomes conditional on the information in the study methods and data as posterior probabilities. Similarly, they might regard the distribution of possible means based on a SEM as a posterior probability distribution too. If the converse likelihood distribution of the observed mean conditional on any hypothetical mean (e.g. the null hypothesis) is assumed to be the same as the above posterior distribution (as is customary) then by Bayes rule, the prior distribution of all possible hypothetical means is uniform. It follows that the probability Q of any theoretically true mean falling into a tail beyond a null hypothesis would be equal to that tails area as a proportion of the whole. It also follows that the P value (the probability of the observed mean or something more extreme conditional on the null hypothesis) is equal to Q. Replication involves doing two independent studies, thus doubling the variance for the combined posterior probability distribution. So, if the original effect size was 1.96, the number of observations was 100, the SEM was 1 and the original P value was 0.025, the theoretical probability of a replicating study getting a P value of up to 0.025 again is only 0.283. By applying this double variance to achieve a power of 80%, the required number of observations is doubled compared to conventional approaches. If some replicating study is to achieve a P value of up to 0.025 yet again with a probability of 0.8, then this requires 3 times as many observations in the power calculation. This might explain the replication crisis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.16906&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Huw Llewelyn</name></author><category term="stat.ME" /><summary type="html">Diagnosticians use an observed proportion as a direct estimate of the posterior probability of a diagnosis. Therefore, a diagnostician might regard a continuous Gaussian probability distribution of possible numerical outcomes conditional on the information in the study methods and data as posterior probabilities. Similarly, they might regard the distribution of possible means based on a SEM as a posterior probability distribution too. If the converse likelihood distribution of the observed mean conditional on any hypothetical mean (e.g. the null hypothesis) is assumed to be the same as the above posterior distribution (as is customary) then by Bayes rule, the prior distribution of all possible hypothetical means is uniform. It follows that the probability Q of any theoretically true mean falling into a tail beyond a null hypothesis would be equal to that tails area as a proportion of the whole. It also follows that the P value (the probability of the observed mean or something more extreme conditional on the null hypothesis) is equal to Q. Replication involves doing two independent studies, thus doubling the variance for the combined posterior probability distribution. So, if the original effect size was 1.96, the number of observations was 100, the SEM was 1 and the original P value was 0.025, the theoretical probability of a replicating study getting a P value of up to 0.025 again is only 0.283. By applying this double variance to achieve a power of 80%, the required number of observations is doubled compared to conventional approaches. If some replicating study is to achieve a P value of up to 0.025 yet again with a probability of 0.8, then this requires 3 times as many observations in the power calculation. This might explain the replication crisis.</summary></entry><entry><title type="html">Density regression via Dirichlet process mixtures of normal structured additive regression models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/DensityregressionviaDirichletprocessmixturesofnormalstructuredadditiveregressionmodels.html" rel="alternate" type="text/html" title="Density regression via Dirichlet process mixtures of normal structured additive regression models" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/DensityregressionviaDirichletprocessmixturesofnormalstructuredadditiveregressionmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/DensityregressionviaDirichletprocessmixturesofnormalstructuredadditiveregressionmodels.html">&lt;p&gt;Within Bayesian nonparametrics, dependent Dirichlet process mixture models provide a highly flexible approach for conducting inference about the conditional density function. However, several formulations of this class make either rather restrictive modelling assumptions or involve intricate algorithms for posterior inference, thus preventing their widespread use. In response to these challenges, we present a flexible, versatile, and computationally tractable model for density regression based on a single-weights dependent Dirichlet process mixture of normal distributions model for univariate continuous responses. We assume an additive structure for the mean of each mixture component and incorporate the effects of continuous covariates through smooth nonlinear functions. The key components of our modelling approach are penalised B-splines and their bivariate tensor product extension. Our proposed method also seamlessly accommodates parametric effects of categorical covariates, linear effects of continuous covariates, interactions between categorical and/or continuous covariates, varying coefficient terms, and random effects, which is why we refer our model as a Dirichlet process mixture of normal structured additive regression models. A noteworthy feature of our method is its efficiency in posterior simulation through Gibbs sampling, as closed-form full conditional distributions for all model parameters are available. Results from a simulation study demonstrate that our approach successfully recovers true conditional densities and other regression functionals in various challenging scenarios. Applications to a toxicology, disease diagnosis, and agricultural study are provided and further underpin the broad applicability of our modelling framework. An R package, DDPstar, implementing the proposed method is publicly available at https://bitbucket.org/mxrodriguez/ddpstar.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.03881&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>María Xosé Rodríguez-Álvarez, Vanda Inácio, Nadja Klein</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Within Bayesian nonparametrics, dependent Dirichlet process mixture models provide a highly flexible approach for conducting inference about the conditional density function. However, several formulations of this class make either rather restrictive modelling assumptions or involve intricate algorithms for posterior inference, thus preventing their widespread use. In response to these challenges, we present a flexible, versatile, and computationally tractable model for density regression based on a single-weights dependent Dirichlet process mixture of normal distributions model for univariate continuous responses. We assume an additive structure for the mean of each mixture component and incorporate the effects of continuous covariates through smooth nonlinear functions. The key components of our modelling approach are penalised B-splines and their bivariate tensor product extension. Our proposed method also seamlessly accommodates parametric effects of categorical covariates, linear effects of continuous covariates, interactions between categorical and/or continuous covariates, varying coefficient terms, and random effects, which is why we refer our model as a Dirichlet process mixture of normal structured additive regression models. A noteworthy feature of our method is its efficiency in posterior simulation through Gibbs sampling, as closed-form full conditional distributions for all model parameters are available. Results from a simulation study demonstrate that our approach successfully recovers true conditional densities and other regression functionals in various challenging scenarios. Applications to a toxicology, disease diagnosis, and agricultural study are provided and further underpin the broad applicability of our modelling framework. An R package, DDPstar, implementing the proposed method is publicly available at https://bitbucket.org/mxrodriguez/ddpstar.</summary></entry><entry><title type="html">Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation.html" rel="alternate" type="text/html" title="Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation.html">&lt;p&gt;There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using inverse probability weighting (IPW). However, due to their numerically unstable IPW weights, these methods suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator by weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Our experimental results show that by effectively correcting the weight values, our proposed method outperforms the existing ones, including traditional weighting schemes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17483&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yoichi Chikahara, Kansei Ushiyama</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using inverse probability weighting (IPW). However, due to their numerically unstable IPW weights, these methods suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator by weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Our experimental results show that by effectively correcting the weight values, our proposed method outperforms the existing ones, including traditional weighting schemes.</summary></entry><entry><title type="html">Distributed High-Dimensional Quantile Regression: Estimation Efficiency and Support Recovery</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/DistributedHighDimensionalQuantileRegressionEstimationEfficiencyandSupportRecovery.html" rel="alternate" type="text/html" title="Distributed High-Dimensional Quantile Regression: Estimation Efficiency and Support Recovery" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/DistributedHighDimensionalQuantileRegressionEstimationEfficiencyandSupportRecovery</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/DistributedHighDimensionalQuantileRegressionEstimationEfficiencyandSupportRecovery.html">&lt;p&gt;In this paper, we focus on distributed estimation and support recovery for high-dimensional linear quantile regression. Quantile regression is a popular alternative tool to the least squares regression for robustness against outliers and data heterogeneity. However, the non-smoothness of the check loss function poses big challenges to both computation and theory in the distributed setting. To tackle these problems, we transform the original quantile regression into the least-squares optimization. By applying a double-smoothing approach, we extend a previous Newton-type distributed approach without the restrictive independent assumption between the error term and covariates. An efficient algorithm is developed, which enjoys high computation and communication efficiency. Theoretically, the proposed distributed estimator achieves a near-oracle convergence rate and high support recovery accuracy after a constant number of iterations. Extensive experiments on synthetic examples and a real data application further demonstrate the effectiveness of the proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07552&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Caixing Wang, Ziliang Shen</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">In this paper, we focus on distributed estimation and support recovery for high-dimensional linear quantile regression. Quantile regression is a popular alternative tool to the least squares regression for robustness against outliers and data heterogeneity. However, the non-smoothness of the check loss function poses big challenges to both computation and theory in the distributed setting. To tackle these problems, we transform the original quantile regression into the least-squares optimization. By applying a double-smoothing approach, we extend a previous Newton-type distributed approach without the restrictive independent assumption between the error term and covariates. An efficient algorithm is developed, which enjoys high computation and communication efficiency. Theoretically, the proposed distributed estimator achieves a near-oracle convergence rate and high support recovery accuracy after a constant number of iterations. Extensive experiments on synthetic examples and a real data application further demonstrate the effectiveness of the proposed method.</summary></entry><entry><title type="html">Dynamic Contextual Pricing with Doubly Non-Parametric Random Utility Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/DynamicContextualPricingwithDoublyNonParametricRandomUtilityModels.html" rel="alternate" type="text/html" title="Dynamic Contextual Pricing with Doubly Non-Parametric Random Utility Models" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/DynamicContextualPricingwithDoublyNonParametricRandomUtilityModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/DynamicContextualPricingwithDoublyNonParametricRandomUtilityModels.html">&lt;p&gt;In the evolving landscape of digital commerce, adaptive dynamic pricing strategies are essential for gaining a competitive edge. This paper introduces novel {\em doubly nonparametric random utility models} that eschew traditional parametric assumptions used in estimating consumer demand’s mean utility function and noise distribution. Existing nonparametric methods like multi-scale {\em Distributional Nearest Neighbors (DNN and TDNN)}, initially designed for offline regression, face challenges in dynamic online pricing due to design limitations, such as the indirect observability of utility-related variables and the absence of uniform convergence guarantees. We address these challenges with innovative population equations that facilitate nonparametric estimation within decision-making frameworks and establish new analytical results on the uniform convergence rates of DNN and TDNN, enhancing their applicability in dynamic environments.
  Our theoretical analysis confirms that the statistical learning rates for the mean utility function and noise distribution are minimax optimal. We also derive a regret bound that illustrates the critical interaction between model dimensionality and noise distribution smoothness, deepening our understanding of dynamic pricing under varied market conditions. These contributions offer substantial theoretical insights and practical tools for implementing effective, data-driven pricing strategies, advancing the theoretical framework of pricing models and providing robust methodologies for navigating the complexities of modern markets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06866&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elynn Chen, Xi Chen, Lan Gao, Jiayu Li</name></author><category term="stat.ME" /><summary type="html">In the evolving landscape of digital commerce, adaptive dynamic pricing strategies are essential for gaining a competitive edge. This paper introduces novel {\em doubly nonparametric random utility models} that eschew traditional parametric assumptions used in estimating consumer demand’s mean utility function and noise distribution. Existing nonparametric methods like multi-scale {\em Distributional Nearest Neighbors (DNN and TDNN)}, initially designed for offline regression, face challenges in dynamic online pricing due to design limitations, such as the indirect observability of utility-related variables and the absence of uniform convergence guarantees. We address these challenges with innovative population equations that facilitate nonparametric estimation within decision-making frameworks and establish new analytical results on the uniform convergence rates of DNN and TDNN, enhancing their applicability in dynamic environments. Our theoretical analysis confirms that the statistical learning rates for the mean utility function and noise distribution are minimax optimal. We also derive a regret bound that illustrates the critical interaction between model dimensionality and noise distribution smoothness, deepening our understanding of dynamic pricing under varied market conditions. These contributions offer substantial theoretical insights and practical tools for implementing effective, data-driven pricing strategies, advancing the theoretical framework of pricing models and providing robust methodologies for navigating the complexities of modern markets.</summary></entry><entry><title type="html">Efficient Nested Simulation Experiment Design via the Likelihood Ratio Method</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/EfficientNestedSimulationExperimentDesignviatheLikelihoodRatioMethod.html" rel="alternate" type="text/html" title="Efficient Nested Simulation Experiment Design via the Likelihood Ratio Method" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/EfficientNestedSimulationExperimentDesignviatheLikelihoodRatioMethod</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/EfficientNestedSimulationExperimentDesignviatheLikelihoodRatioMethod.html">&lt;p&gt;In nested simulation literature, a common assumption is that the experimenter can choose the number of outer scenarios to sample. This paper considers the case when the experimenter is given a fixed set of outer scenarios from an external entity. We propose a nested simulation experiment design that pools inner replications from one scenario to estimate another scenario’s conditional mean via the likelihood ratio method. Given the outer scenarios, we decide how many inner replications to run at each outer scenario as well as how to pool the inner replications by solving a bi-level optimization problem that minimizes the total simulation effort. We provide asymptotic analyses on the convergence rates of the performance measure estimators computed from the optimized experiment design. Under some assumptions, the optimized design achieves $\cO(\Gamma^{-1})$ mean squared error of the estimators given simulation budget $\Gamma$. Numerical experiments demonstrate that our design outperforms a state-of-the-art design that pools replications via regression.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2008.13087&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mingbin Ben Feng, Eunhye Song</name></author><category term="stat.ME" /><summary type="html">In nested simulation literature, a common assumption is that the experimenter can choose the number of outer scenarios to sample. This paper considers the case when the experimenter is given a fixed set of outer scenarios from an external entity. We propose a nested simulation experiment design that pools inner replications from one scenario to estimate another scenario’s conditional mean via the likelihood ratio method. Given the outer scenarios, we decide how many inner replications to run at each outer scenario as well as how to pool the inner replications by solving a bi-level optimization problem that minimizes the total simulation effort. We provide asymptotic analyses on the convergence rates of the performance measure estimators computed from the optimized experiment design. Under some assumptions, the optimized design achieves $\cO(\Gamma^{-1})$ mean squared error of the estimators given simulation budget $\Gamma$. Numerical experiments demonstrate that our design outperforms a state-of-the-art design that pools replications via regression.</summary></entry><entry><title type="html">EgoCor: an R package to facilitate the use of exponential semi-variograms for modelling the local spatial correlation structure in social epidemiology</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/EgoCoranRpackagetofacilitatetheuseofexponentialsemivariogramsformodellingthelocalspatialcorrelationstructureinsocialepidemiology.html" rel="alternate" type="text/html" title="EgoCor: an R package to facilitate the use of exponential semi-variograms for modelling the local spatial correlation structure in social epidemiology" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/EgoCoranRpackagetofacilitatetheuseofexponentialsemivariogramsformodellingthelocalspatialcorrelationstructureinsocialepidemiology</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/EgoCoranRpackagetofacilitatetheuseofexponentialsemivariogramsformodellingthelocalspatialcorrelationstructureinsocialepidemiology.html">&lt;p&gt;As an alternative to using administrative areas for the evaluation of small-area health inequalities, Sauzet et al. suggested to take an ego-centred approach and model the spatial correlation structure of health outcomes at the individual level. Existing tools for the analysis of spatial data in R might appear too complex to non-specialists which could limit the use of the approach. We present the R package EgoCor which offers a user-friendly interface displaying in one function a range of graphics and tables of parameters to facilitate the decision making about which exponential parameters fit best either raw data or residuals. This function is based on the functions of the R package gstat. Moreover, we implemented a function providing the measure of uncertainty proposed by Dyck and Sauzet. With the R package EgoCor the modelling of spatial correlation structure of health outcomes or spatially structured predictors of health with a measure of uncertainty is made available to non-specialists.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.12979&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Julia Dyck, Jan-Ole Koslik, Odile Sauzet</name></author><category term="stat.CO" /><summary type="html">As an alternative to using administrative areas for the evaluation of small-area health inequalities, Sauzet et al. suggested to take an ego-centred approach and model the spatial correlation structure of health outcomes at the individual level. Existing tools for the analysis of spatial data in R might appear too complex to non-specialists which could limit the use of the approach. We present the R package EgoCor which offers a user-friendly interface displaying in one function a range of graphics and tables of parameters to facilitate the decision making about which exponential parameters fit best either raw data or residuals. This function is based on the functions of the R package gstat. Moreover, we implemented a function providing the measure of uncertainty proposed by Dyck and Sauzet. With the R package EgoCor the modelling of spatial correlation structure of health outcomes or spatially structured predictors of health with a measure of uncertainty is made available to non-specialists.</summary></entry><entry><title type="html">Enhancing Scalability in Bayesian Nonparametric Factor Analysis of Spatiotemporal Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/EnhancingScalabilityinBayesianNonparametricFactorAnalysisofSpatiotemporalData.html" rel="alternate" type="text/html" title="Enhancing Scalability in Bayesian Nonparametric Factor Analysis of Spatiotemporal Data" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/EnhancingScalabilityinBayesianNonparametricFactorAnalysisofSpatiotemporalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/EnhancingScalabilityinBayesianNonparametricFactorAnalysisofSpatiotemporalData.html">&lt;p&gt;This manuscript puts forward novel practicable spatiotemporal Bayesian factor analysis frameworks computationally feasible for moderate to large data. Our models exhibit significantly enhanced computational scalability and storage efficiency, deliver high overall modeling performances, and possess powerful inferential capabilities for adequately predicting outcomes at future time points or new spatial locations and satisfactorily clustering spatial locations into regions with similar temporal trajectories, a frequently encountered crucial task. We integrate on top of a baseline separable factor model with temporally dependent latent factors and spatially dependent factor loadings under a probit stick breaking process (PSBP) prior a new slice sampling algorithm that permits unknown varying numbers of spatial mixture components across all factors and guarantees them to be non-increasing through the MCMC iterations, thus considerably enhancing model flexibility, efficiency, and scalability. We further introduce a novel spatial latent nearest-neighbor Gaussian process (NNGP) prior and new sequential updating algorithms for the spatially varying latent variables in the PSBP prior, thereby attaining high spatial scalability. The markedly accelerated posterior sampling and spatial prediction as well as the great modeling and inferential performances of our models are substantiated by our simulation experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.05802&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yifan Cheng, Cheng Li</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">This manuscript puts forward novel practicable spatiotemporal Bayesian factor analysis frameworks computationally feasible for moderate to large data. Our models exhibit significantly enhanced computational scalability and storage efficiency, deliver high overall modeling performances, and possess powerful inferential capabilities for adequately predicting outcomes at future time points or new spatial locations and satisfactorily clustering spatial locations into regions with similar temporal trajectories, a frequently encountered crucial task. We integrate on top of a baseline separable factor model with temporally dependent latent factors and spatially dependent factor loadings under a probit stick breaking process (PSBP) prior a new slice sampling algorithm that permits unknown varying numbers of spatial mixture components across all factors and guarantees them to be non-increasing through the MCMC iterations, thus considerably enhancing model flexibility, efficiency, and scalability. We further introduce a novel spatial latent nearest-neighbor Gaussian process (NNGP) prior and new sequential updating algorithms for the spatially varying latent variables in the PSBP prior, thereby attaining high spatial scalability. The markedly accelerated posterior sampling and spatial prediction as well as the great modeling and inferential performances of our models are substantiated by our simulation experiments.</summary></entry><entry><title type="html">Estimating Value at Risk and Expected Shortfall: A Brief Review and Some New Developments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/EstimatingValueatRiskandExpectedShortfallABriefReviewandSomeNewDevelopments.html" rel="alternate" type="text/html" title="Estimating Value at Risk and Expected Shortfall: A Brief Review and Some New Developments" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/EstimatingValueatRiskandExpectedShortfallABriefReviewandSomeNewDevelopments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/EstimatingValueatRiskandExpectedShortfallABriefReviewandSomeNewDevelopments.html">&lt;p&gt;Value-at-risk (VaR) and expected shortfall (ES) are two commonly utilized metrics for quantifying financial risk. In this study, we review the widely employed Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models. These models are explored with diverse distributional assumptions on innovation, including parametric, non-parametric, and `semi-parametric’ that incorporates a parametric tail distribution based on extreme value theory. Additionally, we introduce a non-parametric local linear quantile autoregression (LLQAR) with kernel weights depending on the distance between the current loss and past losses, and decreasing in the time lag.
  To evaluate the performance of different methods for VaR and ES estimation, we employ a multi-criteria approach. This involves mean squared error assessment using simulated data, backtesting on both simulated data and US stocks, and application of the ESBootstrap test. The LLQAR method, which does not necessarily require stationarity assumptions, seems to perform better for simulated non-stationary data as well as real-world data, for estimating VaR and ES.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06798&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kanon Kamronnaher, Andrew Bellucco, Whitney K. Huang, Colin M. Gallagher</name></author><category term="stat.CO" /><summary type="html">Value-at-risk (VaR) and expected shortfall (ES) are two commonly utilized metrics for quantifying financial risk. In this study, we review the widely employed Generalized Autoregressive Conditional Heteroskedasticity (GARCH) models. These models are explored with diverse distributional assumptions on innovation, including parametric, non-parametric, and `semi-parametric’ that incorporates a parametric tail distribution based on extreme value theory. Additionally, we introduce a non-parametric local linear quantile autoregression (LLQAR) with kernel weights depending on the distance between the current loss and past losses, and decreasing in the time lag. To evaluate the performance of different methods for VaR and ES estimation, we employ a multi-criteria approach. This involves mean squared error assessment using simulated data, backtesting on both simulated data and US stocks, and application of the ESBootstrap test. The LLQAR method, which does not necessarily require stationarity assumptions, seems to perform better for simulated non-stationary data as well as real-world data, for estimating VaR and ES.</summary></entry><entry><title type="html">Exploring Convergence in Relation using Association Rules Mining: A Case Study in Collaborative Knowledge Production</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ExploringConvergenceinRelationusingAssociationRulesMiningACaseStudyinCollaborativeKnowledgeProduction.html" rel="alternate" type="text/html" title="Exploring Convergence in Relation using Association Rules Mining: A Case Study in Collaborative Knowledge Production" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ExploringConvergenceinRelationusingAssociationRulesMiningACaseStudyinCollaborativeKnowledgeProduction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ExploringConvergenceinRelationusingAssociationRulesMiningACaseStudyinCollaborativeKnowledgeProduction.html">&lt;p&gt;This study delves into the pivotal role played by non-experts in knowledge production on open collaboration platforms, with a particular focus on the intricate process of tag development that culminates in the proposal of new glitch classes. Leveraging the power of Association Rule Mining (ARM), this research endeavors to unravel the underlying dynamics of collaboration among citizen scientists. By meticulously quantifying tag associations and scrutinizing their temporal dynamics, the study provides a comprehensive and nuanced understanding of how non-experts collaborate to generate valuable scientific insights. Furthermore, this investigation extends its purview to examine the phenomenon of ideological convergence within online citizen science knowledge production. To accomplish this, a novel measurement algorithm, based on the Mann-Kendall Trend Test, is introduced. This innovative approach sheds illuminating light on the dynamics of collaborative knowledge production, revealing both the vast opportunities and daunting challenges inherent in leveraging non-expert contributions for scientific research endeavors. Notably, the study uncovers a robust pattern of convergence in ideology, employing both the newly proposed convergence testing method and the traditional approach based on the stationarity of time series data. This groundbreaking discovery holds significant implications for understanding the dynamics of online citizen science communities and underscores the crucial role played by non-experts in shaping the scientific landscape of the digital age. Ultimately, this study contributes significantly to our understanding of online citizen science communities, highlighting their potential to harness collective intelligence for tackling complex scientific tasks and enriching our comprehension of collaborative knowledge production processes in the digital age.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.15440&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiahe Ling, Corey B. Jackson</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">This study delves into the pivotal role played by non-experts in knowledge production on open collaboration platforms, with a particular focus on the intricate process of tag development that culminates in the proposal of new glitch classes. Leveraging the power of Association Rule Mining (ARM), this research endeavors to unravel the underlying dynamics of collaboration among citizen scientists. By meticulously quantifying tag associations and scrutinizing their temporal dynamics, the study provides a comprehensive and nuanced understanding of how non-experts collaborate to generate valuable scientific insights. Furthermore, this investigation extends its purview to examine the phenomenon of ideological convergence within online citizen science knowledge production. To accomplish this, a novel measurement algorithm, based on the Mann-Kendall Trend Test, is introduced. This innovative approach sheds illuminating light on the dynamics of collaborative knowledge production, revealing both the vast opportunities and daunting challenges inherent in leveraging non-expert contributions for scientific research endeavors. Notably, the study uncovers a robust pattern of convergence in ideology, employing both the newly proposed convergence testing method and the traditional approach based on the stationarity of time series data. This groundbreaking discovery holds significant implications for understanding the dynamics of online citizen science communities and underscores the crucial role played by non-experts in shaping the scientific landscape of the digital age. Ultimately, this study contributes significantly to our understanding of online citizen science communities, highlighting their potential to harness collective intelligence for tackling complex scientific tasks and enriching our comprehension of collaborative knowledge production processes in the digital age.</summary></entry><entry><title type="html">Factor Strength Estimation in Vector and Matrix Time Series Factor Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/FactorStrengthEstimationinVectorandMatrixTimeSeriesFactorModels.html" rel="alternate" type="text/html" title="Factor Strength Estimation in Vector and Matrix Time Series Factor Models" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/FactorStrengthEstimationinVectorandMatrixTimeSeriesFactorModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/FactorStrengthEstimationinVectorandMatrixTimeSeriesFactorModels.html">&lt;p&gt;Most factor modelling research in vector or matrix-valued time series assume all factors are pervasive/strong and leave weaker factors and their corresponding series to the noise. Weaker factors can in fact be important to a group of observed variables, for instance a sector factor in a large portfolio of stocks may only affect particular sectors, but can be important both in interpretations and predictions for those stocks. While more recent factor modelling researches do consider ``local’’ factors which are weak factors with sparse corresponding factor loadings, there are real data examples in the literature where factors are weak because of weak influence on most/all observed variables, so that the corresponding factor loadings are not sparse (non-local). As a first in the literature, we propose estimators of factor strengths for both local and non-local weak factors, and prove their consistency with rates of convergence spelt out for both vector and matrix-valued time series factor models. Factor strength has an important indication in what estimation procedure of factor models to follow, as well as the estimation accuracy of various estimators (Chen and Lam, 2024). Simulation results show that our estimators have good performance in recovering the true factor strengths, and an analysis on the NYC taxi traffic data indicates the existence of weak factors in the data which may not be localized.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07294&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Weilin Chen, Clifford Lam</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Most factor modelling research in vector or matrix-valued time series assume all factors are pervasive/strong and leave weaker factors and their corresponding series to the noise. Weaker factors can in fact be important to a group of observed variables, for instance a sector factor in a large portfolio of stocks may only affect particular sectors, but can be important both in interpretations and predictions for those stocks. While more recent factor modelling researches do consider ``local’’ factors which are weak factors with sparse corresponding factor loadings, there are real data examples in the literature where factors are weak because of weak influence on most/all observed variables, so that the corresponding factor loadings are not sparse (non-local). As a first in the literature, we propose estimators of factor strengths for both local and non-local weak factors, and prove their consistency with rates of convergence spelt out for both vector and matrix-valued time series factor models. Factor strength has an important indication in what estimation procedure of factor models to follow, as well as the estimation accuracy of various estimators (Chen and Lam, 2024). Simulation results show that our estimators have good performance in recovering the true factor strengths, and an analysis on the NYC taxi traffic data indicates the existence of weak factors in the data which may not be localized.</summary></entry><entry><title type="html">Fast Bayesian inference for spatial mean-parameterized Conway-Maxwell-Poisson models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/FastBayesianinferenceforspatialmeanparameterizedConwayMaxwellPoissonmodels.html" rel="alternate" type="text/html" title="Fast Bayesian inference for spatial mean-parameterized Conway-Maxwell-Poisson models" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/FastBayesianinferenceforspatialmeanparameterizedConwayMaxwellPoissonmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/FastBayesianinferenceforspatialmeanparameterizedConwayMaxwellPoissonmodels.html">&lt;p&gt;Count data with complex features arise in many disciplines, including ecology, agriculture, criminology, medicine, and public health. Zero inflation, spatial dependence, and non-equidispersion are common features in count data. There are two classes of models that allow for these features – he mode-parameterized Conway–Maxwell–Poisson (COMP) distribution and the generalized Poisson model. However both require the use of either constraints on the parameter space or a parameterization that leads to challenges in interpretability. We propose a spatial mean-parameterized COMP model that retains the flexibility of these models while resolving the above issues. We use a Bayesian spatial filtering approach in order to efficiently handle high-dimensional spatial data and we use reversible-jump MCMC to automatically choose the basis vectors for spatial filtering. The COMP distribution poses two additional computational challenges – an intractable normalizing function in the likelihood and no closed-form expression for the mean. We propose a fast computational approach that addresses these challenges by, respectively, introducing an efficient auxiliary variable algorithm and pre-computing key approximations for fast likelihood evaluation. We illustrate the application of our methodology to simulated and real datasets, including Texas HPV-cancer data and US vaccine refusal data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.11472&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bokgyeong Kang, John Hughes, Murali Haran</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Count data with complex features arise in many disciplines, including ecology, agriculture, criminology, medicine, and public health. Zero inflation, spatial dependence, and non-equidispersion are common features in count data. There are two classes of models that allow for these features – he mode-parameterized Conway–Maxwell–Poisson (COMP) distribution and the generalized Poisson model. However both require the use of either constraints on the parameter space or a parameterization that leads to challenges in interpretability. We propose a spatial mean-parameterized COMP model that retains the flexibility of these models while resolving the above issues. We use a Bayesian spatial filtering approach in order to efficiently handle high-dimensional spatial data and we use reversible-jump MCMC to automatically choose the basis vectors for spatial filtering. The COMP distribution poses two additional computational challenges – an intractable normalizing function in the likelihood and no closed-form expression for the mean. We propose a fast computational approach that addresses these challenges by, respectively, introducing an efficient auxiliary variable algorithm and pre-computing key approximations for fast likelihood evaluation. We illustrate the application of our methodology to simulated and real datasets, including Texas HPV-cancer data and US vaccine refusal data.</summary></entry><entry><title type="html">Fast Machine-Precision Spectral Likelihoods for Stationary Time Series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/FastMachinePrecisionSpectralLikelihoodsforStationaryTimeSeries.html" rel="alternate" type="text/html" title="Fast Machine-Precision Spectral Likelihoods for Stationary Time Series" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/FastMachinePrecisionSpectralLikelihoodsforStationaryTimeSeries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/FastMachinePrecisionSpectralLikelihoodsforStationaryTimeSeries.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;We provide in this work an algorithm for approximating a very broad class of symmetric Toeplitz matrices to machine precision in $\mathcal{O}(n \log n)$ time with applications to fitting time series models. In particular, for a symmetric Toeplitz matrix $\mathbf{\Sigma}$ with values $\mathbf{\Sigma}&lt;em&gt;{j,k} = h&lt;/em&gt;{&lt;/td&gt;
      &lt;td&gt;j-k&lt;/td&gt;
      &lt;td&gt;} = \int_{-1/2}^{1/2} e^{2 \pi i&lt;/td&gt;
      &lt;td&gt;j-k&lt;/td&gt;
      &lt;td&gt;\omega} S(\omega) \mathrm{d} \omega$ where $S(\omega)$ is piecewise smooth, we give an approximation $\mathbf{\mathcal{F}} \mathbf{\Sigma} \mathbf{\mathcal{F}}^H \approx \mathbf{D} + \mathbf{U} \mathbf{V}^H$, where $\mathbf{\mathcal{F}}$ is the DFT matrix, $\mathbf{D}$ is diagonal, and the matrices $\mathbf{U}$ and $\mathbf{V}$ are in $\mathbb{C}^{n \times r}$ with $r \ll n$. Studying these matrices in the context of time series, we offer a theoretical explanation of this structure and connect it to existing spectral-domain approximation frameworks. We then give a complete discussion of the numerical method for assembling the approximation and demonstrate its efficiency for improving Whittle-type likelihood approximations, including dramatic examples where a correction of rank $r = 2$ to the standard Whittle approximation increases the accuracy from $3$ to $14$ digits for a matrix $\mathbf{\Sigma} \in \mathbb{R}^{10^5 \times 10^5}$. The method and analysis of this work applies well beyond time series analysis, providing an algorithm for extremely accurate direct solves with a wide variety of symmetric Toeplitz matrices. The analysis employed here largely depends on asymptotic expansions of oscillatory integrals, and also provides a new perspective on when existing spectral-domain approximation methods for Gaussian log-likelihoods can be particularly problematic.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.16583&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Christopher J. Geoga</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">We provide in this work an algorithm for approximating a very broad class of symmetric Toeplitz matrices to machine precision in $\mathcal{O}(n \log n)$ time with applications to fitting time series models. In particular, for a symmetric Toeplitz matrix $\mathbf{\Sigma}$ with values $\mathbf{\Sigma}{j,k} = h{ j-k } = \int_{-1/2}^{1/2} e^{2 \pi i j-k \omega} S(\omega) \mathrm{d} \omega$ where $S(\omega)$ is piecewise smooth, we give an approximation $\mathbf{\mathcal{F}} \mathbf{\Sigma} \mathbf{\mathcal{F}}^H \approx \mathbf{D} + \mathbf{U} \mathbf{V}^H$, where $\mathbf{\mathcal{F}}$ is the DFT matrix, $\mathbf{D}$ is diagonal, and the matrices $\mathbf{U}$ and $\mathbf{V}$ are in $\mathbb{C}^{n \times r}$ with $r \ll n$. Studying these matrices in the context of time series, we offer a theoretical explanation of this structure and connect it to existing spectral-domain approximation frameworks. We then give a complete discussion of the numerical method for assembling the approximation and demonstrate its efficiency for improving Whittle-type likelihood approximations, including dramatic examples where a correction of rank $r = 2$ to the standard Whittle approximation increases the accuracy from $3$ to $14$ digits for a matrix $\mathbf{\Sigma} \in \mathbb{R}^{10^5 \times 10^5}$. The method and analysis of this work applies well beyond time series analysis, providing an algorithm for extremely accurate direct solves with a wide variety of symmetric Toeplitz matrices. The analysis employed here largely depends on asymptotic expansions of oscillatory integrals, and also provides a new perspective on when existing spectral-domain approximation methods for Gaussian log-likelihoods can be particularly problematic.</summary></entry><entry><title type="html">Forecasting with Hyper-Trees</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ForecastingwithHyperTrees.html" rel="alternate" type="text/html" title="Forecasting with Hyper-Trees" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ForecastingwithHyperTrees</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ForecastingwithHyperTrees.html">&lt;p&gt;This paper introduces the concept of Hyper-Trees and offers a new direction in applying tree-based models to time series data. Unlike conventional applications of decision trees that forecast time series directly, Hyper-Trees are designed to learn the parameters of a target time series model. Our framework leverages the gradient-based nature of boosted trees, which allows us to extend the concept of Hyper-Networks to Hyper-Trees and to induce a time-series inductive bias to tree models. By relating the parameters of a target time series model to features, Hyper-Trees address the challenge of parameter non-stationarity and enable tree-based forecasts to extend beyond their initial training range. With our research, we aim to explore the effectiveness of Hyper-Trees across various forecasting scenarios and to expand the application of gradient boosted decision trees past their conventional use in time series forecasting.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07836&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexander März, Kashif Rasul</name></author><category term="stat.ME" /><summary type="html">This paper introduces the concept of Hyper-Trees and offers a new direction in applying tree-based models to time series data. Unlike conventional applications of decision trees that forecast time series directly, Hyper-Trees are designed to learn the parameters of a target time series model. Our framework leverages the gradient-based nature of boosted trees, which allows us to extend the concept of Hyper-Networks to Hyper-Trees and to induce a time-series inductive bias to tree models. By relating the parameters of a target time series model to features, Hyper-Trees address the challenge of parameter non-stationarity and enable tree-based forecasts to extend beyond their initial training range. With our research, we aim to explore the effectiveness of Hyper-Trees across various forecasting scenarios and to expand the application of gradient boosted decision trees past their conventional use in time series forecasting.</summary></entry><entry><title type="html">Forecasting with an N-dimensional Langevin Equation and a Neural-Ordinary Differential Equation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ForecastingwithanNdimensionalLangevinEquationandaNeuralOrdinaryDifferentialEquation.html" rel="alternate" type="text/html" title="Forecasting with an N-dimensional Langevin Equation and a Neural-Ordinary Differential Equation" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ForecastingwithanNdimensionalLangevinEquationandaNeuralOrdinaryDifferentialEquation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ForecastingwithanNdimensionalLangevinEquationandaNeuralOrdinaryDifferentialEquation.html">&lt;p&gt;Accurate prediction of electricity day-ahead prices is essential in competitive electricity markets. Although stationary electricity-price forecasting techniques have received considerable attention, research on non-stationary methods is comparatively scarce, despite the common prevalence of non-stationary features in electricity markets. Specifically, existing non-stationary techniques will often aim to address individual non-stationary features in isolation, leaving aside the exploration of concurrent multiple non-stationary effects. Our overarching objective here is the formulation of a framework to systematically model and forecast non-stationary electricity-price time series, encompassing the broader scope of non-stationary behavior. For this purpose we develop a data-driven model that combines an N-dimensional Langevin equation (LE) with a neural-ordinary differential equation (NODE). The LE captures fine-grained details of the electricity-price behavior in stationary regimes but is inadequate for non-stationary conditions. To overcome this inherent limitation, we adopt a NODE approach to learn, and at the same time predict, the difference between the actual electricity-price time series and the simulated price trajectories generated by the LE. By learning this difference, the NODE reconstructs the non-stationary components of the time series that the LE is not able to capture. We exemplify the effectiveness of our framework using the Spanish electricity day-ahead market as a prototypical case study. Our findings reveal that the NODE nicely complements the LE, providing a comprehensive strategy to tackle both stationary and non-stationary electricity-price behavior. The framework’s dependability and robustness is demonstrated through different non-stationary scenarios by comparing it against a range of basic naive methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07359&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Antonio Malpica-Morales, Miguel A. Duran-Olivencia, Serafim Kalliadasis</name></author><category term="stat.ME" /><summary type="html">Accurate prediction of electricity day-ahead prices is essential in competitive electricity markets. Although stationary electricity-price forecasting techniques have received considerable attention, research on non-stationary methods is comparatively scarce, despite the common prevalence of non-stationary features in electricity markets. Specifically, existing non-stationary techniques will often aim to address individual non-stationary features in isolation, leaving aside the exploration of concurrent multiple non-stationary effects. Our overarching objective here is the formulation of a framework to systematically model and forecast non-stationary electricity-price time series, encompassing the broader scope of non-stationary behavior. For this purpose we develop a data-driven model that combines an N-dimensional Langevin equation (LE) with a neural-ordinary differential equation (NODE). The LE captures fine-grained details of the electricity-price behavior in stationary regimes but is inadequate for non-stationary conditions. To overcome this inherent limitation, we adopt a NODE approach to learn, and at the same time predict, the difference between the actual electricity-price time series and the simulated price trajectories generated by the LE. By learning this difference, the NODE reconstructs the non-stationary components of the time series that the LE is not able to capture. We exemplify the effectiveness of our framework using the Spanish electricity day-ahead market as a prototypical case study. Our findings reveal that the NODE nicely complements the LE, providing a comprehensive strategy to tackle both stationary and non-stationary electricity-price behavior. The framework’s dependability and robustness is demonstrated through different non-stationary scenarios by comparing it against a range of basic naive methods.</summary></entry><entry><title type="html">Generalization Problems in Experiments Involving Multidimensional Decisions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/GeneralizationProblemsinExperimentsInvolvingMultidimensionalDecisions.html" rel="alternate" type="text/html" title="Generalization Problems in Experiments Involving Multidimensional Decisions" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/GeneralizationProblemsinExperimentsInvolvingMultidimensionalDecisions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/GeneralizationProblemsinExperimentsInvolvingMultidimensionalDecisions.html">&lt;p&gt;Can the causal effects estimated in experiment be generalized to real-world scenarios? This question lies at the heart of social science studies. External validity primarily assesses whether experimental effects persist across different settings, implicitly presuming the experiment’s ecological validity-that is, the consistency of experimental effects with their real-life counterparts. However, we argue that this presumed consistency may not always hold, especially in experiments involving multidimensional decision processes, such as conjoint experiments. We introduce a formal model to elucidate how attention and salience effects lead to three types of inconsistencies between experimental findings and real-world phenomena: amplified effect magnitude, effect sign reversal, and effect importance reversal. We derive testable hypotheses from each theoretical outcome and test these hypotheses using data from various existing conjoint experiments and our own experiments. Drawing on our theoretical framework, we propose several recommendations for experimental design aimed at enhancing the generalizability of survey experiment findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06779&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiawei Fu, Xiaojun Li</name></author><category term="stat.AP" /><summary type="html">Can the causal effects estimated in experiment be generalized to real-world scenarios? This question lies at the heart of social science studies. External validity primarily assesses whether experimental effects persist across different settings, implicitly presuming the experiment’s ecological validity-that is, the consistency of experimental effects with their real-life counterparts. However, we argue that this presumed consistency may not always hold, especially in experiments involving multidimensional decision processes, such as conjoint experiments. We introduce a formal model to elucidate how attention and salience effects lead to three types of inconsistencies between experimental findings and real-world phenomena: amplified effect magnitude, effect sign reversal, and effect importance reversal. We derive testable hypotheses from each theoretical outcome and test these hypotheses using data from various existing conjoint experiments and our own experiments. Drawing on our theoretical framework, we propose several recommendations for experimental design aimed at enhancing the generalizability of survey experiment findings.</summary></entry><entry><title type="html">Graphical models for cardinal paired comparisons data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Graphicalmodelsforcardinalpairedcomparisonsdata.html" rel="alternate" type="text/html" title="Graphical models for cardinal paired comparisons data" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Graphicalmodelsforcardinalpairedcomparisonsdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Graphicalmodelsforcardinalpairedcomparisonsdata.html">&lt;p&gt;Graphical models for cardinal paired comparison data with and without covariates are rigorously analyzed. Novel, graph–based, necessary and sufficient conditions which guarantee strong consistency, asymptotic normality and the exponential convergence of the estimated ranks are emphasized. A complete theory for models with covariates is laid out. In particular conditions under which covariates can be safely omitted from the model are provided. The methodology is employed in the analysis of both finite and infinite sets of ranked items specifically in the case of large sparse comparison graphs. The proposed methods are explored by simulation and applied to the ranking of teams in the National Basketball Association (NBA).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.07018&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rahul Singh, George Iliopoulos, Ori Davidov</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Graphical models for cardinal paired comparison data with and without covariates are rigorously analyzed. Novel, graph–based, necessary and sufficient conditions which guarantee strong consistency, asymptotic normality and the exponential convergence of the estimated ranks are emphasized. A complete theory for models with covariates is laid out. In particular conditions under which covariates can be safely omitted from the model are provided. The methodology is employed in the analysis of both finite and infinite sets of ranked items specifically in the case of large sparse comparison graphs. The proposed methods are explored by simulation and applied to the ranking of teams in the National Basketball Association (NBA).</summary></entry><entry><title type="html">Graph neural networks for power grid operational risk assessment under evolving grid topology</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Graphneuralnetworksforpowergridoperationalriskassessmentunderevolvinggridtopology.html" rel="alternate" type="text/html" title="Graph neural networks for power grid operational risk assessment under evolving grid topology" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Graphneuralnetworksforpowergridoperationalriskassessmentunderevolvinggridtopology</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Graphneuralnetworksforpowergridoperationalriskassessmentunderevolvinggridtopology.html">&lt;p&gt;This article investigates the ability of graph neural networks (GNNs) to identify risky conditions in a power grid over the subsequent few hours, without explicit, high-resolution information regarding future generator on/off status (grid topology) or power dispatch decisions. The GNNs are trained using supervised learning, to predict the power grid’s aggregated bus-level (either zonal or system-level) or individual branch-level state under different power supply and demand conditions. The variability of the stochastic grid variables (wind/solar generation and load demand), and their statistical correlations, are rigorously considered while generating the inputs for the training data. The outputs in the training data, obtained by solving numerous mixed-integer linear programming (MILP) optimal power flow problems, correspond to system-level, zonal and transmission line-level quantities of interest (QoIs). The QoIs predicted by the GNNs are used to conduct hours-ahead, sampling-based reliability and risk assessment w.r.t. zonal and system-level (load shedding) as well as branch-level (overloading) failure events. The proposed methodology is demonstrated for three synthetic grids with sizes ranging from 118 to 2848 buses. Our results demonstrate that GNNs are capable of providing fast and accurate prediction of QoIs and can be good proxies for computationally expensive MILP algorithms. The excellent accuracy of GNN-based reliability and risk assessment suggests that GNN models can substantially improve situational awareness by quickly providing rigorous reliability and risk estimates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07343&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yadong Zhang, Pranav M Karve, Sankaran Mahadevan</name></author><category term="stat.ME" /><summary type="html">This article investigates the ability of graph neural networks (GNNs) to identify risky conditions in a power grid over the subsequent few hours, without explicit, high-resolution information regarding future generator on/off status (grid topology) or power dispatch decisions. The GNNs are trained using supervised learning, to predict the power grid’s aggregated bus-level (either zonal or system-level) or individual branch-level state under different power supply and demand conditions. The variability of the stochastic grid variables (wind/solar generation and load demand), and their statistical correlations, are rigorously considered while generating the inputs for the training data. The outputs in the training data, obtained by solving numerous mixed-integer linear programming (MILP) optimal power flow problems, correspond to system-level, zonal and transmission line-level quantities of interest (QoIs). The QoIs predicted by the GNNs are used to conduct hours-ahead, sampling-based reliability and risk assessment w.r.t. zonal and system-level (load shedding) as well as branch-level (overloading) failure events. The proposed methodology is demonstrated for three synthetic grids with sizes ranging from 118 to 2848 buses. Our results demonstrate that GNNs are capable of providing fast and accurate prediction of QoIs and can be good proxies for computationally expensive MILP algorithms. The excellent accuracy of GNN-based reliability and risk assessment suggests that GNN models can substantially improve situational awareness by quickly providing rigorous reliability and risk estimates.</summary></entry><entry><title type="html">Hierarchical inference of evidence using posterior samples</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Hierarchicalinferenceofevidenceusingposteriorsamples.html" rel="alternate" type="text/html" title="Hierarchical inference of evidence using posterior samples" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Hierarchicalinferenceofevidenceusingposteriorsamples</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Hierarchicalinferenceofevidenceusingposteriorsamples.html">&lt;p&gt;The Bayesian evidence, crucial ingredient for model selection, is arguably the most important quantity in Bayesian data analysis: at the same time, however, it is also one of the most difficult to compute. In this paper we present a hierarchical method that leverages on a multivariate normalised approximant for the posterior probability density to infer the evidence for a model in a hierarchical fashion using a set of posterior samples drawn using an arbitrary sampling scheme.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07504&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Stefano Rinaldi, Gabriele Demasi, Walter Del Pozzo, Otto A. Hannuksela</name></author><category term="stat.ME" /><summary type="html">The Bayesian evidence, crucial ingredient for model selection, is arguably the most important quantity in Bayesian data analysis: at the same time, however, it is also one of the most difficult to compute. In this paper we present a hierarchical method that leverages on a multivariate normalised approximant for the posterior probability density to infer the evidence for a model in a hierarchical fashion using a set of posterior samples drawn using an arbitrary sampling scheme.</summary></entry><entry><title type="html">Improved LARS algorithm for adaptive LASSO in the linear regression model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ImprovedLARSalgorithmforadaptiveLASSOinthelinearregressionmodel.html" rel="alternate" type="text/html" title="Improved LARS algorithm for adaptive LASSO in the linear regression model" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ImprovedLARSalgorithmforadaptiveLASSOinthelinearregressionmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ImprovedLARSalgorithmforadaptiveLASSOinthelinearregressionmodel.html">&lt;p&gt;The adaptive LASSO has been used for consistent variable selection in place of LASSO in the linear regression model. In this article, we propose a modified LARS algorithm to combine adaptive LASSO with some biased estimators, namely the Almost Unbiased Ridge Estimator (AURE), Liu Estimator (LE), Almost Unbiased Liu Estimator (AULE), Principal Component Regression Estimator (PCRE), r-k class estimator, and r-d class estimator. Furthermore, we examine the performance of the proposed algorithm using a Monte Carlo simulation study and real-world examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07985&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Manickavasagar Kayanan, Pushpakanthie Wijekoon</name></author><category term="stat.ME" /><summary type="html">The adaptive LASSO has been used for consistent variable selection in place of LASSO in the linear regression model. In this article, we propose a modified LARS algorithm to combine adaptive LASSO with some biased estimators, namely the Almost Unbiased Ridge Estimator (AURE), Liu Estimator (LE), Almost Unbiased Liu Estimator (AULE), Principal Component Regression Estimator (PCRE), r-k class estimator, and r-d class estimator. Furthermore, we examine the performance of the proposed algorithm using a Monte Carlo simulation study and real-world examples.</summary></entry><entry><title type="html">Improving prediction models by incorporating external data with weights based on similarity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Improvingpredictionmodelsbyincorporatingexternaldatawithweightsbasedonsimilarity.html" rel="alternate" type="text/html" title="Improving prediction models by incorporating external data with weights based on similarity" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Improvingpredictionmodelsbyincorporatingexternaldatawithweightsbasedonsimilarity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Improvingpredictionmodelsbyincorporatingexternaldatawithweightsbasedonsimilarity.html">&lt;p&gt;In clinical settings, we often face the challenge of building prediction models based on small observational data sets. For example, such a data set might be from a medical center in a multi-center study. Differences between centers might be large, thus requiring specific models based on the data set from the target center. Still, we want to borrow information from the external centers, to deal with small sample sizes. There are approaches that either assign weights to each external data set or each external observation. To incorporate information on differences between data sets and observations, we propose an approach that combines both into weights that can be incorporated into a likelihood for fitting regression models. Specifically, we suggest weights at the data set level that incorporate information on how well the models that provide the observation weights distinguish between data sets. Technically, this takes the form of inverse probability weighting. We explore different scenarios where covariates and outcomes differ among data sets, informing our simulation design for method evaluation. The concept of effective sample size is used for understanding the effectiveness of our subgroup modeling approach. We demonstrate our approach through a clinical application, predicting applied radiotherapy doses for cancer patients. Generally, the proposed approach provides improved prediction performance when external data sets are similar. We thus provide a method for quantifying similarity of external data sets to the target data set and use this similarity to include external observations for improving performance in a target data set prediction modeling task with small data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07631&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Max Behrens, Maryam Farhadizadeh, Angelika Rohde, Alexander Rühle, Nils H. Nicolay, Harald Binder, Daniela Zöller</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In clinical settings, we often face the challenge of building prediction models based on small observational data sets. For example, such a data set might be from a medical center in a multi-center study. Differences between centers might be large, thus requiring specific models based on the data set from the target center. Still, we want to borrow information from the external centers, to deal with small sample sizes. There are approaches that either assign weights to each external data set or each external observation. To incorporate information on differences between data sets and observations, we propose an approach that combines both into weights that can be incorporated into a likelihood for fitting regression models. Specifically, we suggest weights at the data set level that incorporate information on how well the models that provide the observation weights distinguish between data sets. Technically, this takes the form of inverse probability weighting. We explore different scenarios where covariates and outcomes differ among data sets, informing our simulation design for method evaluation. The concept of effective sample size is used for understanding the effectiveness of our subgroup modeling approach. We demonstrate our approach through a clinical application, predicting applied radiotherapy doses for cancer patients. Generally, the proposed approach provides improved prediction performance when external data sets are similar. We thus provide a method for quantifying similarity of external data sets to the target data set and use this similarity to include external observations for improving performance in a target data set prediction modeling task with small data.</summary></entry><entry><title type="html">Interactive identification of individuals with positive treatment effect while controlling false discoveries</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Interactiveidentificationofindividualswithpositivetreatmenteffectwhilecontrollingfalsediscoveries.html" rel="alternate" type="text/html" title="Interactive identification of individuals with positive treatment effect while controlling false discoveries" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Interactiveidentificationofindividualswithpositivetreatmenteffectwhilecontrollingfalsediscoveries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Interactiveidentificationofindividualswithpositivetreatmenteffectwhilecontrollingfalsediscoveries.html">&lt;p&gt;Out of the participants in a randomized experiment with anticipated heterogeneous treatment effects, is it possible to identify which subjects have a positive treatment effect? While subgroup analysis has received attention, claims about individual participants are much more challenging. We frame the problem in terms of multiple hypothesis testing: each individual has a null hypothesis (stating that the potential outcomes are equal, for example) and we aim to identify those for whom the null is false (the treatment potential outcome stochastically dominates the control one, for example). We develop a novel algorithm that identifies such a subset, with nonasymptotic control of the false discovery rate (FDR). Our algorithm allows for interaction – a human data scientist (or a computer program) may adaptively guide the algorithm in a data-dependent manner to gain power. We show how to extend the methods to observational settings and achieve a type of doubly-robust FDR control. We also propose several extensions: (a) relaxing the null to nonpositive effects, (b) moving from unpaired to paired samples, and (c) subgroup identification. We demonstrate via numerical experiments and theoretical analysis that the proposed method has valid FDR control in finite samples and reasonably high identification power.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2102.10778&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Boyan Duan, Larry Wasserman, Aaditya Ramdas</name></author><category term="stat.ME" /><summary type="html">Out of the participants in a randomized experiment with anticipated heterogeneous treatment effects, is it possible to identify which subjects have a positive treatment effect? While subgroup analysis has received attention, claims about individual participants are much more challenging. We frame the problem in terms of multiple hypothesis testing: each individual has a null hypothesis (stating that the potential outcomes are equal, for example) and we aim to identify those for whom the null is false (the treatment potential outcome stochastically dominates the control one, for example). We develop a novel algorithm that identifies such a subset, with nonasymptotic control of the false discovery rate (FDR). Our algorithm allows for interaction – a human data scientist (or a computer program) may adaptively guide the algorithm in a data-dependent manner to gain power. We show how to extend the methods to observational settings and achieve a type of doubly-robust FDR control. We also propose several extensions: (a) relaxing the null to nonpositive effects, (b) moving from unpaired to paired samples, and (c) subgroup identification. We demonstrate via numerical experiments and theoretical analysis that the proposed method has valid FDR control in finite samples and reasonably high identification power.</summary></entry><entry><title type="html">Kernel Three Pass Regression Filter</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/KernelThreePassRegressionFilter.html" rel="alternate" type="text/html" title="Kernel Three Pass Regression Filter" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/KernelThreePassRegressionFilter</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/KernelThreePassRegressionFilter.html">&lt;p&gt;We forecast a single time series using a high-dimensional set of predictors. When these predictors share common underlying dynamics, an approximate latent factor model provides a powerful characterization of their co-movements Bai(2003). These latent factors succinctly summarize the data and can also be used for prediction, alleviating the curse of dimensionality in high-dimensional prediction exercises, see Stock &amp;amp; Watson (2002a). However, forecasting using these latent factors suffers from two potential drawbacks. First, not all pervasive factors among the set of predictors may be relevant, and using all of them can lead to inefficient forecasts. The second shortcoming is the assumption of linear dependence of predictors on the underlying factors. The first issue can be addressed by using some form of supervision, which leads to the omission of irrelevant information. One example is the three-pass regression filter proposed by Kelly &amp;amp; Pruitt (2015). We extend their framework to cases where the form of dependence might be nonlinear by developing a new estimator, which we refer to as the Kernel Three-Pass Regression Filter (K3PRF). This alleviates the aforementioned second shortcoming. The estimator is computationally efficient and performs well empirically. The short-term performance matches or exceeds that of established models, while the long-term performance shows significant improvement.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07292&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rajveer Jat, Daanish Padha</name></author><category term="stat.ME" /><summary type="html">We forecast a single time series using a high-dimensional set of predictors. When these predictors share common underlying dynamics, an approximate latent factor model provides a powerful characterization of their co-movements Bai(2003). These latent factors succinctly summarize the data and can also be used for prediction, alleviating the curse of dimensionality in high-dimensional prediction exercises, see Stock &amp;amp; Watson (2002a). However, forecasting using these latent factors suffers from two potential drawbacks. First, not all pervasive factors among the set of predictors may be relevant, and using all of them can lead to inefficient forecasts. The second shortcoming is the assumption of linear dependence of predictors on the underlying factors. The first issue can be addressed by using some form of supervision, which leads to the omission of irrelevant information. One example is the three-pass regression filter proposed by Kelly &amp;amp; Pruitt (2015). We extend their framework to cases where the form of dependence might be nonlinear by developing a new estimator, which we refer to as the Kernel Three-Pass Regression Filter (K3PRF). This alleviates the aforementioned second shortcoming. The estimator is computationally efficient and performs well empirically. The short-term performance matches or exceeds that of established models, while the long-term performance shows significant improvement.</summary></entry><entry><title type="html">LLM4ED: Large Language Models for Automatic Equation Discovery</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/LLM4EDLargeLanguageModelsforAutomaticEquationDiscovery.html" rel="alternate" type="text/html" title="LLM4ED: Large Language Models for Automatic Equation Discovery" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/LLM4EDLargeLanguageModelsforAutomaticEquationDiscovery</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/LLM4EDLargeLanguageModelsforAutomaticEquationDiscovery.html">&lt;p&gt;Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain. Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms. In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations. In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively. The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance. The second strategy is to instruct LLMs to perform evolutionary operators for global search. Experiments are extensively conducted on both partial differential equations and ordinary differential equations. Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems. Further comparisons are made with state-of-the-art models, demonstrating good stability and usability. Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07761&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mengge Du, Yuntian Chen, Zhongzheng Wang, Longfeng Nie, Dongxiao Zhang</name></author><category term="stat.AP" /><summary type="html">Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain. Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms. In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations. In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively. The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance. The second strategy is to instruct LLMs to perform evolutionary operators for global search. Experiments are extensively conducted on both partial differential equations and ordinary differential equations. Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems. Further comparisons are made with state-of-the-art models, demonstrating good stability and usability. Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery.</summary></entry><entry><title type="html">Large-dimensional Robust Factor Analysis with Group Structure</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/LargedimensionalRobustFactorAnalysiswithGroupStructure.html" rel="alternate" type="text/html" title="Large-dimensional Robust Factor Analysis with Group Structure" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/LargedimensionalRobustFactorAnalysiswithGroupStructure</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/LargedimensionalRobustFactorAnalysiswithGroupStructure.html">&lt;p&gt;In this paper, we focus on exploiting the group structure for large-dimensional factor models, which captures the homogeneous effects of common factors on individuals within the same group. In view of the fact that datasets in macroeconomics and finance are typically heavy-tailed, we propose to identify the unknown group structure using the agglomerative hierarchical clustering algorithm and an information criterion with the robust two-step (RTS) estimates as initial values. The loadings and factors are then re-estimated conditional on the identified groups. Theoretically, we demonstrate the consistency of the estimators for both group membership and the number of groups determined by the information criterion. Under finite second moment condition, we provide the convergence rate for the newly estimated factor loadings with group information, which are shown to achieve efficiency gains compared to those obtained without group structure information. Numerical simulations and real data analysis demonstrate the nice finite sample performance of our proposed approach in the presence of both group structure and heavy-tailedness.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07138&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yong He, Xiaoyang Ma, Xingheng Wang, Yalin Wang</name></author><category term="stat.ME" /><summary type="html">In this paper, we focus on exploiting the group structure for large-dimensional factor models, which captures the homogeneous effects of common factors on individuals within the same group. In view of the fact that datasets in macroeconomics and finance are typically heavy-tailed, we propose to identify the unknown group structure using the agglomerative hierarchical clustering algorithm and an information criterion with the robust two-step (RTS) estimates as initial values. The loadings and factors are then re-estimated conditional on the identified groups. Theoretically, we demonstrate the consistency of the estimators for both group membership and the number of groups determined by the information criterion. Under finite second moment condition, we provide the convergence rate for the newly estimated factor loadings with group information, which are shown to achieve efficiency gains compared to those obtained without group structure information. Numerical simulations and real data analysis demonstrate the nice finite sample performance of our proposed approach in the presence of both group structure and heavy-tailedness.</summary></entry><entry><title type="html">Liouville Flow Importance Sampler</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/LiouvilleFlowImportanceSampler.html" rel="alternate" type="text/html" title="Liouville Flow Importance Sampler" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/LiouvilleFlowImportanceSampler</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/LiouvilleFlowImportanceSampler.html">&lt;p&gt;We present the Liouville Flow Importance Sampler (LFIS), an innovative flow-based model for generating samples from unnormalized density functions. LFIS learns a time-dependent velocity field that deterministically transports samples from a simple initial distribution to a complex target distribution, guided by a prescribed path of annealed distributions. The training of LFIS utilizes a unique method that enforces the structure of a derived partial differential equation to neural networks modeling velocity fields. By considering the neural velocity field as an importance sampler, sample weights can be computed through accumulating errors along the sample trajectories driven by neural velocity fields, ensuring unbiased and consistent estimation of statistical quantities. We demonstrate the effectiveness of LFIS through its application to a range of benchmark problems, on many of which LFIS achieved state-of-the-art performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06672&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yifeng Tian, Nishant Panda, Yen Ting Lin</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">We present the Liouville Flow Importance Sampler (LFIS), an innovative flow-based model for generating samples from unnormalized density functions. LFIS learns a time-dependent velocity field that deterministically transports samples from a simple initial distribution to a complex target distribution, guided by a prescribed path of annealed distributions. The training of LFIS utilizes a unique method that enforces the structure of a derived partial differential equation to neural networks modeling velocity fields. By considering the neural velocity field as an importance sampler, sample weights can be computed through accumulating errors along the sample trajectories driven by neural velocity fields, ensuring unbiased and consistent estimation of statistical quantities. We demonstrate the effectiveness of LFIS through its application to a range of benchmark problems, on many of which LFIS achieved state-of-the-art performance.</summary></entry><entry><title type="html">Low-order outcomes and clustered designs: combining design and analysis for causal inference under network interference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Loworderoutcomesandclustereddesignscombiningdesignandanalysisforcausalinferenceundernetworkinterference.html" rel="alternate" type="text/html" title="Low-order outcomes and clustered designs: combining design and analysis for causal inference under network interference" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Loworderoutcomesandclustereddesignscombiningdesignandanalysisforcausalinferenceundernetworkinterference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Loworderoutcomesandclustereddesignscombiningdesignandanalysisforcausalinferenceundernetworkinterference.html">&lt;p&gt;Variance reduction for causal inference in the presence of network interference is often achieved through either outcome modeling, which is typically analyzed under unit-randomized Bernoulli designs, or clustered experimental designs, which are typically analyzed without strong parametric assumptions. In this work, we study the intersection of these two approaches and consider the problem of estimation in low-order outcome models using data from a general experimental design. Our contributions are threefold. First, we present an estimator of the total treatment effect (also called the global average treatment effect) in a low-degree outcome model when the data are collected under general experimental designs, generalizing previous results for Bernoulli designs. We refer to this estimator as the pseudoinverse estimator and give bounds on its bias and variance in terms of properties of the experimental design. Second, we evaluate these bounds for the case of cluster randomized designs with both Bernoulli and complete randomization. For clustered Bernoulli randomization, we find that our estimator is always unbiased and that its variance scales like the smaller of the variance obtained from a low-order assumption and the variance obtained from cluster randomization, showing that combining these variance reduction strategies is preferable to using either individually. For clustered complete randomization, we find a notable bias-variance trade-off mediated by specific features of the clustering. Third, when choosing a clustered experimental design, our bounds can be used to select a clustering from a set of candidate clusterings. Across a range of graphs and clustering algorithms, we show that our method consistently selects clusterings that perform well on a range of response models, suggesting that our bounds are useful to practitioners.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07979&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matthew Eichhorn, Samir Khan, Johan Ugander, Christina Lee Yu</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Variance reduction for causal inference in the presence of network interference is often achieved through either outcome modeling, which is typically analyzed under unit-randomized Bernoulli designs, or clustered experimental designs, which are typically analyzed without strong parametric assumptions. In this work, we study the intersection of these two approaches and consider the problem of estimation in low-order outcome models using data from a general experimental design. Our contributions are threefold. First, we present an estimator of the total treatment effect (also called the global average treatment effect) in a low-degree outcome model when the data are collected under general experimental designs, generalizing previous results for Bernoulli designs. We refer to this estimator as the pseudoinverse estimator and give bounds on its bias and variance in terms of properties of the experimental design. Second, we evaluate these bounds for the case of cluster randomized designs with both Bernoulli and complete randomization. For clustered Bernoulli randomization, we find that our estimator is always unbiased and that its variance scales like the smaller of the variance obtained from a low-order assumption and the variance obtained from cluster randomization, showing that combining these variance reduction strategies is preferable to using either individually. For clustered complete randomization, we find a notable bias-variance trade-off mediated by specific features of the clustering. Third, when choosing a clustered experimental design, our bounds can be used to select a clustering from a set of candidate clusterings. Across a range of graphs and clustering algorithms, we show that our method consistently selects clusterings that perform well on a range of response models, suggesting that our bounds are useful to practitioners.</summary></entry><entry><title type="html">Multilayer Network Regression with Eigenvector Centrality and Community Structure</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/MultilayerNetworkRegressionwithEigenvectorCentralityandCommunityStructure.html" rel="alternate" type="text/html" title="Multilayer Network Regression with Eigenvector Centrality and Community Structure" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/MultilayerNetworkRegressionwithEigenvectorCentralityandCommunityStructure</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/MultilayerNetworkRegressionwithEigenvectorCentralityandCommunityStructure.html">&lt;p&gt;In the analysis of complex networks, centrality measures and community structures are two important aspects. For multilayer networks, one crucial task is to integrate information across different layers, especially taking the dependence structure within and between layers into consideration. In this study, we introduce a novel two-stage regression model (CC-MNetR) that leverages the eigenvector centrality and network community structure of fourth-order tensor-like multilayer networks. In particular, we construct community-based centrality measures, which are then incorporated into the regression model. In addition, considering the noise of network data, we analyze the centrality measure with and without measurement errors respectively, and establish the consistent properties of the least squares estimates in the regression. Our proposed method is then applied to the World Input-Output Database (WIOD) dataset to explore how input-output network data between different countries and different industries affect the Gross Output of each industry.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.06204&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhuoye Han, Tiandong Wang</name></author><category term="stat.ME" /><summary type="html">In the analysis of complex networks, centrality measures and community structures are two important aspects. For multilayer networks, one crucial task is to integrate information across different layers, especially taking the dependence structure within and between layers into consideration. In this study, we introduce a novel two-stage regression model (CC-MNetR) that leverages the eigenvector centrality and network community structure of fourth-order tensor-like multilayer networks. In particular, we construct community-based centrality measures, which are then incorporated into the regression model. In addition, considering the noise of network data, we analyze the centrality measure with and without measurement errors respectively, and establish the consistent properties of the least squares estimates in the regression. Our proposed method is then applied to the World Input-Output Database (WIOD) dataset to explore how input-output network data between different countries and different industries affect the Gross Output of each industry.</summary></entry><entry><title type="html">Nested Instrumental Variables Design: Switcher Average Treatment Effect, Identification, Efficient Estimation and Generalizability</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/NestedInstrumentalVariablesDesignSwitcherAverageTreatmentEffectIdentificationEfficientEstimationandGeneralizability.html" rel="alternate" type="text/html" title="Nested Instrumental Variables Design: Switcher Average Treatment Effect, Identification, Efficient Estimation and Generalizability" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/NestedInstrumentalVariablesDesignSwitcherAverageTreatmentEffectIdentificationEfficientEstimationandGeneralizability</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/NestedInstrumentalVariablesDesignSwitcherAverageTreatmentEffectIdentificationEfficientEstimationandGeneralizability.html">&lt;p&gt;Instrumental variables (IV) are a commonly used tool to estimate causal effects from non-randomized data. A prototype of an IV is a randomized trial with non-compliance where the randomized treatment assignment serves as an IV for the non-ignorable treatment received. Under a monotonicity assumption, a valid IV non-parametrically identifies the average treatment effect among a non-identifiable complier subgroup, whose generalizability is often under debate. In many studies, there could exist multiple versions of an IV, for instance, different nudges to take the same treatment in different study sites in a multi-center clinical trial. These different versions of an IV may result in different compliance rates and offer a unique opportunity to study IV estimates’ generalizability. In this article, we introduce a novel nested IV assumption and study identification of the average treatment effect among two latent subgroups: always-compliers and switchers, who are defined based on the joint potential treatment received under two versions of a binary IV. We derive the efficient influence function for the SWitcher Average Treatment Effect (SWATE) and propose efficient estimators. We then propose formal statistical tests of the generalizability of IV estimates based on comparing the conditional average treatment effect among the always-compliers and that among the switchers under the nested IV framework. We apply the proposed framework and method to the Prostate, Lung, Colorectal and Ovarian (PLCO) Cancer Screening Trial and study the causal effect of colorectal cancer screening and its generalizability.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07102&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rui Wang, Ying-Qi Zhao, Oliver Dukes, Bo Zhang</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.OT" /><summary type="html">Instrumental variables (IV) are a commonly used tool to estimate causal effects from non-randomized data. A prototype of an IV is a randomized trial with non-compliance where the randomized treatment assignment serves as an IV for the non-ignorable treatment received. Under a monotonicity assumption, a valid IV non-parametrically identifies the average treatment effect among a non-identifiable complier subgroup, whose generalizability is often under debate. In many studies, there could exist multiple versions of an IV, for instance, different nudges to take the same treatment in different study sites in a multi-center clinical trial. These different versions of an IV may result in different compliance rates and offer a unique opportunity to study IV estimates’ generalizability. In this article, we introduce a novel nested IV assumption and study identification of the average treatment effect among two latent subgroups: always-compliers and switchers, who are defined based on the joint potential treatment received under two versions of a binary IV. We derive the efficient influence function for the SWitcher Average Treatment Effect (SWATE) and propose efficient estimators. We then propose formal statistical tests of the generalizability of IV estimates based on comparing the conditional average treatment effect among the always-compliers and that among the switchers under the nested IV framework. We apply the proposed framework and method to the Prostate, Lung, Colorectal and Ovarian (PLCO) Cancer Screening Trial and study the causal effect of colorectal cancer screening and its generalizability.</summary></entry><entry><title type="html">On Doubly Robust Estimation with Nonignorable Missing Data Using Instrumental Variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/OnDoublyRobustEstimationwithNonignorableMissingDataUsingInstrumentalVariables.html" rel="alternate" type="text/html" title="On Doubly Robust Estimation with Nonignorable Missing Data Using Instrumental Variables" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/OnDoublyRobustEstimationwithNonignorableMissingDataUsingInstrumentalVariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/OnDoublyRobustEstimationwithNonignorableMissingDataUsingInstrumentalVariables.html">&lt;p&gt;Suppose we are interested in the mean of an outcome that is subject to nonignorable nonresponse. This paper develops new semiparametric estimation methods with instrumental variables which affect nonresponse, but not the outcome. The proposed estimators remain consistent and asymptotically normal even under partial model misspecifications for two variation independent nuisance components. We evaluate the performance of the proposed estimators via a simulation study, and apply them in adjusting for missing data induced by HIV testing refusal in the evaluation of HIV seroprevalence in Mochudi, Botswana, using interviewer experience as an instrumental variable.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.08691&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Baoluo Sun, Wang Miao, Deshanee S. Wickramarachchi</name></author><category term="stat.ME" /><summary type="html">Suppose we are interested in the mean of an outcome that is subject to nonignorable nonresponse. This paper develops new semiparametric estimation methods with instrumental variables which affect nonresponse, but not the outcome. The proposed estimators remain consistent and asymptotically normal even under partial model misspecifications for two variation independent nuisance components. We evaluate the performance of the proposed estimators via a simulation study, and apply them in adjusting for missing data induced by HIV testing refusal in the evaluation of HIV seroprevalence in Mochudi, Botswana, using interviewer experience as an instrumental variable.</summary></entry><entry><title type="html">On Joint Marginal Expected Shortfall and Associated Contribution Risk Measures</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/OnJointMarginalExpectedShortfallandAssociatedContributionRiskMeasures.html" rel="alternate" type="text/html" title="On Joint Marginal Expected Shortfall and Associated Contribution Risk Measures" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/OnJointMarginalExpectedShortfallandAssociatedContributionRiskMeasures</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/OnJointMarginalExpectedShortfallandAssociatedContributionRiskMeasures.html">&lt;p&gt;Systemic risk is the risk that a company- or industry-level risk could trigger a huge collapse of another or even the whole institution. Various systemic risk measures have been proposed in the literature to quantify the domino and (relative) spillover effects induced by systemic risks such as the well-known CoVaR, CoES, MES and CoD risk measures, and associated contribution measures. This paper proposes another new type of systemic risk measure, called the joint marginal expected shortfall (JMES), to measure whether the MES of one entity’s risk-taking adds to another one or the overall risk conditioned on the event that the entity is already in some specified distress level. We further introduce two useful systemic risk contribution measures based on the difference function or relative ratio function of the JMES and the conventional ES, respectively. Some basic properties of these proposed measures are studied such as monotonicity, comonotonic additivity, non-identifiability and non-elicitability. For both risk measures and two different vectors of bivariate risks, we establish sufficient conditions imposed on copula structure, stress levels, and stochastic orders to compare these new measures. We further provide some numerical examples to illustrate our main findings. A real application in analyzing the risk contagion among several stock market indices is implemented to show the performances of our proposed measures compared with other commonly used measures including CoVaR, CoES, MES, and their associated contribution measures.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07549&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tong Pu, Yifei Zhang, Yiying Zhang</name></author><category term="stat.AP" /><summary type="html">Systemic risk is the risk that a company- or industry-level risk could trigger a huge collapse of another or even the whole institution. Various systemic risk measures have been proposed in the literature to quantify the domino and (relative) spillover effects induced by systemic risks such as the well-known CoVaR, CoES, MES and CoD risk measures, and associated contribution measures. This paper proposes another new type of systemic risk measure, called the joint marginal expected shortfall (JMES), to measure whether the MES of one entity’s risk-taking adds to another one or the overall risk conditioned on the event that the entity is already in some specified distress level. We further introduce two useful systemic risk contribution measures based on the difference function or relative ratio function of the JMES and the conventional ES, respectively. Some basic properties of these proposed measures are studied such as monotonicity, comonotonic additivity, non-identifiability and non-elicitability. For both risk measures and two different vectors of bivariate risks, we establish sufficient conditions imposed on copula structure, stress levels, and stochastic orders to compare these new measures. We further provide some numerical examples to illustrate our main findings. A real application in analyzing the risk contagion among several stock market indices is implemented to show the performances of our proposed measures compared with other commonly used measures including CoVaR, CoES, MES, and their associated contribution measures.</summary></entry><entry><title type="html">On the Relation Between Autoencoders and Non-negative Matrix Factorization, and Their Application for Mutational Signature Extraction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/OntheRelationBetweenAutoencodersandNonnegativeMatrixFactorizationandTheirApplicationforMutationalSignatureExtraction.html" rel="alternate" type="text/html" title="On the Relation Between Autoencoders and Non-negative Matrix Factorization, and Their Application for Mutational Signature Extraction" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/OntheRelationBetweenAutoencodersandNonnegativeMatrixFactorizationandTheirApplicationforMutationalSignatureExtraction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/OntheRelationBetweenAutoencodersandNonnegativeMatrixFactorizationandTheirApplicationforMutationalSignatureExtraction.html">&lt;p&gt;The aim of this study is to provide a foundation to understand the relationship between non-negative matrix factorization (NMF) and non-negative autoencoders enabling proper interpretation and understanding of autoencoder-based alternatives to NMF. Since its introduction, NMF has been a popular tool for extracting interpretable, low-dimensional representations of high-dimensional data. However, recently, several studies have proposed to replace NMF with autoencoders. This increasing popularity of autoencoders warrants an investigation on whether this replacement is in general valid and reasonable. Moreover, the exact relationship between non-negative autoencoders and NMF has not been thoroughly explored. Thus, a main aim of this study is to investigate in detail the relationship between non-negative autoencoders and NMF. We find that the connection between the two models can be established through convex NMF, which is a restricted case of NMF. In particular, convex NMF is a special case of an autoencoder. The performance of NMF and autoencoders is compared within the context of extraction of mutational signatures from cancer genomics data. We find that the reconstructions based on NMF are more accurate compared to autoencoders, while the signatures extracted using both methods show comparable consistencies and values when externally validated. These findings suggest that the non-negative autoencoders investigated in this article do not provide an improvement of NMF in the field of mutational signature extraction.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07879&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ida Egendal, Rasmus Froberg Br{\o}ndum, Marta Pelizzola, Asger Hobolth, Martin B{\o}gsted</name></author><category term="stat.AP" /><summary type="html">The aim of this study is to provide a foundation to understand the relationship between non-negative matrix factorization (NMF) and non-negative autoencoders enabling proper interpretation and understanding of autoencoder-based alternatives to NMF. Since its introduction, NMF has been a popular tool for extracting interpretable, low-dimensional representations of high-dimensional data. However, recently, several studies have proposed to replace NMF with autoencoders. This increasing popularity of autoencoders warrants an investigation on whether this replacement is in general valid and reasonable. Moreover, the exact relationship between non-negative autoencoders and NMF has not been thoroughly explored. Thus, a main aim of this study is to investigate in detail the relationship between non-negative autoencoders and NMF. We find that the connection between the two models can be established through convex NMF, which is a restricted case of NMF. In particular, convex NMF is a special case of an autoencoder. The performance of NMF and autoencoders is compared within the context of extraction of mutational signatures from cancer genomics data. We find that the reconstructions based on NMF are more accurate compared to autoencoders, while the signatures extracted using both methods show comparable consistencies and values when externally validated. These findings suggest that the non-negative autoencoders investigated in this article do not provide an improvement of NMF in the field of mutational signature extraction.</summary></entry><entry><title type="html">Optimal accuracy for linear sets of equations with the graph Laplacian</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/OptimalaccuracyforlinearsetsofequationswiththegraphLaplacian.html" rel="alternate" type="text/html" title="Optimal accuracy for linear sets of equations with the graph Laplacian" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/OptimalaccuracyforlinearsetsofequationswiththegraphLaplacian</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/OptimalaccuracyforlinearsetsofequationswiththegraphLaplacian.html">&lt;p&gt;We show that certain Graph Laplacian linear sets of equations exhibit optimal accuracy, guaranteeing that the relative error is no larger than the norm of the relative residual and that optimality occurs for carefully chosen right-hand sides. Such sets of equations arise in PageRank and Markov chain theory. We establish new relationships among the PageRank teleportation parameter, the Markov chain discount, and approximations to linear sets of equations. The set of optimally accurate systems can be separated into two groups for an undirected graph – those that achieve optimality asymptotically with the graph size and those that do not – determined by the angle between the right-hand side of the linear system and the vector of all ones. We provide supporting numerical experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07877&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Richard B. Lehoucq, Michael Weylandt, Jonathan W. Berry</name></author><category term="stat.CO" /><summary type="html">We show that certain Graph Laplacian linear sets of equations exhibit optimal accuracy, guaranteeing that the relative error is no larger than the norm of the relative residual and that optimality occurs for carefully chosen right-hand sides. Such sets of equations arise in PageRank and Markov chain theory. We establish new relationships among the PageRank teleportation parameter, the Markov chain discount, and approximations to linear sets of equations. The set of optimally accurate systems can be separated into two groups for an undirected graph – those that achieve optimality asymptotically with the graph size and those that do not – determined by the angle between the right-hand side of the linear system and the vector of all ones. We provide supporting numerical experiments.</summary></entry><entry><title type="html">Post-selection inference for causal effects after causal discovery</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Postselectioninferenceforcausaleffectsaftercausaldiscovery.html" rel="alternate" type="text/html" title="Post-selection inference for causal effects after causal discovery" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Postselectioninferenceforcausaleffectsaftercausaldiscovery</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Postselectioninferenceforcausaleffectsaftercausaldiscovery.html">&lt;p&gt;Algorithms for constraint-based causal discovery select graphical causal models among a space of possible candidates (e.g., all directed acyclic graphs) by executing a sequence of conditional independence tests. These may be used to inform the estimation of causal effects (e.g., average treatment effects) when there is uncertainty about which covariates ought to be adjusted for, or which variables act as confounders versus mediators. However, naively using the data twice, for model selection and estimation, would lead to invalid confidence intervals. Moreover, if the selected graph is incorrect, the inferential claims may apply to a selected functional that is distinct from the actual causal effect. We propose an approach to post-selection inference that is based on a resampling and screening procedure, which essentially performs causal discovery multiple times with randomly varying intermediate test statistics. Then, an estimate of the target causal effect and corresponding confidence sets are constructed from a union of individual graph-based estimates and intervals. We show that this construction has asymptotically correct coverage for the true causal effect parameter. Importantly, the guarantee holds for a fixed population-level effect, not a data-dependent or selection-dependent quantity. Most of our exposition focuses on the PC-algorithm for learning directed acyclic graphs and the multivariate Gaussian case for simplicity, but the approach is general and modular, so it may be used with other conditional independence based discovery algorithms and distributional families.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06763&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ting-Hsuan Chang, Zijian Guo, Daniel Malinsky</name></author><category term="stat.ME" /><summary type="html">Algorithms for constraint-based causal discovery select graphical causal models among a space of possible candidates (e.g., all directed acyclic graphs) by executing a sequence of conditional independence tests. These may be used to inform the estimation of causal effects (e.g., average treatment effects) when there is uncertainty about which covariates ought to be adjusted for, or which variables act as confounders versus mediators. However, naively using the data twice, for model selection and estimation, would lead to invalid confidence intervals. Moreover, if the selected graph is incorrect, the inferential claims may apply to a selected functional that is distinct from the actual causal effect. We propose an approach to post-selection inference that is based on a resampling and screening procedure, which essentially performs causal discovery multiple times with randomly varying intermediate test statistics. Then, an estimate of the target causal effect and corresponding confidence sets are constructed from a union of individual graph-based estimates and intervals. We show that this construction has asymptotically correct coverage for the true causal effect parameter. Importantly, the guarantee holds for a fixed population-level effect, not a data-dependent or selection-dependent quantity. Most of our exposition focuses on the PC-algorithm for learning directed acyclic graphs and the multivariate Gaussian case for simplicity, but the approach is general and modular, so it may be used with other conditional independence based discovery algorithms and distributional families.</summary></entry><entry><title type="html">Proximal Langevin Sampling With Inexact Proximal Mapping</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ProximalLangevinSamplingWithInexactProximalMapping.html" rel="alternate" type="text/html" title="Proximal Langevin Sampling With Inexact Proximal Mapping" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ProximalLangevinSamplingWithInexactProximalMapping</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ProximalLangevinSamplingWithInexactProximalMapping.html">&lt;p&gt;In order to solve tasks like uncertainty quantification or hypothesis tests in Bayesian imaging inverse problems, we often have to draw samples from the arising posterior distribution. For the usually log-concave but high-dimensional posteriors, Markov chain Monte Carlo methods based on time discretizations of Langevin diffusion are a popular tool. If the potential defining the distribution is non-smooth, these discretizations are usually of an implicit form leading to Langevin sampling algorithms that require the evaluation of proximal operators. For some of the potentials relevant in imaging problems this is only possible approximately using an iterative scheme. We investigate the behaviour of a proximal Langevin algorithm under the presence of errors in the evaluation of proximal mappings. We generalize existing non-asymptotic and asymptotic convergence results of the exact algorithm to our inexact setting and quantify the bias between the target and the algorithm’s stationary distribution due to the errors. We show that the additional bias stays bounded for bounded errors and converges to zero for decaying errors in a strongly convex setting. We apply the inexact algorithm to sample numerically from the posterior of typical imaging inverse problems in which we can only approximate the proximal operator by an iterative scheme and validate our theoretical convergence results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.17737&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matthias J. Ehrhardt, Lorenz Kuger, Carola-Bibiane Schönlieb</name></author><category term="stat.CO" /><summary type="html">In order to solve tasks like uncertainty quantification or hypothesis tests in Bayesian imaging inverse problems, we often have to draw samples from the arising posterior distribution. For the usually log-concave but high-dimensional posteriors, Markov chain Monte Carlo methods based on time discretizations of Langevin diffusion are a popular tool. If the potential defining the distribution is non-smooth, these discretizations are usually of an implicit form leading to Langevin sampling algorithms that require the evaluation of proximal operators. For some of the potentials relevant in imaging problems this is only possible approximately using an iterative scheme. We investigate the behaviour of a proximal Langevin algorithm under the presence of errors in the evaluation of proximal mappings. We generalize existing non-asymptotic and asymptotic convergence results of the exact algorithm to our inexact setting and quantify the bias between the target and the algorithm’s stationary distribution due to the errors. We show that the additional bias stays bounded for bounded errors and converges to zero for decaying errors in a strongly convex setting. We apply the inexact algorithm to sample numerically from the posterior of typical imaging inverse problems in which we can only approximate the proximal operator by an iterative scheme and validate our theoretical convergence results.</summary></entry><entry><title type="html">Recursive identification with regularization and on-line hyperparameters estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Recursiveidentificationwithregularizationandonlinehyperparametersestimation.html" rel="alternate" type="text/html" title="Recursive identification with regularization and on-line hyperparameters estimation" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Recursiveidentificationwithregularizationandonlinehyperparametersestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Recursiveidentificationwithregularizationandonlinehyperparametersestimation.html">&lt;p&gt;This paper presents a regularized recursive identification algorithm with simultaneous on-line estimation of both the model parameters and the algorithms hyperparameters. A new kernel is proposed to facilitate the algorithm development. The performance of this novel scheme is compared with that of the recursive least squares algorithm in simulation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.00097&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bernard Vau, Tudor-Bogdan Airimitoaie</name></author><category term="stat.ME" /><summary type="html">This paper presents a regularized recursive identification algorithm with simultaneous on-line estimation of both the model parameters and the algorithms hyperparameters. A new kernel is proposed to facilitate the algorithm development. The performance of this novel scheme is compared with that of the recursive least squares algorithm in simulation.</summary></entry><entry><title type="html">Reinforcement Learning in Modern Biostatistics: Constructing Optimal Adaptive Interventions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ReinforcementLearninginModernBiostatisticsConstructingOptimalAdaptiveInterventions.html" rel="alternate" type="text/html" title="Reinforcement Learning in Modern Biostatistics: Constructing Optimal Adaptive Interventions" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ReinforcementLearninginModernBiostatisticsConstructingOptimalAdaptiveInterventions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ReinforcementLearninginModernBiostatisticsConstructingOptimalAdaptiveInterventions.html">&lt;p&gt;In recent years, reinforcement learning (RL) has acquired a prominent position in health-related sequential decision-making problems, gaining traction as a valuable tool for delivering adaptive interventions (AIs). However, in part due to a poor synergy between the methodological and the applied communities, its real-life application is still limited and its potential is still to be realized. To address this gap, our work provides the first unified technical survey on RL methods, complemented with case studies, for constructing various types of AIs in healthcare. In particular, using the common methodological umbrella of RL, we bridge two seemingly different AI domains, dynamic treatment regimes and just-in-time adaptive interventions in mobile health, highlighting similarities and differences between them and discussing the implications of using RL. Open problems and considerations for future research directions are outlined. Finally, we leverage our experience in designing case studies in both areas to showcase the significant collaborative opportunities between statistical, RL, and healthcare researchers in advancing AIs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.02605&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nina Deliu, Joseph Jay Williams, Bibhas Chakraborty</name></author><category term="stat.ML," /><category term="stat.AP," /><category term="stat.ME" /><summary type="html">In recent years, reinforcement learning (RL) has acquired a prominent position in health-related sequential decision-making problems, gaining traction as a valuable tool for delivering adaptive interventions (AIs). However, in part due to a poor synergy between the methodological and the applied communities, its real-life application is still limited and its potential is still to be realized. To address this gap, our work provides the first unified technical survey on RL methods, complemented with case studies, for constructing various types of AIs in healthcare. In particular, using the common methodological umbrella of RL, we bridge two seemingly different AI domains, dynamic treatment regimes and just-in-time adaptive interventions in mobile health, highlighting similarities and differences between them and discussing the implications of using RL. Open problems and considerations for future research directions are outlined. Finally, we leverage our experience in designing case studies in both areas to showcase the significant collaborative opportunities between statistical, RL, and healthcare researchers in advancing AIs.</summary></entry><entry><title type="html">ResSurv: Cancer Survival Analysis Prediction Model Based on Residual Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ResSurvCancerSurvivalAnalysisPredictionModelBasedonResidualNetworks.html" rel="alternate" type="text/html" title="ResSurv: Cancer Survival Analysis Prediction Model Based on Residual Networks" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ResSurvCancerSurvivalAnalysisPredictionModelBasedonResidualNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/ResSurvCancerSurvivalAnalysisPredictionModelBasedonResidualNetworks.html">&lt;p&gt;Survival prediction is an important branch of cancer prognosis analysis. The model that predicts survival risk through TCGA genomics data can discover genes related to cancer and provide diagnosis and treatment recommendations based on patient characteristics. We found that deep learning models based on Cox proportional hazards often suffer from overfitting when dealing with high-throughput data. Moreover, we found that as the number of network layers increases, the experimental results will not get better, and network degradation will occur. Based on this problem, we propose a new framework based on Deep Residual Learning. Combine the ideas of Cox proportional hazards and Residual. And name it ResSurv. First, ResSurv is a feed-forward deep learning network stacked by multiple basic ResNet Blocks. In each ResNet Block, we add a Normalization Layer to prevent gradient disappearance and gradient explosion. Secondly, for the loss function of the neural network, we inherited the Cox proportional hazards methods, applied the semi-parametric of the CPH model to the neural network, combined with the partial likelihood model, established the loss function, and performed backpropagation and gradient update. Finally, we compared ResSurv networks of different depths and found that we can effectively extract high-dimensional features. Ablation experiments and comparative experiments prove that our model has reached SOTA(state of the art) in the field of deep learning, and our network can effectively extract deep information.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06992&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wankang Zhai</name></author><category term="stat.AP" /><summary type="html">Survival prediction is an important branch of cancer prognosis analysis. The model that predicts survival risk through TCGA genomics data can discover genes related to cancer and provide diagnosis and treatment recommendations based on patient characteristics. We found that deep learning models based on Cox proportional hazards often suffer from overfitting when dealing with high-throughput data. Moreover, we found that as the number of network layers increases, the experimental results will not get better, and network degradation will occur. Based on this problem, we propose a new framework based on Deep Residual Learning. Combine the ideas of Cox proportional hazards and Residual. And name it ResSurv. First, ResSurv is a feed-forward deep learning network stacked by multiple basic ResNet Blocks. In each ResNet Block, we add a Normalization Layer to prevent gradient disappearance and gradient explosion. Secondly, for the loss function of the neural network, we inherited the Cox proportional hazards methods, applied the semi-parametric of the CPH model to the neural network, combined with the partial likelihood model, established the loss function, and performed backpropagation and gradient update. Finally, we compared ResSurv networks of different depths and found that we can effectively extract high-dimensional features. Ablation experiments and comparative experiments prove that our model has reached SOTA(state of the art) in the field of deep learning, and our network can effectively extract deep information.</summary></entry><entry><title type="html">Riemannian Statistics for Any Type of Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/RiemannianStatisticsforAnyTypeofData.html" rel="alternate" type="text/html" title="Riemannian Statistics for Any Type of Data" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/RiemannianStatisticsforAnyTypeofData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/RiemannianStatisticsforAnyTypeofData.html">&lt;p&gt;This paper introduces a novel approach to statistics and data analysis, departing from the conventional assumption of data residing in Euclidean space to consider a Riemannian Manifold. The challenge lies in the absence of vector space operations on such manifolds. Pennec X. et al. in their book Riemannian Geometric Statistics in Medical Image Analysis proposed analyzing data on Riemannian manifolds through geometry, this approach is effective with structured data like medical images, where the intrinsic manifold structure is apparent. Yet, its applicability to general data lacking implicit local distance notions is limited. We propose a solution to generalize Riemannian statistics for any type of data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06799&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Oldemar Rodriguez Rojas</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This paper introduces a novel approach to statistics and data analysis, departing from the conventional assumption of data residing in Euclidean space to consider a Riemannian Manifold. The challenge lies in the absence of vector space operations on such manifolds. Pennec X. et al. in their book Riemannian Geometric Statistics in Medical Image Analysis proposed analyzing data on Riemannian manifolds through geometry, this approach is effective with structured data like medical images, where the intrinsic manifold structure is apparent. Yet, its applicability to general data lacking implicit local distance notions is limited. We propose a solution to generalize Riemannian statistics for any type of data.</summary></entry><entry><title type="html">Selective Randomization Inference for Adaptive Experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/SelectiveRandomizationInferenceforAdaptiveExperiments.html" rel="alternate" type="text/html" title="Selective Randomization Inference for Adaptive Experiments" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/SelectiveRandomizationInferenceforAdaptiveExperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/SelectiveRandomizationInferenceforAdaptiveExperiments.html">&lt;p&gt;Adaptive experiments use preliminary analyses of the data to inform further course of action and are commonly used in many disciplines including medical and social sciences. Because the null hypothesis and experimental design are not pre-specified, it has long been recognized that statistical inference for adaptive experiments is not straightforward. Most existing methods only apply to specific adaptive designs and rely on strong assumptions. In this work, we propose selective randomization inference as a general framework for analyzing adaptive experiments. In a nutshell, our approach applies conditional post-selection inference to randomization tests. By using directed acyclic graphs to describe the data generating process, we derive a selective randomization p-value that controls the selective type-I error without requiring independent and identically distributed data or any other modelling assumptions. We show how rejection sampling and Markov Chain Monte Carlo can be used to compute the selective randomization p-values and construct confidence intervals for a homogeneous treatment effect. To mitigate the risk of disconnected confidence intervals, we propose the use of hold-out units. Lastly, we demonstrate our method and compare it with other randomization tests using synthetic and real-world data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07026&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tobias Freidling, Qingyuan Zhao, Zijun Gao</name></author><category term="stat.ME" /><summary type="html">Adaptive experiments use preliminary analyses of the data to inform further course of action and are commonly used in many disciplines including medical and social sciences. Because the null hypothesis and experimental design are not pre-specified, it has long been recognized that statistical inference for adaptive experiments is not straightforward. Most existing methods only apply to specific adaptive designs and rely on strong assumptions. In this work, we propose selective randomization inference as a general framework for analyzing adaptive experiments. In a nutshell, our approach applies conditional post-selection inference to randomization tests. By using directed acyclic graphs to describe the data generating process, we derive a selective randomization p-value that controls the selective type-I error without requiring independent and identically distributed data or any other modelling assumptions. We show how rejection sampling and Markov Chain Monte Carlo can be used to compute the selective randomization p-values and construct confidence intervals for a homogeneous treatment effect. To mitigate the risk of disconnected confidence intervals, we propose the use of hold-out units. Lastly, we demonstrate our method and compare it with other randomization tests using synthetic and real-world data.</summary></entry><entry><title type="html">Sensitivity Analysis for Active Sampling, with Applications to the Simulation of Analog Circuits</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/SensitivityAnalysisforActiveSamplingwithApplicationstotheSimulationofAnalogCircuits.html" rel="alternate" type="text/html" title="Sensitivity Analysis for Active Sampling, with Applications to the Simulation of Analog Circuits" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/SensitivityAnalysisforActiveSamplingwithApplicationstotheSimulationofAnalogCircuits</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/SensitivityAnalysisforActiveSamplingwithApplicationstotheSimulationofAnalogCircuits.html">&lt;p&gt;We propose an active sampling flow, with the use-case of simulating the impact of combined variations on analog circuits. In such a context, given the large number of parameters, it is difficult to fit a surrogate model and to efficiently explore the space of design features.
  By combining a drastic dimension reduction using sensitivity analysis and Bayesian surrogate modeling, we obtain a flexible active sampling flow. On synthetic and real datasets, this flow outperforms the usual Monte-Carlo sampling which often forms the foundation of design space exploration.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07971&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Reda Chhaibi, Fabrice Gamboa, Christophe Oger, Vinicius Oliveira, Clément Pellegrini, Damien Remot</name></author><category term="stat.ML," /><category term="stat.AP," /><category term="stat.ME" /><summary type="html">We propose an active sampling flow, with the use-case of simulating the impact of combined variations on analog circuits. In such a context, given the large number of parameters, it is difficult to fit a surrogate model and to efficiently explore the space of design features. By combining a drastic dimension reduction using sensitivity analysis and Bayesian surrogate modeling, we obtain a flexible active sampling flow. On synthetic and real datasets, this flow outperforms the usual Monte-Carlo sampling which often forms the foundation of design space exploration.</summary></entry><entry><title type="html">Single-seed generation of Brownian paths and integrals for adaptive and high order SDE solvers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/SingleseedgenerationofBrownianpathsandintegralsforadaptiveandhighorderSDEsolvers.html" rel="alternate" type="text/html" title="Single-seed generation of Brownian paths and integrals for adaptive and high order SDE solvers" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/SingleseedgenerationofBrownianpathsandintegralsforadaptiveandhighorderSDEsolvers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/SingleseedgenerationofBrownianpathsandintegralsforadaptiveandhighorderSDEsolvers.html">&lt;p&gt;Despite the success of adaptive time-stepping in ODE simulation, it has so far seen few applications for Stochastic Differential Equations (SDEs). To simulate SDEs adaptively, methods such as the Virtual Brownian Tree (VBT) have been developed, which can generate Brownian motion (BM) non-chronologically. However, in most applications, knowing only the values of Brownian motion is not enough to achieve a high order of convergence; for that, we must compute time-integrals of BM such as $\int_s^t W_r \, dr$. With the aim of using high order SDE solvers adaptively, we extend the VBT to generate these integrals of BM in addition to the Brownian increments. A JAX-based implementation of our construction is included in the popular Diffrax library (https://github.com/patrick-kidger/diffrax).
  Since the entire Brownian path produced by VBT is uniquely determined by a single PRNG seed, previously generated samples need not be stored, which results in a constant memory footprint and enables experiment repeatability and strong error estimation. Based on binary search, the VBT’s time complexity is logarithmic in the tolerance parameter $\varepsilon$. Unlike the original VBT algorithm, which was only precise at some dyadic times, we prove that our construction exactly matches the joint distribution of the Brownian motion and its time integrals at any query times, provided they are at least $\varepsilon$ apart.
  We present two applications of adaptive high order solvers enabled by our new VBT. Using adaptive solvers to simulate a high-volatility CIR model, we achieve more than twice the convergence order of constant stepping. We apply an adaptive third order underdamped or kinetic Langevin solver to an MCMC problem, where our approach outperforms the No U-Turn Sampler, while using only a tenth of its function evaluations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06464&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andraž Jelinčič, James Foster, Patrick Kidger</name></author><category term="stat.CO" /><summary type="html">Despite the success of adaptive time-stepping in ODE simulation, it has so far seen few applications for Stochastic Differential Equations (SDEs). To simulate SDEs adaptively, methods such as the Virtual Brownian Tree (VBT) have been developed, which can generate Brownian motion (BM) non-chronologically. However, in most applications, knowing only the values of Brownian motion is not enough to achieve a high order of convergence; for that, we must compute time-integrals of BM such as $\int_s^t W_r \, dr$. With the aim of using high order SDE solvers adaptively, we extend the VBT to generate these integrals of BM in addition to the Brownian increments. A JAX-based implementation of our construction is included in the popular Diffrax library (https://github.com/patrick-kidger/diffrax). Since the entire Brownian path produced by VBT is uniquely determined by a single PRNG seed, previously generated samples need not be stored, which results in a constant memory footprint and enables experiment repeatability and strong error estimation. Based on binary search, the VBT’s time complexity is logarithmic in the tolerance parameter $\varepsilon$. Unlike the original VBT algorithm, which was only precise at some dyadic times, we prove that our construction exactly matches the joint distribution of the Brownian motion and its time integrals at any query times, provided they are at least $\varepsilon$ apart. We present two applications of adaptive high order solvers enabled by our new VBT. Using adaptive solvers to simulate a high-volatility CIR model, we achieve more than twice the convergence order of constant stepping. We apply an adaptive third order underdamped or kinetic Langevin solver to an MCMC problem, where our approach outperforms the No U-Turn Sampler, while using only a tenth of its function evaluations.</summary></entry><entry><title type="html">The Multiple Change-in-Gaussian-Mean Problem</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/TheMultipleChangeinGaussianMeanProblem.html" rel="alternate" type="text/html" title="The Multiple Change-in-Gaussian-Mean Problem" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/TheMultipleChangeinGaussianMeanProblem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/TheMultipleChangeinGaussianMeanProblem.html">&lt;p&gt;A manuscript version of the chapter “The Multiple Change-in-Gaussian-Mean Problem” from the book “Change-Point Detection and Data Segmentation” by Fearnhead and Fryzlewicz, currently in preparation. All R code and data to accompany this chapter and the book are gradually being made available through https://github.com/pfryz/cpdds.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06796&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Paul Fearnhead, Piotr Fryzlewicz</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">A manuscript version of the chapter “The Multiple Change-in-Gaussian-Mean Problem” from the book “Change-Point Detection and Data Segmentation” by Fearnhead and Fryzlewicz, currently in preparation. All R code and data to accompany this chapter and the book are gradually being made available through https://github.com/pfryz/cpdds.</summary></entry><entry><title type="html">The Spike-and-Slab Quantile LASSO for Robust Variable Selection in Cancer Genomics Studies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/TheSpikeandSlabQuantileLASSOforRobustVariableSelectioninCancerGenomicsStudies.html" rel="alternate" type="text/html" title="The Spike-and-Slab Quantile LASSO for Robust Variable Selection in Cancer Genomics Studies" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/TheSpikeandSlabQuantileLASSOforRobustVariableSelectioninCancerGenomicsStudies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/TheSpikeandSlabQuantileLASSOforRobustVariableSelectioninCancerGenomicsStudies.html">&lt;p&gt;Data irregularity in cancer genomics studies has been widely observed in the form of outliers and heavy-tailed distributions in the complex traits. In the past decade, robust variable selection methods have emerged as powerful alternatives to the non-robust ones to identify important genes associated with heterogeneous disease traits and build superior predictive models. In this study, to keep the remarkable features of the quantile LASSO and fully Bayesian regularized quantile regression while overcoming their disadvantage in the analysis of high-dimensional genomics data, we propose the spike-and-slab quantile LASSO through a fully Bayesian spike-and-slab formulation under the robust likelihood by adopting the asymmetric Laplace distribution (ALD). The proposed robust method has inherited the prominent properties of selective shrinkage and self-adaptivity to the sparsity pattern from the spike-and-slab LASSO (Ro\v{c}kov&apos;a and George, 2018). Furthermore, the spike-and-slab quantile LASSO has a computational advantage to locate the posterior modes via soft-thresholding rule guided Expectation-Maximization (EM) steps in the coordinate descent framework, a phenomenon rarely observed for robust regularization with non-differentiable loss functions. We have conducted comprehensive simulation studies with a variety of heavy-tailed errors in both homogeneous and heterogeneous model settings to demonstrate the superiority of the spike-and-slab quantile LASSO over its competing methods. The advantage of the proposed method has been further demonstrated in case studies of the lung adenocarcinomas (LUAD) and skin cutaneous melanoma (SKCM) data from The Cancer Genome Atlas (TCGA).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07397&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuwen Liu, Jie Ren, Shuangge Ma, Cen Wu</name></author><category term="stat.ME" /><summary type="html">Data irregularity in cancer genomics studies has been widely observed in the form of outliers and heavy-tailed distributions in the complex traits. In the past decade, robust variable selection methods have emerged as powerful alternatives to the non-robust ones to identify important genes associated with heterogeneous disease traits and build superior predictive models. In this study, to keep the remarkable features of the quantile LASSO and fully Bayesian regularized quantile regression while overcoming their disadvantage in the analysis of high-dimensional genomics data, we propose the spike-and-slab quantile LASSO through a fully Bayesian spike-and-slab formulation under the robust likelihood by adopting the asymmetric Laplace distribution (ALD). The proposed robust method has inherited the prominent properties of selective shrinkage and self-adaptivity to the sparsity pattern from the spike-and-slab LASSO (Ro\v{c}kov&apos;a and George, 2018). Furthermore, the spike-and-slab quantile LASSO has a computational advantage to locate the posterior modes via soft-thresholding rule guided Expectation-Maximization (EM) steps in the coordinate descent framework, a phenomenon rarely observed for robust regularization with non-differentiable loss functions. We have conducted comprehensive simulation studies with a variety of heavy-tailed errors in both homogeneous and heterogeneous model settings to demonstrate the superiority of the spike-and-slab quantile LASSO over its competing methods. The advantage of the proposed method has been further demonstrated in case studies of the lung adenocarcinomas (LUAD) and skin cutaneous melanoma (SKCM) data from The Cancer Genome Atlas (TCGA).</summary></entry><entry><title type="html">Tuning parameter selection for the adaptive nuclear norm regularized trace regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Tuningparameterselectionfortheadaptivenuclearnormregularizedtraceregression.html" rel="alternate" type="text/html" title="Tuning parameter selection for the adaptive nuclear norm regularized trace regression" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Tuningparameterselectionfortheadaptivenuclearnormregularizedtraceregression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/Tuningparameterselectionfortheadaptivenuclearnormregularizedtraceregression.html">&lt;p&gt;Regularized models have been applied in lots of areas, with high-dimensional data sets being popular. Because tuning parameter decides the theoretical performance and computational efficiency of the regularized models, tuning parameter selection is a basic and important issue. We consider the tuning parameter selection for adaptive nuclear norm regularized trace regression, which achieves by the Bayesian information criterion (BIC). The proposed BIC is established with the help of an unbiased estimator of degrees of freedom. Under some regularized conditions, this BIC is proved to achieve the rank consistency of the tuning parameter selection. That is the model solution under selected tuning parameter converges to the true solution and has the same rank with that of the true solution in probability. Some numerical results are presented to evaluate the performance of the proposed BIC on tuning parameter selection.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06889&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Pan Shang, Lingchen Kong, Yiting Ma</name></author><category term="stat.ME" /><summary type="html">Regularized models have been applied in lots of areas, with high-dimensional data sets being popular. Because tuning parameter decides the theoretical performance and computational efficiency of the regularized models, tuning parameter selection is a basic and important issue. We consider the tuning parameter selection for adaptive nuclear norm regularized trace regression, which achieves by the Bayesian information criterion (BIC). The proposed BIC is established with the help of an unbiased estimator of degrees of freedom. Under some regularized conditions, this BIC is proved to achieve the rank consistency of the tuning parameter selection. That is the model solution under selected tuning parameter converges to the true solution and has the same rank with that of the true solution in probability. Some numerical results are presented to evaluate the performance of the proposed BIC on tuning parameter selection.</summary></entry><entry><title type="html">spAbundance: An R package for single-species and multi-species spatially explicit abundance models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/spAbundanceAnRpackageforsinglespeciesandmultispeciesspatiallyexplicitabundancemodels.html" rel="alternate" type="text/html" title="spAbundance: An R package for single-species and multi-species spatially explicit abundance models" /><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/spAbundanceAnRpackageforsinglespeciesandmultispeciesspatiallyexplicitabundancemodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/14/spAbundanceAnRpackageforsinglespeciesandmultispeciesspatiallyexplicitabundancemodels.html">&lt;p&gt;Numerous modeling techniques exist to estimate abundance of plant and wildlife species. These methods seek to estimate abundance while accounting for multiple complexities found in ecological data, such as observational biases, spatial autocorrelation, and species correlations. There is, however, a lack of user-friendly and computationally efficient software to implement the various models, particularly for large data sets. We developed the spAbundance R package for fitting spatially-explicit Bayesian single-species and multi-species hierarchical distance sampling models, N-mixture models, and generalized linear mixed models. The models within the package can account for spatial autocorrelation using Nearest Neighbor Gaussian Processes and accommodate species correlations in multi-species models using a latent factor approach, which enables model fitting for data sets with large numbers of sites and/or species. We provide three vignettes and three case studies that highlight spAbundance functionality. We used spatially-explicit multi-species distance sampling models to estimate density of 16 bird species in Florida, USA, an N-mixture model to estimate Black-throated Blue Warbler (Setophaga caerulescens) abundance in New Hampshire, USA, and a spatial linear mixed model to estimate forest aboveground biomass across the continental USA. spAbundance provides a user-friendly, formula-based interface to fit a variety of univariate and multivariate spatially-explicit abundance models. The package serves as a useful tool for ecologists and conservation practitioners to generate improved inference and predictions on the spatial drivers of populations and communities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.19446&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jeffrey W. Doser, Andrew O. Finley, Marc Kéry, Elise F. Zipkin</name></author><category term="stat.AP" /><summary type="html">Numerous modeling techniques exist to estimate abundance of plant and wildlife species. These methods seek to estimate abundance while accounting for multiple complexities found in ecological data, such as observational biases, spatial autocorrelation, and species correlations. There is, however, a lack of user-friendly and computationally efficient software to implement the various models, particularly for large data sets. We developed the spAbundance R package for fitting spatially-explicit Bayesian single-species and multi-species hierarchical distance sampling models, N-mixture models, and generalized linear mixed models. The models within the package can account for spatial autocorrelation using Nearest Neighbor Gaussian Processes and accommodate species correlations in multi-species models using a latent factor approach, which enables model fitting for data sets with large numbers of sites and/or species. We provide three vignettes and three case studies that highlight spAbundance functionality. We used spatially-explicit multi-species distance sampling models to estimate density of 16 bird species in Florida, USA, an N-mixture model to estimate Black-throated Blue Warbler (Setophaga caerulescens) abundance in New Hampshire, USA, and a spatial linear mixed model to estimate forest aboveground biomass across the continental USA. spAbundance provides a user-friendly, formula-based interface to fit a variety of univariate and multivariate spatially-explicit abundance models. The package serves as a useful tool for ecologists and conservation practitioners to generate improved inference and predictions on the spatial drivers of populations and communities.</summary></entry></feed>