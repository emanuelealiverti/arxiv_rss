<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-31T07:13:56+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Multinomial Canonical Decomposition Model, with emphasis on the analysis of Multivariate Binary data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/AMultinomialCanonicalDecompositionModelwithemphasisontheanalysisofMultivariateBinarydata.html" rel="alternate" type="text/html" title="A Multinomial Canonical Decomposition Model, with emphasis on the analysis of Multivariate Binary data" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/AMultinomialCanonicalDecompositionModelwithemphasisontheanalysisofMultivariateBinarydata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/AMultinomialCanonicalDecompositionModelwithemphasisontheanalysisofMultivariateBinarydata.html">&lt;p&gt;In this paper, we propose to decompose the canonical parameter of a multinomial model into a set of participant scores and category scores. Both sets of scores are linearly constraint to represent external information about the participants and categories. For the estimation of the parameters of the decomposition, we derive a majorization-minimization algorithm. We place special emphasis on the case where the categories represent profiles of binary response variables. In that case, the multinomial model becomes a regression model for multiple binary response variables and researchers might be interested in the relationship of an external variable for the participant (i.e., a predictor) and one of the binary response variable or in the relationship between this predictor and the association among binary response variables. We derive interpretational rules for these relationships in terms of changes in log odds or log odds ratios. Connections between our multinomial canonical decomposition and loglinear models, multinomial logistic regression, multinomial reduced rank logistic regression, and double constrained correspondence analysis are discussed. We illustrate our methodology with two empirical data sets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.07634&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mark de Rooij</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">In this paper, we propose to decompose the canonical parameter of a multinomial model into a set of participant scores and category scores. Both sets of scores are linearly constraint to represent external information about the participants and categories. For the estimation of the parameters of the decomposition, we derive a majorization-minimization algorithm. We place special emphasis on the case where the categories represent profiles of binary response variables. In that case, the multinomial model becomes a regression model for multiple binary response variables and researchers might be interested in the relationship of an external variable for the participant (i.e., a predictor) and one of the binary response variable or in the relationship between this predictor and the association among binary response variables. We derive interpretational rules for these relationships in terms of changes in log odds or log odds ratios. Connections between our multinomial canonical decomposition and loglinear models, multinomial logistic regression, multinomial reduced rank logistic regression, and double constrained correspondence analysis are discussed. We illustrate our methodology with two empirical data sets.</summary></entry><entry><title type="html">A Unification of Exchangeability and Continuous Exposure and Confounder Measurement Errors: Probabilistic Exchangeability</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/AUnificationofExchangeabilityandContinuousExposureandConfounderMeasurementErrorsProbabilisticExchangeability.html" rel="alternate" type="text/html" title="A Unification of Exchangeability and Continuous Exposure and Confounder Measurement Errors: Probabilistic Exchangeability" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/AUnificationofExchangeabilityandContinuousExposureandConfounderMeasurementErrorsProbabilisticExchangeability</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/AUnificationofExchangeabilityandContinuousExposureandConfounderMeasurementErrorsProbabilisticExchangeability.html">&lt;p&gt;Exchangeability concerning a continuous exposure, X, implies no confounding bias when identifying average exposure effects of X, AEE(X). When X is measured with error (Xep), two challenges arise in identifying AEE(X). Firstly, exchangeability regarding Xep does not equal exchangeability regarding X. Secondly, the non-differential error assumption (NDEA) could be overly stringent in practice. To address them, this article proposes unifying exchangeability and exposure and confounder measurement errors with three novel concepts. The first, Probabilistic Exchangeability (PE), states that the outcomes of those with Xep=e are probabilistically exchangeable with the outcomes of those truly exposed to X=eT. The relationship between AEE(Xep) and AEE(X) in risk difference and ratio scales is mathematically expressed as a probabilistic certainty, termed exchangeability probability (Pe). Squared Pe (Pe2) quantifies the extent to which AEE(Xep) differs from AEE(X) due to exposure measurement error through mechanisms not akin to confounding mechanisms. The coefficient of determination (R2) in the regression of Xep against X may sometimes be sufficient to measure Pe2. The second concept, Emergent Pseudo Confounding (EPC), describes the bias introduced by exposure measurement error through mechanisms akin to confounding mechanisms. PE requires controlling for EPC, which is weaker than NDEA. The third, Emergent Confounding, describes when bias due to confounder measurement error arises. Adjustment for E(P)C can be performed like confounding adjustment. This paper provides maximum insight into when AEE(Xep) is an appropriate surrogate of AEE(X) and how to measure the difference between these two. Differential errors could be addressed and may not compromise causal inference.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07910&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Honghyok Kim</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Exchangeability concerning a continuous exposure, X, implies no confounding bias when identifying average exposure effects of X, AEE(X). When X is measured with error (Xep), two challenges arise in identifying AEE(X). Firstly, exchangeability regarding Xep does not equal exchangeability regarding X. Secondly, the non-differential error assumption (NDEA) could be overly stringent in practice. To address them, this article proposes unifying exchangeability and exposure and confounder measurement errors with three novel concepts. The first, Probabilistic Exchangeability (PE), states that the outcomes of those with Xep=e are probabilistically exchangeable with the outcomes of those truly exposed to X=eT. The relationship between AEE(Xep) and AEE(X) in risk difference and ratio scales is mathematically expressed as a probabilistic certainty, termed exchangeability probability (Pe). Squared Pe (Pe2) quantifies the extent to which AEE(Xep) differs from AEE(X) due to exposure measurement error through mechanisms not akin to confounding mechanisms. The coefficient of determination (R2) in the regression of Xep against X may sometimes be sufficient to measure Pe2. The second concept, Emergent Pseudo Confounding (EPC), describes the bias introduced by exposure measurement error through mechanisms akin to confounding mechanisms. PE requires controlling for EPC, which is weaker than NDEA. The third, Emergent Confounding, describes when bias due to confounder measurement error arises. Adjustment for E(P)C can be performed like confounding adjustment. This paper provides maximum insight into when AEE(Xep) is an appropriate surrogate of AEE(X) and how to measure the difference between these two. Differential errors could be addressed and may not compromise causal inference.</summary></entry><entry><title type="html">Accounting for Mismatch Error in Small Area Estimation with Linked Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/AccountingforMismatchErrorinSmallAreaEstimationwithLinkedData.html" rel="alternate" type="text/html" title="Accounting for Mismatch Error in Small Area Estimation with Linked Data" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/AccountingforMismatchErrorinSmallAreaEstimationwithLinkedData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/AccountingforMismatchErrorinSmallAreaEstimationwithLinkedData.html">&lt;p&gt;In small area estimation different data sources are integrated in order to produce reliable estimates of target parameters (e.g., a mean or a proportion) for a collection of small subsets (areas) of a finite population. Regression models such as the linear mixed effects model or M-quantile regression are often used to improve the precision of survey sample estimates by leveraging auxiliary information for which means or totals are known at the area level. In many applications, the unit-level linkage of records from different sources is probabilistic and potentially error-prone. In this paper, we present adjustments of the small area predictors that are based on either the linear mixed effects model or M-quantile regression to account for the presence of linkage error. These adjustments are developed from a two-component mixture model that hinges on the assumption of independence of the target and auxiliary variable given incorrect linkage. Estimation and inference is based on composite likelihoods and machinery revolving around the Expectation-Maximization Algorithm. For each of the two regression methods, we propose modified small area predictors and approximations for their mean squared errors. The empirical performance of the proposed approaches is studied in both design-based and model-based simulations that include comparisons to a variety of baselines.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20149&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Enrico Fabrizi, Nicola Salvati, Martin Slawski</name></author><category term="stat.ME" /><summary type="html">In small area estimation different data sources are integrated in order to produce reliable estimates of target parameters (e.g., a mean or a proportion) for a collection of small subsets (areas) of a finite population. Regression models such as the linear mixed effects model or M-quantile regression are often used to improve the precision of survey sample estimates by leveraging auxiliary information for which means or totals are known at the area level. In many applications, the unit-level linkage of records from different sources is probabilistic and potentially error-prone. In this paper, we present adjustments of the small area predictors that are based on either the linear mixed effects model or M-quantile regression to account for the presence of linkage error. These adjustments are developed from a two-component mixture model that hinges on the assumption of independence of the target and auxiliary variable given incorrect linkage. Estimation and inference is based on composite likelihoods and machinery revolving around the Expectation-Maximization Algorithm. For each of the two regression methods, we propose modified small area predictors and approximations for their mean squared errors. The empirical performance of the proposed approaches is studied in both design-based and model-based simulations that include comparisons to a variety of baselines.</summary></entry><entry><title type="html">Algebraic and Statistical Properties of the Ordinary Least Squares Interpolator</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/AlgebraicandStatisticalPropertiesoftheOrdinaryLeastSquaresInterpolator.html" rel="alternate" type="text/html" title="Algebraic and Statistical Properties of the Ordinary Least Squares Interpolator" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/AlgebraicandStatisticalPropertiesoftheOrdinaryLeastSquaresInterpolator</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/AlgebraicandStatisticalPropertiesoftheOrdinaryLeastSquaresInterpolator.html">&lt;p&gt;Deep learning research has uncovered the phenomenon of benign overfitting for overparameterized statistical models, which has drawn significant theoretical interest in recent years. Given its simplicity and practicality, the ordinary least squares (OLS) interpolator has become essential to gain foundational insights into this phenomenon. While properties of OLS are well established in classical, underparameterized settings, its behavior in high-dimensional, overparameterized regimes is less explored (unlike for ridge or lasso regression) though significant progress has been made of late. We contribute to this growing literature by providing fundamental algebraic and statistical results for the minimum $\ell_2$-norm OLS interpolator. In particular, we provide algebraic equivalents of (i) the leave-$k$-out residual formula, (ii) Cochran’s formula, and (iii) the Frisch-Waugh-Lovell theorem in the overparameterized regime. These results aid in understanding the OLS interpolator’s ability to generalize and have substantive implications for causal inference. Under the Gauss-Markov model, we present statistical results such as an extension of the Gauss-Markov theorem and an analysis of variance estimation under homoskedastic errors for the overparameterized regime. To substantiate our theoretical contributions, we conduct simulations that further explore the stochastic properties of the OLS interpolator.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.15769&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Dennis Shen, Dogyoon Song, Peng Ding, Jasjeet S. Sekhon</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Deep learning research has uncovered the phenomenon of benign overfitting for overparameterized statistical models, which has drawn significant theoretical interest in recent years. Given its simplicity and practicality, the ordinary least squares (OLS) interpolator has become essential to gain foundational insights into this phenomenon. While properties of OLS are well established in classical, underparameterized settings, its behavior in high-dimensional, overparameterized regimes is less explored (unlike for ridge or lasso regression) though significant progress has been made of late. We contribute to this growing literature by providing fundamental algebraic and statistical results for the minimum $\ell_2$-norm OLS interpolator. In particular, we provide algebraic equivalents of (i) the leave-$k$-out residual formula, (ii) Cochran’s formula, and (iii) the Frisch-Waugh-Lovell theorem in the overparameterized regime. These results aid in understanding the OLS interpolator’s ability to generalize and have substantive implications for causal inference. Under the Gauss-Markov model, we present statistical results such as an extension of the Gauss-Markov theorem and an analysis of variance estimation under homoskedastic errors for the overparameterized regime. To substantiate our theoretical contributions, we conduct simulations that further explore the stochastic properties of the OLS interpolator.</summary></entry><entry><title type="html">Algorithm-agnostic significance testing in supervised learning with multimodal data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Algorithmagnosticsignificancetestinginsupervisedlearningwithmultimodaldata.html" rel="alternate" type="text/html" title="Algorithm-agnostic significance testing in supervised learning with multimodal data" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Algorithmagnosticsignificancetestinginsupervisedlearningwithmultimodaldata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Algorithmagnosticsignificancetestinginsupervisedlearningwithmultimodaldata.html">&lt;p&gt;Valid statistical inference is crucial for decision-making but difficult to obtain in supervised learning with multimodal data, e.g., combinations of clinical features, genomic data, and medical images. Multimodal data often warrants the use of black-box algorithms, for instance, random forests or neural networks, which impede the use of traditional variable significance tests. We address this problem by proposing the use of COvariance Measure Tests (COMETs), which are calibrated and powerful tests that can be combined with any sufficiently predictive supervised learning algorithm. We apply COMETs to several high-dimensional, multimodal data sets to illustrate (i) variable significance testing for finding relevant mutations modulating drug-activity, (ii) modality selection for predicting survival in liver cancer patients with multiomics data, and (iii) modality selection with clinical features and medical imaging data. In all applications, COMETs yield results consistent with domain knowledge without requiring data-driven pre-processing which may invalidate type I error control. These novel applications with high-dimensional multimodal data corroborate prior results on the power and robustness of COMETs for significance testing. COMETs are implemented in the comets R package available on CRAN and pycomets Python library available on GitHub. Source code for reproducing all results is available at https://github.com/LucasKook/comets. All data sets used in this work are openly available.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.14416&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lucas Kook, Anton Rask Lundborg</name></author><category term="stat.AP" /><summary type="html">Valid statistical inference is crucial for decision-making but difficult to obtain in supervised learning with multimodal data, e.g., combinations of clinical features, genomic data, and medical images. Multimodal data often warrants the use of black-box algorithms, for instance, random forests or neural networks, which impede the use of traditional variable significance tests. We address this problem by proposing the use of COvariance Measure Tests (COMETs), which are calibrated and powerful tests that can be combined with any sufficiently predictive supervised learning algorithm. We apply COMETs to several high-dimensional, multimodal data sets to illustrate (i) variable significance testing for finding relevant mutations modulating drug-activity, (ii) modality selection for predicting survival in liver cancer patients with multiomics data, and (iii) modality selection with clinical features and medical imaging data. In all applications, COMETs yield results consistent with domain knowledge without requiring data-driven pre-processing which may invalidate type I error control. These novel applications with high-dimensional multimodal data corroborate prior results on the power and robustness of COMETs for significance testing. COMETs are implemented in the comets R package available on CRAN and pycomets Python library available on GitHub. Source code for reproducing all results is available at https://github.com/LucasKook/comets. All data sets used in this work are openly available.</summary></entry><entry><title type="html">A unified framework of principal component analysis and factor analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Aunifiedframeworkofprincipalcomponentanalysisandfactoranalysis.html" rel="alternate" type="text/html" title="A unified framework of principal component analysis and factor analysis" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Aunifiedframeworkofprincipalcomponentanalysisandfactoranalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Aunifiedframeworkofprincipalcomponentanalysisandfactoranalysis.html">&lt;p&gt;Principal component analysis and factor analysis are fundamental multivariate analysis methods. In this paper a unified framework to connect them is introduced. Under a general latent variable model, we present matrix optimization problems from the viewpoint of loss function minimization, and show that the two methods can be viewed as solutions to the optimization problems with specific loss functions. Specifically, principal component analysis can be derived from a broad class of loss functions including the L2 norm, while factor analysis corresponds to a modified L0 norm problem. Related problems are discussed, including algorithms, penalized maximum likelihood estimation under the latent variable model, and a principal component factor model. These results can lead to new tools of data analysis and research topics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20137&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shifeng Xiong</name></author><category term="stat.ME" /><summary type="html">Principal component analysis and factor analysis are fundamental multivariate analysis methods. In this paper a unified framework to connect them is introduced. Under a general latent variable model, we present matrix optimization problems from the viewpoint of loss function minimization, and show that the two methods can be viewed as solutions to the optimization problems with specific loss functions. Specifically, principal component analysis can be derived from a broad class of loss functions including the L2 norm, while factor analysis corresponds to a modified L0 norm problem. Related problems are discussed, including algorithms, penalized maximum likelihood estimation under the latent variable model, and a principal component factor model. These results can lead to new tools of data analysis and research topics.</summary></entry><entry><title type="html">Bayesian Joint Modeling for Longitudinal Magnitude Data with Informative Dropout: an Application to Critical Care Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/BayesianJointModelingforLongitudinalMagnitudeDatawithInformativeDropoutanApplicationtoCriticalCareData.html" rel="alternate" type="text/html" title="Bayesian Joint Modeling for Longitudinal Magnitude Data with Informative Dropout: an Application to Critical Care Data" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/BayesianJointModelingforLongitudinalMagnitudeDatawithInformativeDropoutanApplicationtoCriticalCareData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/BayesianJointModelingforLongitudinalMagnitudeDatawithInformativeDropoutanApplicationtoCriticalCareData.html">&lt;p&gt;In various biomedical studies, the focus of analysis centers on the magnitudes of data, particularly when algebraic signs are irrelevant or lost. To analyze the magnitude outcomes in repeated measures studies, using models with random effects is essential. This is because random effects can account for individual heterogeneity, enhancing parameter estimation precision. However, there are currently no established regression methods that incorporate random effects and are specifically designed for magnitude outcomes. This article bridges this gap by introducing Bayesian regression modeling approaches for analyzing magnitude data, with a key focus on the incorporation of random effects. Additionally, the proposed method is extended to address multiple causes of informative dropout, commonly encountered in repeated measures studies. To tackle the missing data challenge arising from dropout, a joint modeling strategy is developed, building upon the previously introduced regression techniques. Two numerical simulation studies are conducted to assess the validity of our method. The chosen simulation scenarios aim to resemble the conditions of our motivating study. The results demonstrate that the proposed method for magnitude data exhibits good performance in terms of both estimation accuracy and precision, and the joint models effectively mitigate bias due to missing data. Finally, we apply proposed models to analyze the magnitude data from the motivating study, investigating if sex impacts the magnitude change in diaphragm thickness over time for ICU patients.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19666&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wen Teng, Niall D. Ferguson, Ewan C. Goligher, Anna Heath</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In various biomedical studies, the focus of analysis centers on the magnitudes of data, particularly when algebraic signs are irrelevant or lost. To analyze the magnitude outcomes in repeated measures studies, using models with random effects is essential. This is because random effects can account for individual heterogeneity, enhancing parameter estimation precision. However, there are currently no established regression methods that incorporate random effects and are specifically designed for magnitude outcomes. This article bridges this gap by introducing Bayesian regression modeling approaches for analyzing magnitude data, with a key focus on the incorporation of random effects. Additionally, the proposed method is extended to address multiple causes of informative dropout, commonly encountered in repeated measures studies. To tackle the missing data challenge arising from dropout, a joint modeling strategy is developed, building upon the previously introduced regression techniques. Two numerical simulation studies are conducted to assess the validity of our method. The chosen simulation scenarios aim to resemble the conditions of our motivating study. The results demonstrate that the proposed method for magnitude data exhibits good performance in terms of both estimation accuracy and precision, and the joint models effectively mitigate bias due to missing data. Finally, we apply proposed models to analyze the magnitude data from the motivating study, investigating if sex impacts the magnitude change in diaphragm thickness over time for ICU patients.</summary></entry><entry><title type="html">Bayesian Online Natural Gradient (BONG)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/BayesianOnlineNaturalGradientBONG.html" rel="alternate" type="text/html" title="Bayesian Online Natural Gradient (BONG)" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/BayesianOnlineNaturalGradientBONG</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/BayesianOnlineNaturalGradientBONG.html">&lt;p&gt;We propose a novel approach to sequential Bayesian inference based on variational Bayes. The key insight is that, in the online setting, we do not need to add the KL term to regularize to the prior (which comes from the posterior at the previous timestep); instead we can optimize just the expected log-likelihood, performing a single step of natural gradient descent starting at the prior predictive. We prove this method recovers exact Bayesian inference if the model is conjugate, and empirically outperforms other online VB methods in the non-conjugate setting, such as online learning for neural networks, especially when controlling for computational costs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19681&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matt Jones, Peter Chang, Kevin Murphy</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">We propose a novel approach to sequential Bayesian inference based on variational Bayes. The key insight is that, in the online setting, we do not need to add the KL term to regularize to the prior (which comes from the posterior at the previous timestep); instead we can optimize just the expected log-likelihood, performing a single step of natural gradient descent starting at the prior predictive. We prove this method recovers exact Bayesian inference if the model is conjugate, and empirically outperforms other online VB methods in the non-conjugate setting, such as online learning for neural networks, especially when controlling for computational costs.</summary></entry><entry><title type="html">CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/CaRiNGLearningTemporalCausalRepresentationunderNonInvertibleGenerationProcess.html" rel="alternate" type="text/html" title="CaRiNG: Learning Temporal Causal Representation under Non-Invertible Generation Process" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/CaRiNGLearningTemporalCausalRepresentationunderNonInvertibleGenerationProcess</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/CaRiNGLearningTemporalCausalRepresentationunderNonInvertibleGenerationProcess.html">&lt;p&gt;Identifying the underlying time-delayed latent causal processes in sequential data is vital for grasping temporal dynamics and making downstream reasoning. While some recent methods can robustly identify these latent causal variables, they rely on strict assumptions about the invertible generation process from latent variables to observed data. However, these assumptions are often hard to satisfy in real-world applications containing information loss. For instance, the visual perception process translates a 3D space into 2D images, or the phenomenon of persistence of vision incorporates historical data into current perceptions. To address this challenge, we establish an identifiability theory that allows for the recovery of independent latent components even when they come from a nonlinear and non-invertible mix. Using this theory as a foundation, we propose a principled approach, CaRiNG, to learn the CAusal RepresentatIon of Non-invertible Generative temporal data with identifiability guarantees. Specifically, we utilize temporal context to recover lost latent information and apply the conditions in our theory to guide the training process. Through experiments conducted on synthetic datasets, we validate that our CaRiNG method reliably identifies the causal process, even when the generation process is non-invertible. Moreover, we demonstrate that our approach considerably improves temporal understanding and reasoning in practical applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.14535&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Guangyi Chen, Yifan Shen, Zhenhao Chen, Xiangchen Song, Yuewen Sun, Weiran Yao, Xiao Liu, Kun Zhang</name></author><category term="stat.ME" /><summary type="html">Identifying the underlying time-delayed latent causal processes in sequential data is vital for grasping temporal dynamics and making downstream reasoning. While some recent methods can robustly identify these latent causal variables, they rely on strict assumptions about the invertible generation process from latent variables to observed data. However, these assumptions are often hard to satisfy in real-world applications containing information loss. For instance, the visual perception process translates a 3D space into 2D images, or the phenomenon of persistence of vision incorporates historical data into current perceptions. To address this challenge, we establish an identifiability theory that allows for the recovery of independent latent components even when they come from a nonlinear and non-invertible mix. Using this theory as a foundation, we propose a principled approach, CaRiNG, to learn the CAusal RepresentatIon of Non-invertible Generative temporal data with identifiability guarantees. Specifically, we utilize temporal context to recover lost latent information and apply the conditions in our theory to guide the training process. Through experiments conducted on synthetic datasets, we validate that our CaRiNG method reliably identifies the causal process, even when the generation process is non-invertible. Moreover, we demonstrate that our approach considerably improves temporal understanding and reasoning in practical applications.</summary></entry><entry><title type="html">Canonical Correlation Analysis as Reduced Rank Regression in High Dimensions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/CanonicalCorrelationAnalysisasReducedRankRegressioninHighDimensions.html" rel="alternate" type="text/html" title="Canonical Correlation Analysis as Reduced Rank Regression in High Dimensions" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/CanonicalCorrelationAnalysisasReducedRankRegressioninHighDimensions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/CanonicalCorrelationAnalysisasReducedRankRegressioninHighDimensions.html">&lt;p&gt;Canonical Correlation Analysis (CCA) is a widespread technique for discovering linear relationships between two sets of variables $X \in \mathbb{R}^{n \times p}$ and $Y \in \mathbb{R}^{n \times q}$. In high dimensions however, standard estimates of the canonical directions cease to be consistent without assuming further structure. In this setting, a possible solution consists in leveraging the presumed sparsity of the solution: only a subset of the covariates span the canonical directions. While the last decade has seen a proliferation of sparse CCA methods, practical challenges regarding the scalability and adaptability of these methods still persist. To circumvent these issues, this paper suggests an alternative strategy that uses reduced rank regression to estimate the canonical directions when one of the datasets is high-dimensional while the other remains low-dimensional. By casting the problem of estimating the canonical direction as a regression problem, our estimator is able to leverage the rich statistics literature on high-dimensional regression and is easily adaptable to accommodate a wider range of structural priors. Our proposed solution maintains computational efficiency and accuracy, even in the presence of very high-dimensional data. We validate the benefits of our approach through a series of simulated experiments and further illustrate its practicality by applying it to three real-world datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19539&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Claire Donnat, Elena Tuzhilina</name></author><category term="stat.ME" /><summary type="html">Canonical Correlation Analysis (CCA) is a widespread technique for discovering linear relationships between two sets of variables $X \in \mathbb{R}^{n \times p}$ and $Y \in \mathbb{R}^{n \times q}$. In high dimensions however, standard estimates of the canonical directions cease to be consistent without assuming further structure. In this setting, a possible solution consists in leveraging the presumed sparsity of the solution: only a subset of the covariates span the canonical directions. While the last decade has seen a proliferation of sparse CCA methods, practical challenges regarding the scalability and adaptability of these methods still persist. To circumvent these issues, this paper suggests an alternative strategy that uses reduced rank regression to estimate the canonical directions when one of the datasets is high-dimensional while the other remains low-dimensional. By casting the problem of estimating the canonical direction as a regression problem, our estimator is able to leverage the rich statistics literature on high-dimensional regression and is easily adaptable to accommodate a wider range of structural priors. Our proposed solution maintains computational efficiency and accuracy, even in the presence of very high-dimensional data. We validate the benefits of our approach through a series of simulated experiments and further illustrate its practicality by applying it to three real-world datasets.</summary></entry><entry><title type="html">Comparison of Point Process Learning and its special case Takacs-Fiksel estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ComparisonofPointProcessLearninganditsspecialcaseTakacsFikselestimation.html" rel="alternate" type="text/html" title="Comparison of Point Process Learning and its special case Takacs-Fiksel estimation" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ComparisonofPointProcessLearninganditsspecialcaseTakacsFikselestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ComparisonofPointProcessLearninganditsspecialcaseTakacsFikselestimation.html">&lt;p&gt;Recently, Cronie et al. (2024) introduced the notion of cross-validation for point processes and a new statistical methodology called Point Process Learning (PPL). In PPL one splits a point process/pattern into a training and a validation set, and then predicts the latter from the former through a parametrised Papangelou conditional intensity. The model parameters are estimated by minimizing a point process prediction error; this notion was introduced as the second building block of PPL. It was shown that PPL outperforms the state-of-the-art in both kernel intensity estimation and estimation of the parameters of the Gibbs hard-core process. In the latter case, the state-of-the-art was represented by pseudolikelihood estimation. In this paper we study PPL in relation to Takacs-Fiksel estimation, of which pseudolikelihood is a special case. We show that Takacs-Fiksel estimation is a special case of PPL in the sense that PPL with a specific loss function asymptotically reduces to Takacs-Fiksel estimation if we let the cross-validation regime tend to leave-one-out cross-validation. Moreover, PPL involves a certain type of hyperparameter given by a weight function which ensures that the prediction errors have expectation zero if and only if we have the correct parametrisation. We show that the weight function takes an explicit but intractable form for general Gibbs models. Consequently, we propose different approaches to estimate the weight function in practice. In order to assess how the general PPL setup performs in relation to its special case Takacs-Fiksel estimation, we conduct a simulation study where we find that for common Gibbs models we can find loss functions and hyperparameters so that PPL typically outperforms Takacs-Fiksel estimation significantly in terms of mean square error. Here, the hyperparameters are the cross-validation parameters and the weight function estimate.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19523&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Julia Jansson, Ottmar Cronie</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Recently, Cronie et al. (2024) introduced the notion of cross-validation for point processes and a new statistical methodology called Point Process Learning (PPL). In PPL one splits a point process/pattern into a training and a validation set, and then predicts the latter from the former through a parametrised Papangelou conditional intensity. The model parameters are estimated by minimizing a point process prediction error; this notion was introduced as the second building block of PPL. It was shown that PPL outperforms the state-of-the-art in both kernel intensity estimation and estimation of the parameters of the Gibbs hard-core process. In the latter case, the state-of-the-art was represented by pseudolikelihood estimation. In this paper we study PPL in relation to Takacs-Fiksel estimation, of which pseudolikelihood is a special case. We show that Takacs-Fiksel estimation is a special case of PPL in the sense that PPL with a specific loss function asymptotically reduces to Takacs-Fiksel estimation if we let the cross-validation regime tend to leave-one-out cross-validation. Moreover, PPL involves a certain type of hyperparameter given by a weight function which ensures that the prediction errors have expectation zero if and only if we have the correct parametrisation. We show that the weight function takes an explicit but intractable form for general Gibbs models. Consequently, we propose different approaches to estimate the weight function in practice. In order to assess how the general PPL setup performs in relation to its special case Takacs-Fiksel estimation, we conduct a simulation study where we find that for common Gibbs models we can find loss functions and hyperparameters so that PPL typically outperforms Takacs-Fiksel estimation significantly in terms of mean square error. Here, the hyperparameters are the cross-validation parameters and the weight function estimate.</summary></entry><entry><title type="html">Continuously Optimizing Radar Placement with Model Predictive Path Integrals</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ContinuouslyOptimizingRadarPlacementwithModelPredictivePathIntegrals.html" rel="alternate" type="text/html" title="Continuously Optimizing Radar Placement with Model Predictive Path Integrals" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ContinuouslyOptimizingRadarPlacementwithModelPredictivePathIntegrals</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ContinuouslyOptimizingRadarPlacementwithModelPredictivePathIntegrals.html">&lt;p&gt;Continuously optimizing sensor placement is essential for precise target localization in various military and civilian applications. While information theory has shown promise in optimizing sensor placement, many studies oversimplify sensor measurement models or neglect dynamic constraints of mobile sensors. To address these challenges, we employ a range measurement model that incorporates radar parameters and radar-target distance, coupled with Model Predictive Path Integral (MPPI) control to manage complex environmental obstacles and dynamic constraints. We compare the proposed approach against stationary radars or simplified range measurement models based on the root mean squared error (RMSE) of the Cubature Kalman Filter (CKF) estimator for the targets’ state. Additionally, we visualize the evolving geometry of radars and targets over time, highlighting areas of highest measurement information gain, demonstrating the strengths of the approach. The proposed strategy outperforms stationary radars and simplified range measurement models in target localization, achieving a 38-74% reduction in mean RMSE and a 33-79% reduction in the upper tail of the 90% Highest Density Interval (HDI) over 500 Monte Carl (MC) trials across all time steps.
  Code will be made publicly available upon acceptance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18999&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael Potter, Shuo Tang, Paul Ghanem, Milica Stojanovic, Pau Closas, Murat Akcakaya, Ben Wright, Marius Necsoiu, Deniz Erdogmus, Michael Everett, Tales Imbiriba</name></author><category term="stat.AP" /><summary type="html">Continuously optimizing sensor placement is essential for precise target localization in various military and civilian applications. While information theory has shown promise in optimizing sensor placement, many studies oversimplify sensor measurement models or neglect dynamic constraints of mobile sensors. To address these challenges, we employ a range measurement model that incorporates radar parameters and radar-target distance, coupled with Model Predictive Path Integral (MPPI) control to manage complex environmental obstacles and dynamic constraints. We compare the proposed approach against stationary radars or simplified range measurement models based on the root mean squared error (RMSE) of the Cubature Kalman Filter (CKF) estimator for the targets’ state. Additionally, we visualize the evolving geometry of radars and targets over time, highlighting areas of highest measurement information gain, demonstrating the strengths of the approach. The proposed strategy outperforms stationary radars and simplified range measurement models in target localization, achieving a 38-74% reduction in mean RMSE and a 33-79% reduction in the upper tail of the 90% Highest Density Interval (HDI) over 500 Monte Carl (MC) trials across all time steps. Code will be made publicly available upon acceptance.</summary></entry><entry><title type="html">Dynamic Factor Analysis of High-dimensional Recurrent Events</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/DynamicFactorAnalysisofHighdimensionalRecurrentEvents.html" rel="alternate" type="text/html" title="Dynamic Factor Analysis of High-dimensional Recurrent Events" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/DynamicFactorAnalysisofHighdimensionalRecurrentEvents</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/DynamicFactorAnalysisofHighdimensionalRecurrentEvents.html">&lt;p&gt;Recurrent event time data arise in many studies, including biomedicine, public health, marketing, and social media analysis. High-dimensional recurrent event data involving large numbers of event types and observations become prevalent with the advances in information technology. This paper proposes a semiparametric dynamic factor model for the dimension reduction and prediction of high-dimensional recurrent event data. The proposed model imposes a low-dimensional structure on the mean intensity functions of the event types while allowing for dependencies. A nearly rate-optimal smoothing-based estimator is proposed. An information criterion that consistently selects the number of factors is also developed. Simulation studies demonstrate the effectiveness of these inference tools. The proposed method is applied to grocery shopping data, for which an interpretable factor structure is obtained.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19803&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fangyi Chen, Yunxiao Chen, Zhiliang Ying, Kangjie Zhou</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Recurrent event time data arise in many studies, including biomedicine, public health, marketing, and social media analysis. High-dimensional recurrent event data involving large numbers of event types and observations become prevalent with the advances in information technology. This paper proposes a semiparametric dynamic factor model for the dimension reduction and prediction of high-dimensional recurrent event data. The proposed model imposes a low-dimensional structure on the mean intensity functions of the event types while allowing for dependencies. A nearly rate-optimal smoothing-based estimator is proposed. An information criterion that consistently selects the number of factors is also developed. Simulation studies demonstrate the effectiveness of these inference tools. The proposed method is applied to grocery shopping data, for which an interpretable factor structure is obtained.</summary></entry><entry><title type="html">Eclipse Attack Detection on a Blockchain Network as a Non-Parametric Change Detection Problem</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/EclipseAttackDetectiononaBlockchainNetworkasaNonParametricChangeDetectionProblem.html" rel="alternate" type="text/html" title="Eclipse Attack Detection on a Blockchain Network as a Non-Parametric Change Detection Problem" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/EclipseAttackDetectiononaBlockchainNetworkasaNonParametricChangeDetectionProblem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/EclipseAttackDetectiononaBlockchainNetworkasaNonParametricChangeDetectionProblem.html">&lt;p&gt;This paper introduces a novel non-parametric change detection algorithm to identify eclipse attacks on a blockchain network; the non-parametric algorithm relies only on the empirical mean and variance of the dataset, making it highly adaptable. An eclipse attack occurs when malicious actors isolate blockchain users, disrupting their ability to reach consensus with the broader network, thereby distorting their local copy of the ledger. To detect an eclipse attack, we monitor changes in the Fr&apos;echet mean and variance of the evolving blockchain communication network connecting blockchain users. First, we leverage the Johnson-Lindenstrauss lemma to project large-dimensional networks into a lower-dimensional space, preserving essential statistical properties. Subsequently, we employ a non-parametric change detection procedure, leading to a test statistic that converges weakly to a Brownian bridge process in the absence of an eclipse attack. This enables us to quantify the false alarm rate of the detector. Our detector can be implemented as a smart contract on the blockchain, offering a tamper-proof and reliable solution. Finally, we use numerical examples to compare the proposed eclipse attack detector with a detector based on the random forest model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.00538&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anurag Gupta, Vikram Krishnamurthy, Brian M. Sadler</name></author><category term="stat.AP" /><summary type="html">This paper introduces a novel non-parametric change detection algorithm to identify eclipse attacks on a blockchain network; the non-parametric algorithm relies only on the empirical mean and variance of the dataset, making it highly adaptable. An eclipse attack occurs when malicious actors isolate blockchain users, disrupting their ability to reach consensus with the broader network, thereby distorting their local copy of the ledger. To detect an eclipse attack, we monitor changes in the Fr&apos;echet mean and variance of the evolving blockchain communication network connecting blockchain users. First, we leverage the Johnson-Lindenstrauss lemma to project large-dimensional networks into a lower-dimensional space, preserving essential statistical properties. Subsequently, we employ a non-parametric change detection procedure, leading to a test statistic that converges weakly to a Brownian bridge process in the absence of an eclipse attack. This enables us to quantify the false alarm rate of the detector. Our detector can be implemented as a smart contract on the blockchain, offering a tamper-proof and reliable solution. Finally, we use numerical examples to compare the proposed eclipse attack detector with a detector based on the random forest model.</summary></entry><entry><title type="html">Elementary methods provide more replicable results in microbial differential abundance analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Elementarymethodsprovidemorereplicableresultsinmicrobialdifferentialabundanceanalysis.html" rel="alternate" type="text/html" title="Elementary methods provide more replicable results in microbial differential abundance analysis" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Elementarymethodsprovidemorereplicableresultsinmicrobialdifferentialabundanceanalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Elementarymethodsprovidemorereplicableresultsinmicrobialdifferentialabundanceanalysis.html">&lt;p&gt;Differential abundance analysis is a key component of microbiome studies. While dozens of methods for it exist, currently, there is no consensus on the preferred methods. Correctness of results in differential abundance analysis is an ambiguous concept that cannot be evaluated without employing simulated data, but we argue that consistency of results across datasets should be considered as an essential quality of a well-performing method.
  We compared the performance of 14 differential abundance analysis methods employing datasets from 54 taxonomic profiling studies based on 16S rRNA gene or shotgun sequencing. For each method, we examined how the results replicated between random partitions of each dataset and between datasets from independent studies. While certain methods showed good consistency, some widely used methods were observed to produce a substantial number of conflicting findings. Overall, the highest consistency without unnecessary reduction in sensitivity was attained by analyzing relative abundances with a non-parametric method (Wilcoxon test or ordinal regression model) or linear regression (MaAsLin2). Comparable performance was also attained by analyzing presence/absence of taxa with logistic regression.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.02691&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Juho Pelto, Kari Auranen, Janne Kujala, Leo Lahti</name></author><category term="stat.AP" /><summary type="html">Differential abundance analysis is a key component of microbiome studies. While dozens of methods for it exist, currently, there is no consensus on the preferred methods. Correctness of results in differential abundance analysis is an ambiguous concept that cannot be evaluated without employing simulated data, but we argue that consistency of results across datasets should be considered as an essential quality of a well-performing method. We compared the performance of 14 differential abundance analysis methods employing datasets from 54 taxonomic profiling studies based on 16S rRNA gene or shotgun sequencing. For each method, we examined how the results replicated between random partitions of each dataset and between datasets from independent studies. While certain methods showed good consistency, some widely used methods were observed to produce a substantial number of conflicting findings. Overall, the highest consistency without unnecessary reduction in sensitivity was attained by analyzing relative abundances with a non-parametric method (Wilcoxon test or ordinal regression model) or linear regression (MaAsLin2). Comparable performance was also attained by analyzing presence/absence of taxa with logistic regression.</summary></entry><entry><title type="html">Enhancing Sufficient Dimension Reduction via Hellinger Correlation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/EnhancingSufficientDimensionReductionviaHellingerCorrelation.html" rel="alternate" type="text/html" title="Enhancing Sufficient Dimension Reduction via Hellinger Correlation" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/EnhancingSufficientDimensionReductionviaHellingerCorrelation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/EnhancingSufficientDimensionReductionviaHellingerCorrelation.html">&lt;p&gt;In this work, we develop a new theory and method for sufficient dimension reduction (SDR) in single-index models, where SDR is a sub-field of supervised dimension reduction based on conditional independence. Our work is primarily motivated by the recent introduction of the Hellinger correlation as a dependency measure. Utilizing this measure, we develop a method capable of effectively detecting the dimension reduction subspace, complete with theoretical justification. Through extensive numerical experiments, we demonstrate that our proposed method significantly enhances and outperforms existing SDR methods. This improvement is largely attributed to our proposed method’s deeper understanding of data dependencies and the refinement of existing SDR techniques.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19704&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Seungbeom Hong, Ilmun Kim, Jun Song</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">In this work, we develop a new theory and method for sufficient dimension reduction (SDR) in single-index models, where SDR is a sub-field of supervised dimension reduction based on conditional independence. Our work is primarily motivated by the recent introduction of the Hellinger correlation as a dependency measure. Utilizing this measure, we develop a method capable of effectively detecting the dimension reduction subspace, complete with theoretical justification. Through extensive numerical experiments, we demonstrate that our proposed method significantly enhances and outperforms existing SDR methods. This improvement is largely attributed to our proposed method’s deeper understanding of data dependencies and the refinement of existing SDR techniques.</summary></entry><entry><title type="html">Evaluating Approximations of Count Distributions and Forecasts for Poisson-Lindley Integer Autoregressive Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/EvaluatingApproximationsofCountDistributionsandForecastsforPoissonLindleyIntegerAutoregressiveProcesses.html" rel="alternate" type="text/html" title="Evaluating Approximations of Count Distributions and Forecasts for Poisson-Lindley Integer Autoregressive Processes" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/EvaluatingApproximationsofCountDistributionsandForecastsforPoissonLindleyIntegerAutoregressiveProcesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/EvaluatingApproximationsofCountDistributionsandForecastsforPoissonLindleyIntegerAutoregressiveProcesses.html">&lt;p&gt;Although many time series are realizations from discrete processes, it is often that a continuous Gaussian model is implemented for modeling and forecasting the data, resulting in incoherent forecasts. Forecasts using a Poisson-Lindley integer autoregressive (PLINAR) model are compared to variations of Gaussian forecasts via simulation by equating relevant moments of the marginals of the PLINAR to the Gaussian AR. To illustrate utility, the methods discussed are applied and compared using a discrete series with model parameters being estimated using each of conditional least squares, Yule-Walker, and maximum likelihood.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20342&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rachel D. Gidaro, Jane L. Harvill</name></author><category term="stat.ME" /><summary type="html">Although many time series are realizations from discrete processes, it is often that a continuous Gaussian model is implemented for modeling and forecasting the data, resulting in incoherent forecasts. Forecasts using a Poisson-Lindley integer autoregressive (PLINAR) model are compared to variations of Gaussian forecasts via simulation by equating relevant moments of the marginals of the PLINAR to the Gaussian AR. To illustrate utility, the methods discussed are applied and compared using a discrete series with model parameters being estimated using each of conditional least squares, Yule-Walker, and maximum likelihood.</summary></entry><entry><title type="html">Factor Augmented Tensor-on-Tensor Neural Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/FactorAugmentedTensoronTensorNeuralNetworks.html" rel="alternate" type="text/html" title="Factor Augmented Tensor-on-Tensor Neural Networks" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/FactorAugmentedTensoronTensorNeuralNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/FactorAugmentedTensoronTensorNeuralNetworks.html">&lt;p&gt;This paper studies the prediction task of tensor-on-tensor regression in which both covariates and responses are multi-dimensional arrays (a.k.a., tensors) across time with arbitrary tensor order and data dimension. Existing methods either focused on linear models without accounting for possibly nonlinear relationships between covariates and responses, or directly employed black-box deep learning algorithms that failed to utilize the inherent tensor structure. In this work, we propose a Factor Augmented Tensor-on-Tensor Neural Network (FATTNN) that integrates tensor factor models into deep neural networks. We begin with summarizing and extracting useful predictive information (represented by the ``factor tensor’’) from the complex structured tensor covariates, and then proceed with the prediction task using the estimated factor tensor as input of a temporal convolutional neural network. The proposed methods effectively handle nonlinearity between complex data structures, and improve over traditional statistical models and conventional deep learning approaches in both prediction accuracy and computational cost. By leveraging tensor factor models, our proposed methods exploit the underlying latent factor structure to enhance the prediction, and in the meantime, drastically reduce the data dimensionality that speeds up the computation. The empirical performances of our proposed methods are demonstrated via simulation studies and real-world applications to three public datasets. Numerical results show that our proposed algorithms achieve substantial increases in prediction accuracy and significant reductions in computational time compared to benchmark methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19610&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Guanhao Zhou, Yuefeng Han, Xiufan Yu</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">This paper studies the prediction task of tensor-on-tensor regression in which both covariates and responses are multi-dimensional arrays (a.k.a., tensors) across time with arbitrary tensor order and data dimension. Existing methods either focused on linear models without accounting for possibly nonlinear relationships between covariates and responses, or directly employed black-box deep learning algorithms that failed to utilize the inherent tensor structure. In this work, we propose a Factor Augmented Tensor-on-Tensor Neural Network (FATTNN) that integrates tensor factor models into deep neural networks. We begin with summarizing and extracting useful predictive information (represented by the ``factor tensor’’) from the complex structured tensor covariates, and then proceed with the prediction task using the estimated factor tensor as input of a temporal convolutional neural network. The proposed methods effectively handle nonlinearity between complex data structures, and improve over traditional statistical models and conventional deep learning approaches in both prediction accuracy and computational cost. By leveraging tensor factor models, our proposed methods exploit the underlying latent factor structure to enhance the prediction, and in the meantime, drastically reduce the data dimensionality that speeds up the computation. The empirical performances of our proposed methods are demonstrated via simulation studies and real-world applications to three public datasets. Numerical results show that our proposed algorithms achieve substantial increases in prediction accuracy and significant reductions in computational time compared to benchmark methods.</summary></entry><entry><title type="html">Federated Causal Inference from Observational Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/FederatedCausalInferencefromObservationalData.html" rel="alternate" type="text/html" title="Federated Causal Inference from Observational Data" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/FederatedCausalInferencefromObservationalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/FederatedCausalInferencefromObservationalData.html">&lt;p&gt;Decentralized data sources are prevalent in real-world applications, posing a formidable challenge for causal inference. These sources cannot be consolidated into a single entity owing to privacy constraints. The presence of dissimilar data distributions and missing values within them can potentially introduce bias to the causal estimands. In this article, we propose a framework to estimate causal effects from decentralized data sources. The proposed framework avoid exchanging raw data among the sources, thus contributing towards privacy-preserving causal learning. Three instances of the proposed framework are introduced to estimate causal effects across a wide range of diverse scenarios within a federated setting. (1) FedCI: a Bayesian framework based on Gaussian processes for estimating causal effects from federated observational data sources. It estimates the posterior distributions of the causal effects to compute the higher-order statistics that capture the uncertainty. (2) CausalRFF: an adaptive transfer algorithm that learns the similarities among the data sources by utilizing Random Fourier Features to disentangle the loss function into multiple components, each of which is associated with a data source. It estimates the similarities among the sources through transfer coefficients, and hence requiring no prior information about the similarity measures. (3) CausalFI: a new approach for federated causal inference from incomplete data, enabling the estimation of causal effects from multiple decentralized and incomplete data sources. It accounts for the missing data under the missing at random assumption, while also estimating higher-order statistics of the causal estimands. The proposed federated framework and its instances are an important step towards a privacy-preserving causal learning model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.13047&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thanh Vinh Vo, Young lee, Tze-Yun Leong</name></author><category term="stat.ME" /><summary type="html">Decentralized data sources are prevalent in real-world applications, posing a formidable challenge for causal inference. These sources cannot be consolidated into a single entity owing to privacy constraints. The presence of dissimilar data distributions and missing values within them can potentially introduce bias to the causal estimands. In this article, we propose a framework to estimate causal effects from decentralized data sources. The proposed framework avoid exchanging raw data among the sources, thus contributing towards privacy-preserving causal learning. Three instances of the proposed framework are introduced to estimate causal effects across a wide range of diverse scenarios within a federated setting. (1) FedCI: a Bayesian framework based on Gaussian processes for estimating causal effects from federated observational data sources. It estimates the posterior distributions of the causal effects to compute the higher-order statistics that capture the uncertainty. (2) CausalRFF: an adaptive transfer algorithm that learns the similarities among the data sources by utilizing Random Fourier Features to disentangle the loss function into multiple components, each of which is associated with a data source. It estimates the similarities among the sources through transfer coefficients, and hence requiring no prior information about the similarity measures. (3) CausalFI: a new approach for federated causal inference from incomplete data, enabling the estimation of causal effects from multiple decentralized and incomplete data sources. It accounts for the missing data under the missing at random assumption, while also estimating higher-order statistics of the causal estimands. The proposed federated framework and its instances are an important step towards a privacy-preserving causal learning model.</summary></entry><entry><title type="html">Individualized Dynamic Latent Factor Model for Multi-resolutional Data with Application to Mobile Health</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/IndividualizedDynamicLatentFactorModelforMultiresolutionalDatawithApplicationtoMobileHealth.html" rel="alternate" type="text/html" title="Individualized Dynamic Latent Factor Model for Multi-resolutional Data with Application to Mobile Health" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/IndividualizedDynamicLatentFactorModelforMultiresolutionalDatawithApplicationtoMobileHealth</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/IndividualizedDynamicLatentFactorModelforMultiresolutionalDatawithApplicationtoMobileHealth.html">&lt;p&gt;Mobile health has emerged as a major success for tracking individual health status, due to the popularity and power of smartphones and wearable devices. This has also brought great challenges in handling heterogeneous, multi-resolution data which arise ubiquitously in mobile health due to irregular multivariate measurements collected from individuals. In this paper, we propose an individualized dynamic latent factor model for irregular multi-resolution time series data to interpolate unsampled measurements of time series with low resolution. One major advantage of the proposed method is the capability to integrate multiple irregular time series and multiple subjects by mapping the multi-resolution data to the latent space. In addition, the proposed individualized dynamic latent factor model is applicable to capturing heterogeneous longitudinal information through individualized dynamic latent factors. Our theory provides a bound on the integrated interpolation error and the convergence rate for B-spline approximation methods. Both the simulation studies and the application to smartwatch data demonstrate the superior performance of the proposed method compared to existing methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.12392&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiuchen Zhang, Fei Xue, Qi Xu, Jung-Ah Lee, Annie Qu</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Mobile health has emerged as a major success for tracking individual health status, due to the popularity and power of smartphones and wearable devices. This has also brought great challenges in handling heterogeneous, multi-resolution data which arise ubiquitously in mobile health due to irregular multivariate measurements collected from individuals. In this paper, we propose an individualized dynamic latent factor model for irregular multi-resolution time series data to interpolate unsampled measurements of time series with low resolution. One major advantage of the proposed method is the capability to integrate multiple irregular time series and multiple subjects by mapping the multi-resolution data to the latent space. In addition, the proposed individualized dynamic latent factor model is applicable to capturing heterogeneous longitudinal information through individualized dynamic latent factors. Our theory provides a bound on the integrated interpolation error and the convergence rate for B-spline approximation methods. Both the simulation studies and the application to smartwatch data demonstrate the superior performance of the proposed method compared to existing methods.</summary></entry><entry><title type="html">Inference in semiparametric formation models for directed networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Inferenceinsemiparametricformationmodelsfordirectednetworks.html" rel="alternate" type="text/html" title="Inference in semiparametric formation models for directed networks" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Inferenceinsemiparametricformationmodelsfordirectednetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Inferenceinsemiparametricformationmodelsfordirectednetworks.html">&lt;p&gt;We propose a semiparametric model for dyadic link formations in directed networks. The model contains a set of degree parameters that measure different effects of popularity or outgoingness across nodes, a regression parameter vector that reflects the homophily effect resulting from the nodal attributes or pairwise covariates associated with edges, and a set of latent random noises with unknown distributions. Our interest lies in inferring the unknown degree parameters and homophily parameters. The dimension of the degree parameters increases with the number of nodes. Under the high-dimensional regime, we develop a kernel-based least squares approach to estimate the unknown parameters. The major advantage of our estimator is that it does not encounter the incidental parameter problem for the homophily parameters. We prove consistency of all the resulting estimators of the degree parameters and homophily parameters. We establish high-dimensional central limit theorems for the proposed estimators and provide several applications of our general theory, including testing the existence of degree heterogeneity, testing sparse signals and recovering the support. Simulation studies and a real data application are conducted to illustrate the finite sample performance of the proposed methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19637&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lianqiang Qu, Lu Chen, Ting Yan, Yuguo Chen</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We propose a semiparametric model for dyadic link formations in directed networks. The model contains a set of degree parameters that measure different effects of popularity or outgoingness across nodes, a regression parameter vector that reflects the homophily effect resulting from the nodal attributes or pairwise covariates associated with edges, and a set of latent random noises with unknown distributions. Our interest lies in inferring the unknown degree parameters and homophily parameters. The dimension of the degree parameters increases with the number of nodes. Under the high-dimensional regime, we develop a kernel-based least squares approach to estimate the unknown parameters. The major advantage of our estimator is that it does not encounter the incidental parameter problem for the homophily parameters. We prove consistency of all the resulting estimators of the degree parameters and homophily parameters. We establish high-dimensional central limit theorems for the proposed estimators and provide several applications of our general theory, including testing the existence of degree heterogeneity, testing sparse signals and recovering the support. Simulation studies and a real data application are conducted to illustrate the finite sample performance of the proposed methods.</summary></entry><entry><title type="html">Inferring Synergistic and Antagonistic Interactions in Mixtures of Exposures</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/InferringSynergisticandAntagonisticInteractionsinMixturesofExposures.html" rel="alternate" type="text/html" title="Inferring Synergistic and Antagonistic Interactions in Mixtures of Exposures" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/InferringSynergisticandAntagonisticInteractionsinMixturesofExposures</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/InferringSynergisticandAntagonisticInteractionsinMixturesofExposures.html">&lt;p&gt;There is abundant interest in assessing the joint effects of multiple exposures on human health. This is often referred to as the mixtures problem in environmental epidemiology and toxicology. Classically, studies have examined the adverse health effects of different chemicals one at a time, but there is concern that certain chemicals may act together to amplify each other’s effects. Such amplification is referred to as synergistic interaction, while chemicals that inhibit each other’s effects have antagonistic interactions. Current approaches for assessing the health effects of chemical mixtures do not explicitly consider synergy or antagonism in the modeling, instead focusing on either parametric or unconstrained nonparametric dose response surface modeling. The parametric case can be too inflexible, while nonparametric methods face a curse of dimensionality that leads to overly wiggly and uninterpretable surface estimates. We propose a Bayesian approach that decomposes the response surface into additive main effects and pairwise interaction effects, and then detects synergistic and antagonistic interactions. Variable selection decisions for each interaction component are also provided. This Synergistic Antagonistic Interaction Detection (SAID) framework is evaluated relative to existing approaches using simulation experiments and an application to data from NHANES.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.09279&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shounak Chattopadhyay, Stephanie M. Engel, David Dunson</name></author><category term="stat.AP" /><summary type="html">There is abundant interest in assessing the joint effects of multiple exposures on human health. This is often referred to as the mixtures problem in environmental epidemiology and toxicology. Classically, studies have examined the adverse health effects of different chemicals one at a time, but there is concern that certain chemicals may act together to amplify each other’s effects. Such amplification is referred to as synergistic interaction, while chemicals that inhibit each other’s effects have antagonistic interactions. Current approaches for assessing the health effects of chemical mixtures do not explicitly consider synergy or antagonism in the modeling, instead focusing on either parametric or unconstrained nonparametric dose response surface modeling. The parametric case can be too inflexible, while nonparametric methods face a curse of dimensionality that leads to overly wiggly and uninterpretable surface estimates. We propose a Bayesian approach that decomposes the response surface into additive main effects and pairwise interaction effects, and then detects synergistic and antagonistic interactions. Variable selection decisions for each interaction component are also provided. This Synergistic Antagonistic Interaction Detection (SAID) framework is evaluated relative to existing approaches using simulation experiments and an application to data from NHANES.</summary></entry><entry><title type="html">Item response parameter estimation performance using Gaussian quadrature and Laplace</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ItemresponseparameterestimationperformanceusingGaussianquadratureandLaplace.html" rel="alternate" type="text/html" title="Item response parameter estimation performance using Gaussian quadrature and Laplace" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ItemresponseparameterestimationperformanceusingGaussianquadratureandLaplace</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ItemresponseparameterestimationperformanceusingGaussianquadratureandLaplace.html">&lt;p&gt;Item parameter estimation in pharmacometric item response theory (IRT) models is predominantly performed using the Laplace estimation algorithm as implemented in NONMEM. In psychometrics a wide range of different software tools, including several packages for the open-source software R for implementation of IRT are also available. Each have their own set of benefits and limitations and to date a systematic comparison of the primary estimation algorithms has not been evaluated. A simulation study evaluating varying number of hypothetical sample sizes and item scenarios at baseline was performed using both Laplace and Gauss-hermite quadrature (GHQ-EM). In scenarios with at least 20 items and more than 100 subjects, item parameters were estimated with good precision and were similar between estimation algorithms as demonstrated by several measures of bias and precision. The minimal differences observed for certain parameters or sample size scenarios were reduced when translating to the total score scale. The ease of use, speed of estimation and relative accuracy of the GHQ-EM method employed in mirt make it an appropriate alternative or supportive analytical approach to NONMEM for potential pharmacometrics IRT applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20164&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Leticia Arrington , Sebastian Ueckert</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Item parameter estimation in pharmacometric item response theory (IRT) models is predominantly performed using the Laplace estimation algorithm as implemented in NONMEM. In psychometrics a wide range of different software tools, including several packages for the open-source software R for implementation of IRT are also available. Each have their own set of benefits and limitations and to date a systematic comparison of the primary estimation algorithms has not been evaluated. A simulation study evaluating varying number of hypothetical sample sizes and item scenarios at baseline was performed using both Laplace and Gauss-hermite quadrature (GHQ-EM). In scenarios with at least 20 items and more than 100 subjects, item parameters were estimated with good precision and were similar between estimation algorithms as demonstrated by several measures of bias and precision. The minimal differences observed for certain parameters or sample size scenarios were reduced when translating to the total score scale. The ease of use, speed of estimation and relative accuracy of the GHQ-EM method employed in mirt make it an appropriate alternative or supportive analytical approach to NONMEM for potential pharmacometrics IRT applications.</summary></entry><entry><title type="html">Latent Factor Analysis in Short Panels</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/LatentFactorAnalysisinShortPanels.html" rel="alternate" type="text/html" title="Latent Factor Analysis in Short Panels" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/LatentFactorAnalysisinShortPanels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/LatentFactorAnalysisinShortPanels.html">&lt;p&gt;We develop inferential tools for latent factor analysis in short panels. The pseudo maximum likelihood setting under a large cross-sectional dimension n and a fixed time series dimension T relies on a diagonal TxT covariance matrix of the errors without imposing sphericity nor Gaussianity. We outline the asymptotic distributions of the latent factor and error covariance estimates as well as of an asymptotically uniformly most powerful invariant (AUMPI) test for the number of factors based on the likelihood ratio statistic. We derive the AUMPI characterization from inequalities ensuring the monotone likelihood ratio property for positive definite quadratic forms in normal variables. An empirical application to a large panel of monthly U.S. stock returns separates month after month systematic and idiosyncratic risks in short subperiods of bear vs. bull market based on the selected number of factors. We observe an uptrend in the paths of total and idiosyncratic volatilities while the systematic risk explains a large part of the cross-sectional total variance in bear markets but is not driven by a single factor. Rank tests show that observed factors struggle spanning latent factors with a discrepancy between the dimensions of the two factor spaces decreasing over time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.14004&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alain-Philippe Fortin, Patrick Gagliardini, Olivier Scaillet</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">We develop inferential tools for latent factor analysis in short panels. The pseudo maximum likelihood setting under a large cross-sectional dimension n and a fixed time series dimension T relies on a diagonal TxT covariance matrix of the errors without imposing sphericity nor Gaussianity. We outline the asymptotic distributions of the latent factor and error covariance estimates as well as of an asymptotically uniformly most powerful invariant (AUMPI) test for the number of factors based on the likelihood ratio statistic. We derive the AUMPI characterization from inequalities ensuring the monotone likelihood ratio property for positive definite quadratic forms in normal variables. An empirical application to a large panel of monthly U.S. stock returns separates month after month systematic and idiosyncratic risks in short subperiods of bear vs. bull market based on the selected number of factors. We observe an uptrend in the paths of total and idiosyncratic volatilities while the systematic risk explains a large part of the cross-sectional total variance in bear markets but is not driven by a single factor. Rank tests show that observed factors struggle spanning latent factors with a discrepancy between the dimensions of the two factor spaces decreasing over time.</summary></entry><entry><title type="html">Loop Polarity Analysis to Avoid Underspecification in Deep Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/LoopPolarityAnalysistoAvoidUnderspecificationinDeepLearning.html" rel="alternate" type="text/html" title="Loop Polarity Analysis to Avoid Underspecification in Deep Learning" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/LoopPolarityAnalysistoAvoidUnderspecificationinDeepLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/LoopPolarityAnalysistoAvoidUnderspecificationinDeepLearning.html">&lt;p&gt;Deep learning is a powerful set of techniques for detecting complex patterns in data. However, when the causal structure of that process is underspecified, deep learning models can be brittle, lacking robustness to shifts in the distribution of the data-generating process. In this paper, we turn to loop polarity analysis as a tool for specifying the causal structure of a data-generating process, in order to encode a more robust understanding of the relationship between system structure and system behavior within the deep learning pipeline. We use simulated epidemic data based on an SIR model to demonstrate how measuring the polarity of the different feedback loops that compose a system can lead to more robust inferences on the part of neural networks, improving the out-of-distribution performance of a deep learning model and infusing a system-dynamics-inspired approach into the machine learning development pipeline.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.10211&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Donald Martin, Jr., David Kinney</name></author><category term="stat.ME" /><summary type="html">Deep learning is a powerful set of techniques for detecting complex patterns in data. However, when the causal structure of that process is underspecified, deep learning models can be brittle, lacking robustness to shifts in the distribution of the data-generating process. In this paper, we turn to loop polarity analysis as a tool for specifying the causal structure of a data-generating process, in order to encode a more robust understanding of the relationship between system structure and system behavior within the deep learning pipeline. We use simulated epidemic data based on an SIR model to demonstrate how measuring the polarity of the different feedback loops that compose a system can lead to more robust inferences on the part of neural networks, improving the out-of-distribution performance of a deep learning model and infusing a system-dynamics-inspired approach into the machine learning development pipeline.</summary></entry><entry><title type="html">Multidimensional spatiotemporal clustering – An application to environmental sustainability scores in Europe</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/MultidimensionalspatiotemporalclusteringAnapplicationtoenvironmentalsustainabilityscoresinEurope.html" rel="alternate" type="text/html" title="Multidimensional spatiotemporal clustering – An application to environmental sustainability scores in Europe" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/MultidimensionalspatiotemporalclusteringAnapplicationtoenvironmentalsustainabilityscoresinEurope</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/MultidimensionalspatiotemporalclusteringAnapplicationtoenvironmentalsustainabilityscoresinEurope.html">&lt;p&gt;The assessment of corporate sustainability performance is extremely relevant in facilitating the transition to a green and low-carbon intensity economy. However, companies located in different areas may be subject to different sustainability and environmental risks and policies. Henceforth, the main objective of this paper is to investigate the spatial and temporal pattern of the sustainability evaluations of European firms. We leverage on a large dataset containing information about companies’ sustainability performances, measured by MSCI ESG ratings, and geographical coordinates of firms in Western Europe between 2013 and 2023. By means of a modified version of the Chavent et al. (2018) hierarchical algorithm, we conduct a spatial clustering analysis, combining sustainability and spatial information, and a spatiotemporal clustering analysis, which combines the time dynamics of multiple sustainability features and spatial dissimilarities, to detect groups of firms with homogeneous sustainability performance. We are able to build cross-national and cross-industry clusters with remarkable differences in terms of sustainability scores. Among other results, in the spatio-temporal analysis, we observe a high degree of geographical overlap among clusters, indicating that the temporal dynamics in sustainability assessment are relevant within a multidimensional approach. Our findings help to capture the diversity of ESG ratings across Western Europe and may assist practitioners and policymakers in evaluating companies facing different sustainability-linked risks in different areas.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20191&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Caterina Morelli, Simone Boccaletti, Paolo Maranzano, Philipp Otto</name></author><category term="stat.AP," /><category term="stat.CO" /><summary type="html">The assessment of corporate sustainability performance is extremely relevant in facilitating the transition to a green and low-carbon intensity economy. However, companies located in different areas may be subject to different sustainability and environmental risks and policies. Henceforth, the main objective of this paper is to investigate the spatial and temporal pattern of the sustainability evaluations of European firms. We leverage on a large dataset containing information about companies’ sustainability performances, measured by MSCI ESG ratings, and geographical coordinates of firms in Western Europe between 2013 and 2023. By means of a modified version of the Chavent et al. (2018) hierarchical algorithm, we conduct a spatial clustering analysis, combining sustainability and spatial information, and a spatiotemporal clustering analysis, which combines the time dynamics of multiple sustainability features and spatial dissimilarities, to detect groups of firms with homogeneous sustainability performance. We are able to build cross-national and cross-industry clusters with remarkable differences in terms of sustainability scores. Among other results, in the spatio-temporal analysis, we observe a high degree of geographical overlap among clusters, indicating that the temporal dynamics in sustainability assessment are relevant within a multidimensional approach. Our findings help to capture the diversity of ESG ratings across Western Europe and may assist practitioners and policymakers in evaluating companies facing different sustainability-linked risks in different areas.</summary></entry><entry><title type="html">Neural Networks for Extreme Quantile Regression with an Application to Forecasting of Flood Risk</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/NeuralNetworksforExtremeQuantileRegressionwithanApplicationtoForecastingofFloodRisk.html" rel="alternate" type="text/html" title="Neural Networks for Extreme Quantile Regression with an Application to Forecasting of Flood Risk" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/NeuralNetworksforExtremeQuantileRegressionwithanApplicationtoForecastingofFloodRisk</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/NeuralNetworksforExtremeQuantileRegressionwithanApplicationtoForecastingofFloodRisk.html">&lt;p&gt;Risk assessment for extreme events requires accurate estimation of high quantiles that go beyond the range of historical observations. When the risk depends on the values of observed predictors, regression techniques are used to interpolate in the predictor space. We propose the EQRN model that combines tools from neural networks and extreme value theory into a method capable of extrapolation in the presence of complex predictor dependence. Neural networks can naturally incorporate additional structure in the data. We develop a recurrent version of EQRN that is able to capture complex sequential dependence in time series. We apply this method to forecast flood risk in the Swiss Aare catchment. It exploits information from multiple covariates in space and time to provide one-day-ahead predictions of return levels and exceedance probabilities. This output complements the static return level from a traditional extreme value analysis, and the predictions are able to adapt to distributional shifts as experienced in a changing climate. Our model can help authorities to manage flooding more effectively and to minimize their disastrous impacts through early warning systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.07590&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Olivier C. Pasche, Sebastian Engelke</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Risk assessment for extreme events requires accurate estimation of high quantiles that go beyond the range of historical observations. When the risk depends on the values of observed predictors, regression techniques are used to interpolate in the predictor space. We propose the EQRN model that combines tools from neural networks and extreme value theory into a method capable of extrapolation in the presence of complex predictor dependence. Neural networks can naturally incorporate additional structure in the data. We develop a recurrent version of EQRN that is able to capture complex sequential dependence in time series. We apply this method to forecast flood risk in the Swiss Aare catchment. It exploits information from multiple covariates in space and time to provide one-day-ahead predictions of return levels and exceedance probabilities. This output complements the static return level from a traditional extreme value analysis, and the predictions are able to adapt to distributional shifts as experienced in a changing climate. Our model can help authorities to manage flooding more effectively and to minimize their disastrous impacts through early warning systems.</summary></entry><entry><title type="html">Nuisance Function Tuning for Optimal Doubly Robust Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/NuisanceFunctionTuningforOptimalDoublyRobustEstimation.html" rel="alternate" type="text/html" title="Nuisance Function Tuning for Optimal Doubly Robust Estimation" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/NuisanceFunctionTuningforOptimalDoublyRobustEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/NuisanceFunctionTuningforOptimalDoublyRobustEstimation.html">&lt;p&gt;Estimators of doubly robust functionals typically rely on estimating two complex nuisance functions, such as the propensity score and conditional outcome mean for the average treatment effect functional. We consider the problem of how to estimate nuisance functions to obtain optimal rates of convergence for a doubly robust nonparametric functional that has witnessed applications across the causal inference and conditional independence testing literature. For several plug-in type estimators and a one-step type estimator, we illustrate the interplay between different tuning parameter choices for the nuisance function estimators and sample splitting strategies on the optimal rate of estimating the functional of interest. For each of these estimators and each sample splitting strategy, we show the necessity to undersmooth the nuisance function estimators under low regularity conditions to obtain optimal rates of convergence for the functional of interest. By performing suitable nuisance function tuning and sample splitting strategies, we show that some of these estimators can achieve minimax rates of convergence in all H&quot;older smoothness classes of the nuisance functions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.14857&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sean McGrath, Rajarshi Mukherjee</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Estimators of doubly robust functionals typically rely on estimating two complex nuisance functions, such as the propensity score and conditional outcome mean for the average treatment effect functional. We consider the problem of how to estimate nuisance functions to obtain optimal rates of convergence for a doubly robust nonparametric functional that has witnessed applications across the causal inference and conditional independence testing literature. For several plug-in type estimators and a one-step type estimator, we illustrate the interplay between different tuning parameter choices for the nuisance function estimators and sample splitting strategies on the optimal rate of estimating the functional of interest. For each of these estimators and each sample splitting strategy, we show the necessity to undersmooth the nuisance function estimators under low regularity conditions to obtain optimal rates of convergence for the functional of interest. By performing suitable nuisance function tuning and sample splitting strategies, we show that some of these estimators can achieve minimax rates of convergence in all H&quot;older smoothness classes of the nuisance functions.</summary></entry><entry><title type="html">Participation bias in the estimation of heritability and genetic correlation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Participationbiasintheestimationofheritabilityandgeneticcorrelation.html" rel="alternate" type="text/html" title="Participation bias in the estimation of heritability and genetic correlation" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Participationbiasintheestimationofheritabilityandgeneticcorrelation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Participationbiasintheestimationofheritabilityandgeneticcorrelation.html">&lt;p&gt;It is increasingly recognized that participation bias can pose problems for genetic studies. Recently, to overcome the challenge that genetic information of non-participants is unavailable, it is shown that by comparing the IBD (identity by descent) shared and not-shared segments among the participants, one can estimate the genetic component underlying participation. That, however, does not directly address how to adjust estimates of heritability and genetic correlation for phenotypes correlated with participation. Here, for phenotypes whose mean differences between population and sample are known, we demonstrate a way to do so by adopting a statistical framework that separates out the genetic and non-genetic correlations between participation and these phenotypes. Crucially, our method avoids making the assumption that the effect of the genetic component underlying participation is manifested entirely through these other phenotypes. Applying the method to 12 UK Biobank phenotypes, we found 8 have significant genetic correlations with participation, including body mass index, educational attainment, and smoking status. For most of these phenotypes, without adjustments, estimates of heritability and the absolute value of genetic correlation would have underestimation biases.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19058&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shuang Song, Stefania Benonisdottir, Jun S. Liu, Augustine Kong</name></author><category term="stat.ME" /><summary type="html">It is increasingly recognized that participation bias can pose problems for genetic studies. Recently, to overcome the challenge that genetic information of non-participants is unavailable, it is shown that by comparing the IBD (identity by descent) shared and not-shared segments among the participants, one can estimate the genetic component underlying participation. That, however, does not directly address how to adjust estimates of heritability and genetic correlation for phenotypes correlated with participation. Here, for phenotypes whose mean differences between population and sample are known, we demonstrate a way to do so by adopting a statistical framework that separates out the genetic and non-genetic correlations between participation and these phenotypes. Crucially, our method avoids making the assumption that the effect of the genetic component underlying participation is manifested entirely through these other phenotypes. Applying the method to 12 UK Biobank phenotypes, we found 8 have significant genetic correlations with participation, including body mass index, educational attainment, and smoking status. For most of these phenotypes, without adjustments, estimates of heritability and the absolute value of genetic correlation would have underestimation biases.</summary></entry><entry><title type="html">Personalized Predictions from Population Level Experiments: A Study on Alzheimer’s Disease</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/PersonalizedPredictionsfromPopulationLevelExperimentsAStudyonAlzheimersDisease.html" rel="alternate" type="text/html" title="Personalized Predictions from Population Level Experiments: A Study on Alzheimer’s Disease" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/PersonalizedPredictionsfromPopulationLevelExperimentsAStudyonAlzheimersDisease</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/PersonalizedPredictionsfromPopulationLevelExperimentsAStudyonAlzheimersDisease.html">&lt;p&gt;The purpose of this article is to infer patient level outcomes from population level randomized control trials (RCTs). In this pursuit, we utilize the recently proposed synthetic nearest neighbors (SNN) estimator. At its core, SNN leverages information across patients to impute missing data associated with each patient of interest. We focus on two types of missing data: (i) unrecorded outcomes from discontinuing the assigned treatments and (ii) unobserved outcomes associated with unassigned treatments. Data imputation in the former powers and de-biases RCTs, while data imputation in the latter simulates “synthetic RCTs” to predict the outcomes for each patient under every treatment. The SNN estimator is interpretable, transparent, and causally justified under a broad class of missing data scenarios. Relative to several standard methods, we empirically find that SNN performs well for the above two applications using Phase 3 clinical trial data on patients with Alzheimer’s Disease. Our findings directly suggest that SNN can tackle a current pain point within the clinical trial workflow on patient dropouts and serve as a new tool towards the development of precision medicine. Building on our insights, we discuss how SNN can further generalize to real-world applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20088&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Dennis Shen, Anish Agarwal, Vishal Misra, Bjoern Schelter, Devavrat Shah, Helen Shiells, Claude Wischik</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">The purpose of this article is to infer patient level outcomes from population level randomized control trials (RCTs). In this pursuit, we utilize the recently proposed synthetic nearest neighbors (SNN) estimator. At its core, SNN leverages information across patients to impute missing data associated with each patient of interest. We focus on two types of missing data: (i) unrecorded outcomes from discontinuing the assigned treatments and (ii) unobserved outcomes associated with unassigned treatments. Data imputation in the former powers and de-biases RCTs, while data imputation in the latter simulates “synthetic RCTs” to predict the outcomes for each patient under every treatment. The SNN estimator is interpretable, transparent, and causally justified under a broad class of missing data scenarios. Relative to several standard methods, we empirically find that SNN performs well for the above two applications using Phase 3 clinical trial data on patients with Alzheimer’s Disease. Our findings directly suggest that SNN can tackle a current pain point within the clinical trial workflow on patient dropouts and serve as a new tool towards the development of precision medicine. Building on our insights, we discuss how SNN can further generalize to real-world applications.</summary></entry><entry><title type="html">Reduced Rank Regression for Mixed Predictor and Response Variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ReducedRankRegressionforMixedPredictorandResponseVariables.html" rel="alternate" type="text/html" title="Reduced Rank Regression for Mixed Predictor and Response Variables" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ReducedRankRegressionforMixedPredictorandResponseVariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ReducedRankRegressionforMixedPredictorandResponseVariables.html">&lt;p&gt;In this paper, we propose the generalized mixed reduced rank regression method, GMR$^3$ for short. GMR$^3$ is a regression method for a mix of numeric, binary and ordinal response variables. The predictor variables can be a mix of binary, nominal, ordinal, and numeric variables. For dealing with the categorical predictors we use optimal scaling. A majorization-minimization algorithm is derived for maximum likelihood estimation under a local independence assumption. We discuss in detail model selection for the dimensionality or rank, and the selection of predictor variables. We show an application of GMR$^3$ using the Eurobarometer Surveys data set of 2023.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19865&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mark de Rooij, Lorenza Cotugno, Roberta Siciliano</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">In this paper, we propose the generalized mixed reduced rank regression method, GMR$^3$ for short. GMR$^3$ is a regression method for a mix of numeric, binary and ordinal response variables. The predictor variables can be a mix of binary, nominal, ordinal, and numeric variables. For dealing with the categorical predictors we use optimal scaling. A majorization-minimization algorithm is derived for maximum likelihood estimation under a local independence assumption. We discuss in detail model selection for the dimensionality or rank, and the selection of predictor variables. We show an application of GMR$^3$ using the Eurobarometer Surveys data set of 2023.</summary></entry><entry><title type="html">Scaling up archival text analysis with the blockmodeling of n-gram networks – A case study of Bulgaria’s representation in the Osservatore Romano (January-May 1877)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ScalinguparchivaltextanalysiswiththeblockmodelingofngramnetworksAcasestudyofBulgariasrepresentationintheOsservatoreRomanoJanuaryMay1877.html" rel="alternate" type="text/html" title="Scaling up archival text analysis with the blockmodeling of n-gram networks – A case study of Bulgaria’s representation in the Osservatore Romano (January-May 1877)" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ScalinguparchivaltextanalysiswiththeblockmodelingofngramnetworksAcasestudyofBulgariasrepresentationintheOsservatoreRomanoJanuaryMay1877</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ScalinguparchivaltextanalysiswiththeblockmodelingofngramnetworksAcasestudyofBulgariasrepresentationintheOsservatoreRomanoJanuaryMay1877.html">&lt;p&gt;This paper seeks to bridge the gap between archival text analysis and network analysis by applying network clustering methods to analyze the coverage of Bulgaria in 123 issues of the newspaper Osservatore Romano published between January and May 1877. Utilizing optical character recognition and generalized homogeneity blockmodeling, the study constructs networks of relevant keywords. Those including the sets Bulgaria and Russia are rather isomorphic and they largely overlap with those for Germany, Britain, and War. In structural terms, the blockmodel of the two networks exhibits a clear core-semiperiphery-periphery structure that reflects relations between concepts in the newpaper’s coverage. The newspaper’s lexical choices effectively delegitimised the Bulgarian national revival, highlighting the influence of the Holy See on the newspaper’s editorial line.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20156&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fabio Ashtar Telarico</name></author><category term="stat.AP," /><category term="stat.CO," /><category term="stat.OT" /><summary type="html">This paper seeks to bridge the gap between archival text analysis and network analysis by applying network clustering methods to analyze the coverage of Bulgaria in 123 issues of the newspaper Osservatore Romano published between January and May 1877. Utilizing optical character recognition and generalized homogeneity blockmodeling, the study constructs networks of relevant keywords. Those including the sets Bulgaria and Russia are rather isomorphic and they largely overlap with those for Germany, Britain, and War. In structural terms, the blockmodel of the two networks exhibits a clear core-semiperiphery-periphery structure that reflects relations between concepts in the newpaper’s coverage. The newspaper’s lexical choices effectively delegitimised the Bulgarian national revival, highlighting the influence of the Holy See on the newspaper’s editorial line.</summary></entry><entry><title type="html">Spectral Change Point Estimation for High Dimensional Time Series by Sparse Tensor Decomposition</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/SpectralChangePointEstimationforHighDimensionalTimeSeriesbySparseTensorDecomposition.html" rel="alternate" type="text/html" title="Spectral Change Point Estimation for High Dimensional Time Series by Sparse Tensor Decomposition" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/SpectralChangePointEstimationforHighDimensionalTimeSeriesbySparseTensorDecomposition</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/SpectralChangePointEstimationforHighDimensionalTimeSeriesbySparseTensorDecomposition.html">&lt;p&gt;Multivariate time series may be subject to partial structural changes over certain frequency band, for instance, in neuroscience. We study the change point detection problem with high dimensional time series, within the framework of frequency domain. The overarching goal is to locate all change points and delineate which series are activated by the change, over which frequencies. In practice, the number of activated series per change and frequency could span from a few to full participation. We solve the problem by first computing a CUSUM tensor based on spectra estimated from blocks of the time series. A frequency-specific projection approach is applied for dimension reduction. The projection direction is estimated by a proposed tensor decomposition algorithm that adjusts to the sparsity level of changes. Finally, the projected CUSUM vectors across frequencies are aggregated for change point detection. We provide theoretical guarantees on the number of estimated change points and the convergence rate of their locations. We derive error bounds for the estimated projection direction for identifying the frequency-specific series activated in a change. We provide data-driven rules for the choice of parameters. The efficacy of the proposed method is illustrated by simulation and a stock returns application.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.10656&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinyu Zhang, Kung-Sik Chan</name></author><category term="stat.ME" /><summary type="html">Multivariate time series may be subject to partial structural changes over certain frequency band, for instance, in neuroscience. We study the change point detection problem with high dimensional time series, within the framework of frequency domain. The overarching goal is to locate all change points and delineate which series are activated by the change, over which frequencies. In practice, the number of activated series per change and frequency could span from a few to full participation. We solve the problem by first computing a CUSUM tensor based on spectra estimated from blocks of the time series. A frequency-specific projection approach is applied for dimension reduction. The projection direction is estimated by a proposed tensor decomposition algorithm that adjusts to the sparsity level of changes. Finally, the projected CUSUM vectors across frequencies are aggregated for change point detection. We provide theoretical guarantees on the number of estimated change points and the convergence rate of their locations. We derive error bounds for the estimated projection direction for identifying the frequency-specific series activated in a change. We provide data-driven rules for the choice of parameters. The efficacy of the proposed method is illustrated by simulation and a stock returns application.</summary></entry><entry><title type="html">Targeted Sequential Indirect Experiment Design</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TargetedSequentialIndirectExperimentDesign.html" rel="alternate" type="text/html" title="Targeted Sequential Indirect Experiment Design" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TargetedSequentialIndirectExperimentDesign</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TargetedSequentialIndirectExperimentDesign.html">&lt;p&gt;Scientific hypotheses typically concern specific aspects of complex, imperfectly understood or entirely unknown mechanisms, such as the effect of gene expression levels on phenotypes or how microbial communities influence environmental health. Such queries are inherently causal (rather than purely associational), but in many settings, experiments can not be conducted directly on the target variables of interest, but are indirect. Therefore, they perturb the target variable, but do not remove potential confounding factors. If, additionally, the resulting experimental measurements are multi-dimensional and the studied mechanisms nonlinear, the query of interest is generally not identified. We develop an adaptive strategy to design indirect experiments that optimally inform a targeted query about the ground truth mechanism in terms of sequentially narrowing the gap between an upper and lower bound on the query. While the general formulation consists of a bi-level optimization procedure, we derive an efficiently estimable analytical kernel-based estimator of the bounds for the causal effect, a query of key interest, and demonstrate the efficacy of our approach in confounded, multivariate, nonlinear synthetic settings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19985&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elisabeth Ailer, Niclas Dern, Jason Hartford, Niki Kilbertus</name></author><category term="stat.ME" /><summary type="html">Scientific hypotheses typically concern specific aspects of complex, imperfectly understood or entirely unknown mechanisms, such as the effect of gene expression levels on phenotypes or how microbial communities influence environmental health. Such queries are inherently causal (rather than purely associational), but in many settings, experiments can not be conducted directly on the target variables of interest, but are indirect. Therefore, they perturb the target variable, but do not remove potential confounding factors. If, additionally, the resulting experimental measurements are multi-dimensional and the studied mechanisms nonlinear, the query of interest is generally not identified. We develop an adaptive strategy to design indirect experiments that optimally inform a targeted query about the ground truth mechanism in terms of sequentially narrowing the gap between an upper and lower bound on the query. While the general formulation consists of a bi-level optimization procedure, we derive an efficiently estimable analytical kernel-based estimator of the bounds for the causal effect, a query of key interest, and demonstrate the efficacy of our approach in confounded, multivariate, nonlinear synthetic settings.</summary></entry><entry><title type="html">Task-Agnostic Machine Learning-Assisted Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TaskAgnosticMachineLearningAssistedInference.html" rel="alternate" type="text/html" title="Task-Agnostic Machine Learning-Assisted Inference" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TaskAgnosticMachineLearningAssistedInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TaskAgnosticMachineLearningAssistedInference.html">&lt;p&gt;Machine learning (ML) is playing an increasingly important role in scientific research. In conjunction with classical statistical approaches, ML-assisted analytical strategies have shown great promise in accelerating research findings. This has also opened up a whole new field of methodological research focusing on integrative approaches that leverage both ML and statistics to tackle data science challenges. One type of study that has quickly gained popularity employs ML to predict unobserved outcomes in massive samples and then uses the predicted outcomes in downstream statistical inference. However, existing methods designed to ensure the validity of this type of post-prediction inference are limited to very basic tasks such as linear regression analysis. This is because any extension of these approaches to new, more sophisticated statistical tasks requires task-specific algebraic derivations and software implementations, which ignores the massive library of existing software tools already developed for complex inference tasks and severely constrains the scope of post-prediction inference in real applications. To address this challenge, we propose a novel statistical framework for task-agnostic ML-assisted inference. It provides a post-prediction inference solution that can be easily plugged into almost any established data analysis routine. It delivers valid and efficient inference that is robust to arbitrary choices of ML models, while allowing nearly all existing analytical frameworks to be incorporated into the analysis of ML-predicted outcomes. Through extensive experiments, we showcase the validity, versatility, and superiority of our method compared to existing approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.20039&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiacheng Miao, Qiongshi Lu</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Machine learning (ML) is playing an increasingly important role in scientific research. In conjunction with classical statistical approaches, ML-assisted analytical strategies have shown great promise in accelerating research findings. This has also opened up a whole new field of methodological research focusing on integrative approaches that leverage both ML and statistics to tackle data science challenges. One type of study that has quickly gained popularity employs ML to predict unobserved outcomes in massive samples and then uses the predicted outcomes in downstream statistical inference. However, existing methods designed to ensure the validity of this type of post-prediction inference are limited to very basic tasks such as linear regression analysis. This is because any extension of these approaches to new, more sophisticated statistical tasks requires task-specific algebraic derivations and software implementations, which ignores the massive library of existing software tools already developed for complex inference tasks and severely constrains the scope of post-prediction inference in real applications. To address this challenge, we propose a novel statistical framework for task-agnostic ML-assisted inference. It provides a post-prediction inference solution that can be easily plugged into almost any established data analysis routine. It delivers valid and efficient inference that is robust to arbitrary choices of ML models, while allowing nearly all existing analytical frameworks to be incorporated into the analysis of ML-predicted outcomes. Through extensive experiments, we showcase the validity, versatility, and superiority of our method compared to existing approaches.</summary></entry><entry><title type="html">Tempered Multifidelity Importance Sampling for Gravitational Wave Parameter Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TemperedMultifidelityImportanceSamplingforGravitationalWaveParameterEstimation.html" rel="alternate" type="text/html" title="Tempered Multifidelity Importance Sampling for Gravitational Wave Parameter Estimation" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TemperedMultifidelityImportanceSamplingforGravitationalWaveParameterEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TemperedMultifidelityImportanceSamplingforGravitationalWaveParameterEstimation.html">&lt;p&gt;Estimating the parameters of compact binaries which coalesce and produce gravitational waves is a challenging Bayesian inverse problem. Gravitational-wave parameter estimation lies within the class of multifidelity problems, where a variety of models with differing assumptions, levels of fidelity, and computational cost are available for use in inference. In an effort to accelerate the solution of a Bayesian inverse problem, cheaper surrogates for the best models may be used to reduce the cost of likelihood evaluations when sampling the posterior. Importance sampling can then be used to reweight these samples to represent the true target posterior, incurring a reduction in the effective sample size. In cases when the problem is high dimensional, or when the surrogate model produces a poor approximation of the true posterior, this reduction in effective samples can be dramatic and render multifidelity importance sampling ineffective. We propose a novel method of tempered multifidelity importance sampling in order to remedy this issue. With this method the biasing distribution produced by the low-fidelity model is tempered, allowing for potentially better overlap with the target distribution. There is an optimal temperature which maximizes the efficiency in this setting, and we propose a low-cost strategy for approximating this optimal temperature using samples from the untempered distribution. In this paper, we motivate this method by applying it to Gaussian target and biasing distributions. Finally, we apply it to a series of problems in gravitational wave parameter estimation and demonstrate improved efficiencies when applying the method to real gravitational wave detections.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19407&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bassel Saleh, Aaron Zimmerman, Peng Chen, Omar Ghattas</name></author><category term="stat.ME" /><summary type="html">Estimating the parameters of compact binaries which coalesce and produce gravitational waves is a challenging Bayesian inverse problem. Gravitational-wave parameter estimation lies within the class of multifidelity problems, where a variety of models with differing assumptions, levels of fidelity, and computational cost are available for use in inference. In an effort to accelerate the solution of a Bayesian inverse problem, cheaper surrogates for the best models may be used to reduce the cost of likelihood evaluations when sampling the posterior. Importance sampling can then be used to reweight these samples to represent the true target posterior, incurring a reduction in the effective sample size. In cases when the problem is high dimensional, or when the surrogate model produces a poor approximation of the true posterior, this reduction in effective samples can be dramatic and render multifidelity importance sampling ineffective. We propose a novel method of tempered multifidelity importance sampling in order to remedy this issue. With this method the biasing distribution produced by the low-fidelity model is tempered, allowing for potentially better overlap with the target distribution. There is an optimal temperature which maximizes the efficiency in this setting, and we propose a low-cost strategy for approximating this optimal temperature using samples from the untempered distribution. In this paper, we motivate this method by applying it to Gaussian target and biasing distributions. Finally, we apply it to a series of problems in gravitational wave parameter estimation and demonstrate improved efficiencies when applying the method to real gravitational wave detections.</summary></entry><entry><title type="html">The ARR2 prior: flexible predictive prior definition for Bayesian auto-regressions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TheARR2priorflexiblepredictivepriordefinitionforBayesianautoregressions.html" rel="alternate" type="text/html" title="The ARR2 prior: flexible predictive prior definition for Bayesian auto-regressions" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TheARR2priorflexiblepredictivepriordefinitionforBayesianautoregressions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TheARR2priorflexiblepredictivepriordefinitionforBayesianautoregressions.html">&lt;p&gt;We present the ARR2 prior, a joint prior over the auto-regressive components in Bayesian time-series models and their induced $R^2$. Compared to other priors designed for times-series models, the ARR2 prior allows for flexible and intuitive shrinkage. We derive the prior for pure auto-regressive models, and extend it to auto-regressive models with exogenous inputs, and state-space models. Through both simulations and real-world modelling exercises, we demonstrate the efficacy of the ARR2 prior in improving sparse and reliable inference, while showing greater inference quality and predictive performance than other shrinkage priors. An open-source implementation of the prior is provided.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19920&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David Kohns, Noa Kallionen, Yann McLatchie, Aki Vehtari</name></author><category term="stat.CO" /><summary type="html">We present the ARR2 prior, a joint prior over the auto-regressive components in Bayesian time-series models and their induced $R^2$. Compared to other priors designed for times-series models, the ARR2 prior allows for flexible and intuitive shrinkage. We derive the prior for pure auto-regressive models, and extend it to auto-regressive models with exogenous inputs, and state-space models. Through both simulations and real-world modelling exercises, we demonstrate the efficacy of the ARR2 prior in improving sparse and reliable inference, while showing greater inference quality and predictive performance than other shrinkage priors. An open-source implementation of the prior is provided.</summary></entry><entry><title type="html">The Political Resource Curse Redux</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ThePoliticalResourceCurseRedux.html" rel="alternate" type="text/html" title="The Political Resource Curse Redux" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ThePoliticalResourceCurseRedux</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/ThePoliticalResourceCurseRedux.html">&lt;p&gt;In the study of the Political Resource Curse (Brollo et al.,2013), the authors identified a new channel to investigate whether the windfalls of resources are unambiguously beneficial to society, both with theory and empirical evidence. This paper revisits the framework with a new dataset. Specifically, we implemented a regression discontinuity design and difference-in-difference specification&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19897&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hanyuan Jiang</name></author><category term="stat.AP" /><summary type="html">In the study of the Political Resource Curse (Brollo et al.,2013), the authors identified a new channel to investigate whether the windfalls of resources are unambiguously beneficial to society, both with theory and empirical evidence. This paper revisits the framework with a new dataset. Specifically, we implemented a regression discontinuity design and difference-in-difference specification</summary></entry><entry><title type="html">The assessment of replicability using the sum of p-values</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Theassessmentofreplicabilityusingthesumofpvalues.html" rel="alternate" type="text/html" title="The assessment of replicability using the sum of p-values" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Theassessmentofreplicabilityusingthesumofpvalues</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/Theassessmentofreplicabilityusingthesumofpvalues.html">&lt;p&gt;Statistical significance of both the original and the replication study is a commonly used criterion to assess replication attempts, also known as the two-trials rule in drug development. However, replication studies are sometimes conducted although the original study is non-significant, in which case Type-I error rate control across both studies is no longer guaranteed. We propose an alternative method to assess replicability using the sum of p-values from the two studies. The approach provides a combined p-value and can be calibrated to control the overall Type-I error rate at the same level as the two-trials rule but allows for replication success even if the original study is non-significant. The unweighted version requires a less restrictive level of significance at replication if the original study is already convincing which facilitates sample size reductions of up to 10%. Downweighting the original study accounts for possible bias and requires a more stringent significance level and larger samples sizes at replication. Data from four large-scale replication projects are used to illustrate and compare the proposed method with the two-trials rule, meta-analysis and Fisher’s combination method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.13615&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Leonhard Held, Samuel Pawel, Charlotte Micheloud</name></author><category term="stat.AP" /><summary type="html">Statistical significance of both the original and the replication study is a commonly used criterion to assess replication attempts, also known as the two-trials rule in drug development. However, replication studies are sometimes conducted although the original study is non-significant, in which case Type-I error rate control across both studies is no longer guaranteed. We propose an alternative method to assess replicability using the sum of p-values from the two studies. The approach provides a combined p-value and can be calibrated to control the overall Type-I error rate at the same level as the two-trials rule but allows for replication success even if the original study is non-significant. The unweighted version requires a less restrictive level of significance at replication if the original study is already convincing which facilitates sample size reductions of up to 10%. Downweighting the original study accounts for possible bias and requires a more stringent significance level and larger samples sizes at replication. Data from four large-scale replication projects are used to illustrate and compare the proposed method with the two-trials rule, meta-analysis and Fisher’s combination method.</summary></entry><entry><title type="html">Time-Varying Dispersion Integer-Valued GARCH Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TimeVaryingDispersionIntegerValuedGARCHModels.html" rel="alternate" type="text/html" title="Time-Varying Dispersion Integer-Valued GARCH Models" /><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TimeVaryingDispersionIntegerValuedGARCHModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/31/TimeVaryingDispersionIntegerValuedGARCHModels.html">&lt;p&gt;We propose a general class of INteger-valued Generalized AutoRegressive Conditionally Heteroscedastic (INGARCH) processes by allowing time-varying mean and dispersion parameters, which we call time-varying dispersion INGARCH (tv-DINGARCH) models. More specifically, we consider mixed Poisson INGARCH models and allow for dynamic modeling of the dispersion parameter (as well as the mean), similar to the spirit of the ordinary GARCH models. We derive conditions to obtain first and second-order stationarity, and ergodicity as well. Estimation of the parameters is addressed and their associated asymptotic properties are established as well. A restricted bootstrap procedure is proposed for testing constant dispersion against time-varying dispersion. Monte Carlo simulation studies are presented for checking point estimation, standard errors, and the performance of the restricted bootstrap approach. We apply the tv-DINGARCH process to model the weekly number of reported measles infections in North Rhine-Westphalia, Germany, from January 2001 to May 2013, and compare its performance to the ordinary INGARCH approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.02024&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wagner Barreto-Souza, Luiza S. C. Piancastelli, Konstantinos Fokianos, Hernando Ombao</name></author><category term="stat.ME" /><summary type="html">We propose a general class of INteger-valued Generalized AutoRegressive Conditionally Heteroscedastic (INGARCH) processes by allowing time-varying mean and dispersion parameters, which we call time-varying dispersion INGARCH (tv-DINGARCH) models. More specifically, we consider mixed Poisson INGARCH models and allow for dynamic modeling of the dispersion parameter (as well as the mean), similar to the spirit of the ordinary GARCH models. We derive conditions to obtain first and second-order stationarity, and ergodicity as well. Estimation of the parameters is addressed and their associated asymptotic properties are established as well. A restricted bootstrap procedure is proposed for testing constant dispersion against time-varying dispersion. Monte Carlo simulation studies are presented for checking point estimation, standard errors, and the performance of the restricted bootstrap approach. We apply the tv-DINGARCH process to model the weekly number of reported measles infections in North Rhine-Westphalia, Germany, from January 2001 to May 2013, and compare its performance to the ordinary INGARCH approach.</summary></entry></feed>