<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-04T07:14:33+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Bayesian Generalized Bridge Regression Approach to Covariance Estimation in the Presence of Covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ABayesianGeneralizedBridgeRegressionApproachtoCovarianceEstimationinthePresenceofCovariates.html" rel="alternate" type="text/html" title="A Bayesian Generalized Bridge Regression Approach to Covariance Estimation in the Presence of Covariates" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ABayesianGeneralizedBridgeRegressionApproachtoCovarianceEstimationinthePresenceofCovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ABayesianGeneralizedBridgeRegressionApproachtoCovarianceEstimationinthePresenceofCovariates.html">&lt;p&gt;A hierarchical Bayesian approach that permits simultaneous inference for the regression coefficient matrix and the error precision (inverse covariance) matrix in the multivariate linear model is proposed. Assuming a natural ordering of the elements of the response, the precision matrix is reparameterized so it can be estimated with univariate-response linear regression techniques. A novel generalized bridge regression prior that accommodates both sparse and dense settings and is competitive with alternative methods for univariate-response regression is proposed and used in this framework. Two component-wise Markov chain Monte Carlo algorithms are developed for sampling, including a data augmentation algorithm based on a scale mixture of normals representation. Numerical examples demonstrate that the proposed method is competitive with comparable joint mean-covariance models, particularly in estimation of the precision matrix. The method is also used to estimate the 253 by 253 precision matrices of two classes of spectra extracted from images taken by the Hubble Space Telescope. Some interesting structural patterns in the estimates are discussed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00906&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Christina Zhao , Ding Xiang , Galin L. Jones , Adam J. Rothman</name></author><category term="stat.ME" /><summary type="html">A hierarchical Bayesian approach that permits simultaneous inference for the regression coefficient matrix and the error precision (inverse covariance) matrix in the multivariate linear model is proposed. Assuming a natural ordering of the elements of the response, the precision matrix is reparameterized so it can be estimated with univariate-response linear regression techniques. A novel generalized bridge regression prior that accommodates both sparse and dense settings and is competitive with alternative methods for univariate-response regression is proposed and used in this framework. Two component-wise Markov chain Monte Carlo algorithms are developed for sampling, including a data augmentation algorithm based on a scale mixture of normals representation. Numerical examples demonstrate that the proposed method is competitive with comparable joint mean-covariance models, particularly in estimation of the precision matrix. The method is also used to estimate the 253 by 253 precision matrices of two classes of spectra extracted from images taken by the Hubble Space Telescope. Some interesting structural patterns in the estimates are discussed.</summary></entry><entry><title type="html">A Kernel Test for Causal Association via Noise Contrastive Backdoor Adjustment</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AKernelTestforCausalAssociationviaNoiseContrastiveBackdoorAdjustment.html" rel="alternate" type="text/html" title="A Kernel Test for Causal Association via Noise Contrastive Backdoor Adjustment" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AKernelTestforCausalAssociationviaNoiseContrastiveBackdoorAdjustment</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AKernelTestforCausalAssociationviaNoiseContrastiveBackdoorAdjustment.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Causal inference grows increasingly complex as the number of confounders increases. Given treatments $X$, confounders $Z$ and outcomes $Y$, we develop a non-parametric method to test the \textit{do-null} hypothesis $H_0:\; p(y&lt;/td&gt;
      &lt;td&gt;\text{\it do}(X=x))=p(y)$ against the general alternative. Building on the Hilbert Schmidt Independence Criterion (HSIC) for marginal independence testing, we propose backdoor-HSIC (bd-HSIC) and demonstrate that it is calibrated and has power for both binary and continuous treatments under a large number of confounders. Additionally, we establish convergence properties of the estimators of covariance operators used in bd-HSIC. We investigate the advantages and disadvantages of bd-HSIC against parametric tests as well as the importance of using the do-null testing in contrast to marginal independence testing or conditional independence testing. A complete implementation can be found at \hyperlink{https://github.com/MrHuff/kgformula}{\texttt{https://github.com/MrHuff/kgformula}}.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.13226&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Robert Hu, Dino Sejdinovic, Robin J. Evans</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Causal inference grows increasingly complex as the number of confounders increases. Given treatments $X$, confounders $Z$ and outcomes $Y$, we develop a non-parametric method to test the \textit{do-null} hypothesis $H_0:\; p(y \text{\it do}(X=x))=p(y)$ against the general alternative. Building on the Hilbert Schmidt Independence Criterion (HSIC) for marginal independence testing, we propose backdoor-HSIC (bd-HSIC) and demonstrate that it is calibrated and has power for both binary and continuous treatments under a large number of confounders. Additionally, we establish convergence properties of the estimators of covariance operators used in bd-HSIC. We investigate the advantages and disadvantages of bd-HSIC against parametric tests as well as the importance of using the do-null testing in contrast to marginal independence testing or conditional independence testing. A complete implementation can be found at \hyperlink{https://github.com/MrHuff/kgformula}{\texttt{https://github.com/MrHuff/kgformula}}.</summary></entry><entry><title type="html">A Nonparametric Mixed-Effects Mixture Model for Patterns of Clinical Measurements Associated with COVID-19</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ANonparametricMixedEffectsMixtureModelforPatternsofClinicalMeasurementsAssociatedwithCOVID19.html" rel="alternate" type="text/html" title="A Nonparametric Mixed-Effects Mixture Model for Patterns of Clinical Measurements Associated with COVID-19" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ANonparametricMixedEffectsMixtureModelforPatternsofClinicalMeasurementsAssociatedwithCOVID19</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ANonparametricMixedEffectsMixtureModelforPatternsofClinicalMeasurementsAssociatedwithCOVID19.html">&lt;p&gt;Some patients with COVID-19 show changes in signs and symptoms such as temperature and oxygen saturation days before being positively tested for SARS-CoV-2, while others remain asymptomatic. It is important to identify these subgroups and to understand what biological and clinical predictors are related to these subgroups. This information will provide insights into how the immune system may respond differently to infection and can further be used to identify infected individuals. We propose a flexible nonparametric mixed-effects mixture model that identifies risk factors and classifies patients with biological changes. We model the latent probability of biological changes using a logistic regression model and trajectories in the latent groups using smoothing splines. We developed an EM algorithm to maximize the penalized likelihood for estimating all parameters and mean functions. We evaluate our methods by simulations and apply the proposed model to investigate changes in temperature in a cohort of COVID-19-infected hemodialysis patients.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.04140&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xiaoran Ma, Wensheng Guo, Mengyang Gu, Len Usvyat, Peter Kotanko, Yuedong Wang</name></author><category term="stat.ME" /><summary type="html">Some patients with COVID-19 show changes in signs and symptoms such as temperature and oxygen saturation days before being positively tested for SARS-CoV-2, while others remain asymptomatic. It is important to identify these subgroups and to understand what biological and clinical predictors are related to these subgroups. This information will provide insights into how the immune system may respond differently to infection and can further be used to identify infected individuals. We propose a flexible nonparametric mixed-effects mixture model that identifies risk factors and classifies patients with biological changes. We model the latent probability of biological changes using a logistic regression model and trajectories in the latent groups using smoothing splines. We developed an EM algorithm to maximize the penalized likelihood for estimating all parameters and mean functions. We evaluate our methods by simulations and apply the proposed model to investigate changes in temperature in a cohort of COVID-19-infected hemodialysis patients.</summary></entry><entry><title type="html">A Partition-insensitive Parallel Framework for Distributed Model Fitting</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/APartitioninsensitiveParallelFrameworkforDistributedModelFitting.html" rel="alternate" type="text/html" title="A Partition-insensitive Parallel Framework for Distributed Model Fitting" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/APartitioninsensitiveParallelFrameworkforDistributedModelFitting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/APartitioninsensitiveParallelFrameworkforDistributedModelFitting.html">&lt;p&gt;Distributed model fitting refers to the process of fitting a mathematical or statistical model to the data using distributed computing resources, such that computing tasks are divided among multiple interconnected computers or nodes, often organized in a cluster or network. Most of the existing methods for distributed model fitting are to formulate it in a consensus optimization problem, and then build up algorithms based on the alternating direction method of multipliers (ADMM). This paper introduces a novel parallel framework for achieving a distributed model fitting. In contrast to previous consensus frameworks, the introduced parallel framework offers two notable advantages. Firstly, it exhibits insensitivity to sample partitioning, meaning that the solution of the algorithm remains unaffected by variations in the number of slave nodes or/and the amount of data each node carries. Secondly, fewer variables are required to be updated at each iteration, so that the proposed parallel framework performs in a more succinct and efficient way, and adapts to high-dimensional data. In addition, we prove that the algorithms under the new parallel framework have a worst-case linear convergence rate in theory. Numerical experiments confirm the generality, robustness, and accuracy of our proposed parallel framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00703&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xiaofei Wu, Rongmei Liang, Fabio Roli, Marcello Pelillo, Jing Yuan</name></author><category term="stat.CO" /><summary type="html">Distributed model fitting refers to the process of fitting a mathematical or statistical model to the data using distributed computing resources, such that computing tasks are divided among multiple interconnected computers or nodes, often organized in a cluster or network. Most of the existing methods for distributed model fitting are to formulate it in a consensus optimization problem, and then build up algorithms based on the alternating direction method of multipliers (ADMM). This paper introduces a novel parallel framework for achieving a distributed model fitting. In contrast to previous consensus frameworks, the introduced parallel framework offers two notable advantages. Firstly, it exhibits insensitivity to sample partitioning, meaning that the solution of the algorithm remains unaffected by variations in the number of slave nodes or/and the amount of data each node carries. Secondly, fewer variables are required to be updated at each iteration, so that the proposed parallel framework performs in a more succinct and efficient way, and adapts to high-dimensional data. In addition, we prove that the algorithms under the new parallel framework have a worst-case linear convergence rate in theory. Numerical experiments confirm the generality, robustness, and accuracy of our proposed parallel framework.</summary></entry><entry><title type="html">A Seamless Phase II/III Design with Dose Optimization for Oncology Drug Development</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ASeamlessPhaseIIIIIDesignwithDoseOptimizationforOncologyDrugDevelopment.html" rel="alternate" type="text/html" title="A Seamless Phase II/III Design with Dose Optimization for Oncology Drug Development" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ASeamlessPhaseIIIIIDesignwithDoseOptimizationforOncologyDrugDevelopment</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ASeamlessPhaseIIIIIDesignwithDoseOptimizationforOncologyDrugDevelopment.html">&lt;p&gt;The US FDA’s Project Optimus initiative that emphasizes dose optimization prior to marketing approval represents a pivotal shift in oncology drug development. It has a ripple effect for rethinking what changes may be made to conventional pivotal trial designs to incorporate a dose optimization component. Aligned with this initiative, we propose a novel Seamless Phase II/III Design with Dose Optimization (SDDO framework). The proposed design starts with dose optimization in a randomized setting, leading to an interim analysis focused on optimal dose selection, trial continuation decisions, and sample size re-estimation (SSR). Based on the decision at interim analysis, patient enrollment continues for both the selected dose arm and control arm, and the significance of treatment effects will be determined at final analysis. The SDDO framework offers increased flexibility and cost-efficiency through sample size adjustment, while stringently controlling the Type I error. This proposed design also facilitates both Accelerated Approval (AA) and regular approval in a “one-trial” approach. Extensive simulation studies confirm that our design reliably identifies the optimal dosage and makes preferable decisions with a reduced sample size while retaining statistical power.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00196&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuhan Li, Yiding Zhang, Gu Mi, Ji Lin</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The US FDA’s Project Optimus initiative that emphasizes dose optimization prior to marketing approval represents a pivotal shift in oncology drug development. It has a ripple effect for rethinking what changes may be made to conventional pivotal trial designs to incorporate a dose optimization component. Aligned with this initiative, we propose a novel Seamless Phase II/III Design with Dose Optimization (SDDO framework). The proposed design starts with dose optimization in a randomized setting, leading to an interim analysis focused on optimal dose selection, trial continuation decisions, and sample size re-estimation (SSR). Based on the decision at interim analysis, patient enrollment continues for both the selected dose arm and control arm, and the significance of treatment effects will be determined at final analysis. The SDDO framework offers increased flexibility and cost-efficiency through sample size adjustment, while stringently controlling the Type I error. This proposed design also facilitates both Accelerated Approval (AA) and regular approval in a “one-trial” approach. Extensive simulation studies confirm that our design reliably identifies the optimal dosage and makes preferable decisions with a reduced sample size while retaining statistical power.</summary></entry><entry><title type="html">A Stochastic-Geometrical Framework for Object Pose Estimation based on Mixture Models Avoiding the Correspondence Problem</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AStochasticGeometricalFrameworkforObjectPoseEstimationbasedonMixtureModelsAvoidingtheCorrespondenceProblem.html" rel="alternate" type="text/html" title="A Stochastic-Geometrical Framework for Object Pose Estimation based on Mixture Models Avoiding the Correspondence Problem" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AStochasticGeometricalFrameworkforObjectPoseEstimationbasedonMixtureModelsAvoidingtheCorrespondenceProblem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AStochasticGeometricalFrameworkforObjectPoseEstimationbasedonMixtureModelsAvoidingtheCorrespondenceProblem.html">&lt;p&gt;Background: Pose estimation of rigid objects is a practical challenge in optical metrology and computer vision. This paper presents a novel stochastic-geometrical modeling framework for object pose estimation based on observing multiple feature points.
  Methods: This framework utilizes mixture models for feature point densities in object space and for interpreting real measurements. Advantages are the avoidance to resolve individual feature correspondences and to incorporate correct stochastic dependencies in multi-view applications. First, the general modeling framework is presented, second, a general algorithm for pose estimation is derived, and third, two example models (camera and lateration setup) are presented.
  Results: Numerical experiments show the effectiveness of this modeling and general algorithm by presenting four simulation scenarios for three observation systems, including the dependence on measurement resolution, object deformations and measurement noise. Probabilistic modeling utilizing mixture models shows the potential for accurate and robust pose estimations while avoiding the correspondence problem.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.18107&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wolfgang Hoegele</name></author><category term="stat.AP" /><summary type="html">Background: Pose estimation of rigid objects is a practical challenge in optical metrology and computer vision. This paper presents a novel stochastic-geometrical modeling framework for object pose estimation based on observing multiple feature points. Methods: This framework utilizes mixture models for feature point densities in object space and for interpreting real measurements. Advantages are the avoidance to resolve individual feature correspondences and to incorporate correct stochastic dependencies in multi-view applications. First, the general modeling framework is presented, second, a general algorithm for pose estimation is derived, and third, two example models (camera and lateration setup) are presented. Results: Numerical experiments show the effectiveness of this modeling and general algorithm by presenting four simulation scenarios for three observation systems, including the dependence on measurement resolution, object deformations and measurement noise. Probabilistic modeling utilizing mixture models shows the potential for accurate and robust pose estimations while avoiding the correspondence problem.</summary></entry><entry><title type="html">A Tutorial on Doubly Robust Learning for Causal Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ATutorialonDoublyRobustLearningforCausalInference.html" rel="alternate" type="text/html" title="A Tutorial on Doubly Robust Learning for Causal Inference" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ATutorialonDoublyRobustLearningforCausalInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ATutorialonDoublyRobustLearningforCausalInference.html">&lt;p&gt;Doubly robust learning offers a robust framework for causal inference from observational data by integrating propensity score and outcome modeling. Despite its theoretical appeal, practical adoption remains limited due to perceived complexity and inaccessible software. This tutorial aims to demystify doubly robust methods and demonstrate their application using the EconML package. We provide an introduction to causal inference, discuss the principles of outcome modeling and propensity scores, and illustrate the doubly robust approach through simulated case studies. By simplifying the methodology and offering practical coding examples, we intend to make doubly robust learning accessible to researchers and practitioners in data science and statistics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00853&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hlynur Daví{\dh} Hlynsson</name></author><category term="stat.ML," /><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Doubly robust learning offers a robust framework for causal inference from observational data by integrating propensity score and outcome modeling. Despite its theoretical appeal, practical adoption remains limited due to perceived complexity and inaccessible software. This tutorial aims to demystify doubly robust methods and demonstrate their application using the EconML package. We provide an introduction to causal inference, discuss the principles of outcome modeling and propensity scores, and illustrate the doubly robust approach through simulated case studies. By simplifying the methodology and offering practical coding examples, we intend to make doubly robust learning accessible to researchers and practitioners in data science and statistics.</summary></entry><entry><title type="html">A class of sequential multi-hypothesis tests</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Aclassofsequentialmultihypothesistests.html" rel="alternate" type="text/html" title="A class of sequential multi-hypothesis tests" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Aclassofsequentialmultihypothesistests</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Aclassofsequentialmultihypothesistests.html">&lt;p&gt;In this paper, we deal with sequential testing of multiple hypotheses. In the general scheme of construction of optimal tests based on the backward induction, we propose a modification which provides a simplified (generally speaking, suboptimal) version of the optimal test, for any particular criterion of optimization. We call this DBC version (the one with Dropped Backward Control) of the optimal test. In particular, for the case of two simple hypotheses, dropping backward control in the Bayesian test produces the classical sequential probability ratio test (SPRT). Similarly, dropping backward control in the modified Kiefer-Weiss solutions produces Lorden’s 2-SPRTs .
  In the case of more than two hypotheses, we obtain in this way new classes of sequential multi-hypothesis tests, and investigate their properties. The efficiency of the DBC-tests is evaluated with respect to the optimal Bayesian multi-hypothesis test and with respect to the matrix sequential probability ratio test (MSPRT) by Armitage. In a multihypothesis variant of the Kiefer-Weiss problem for binomial proportions the performance of the DBC-test is numerically compared with that of the exact solution. In a model of normal observations with a linear trend, the performance of of the DBC-test is numerically compared with that of the MSPRT. Some other numerical examples are presented.
  In all the cases the proposed tests exhibit a very high efficiency with respect to the optimal tests (more than 99.3\% when sampling from Bernoulli populations) and/or with respect to the MSPRT (even outperforming the latter in some scenarios).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00930&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrey Novikov</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">In this paper, we deal with sequential testing of multiple hypotheses. In the general scheme of construction of optimal tests based on the backward induction, we propose a modification which provides a simplified (generally speaking, suboptimal) version of the optimal test, for any particular criterion of optimization. We call this DBC version (the one with Dropped Backward Control) of the optimal test. In particular, for the case of two simple hypotheses, dropping backward control in the Bayesian test produces the classical sequential probability ratio test (SPRT). Similarly, dropping backward control in the modified Kiefer-Weiss solutions produces Lorden’s 2-SPRTs . In the case of more than two hypotheses, we obtain in this way new classes of sequential multi-hypothesis tests, and investigate their properties. The efficiency of the DBC-tests is evaluated with respect to the optimal Bayesian multi-hypothesis test and with respect to the matrix sequential probability ratio test (MSPRT) by Armitage. In a multihypothesis variant of the Kiefer-Weiss problem for binomial proportions the performance of the DBC-test is numerically compared with that of the exact solution. In a model of normal observations with a linear trend, the performance of of the DBC-test is numerically compared with that of the MSPRT. Some other numerical examples are presented. In all the cases the proposed tests exhibit a very high efficiency with respect to the optimal tests (more than 99.3\% when sampling from Bernoulli populations) and/or with respect to the MSPRT (even outperforming the latter in some scenarios).</summary></entry><entry><title type="html">Adaptive Penalized Likelihood method for Markov Chains</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AdaptivePenalizedLikelihoodmethodforMarkovChains.html" rel="alternate" type="text/html" title="Adaptive Penalized Likelihood method for Markov Chains" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AdaptivePenalizedLikelihoodmethodforMarkovChains</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AdaptivePenalizedLikelihoodmethodforMarkovChains.html">&lt;p&gt;Maximum Likelihood Estimation (MLE) and Likelihood Ratio Test (LRT) are widely used methods for estimating the transition probability matrix in Markov chains and identifying significant relationships between transitions, such as equality. However, the estimated transition probability matrix derived from MLE lacks accuracy compared to the real one, and LRT is inefficient in high-dimensional Markov chains. In this study, we extended the adaptive Lasso technique from linear models to Markov chains and proposed a novel model by applying penalized maximum likelihood estimation to optimize the estimation of the transition probability matrix. Meanwhile, we demonstrated that the new model enjoys oracle properties, which means the estimated transition probability matrix has the same performance as the real one when given. Simulations show that our new method behave very well overall in comparison with various competitors. Real data analysis further convince the value of our proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00322&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yining Zhou, Ming Gao, Yiting Chen, Xiaoping Shi</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Maximum Likelihood Estimation (MLE) and Likelihood Ratio Test (LRT) are widely used methods for estimating the transition probability matrix in Markov chains and identifying significant relationships between transitions, such as equality. However, the estimated transition probability matrix derived from MLE lacks accuracy compared to the real one, and LRT is inefficient in high-dimensional Markov chains. In this study, we extended the adaptive Lasso technique from linear models to Markov chains and proposed a novel model by applying penalized maximum likelihood estimation to optimize the estimation of the transition probability matrix. Meanwhile, we demonstrated that the new model enjoys oracle properties, which means the estimated transition probability matrix has the same performance as the real one when given. Simulations show that our new method behave very well overall in comparison with various competitors. Real data analysis further convince the value of our proposed method.</summary></entry><entry><title type="html">Aging modeling and lifetime prediction of a proton exchange membrane fuel cell using an extended Kalman filter</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AgingmodelingandlifetimepredictionofaprotonexchangemembranefuelcellusinganextendedKalmanfilter.html" rel="alternate" type="text/html" title="Aging modeling and lifetime prediction of a proton exchange membrane fuel cell using an extended Kalman filter" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AgingmodelingandlifetimepredictionofaprotonexchangemembranefuelcellusinganextendedKalmanfilter</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AgingmodelingandlifetimepredictionofaprotonexchangemembranefuelcellusinganextendedKalmanfilter.html">&lt;p&gt;This article presents a methodology that aims to model and to provide predictive capabilities for the lifetime of Proton Exchange Membrane Fuel Cell (PEMFC). The approach integrates parametric identification, dynamic modeling, and Extended Kalman Filtering (EKF). The foundation is laid with the creation of a representative aging database, emphasizing specific operating conditions. Electrochemical behavior is characterized through the identification of critical parameters. The methodology extends to capture the temporal evolution of the identified parameters. We also address challenges posed by the limiting current density through a differential analysis-based modeling technique and the detection of breakpoints. This approach, involving Monte Carlo simulations, is coupled with an EKF for predicting voltage degradation. The Remaining Useful Life (RUL) is also estimated. The results show that our approach accurately predicts future voltage and RUL with very low relative errors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01259&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Serigne Daouda Pene, Antoine Picot, Fabrice Gamboa, Nicolas Savy, Christophe Turpin, Amine Jaafar</name></author><category term="stat.AP," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">This article presents a methodology that aims to model and to provide predictive capabilities for the lifetime of Proton Exchange Membrane Fuel Cell (PEMFC). The approach integrates parametric identification, dynamic modeling, and Extended Kalman Filtering (EKF). The foundation is laid with the creation of a representative aging database, emphasizing specific operating conditions. Electrochemical behavior is characterized through the identification of critical parameters. The methodology extends to capture the temporal evolution of the identified parameters. We also address challenges posed by the limiting current density through a differential analysis-based modeling technique and the detection of breakpoints. This approach, involving Monte Carlo simulations, is coupled with an EKF for predicting voltage degradation. The Remaining Useful Life (RUL) is also estimated. The results show that our approach accurately predicts future voltage and RUL with very low relative errors.</summary></entry><entry><title type="html">Analyzing trends for agricultural decision support system using twitter data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Analyzingtrendsforagriculturaldecisionsupportsystemusingtwitterdata.html" rel="alternate" type="text/html" title="Analyzing trends for agricultural decision support system using twitter data" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Analyzingtrendsforagriculturaldecisionsupportsystemusingtwitterdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Analyzingtrendsforagriculturaldecisionsupportsystemusingtwitterdata.html">&lt;p&gt;The trends and reactions of the general public towards global events can be analyzed using data from social platforms, including Twitter. The number of tweets has been reported to help detect variations in communication traffic within subsets like countries, age groups and industries. Similarly, publicly accessible data and (in particular) data from social media about agricultural issues provide a great opportunity for obtaining instantaneous snapshots of farmer opinions and a method to track changes in opinion through temporal analysis. In this paper we hypothesize that the presence of keywords like precision agriculture, digital agriculture, Internet of Things (IoT), BigData, remote sensing, GPS, etc., in tweets could serve as an indicator of discussions centered around interest in modern farming practices. We extracted relevant tweets using keywords such as IoT, BigData and Geographical Information System (GIS), and then analyzed their geographical origin and frequency of their mention. We analyzed the Twitter data for the period of 1st -11th January 2018 to understand these trends and the factors affecting them. These factors, such as special events, projects, biogeography, etc., were further analyzed using tweet sources and trending hashtags from the database. The regions with the highest interest in the keywords were United States, Egypt, Brazil, Japan and China. A comparison of frequency of keywords revealed IoT as the most tweeted word (77.6%) in the downloaded data. The most used language was English followed by Spanish, Japanese and French. Periodical tweets on IoT from an account handled by IoT project on Twitter and Seminars on IoT in January in Santa Catarina (Brazil) were found to be the underlying factors for the observed trends.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00577&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sneha Jha, Dharmendra Saraswat, Mark D. Ward</name></author><category term="stat.AP" /><summary type="html">The trends and reactions of the general public towards global events can be analyzed using data from social platforms, including Twitter. The number of tweets has been reported to help detect variations in communication traffic within subsets like countries, age groups and industries. Similarly, publicly accessible data and (in particular) data from social media about agricultural issues provide a great opportunity for obtaining instantaneous snapshots of farmer opinions and a method to track changes in opinion through temporal analysis. In this paper we hypothesize that the presence of keywords like precision agriculture, digital agriculture, Internet of Things (IoT), BigData, remote sensing, GPS, etc., in tweets could serve as an indicator of discussions centered around interest in modern farming practices. We extracted relevant tweets using keywords such as IoT, BigData and Geographical Information System (GIS), and then analyzed their geographical origin and frequency of their mention. We analyzed the Twitter data for the period of 1st -11th January 2018 to understand these trends and the factors affecting them. These factors, such as special events, projects, biogeography, etc., were further analyzed using tweet sources and trending hashtags from the database. The regions with the highest interest in the keywords were United States, Egypt, Brazil, Japan and China. A comparison of frequency of keywords revealed IoT as the most tweeted word (77.6%) in the downloaded data. The most used language was English followed by Spanish, Japanese and French. Periodical tweets on IoT from an account handled by IoT project on Twitter and Seminars on IoT in January in Santa Catarina (Brazil) were found to be the underlying factors for the observed trends.</summary></entry><entry><title type="html">Approaches to biological species delimitation based on genetic and spatial dissimilarity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Approachestobiologicalspeciesdelimitationbasedongeneticandspatialdissimilarity.html" rel="alternate" type="text/html" title="Approaches to biological species delimitation based on genetic and spatial dissimilarity" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Approachestobiologicalspeciesdelimitationbasedongeneticandspatialdissimilarity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Approachestobiologicalspeciesdelimitationbasedongeneticandspatialdissimilarity.html">&lt;p&gt;The delimitation of biological species, i.e., deciding which individuals belong to the same species and whether and how many different species are represented in a data set, is key to the conservation of biodiversity. Much existing work uses only genetic data for species delimitation, often employing some kind of cluster analysis. This can be misleading, because geographically distant groups of individuals can be genetically quite different even if they belong to the same species. We investigate the problem of testing whether two potentially separated groups of individuals can belong to a single species or not based on genetic and spatial data. Existing methods such as the partial Mantel test and jackknife-based distance-distance regression are considered. New approaches, i.e., an adaptation of a mixed effects model, a bootstrap approach, and a jackknife version of partial Mantel, are proposed. All these methods address the issue that distance data violate the independence assumption for standard inference regarding correlation and regression; a standard linear regression is also considered. The approaches are compared on simulated meta-populations generated with SLiM and GSpace - two software packages that can simulate spatially-explicit genetic data at an individual level. Simulations show that the new jackknife version of the partial Mantel test provides a good compromise between power and respecting the nominal type I error rate. Mixed-effects models have larger power than jackknife-based methods, but tend to display type I error rates slightly above the significance level. An application on brassy ringlets concludes the paper.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.12126&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Gabriele d&apos;Angella, Christian Hennig</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">The delimitation of biological species, i.e., deciding which individuals belong to the same species and whether and how many different species are represented in a data set, is key to the conservation of biodiversity. Much existing work uses only genetic data for species delimitation, often employing some kind of cluster analysis. This can be misleading, because geographically distant groups of individuals can be genetically quite different even if they belong to the same species. We investigate the problem of testing whether two potentially separated groups of individuals can belong to a single species or not based on genetic and spatial data. Existing methods such as the partial Mantel test and jackknife-based distance-distance regression are considered. New approaches, i.e., an adaptation of a mixed effects model, a bootstrap approach, and a jackknife version of partial Mantel, are proposed. All these methods address the issue that distance data violate the independence assumption for standard inference regarding correlation and regression; a standard linear regression is also considered. The approaches are compared on simulated meta-populations generated with SLiM and GSpace - two software packages that can simulate spatially-explicit genetic data at an individual level. Simulations show that the new jackknife version of the partial Mantel test provides a good compromise between power and respecting the nominal type I error rate. Mixed-effects models have larger power than jackknife-based methods, but tend to display type I error rates slightly above the significance level. An application on brassy ringlets concludes the paper.</summary></entry><entry><title type="html">Arbitrary Length Generalization for Addition</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ArbitraryLengthGeneralizationforAddition.html" rel="alternate" type="text/html" title="Arbitrary Length Generalization for Addition" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ArbitraryLengthGeneralizationforAddition</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ArbitraryLengthGeneralizationforAddition.html">&lt;p&gt;This paper introduces a novel training methodology that enables a small Transformer model to generalize the addition of two-digit numbers to numbers with unseen lengths of digits. The proposed approach employs an autoregressive generation technique, processing from right to left, which mimics a common manual method for adding large numbers. To the best of my knowledge, this methodology has not been previously explored in the literature. All results are reproducible, and the corresponding R code is available at: \url{https://github.com/AGPatriota/ALGA-R/}.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00075&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexandre Galvao Patriota</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">This paper introduces a novel training methodology that enables a small Transformer model to generalize the addition of two-digit numbers to numbers with unseen lengths of digits. The proposed approach employs an autoregressive generation technique, processing from right to left, which mimics a common manual method for adding large numbers. To the best of my knowledge, this methodology has not been previously explored in the literature. All results are reproducible, and the corresponding R code is available at: \url{https://github.com/AGPatriota/ALGA-R/}.</summary></entry><entry><title type="html">Assessing survival models by interval testing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Assessingsurvivalmodelsbyintervaltesting.html" rel="alternate" type="text/html" title="Assessing survival models by interval testing" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Assessingsurvivalmodelsbyintervaltesting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Assessingsurvivalmodelsbyintervaltesting.html">&lt;p&gt;When considering many survival models, decisions become more challenging in health economic evaluation. In this paper, we present a set of methods to assist with selecting the most appropriate survival models. The methods highlight areas of particularly poor fit. Furthermore, plots and overall p-values provide guidance on whether a survival model should be rejected or not.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00730&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ben Lee</name></author><category term="stat.ME" /><summary type="html">When considering many survival models, decisions become more challenging in health economic evaluation. In this paper, we present a set of methods to assist with selecting the most appropriate survival models. The methods highlight areas of particularly poor fit. Furthermore, plots and overall p-values provide guidance on whether a survival model should be rejected or not.</summary></entry><entry><title type="html">Assessment of Case Influence in the Lasso with a Case-weight Adjusted Solution Path</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AssessmentofCaseInfluenceintheLassowithaCaseweightAdjustedSolutionPath.html" rel="alternate" type="text/html" title="Assessment of Case Influence in the Lasso with a Case-weight Adjusted Solution Path" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AssessmentofCaseInfluenceintheLassowithaCaseweightAdjustedSolutionPath</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AssessmentofCaseInfluenceintheLassowithaCaseweightAdjustedSolutionPath.html">&lt;p&gt;We study case influence in the Lasso regression using Cook’s distance which measures overall change in the fitted values when one observation is deleted. Unlike in ordinary least squares regression, the estimated coefficients in the Lasso do not have a closed form due to the nondifferentiability of the $\ell_1$ penalty, and neither does Cook’s distance. To find the case-deleted Lasso solution without refitting the model, we approach it from the full data solution by introducing a weight parameter ranging from 1 to 0 and generating a solution path indexed by this parameter. We show that the solution path is piecewise linear with respect to a simple function of the weight parameter under a fixed penalty. The resulting case influence is a function of the penalty and weight, and it becomes Cook’s distance when the weight is 0. As the penalty parameter changes, selected variables change, and the magnitude of Cook’s distance for the same data point may vary with the subset of variables selected. In addition, we introduce a case influence graph to visualize how the contribution of each data point changes with the penalty parameter. From the graph, we can identify influential points at different penalty levels and make modeling decisions accordingly. Moreover, we find that case influence graphs exhibit different patterns between underfitting and overfitting phases, which can provide additional information for model selection.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00493&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhenbang Jiao, Yoonkyung Lee</name></author><category term="stat.ME" /><summary type="html">We study case influence in the Lasso regression using Cook’s distance which measures overall change in the fitted values when one observation is deleted. Unlike in ordinary least squares regression, the estimated coefficients in the Lasso do not have a closed form due to the nondifferentiability of the $\ell_1$ penalty, and neither does Cook’s distance. To find the case-deleted Lasso solution without refitting the model, we approach it from the full data solution by introducing a weight parameter ranging from 1 to 0 and generating a solution path indexed by this parameter. We show that the solution path is piecewise linear with respect to a simple function of the weight parameter under a fixed penalty. The resulting case influence is a function of the penalty and weight, and it becomes Cook’s distance when the weight is 0. As the penalty parameter changes, selected variables change, and the magnitude of Cook’s distance for the same data point may vary with the subset of variables selected. In addition, we introduce a case influence graph to visualize how the contribution of each data point changes with the penalty parameter. From the graph, we can identify influential points at different penalty levels and make modeling decisions accordingly. Moreover, we find that case influence graphs exhibit different patterns between underfitting and overfitting phases, which can provide additional information for model selection.</summary></entry><entry><title type="html">Bayesian Joint Additive Factor Models for Multiview Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/BayesianJointAdditiveFactorModelsforMultiviewLearning.html" rel="alternate" type="text/html" title="Bayesian Joint Additive Factor Models for Multiview Learning" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/BayesianJointAdditiveFactorModelsforMultiviewLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/BayesianJointAdditiveFactorModelsforMultiviewLearning.html">&lt;p&gt;It is increasingly common in a wide variety of applied settings to collect data of multiple different types on the same set of samples. Our particular focus in this article is on studying relationships between such multiview features and responses. A motivating application arises in the context of precision medicine where multi-omics data are collected to correlate with clinical outcomes. It is of interest to infer dependence within and across views while combining multimodal information to improve the prediction of outcomes. The signal-to-noise ratio can vary substantially across views, motivating more nuanced statistical tools beyond standard late and early fusion. This challenge comes with the need to preserve interpretability, select features, and obtain accurate uncertainty quantification. We propose a joint additive factor regression model (JAFAR) with a structured additive design, accounting for shared and view-specific components. We ensure identifiability via a novel dependent cumulative shrinkage process (D-CUSP) prior. We provide an efficient implementation via a partially collapsed Gibbs sampler and extend our approach to allow flexible feature and outcome distributions. Prediction of time-to-labor onset from immunome, metabolome, and proteome data illustrates performance gains against state-of-the-art competitors. Our open-source software (R package) is available at https://github.com/niccoloanceschi/jafar.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00778&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Niccolo Anceschi, Federico Ferrari, David B. Dunson, Himel Mallick</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">It is increasingly common in a wide variety of applied settings to collect data of multiple different types on the same set of samples. Our particular focus in this article is on studying relationships between such multiview features and responses. A motivating application arises in the context of precision medicine where multi-omics data are collected to correlate with clinical outcomes. It is of interest to infer dependence within and across views while combining multimodal information to improve the prediction of outcomes. The signal-to-noise ratio can vary substantially across views, motivating more nuanced statistical tools beyond standard late and early fusion. This challenge comes with the need to preserve interpretability, select features, and obtain accurate uncertainty quantification. We propose a joint additive factor regression model (JAFAR) with a structured additive design, accounting for shared and view-specific components. We ensure identifiability via a novel dependent cumulative shrinkage process (D-CUSP) prior. We provide an efficient implementation via a partially collapsed Gibbs sampler and extend our approach to allow flexible feature and outcome distributions. Prediction of time-to-labor onset from immunome, metabolome, and proteome data illustrates performance gains against state-of-the-art competitors. Our open-source software (R package) is available at https://github.com/niccoloanceschi/jafar.</summary></entry><entry><title type="html">Bayesian clustering of high-dimensional data via latent repulsive mixtures</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Bayesianclusteringofhighdimensionaldatavialatentrepulsivemixtures.html" rel="alternate" type="text/html" title="Bayesian clustering of high-dimensional data via latent repulsive mixtures" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Bayesianclusteringofhighdimensionaldatavialatentrepulsivemixtures</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Bayesianclusteringofhighdimensionaldatavialatentrepulsivemixtures.html">&lt;p&gt;Model-based clustering of moderate or large dimensional data is notoriously difficult. We propose a model for simultaneous dimensionality reduction and clustering by assuming a mixture model for a set of latent scores, which are then linked to the observations via a Gaussian latent factor model. This approach was recently investigated by Chandra et al. (2023). The authors use a factor-analytic representation and assume a mixture model for the latent factors. However, performance can deteriorate in the presence of model misspecification. Assuming a repulsive point process prior for the component-specific means of the mixture for the latent scores is shown to yield a more robust model that outperforms the standard mixture model for the latent factors in several simulated scenarios. The repulsive point process must be anisotropic to favor well-separated clusters of data, and its density should be tractable for efficient posterior inference. We address these issues by proposing a general construction for anisotropic determinantal point processes. We illustrate our model in simulations as well as a plant species co-occurrence dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.02438&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lorenzo Ghilotti, Mario Beraha, Alessandra Guglielmi</name></author><category term="stat.ME" /><summary type="html">Model-based clustering of moderate or large dimensional data is notoriously difficult. We propose a model for simultaneous dimensionality reduction and clustering by assuming a mixture model for a set of latent scores, which are then linked to the observations via a Gaussian latent factor model. This approach was recently investigated by Chandra et al. (2023). The authors use a factor-analytic representation and assume a mixture model for the latent factors. However, performance can deteriorate in the presence of model misspecification. Assuming a repulsive point process prior for the component-specific means of the mixture for the latent scores is shown to yield a more robust model that outperforms the standard mixture model for the latent factors in several simulated scenarios. The repulsive point process must be anisotropic to favor well-separated clusters of data, and its density should be tractable for efficient posterior inference. We address these issues by proposing a general construction for anisotropic determinantal point processes. We illustrate our model in simulations as well as a plant species co-occurrence dataset.</summary></entry><entry><title type="html">Bayesian compositional regression with flexible microbiome feature aggregation and selection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Bayesiancompositionalregressionwithflexiblemicrobiomefeatureaggregationandselection.html" rel="alternate" type="text/html" title="Bayesian compositional regression with flexible microbiome feature aggregation and selection" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Bayesiancompositionalregressionwithflexiblemicrobiomefeatureaggregationandselection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Bayesiancompositionalregressionwithflexiblemicrobiomefeatureaggregationandselection.html">&lt;p&gt;Ongoing advances in microbiome profiling have allowed unprecedented insights into the molecular activities of microbial communities. This has fueled a strong scientific interest in understanding the critical role the microbiome plays in governing human health, by identifying microbial features associated with clinical outcomes of interest. Several aspects of microbiome data limit the applicability of existing variable selection approaches. In particular, microbiome data are high-dimensional, extremely sparse, and compositional. Importantly, many of the observed features, although categorized as different taxa, may play related functional roles. To address these challenges, we propose a novel compositional regression approach that leverages the data-adaptive clustering and variable selection properties of the spiked Dirichlet process to identify taxa that exhibit similar functional roles. Our proposed method, Bayesian Regression with Agglomerated Compositional Effects using a dirichLET process (BRACElet), enables the identification of a sparse set of features with shared impacts on the outcome, facilitating dimension reduction and model interpretation. We demonstrate that BRACElet outperforms existing approaches for microbiome variable selection through simulation studies and an application elucidating the impact of oral microbiome composition on insulin resistance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01557&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Satabdi Saha, Liangliang Zhang, Kim-Anh Do, Christine B. Peterson</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Ongoing advances in microbiome profiling have allowed unprecedented insights into the molecular activities of microbial communities. This has fueled a strong scientific interest in understanding the critical role the microbiome plays in governing human health, by identifying microbial features associated with clinical outcomes of interest. Several aspects of microbiome data limit the applicability of existing variable selection approaches. In particular, microbiome data are high-dimensional, extremely sparse, and compositional. Importantly, many of the observed features, although categorized as different taxa, may play related functional roles. To address these challenges, we propose a novel compositional regression approach that leverages the data-adaptive clustering and variable selection properties of the spiked Dirichlet process to identify taxa that exhibit similar functional roles. Our proposed method, Bayesian Regression with Agglomerated Compositional Effects using a dirichLET process (BRACElet), enables the identification of a sparse set of features with shared impacts on the outcome, facilitating dimension reduction and model interpretation. We demonstrate that BRACElet outperforms existing approaches for microbiome variable selection through simulation studies and an application elucidating the impact of oral microbiome composition on insulin resistance.</summary></entry><entry><title type="html">Causal Contrastive Learning for Counterfactual Regression Over Time</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/CausalContrastiveLearningforCounterfactualRegressionOverTime.html" rel="alternate" type="text/html" title="Causal Contrastive Learning for Counterfactual Regression Over Time" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/CausalContrastiveLearningforCounterfactualRegressionOverTime</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/CausalContrastiveLearningforCounterfactualRegressionOverTime.html">&lt;p&gt;Estimating treatment effects over time holds significance in various domains, including precision medicine, epidemiology, economy, and marketing. This paper introduces a unique approach to counterfactual regression over time, emphasizing long-term predictions. Distinguishing itself from existing models like Causal Transformer, our approach highlights the efficacy of employing RNNs for long-term forecasting, complemented by Contrastive Predictive Coding (CPC) and Information Maximization (InfoMax). Emphasizing efficiency, we avoid the need for computationally expensive transformers. Leveraging CPC, our method captures long-term dependencies in the presence of time-varying confounders. Notably, recent models have disregarded the importance of invertible representation, compromising identification assumptions. To remedy this, we employ the InfoMax principle, maximizing a lower bound of mutual information between sequence data and its representation. Our method achieves state-of-the-art counterfactual estimation results using both synthetic and real-world data, marking the pioneering incorporation of Contrastive Predictive Encoding in causal inference.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00535&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mouad El Bouchattaoui, Myriam Tami, Benoit Lepetit, Paul-Henry Cournède</name></author><category term="stat.ME" /><summary type="html">Estimating treatment effects over time holds significance in various domains, including precision medicine, epidemiology, economy, and marketing. This paper introduces a unique approach to counterfactual regression over time, emphasizing long-term predictions. Distinguishing itself from existing models like Causal Transformer, our approach highlights the efficacy of employing RNNs for long-term forecasting, complemented by Contrastive Predictive Coding (CPC) and Information Maximization (InfoMax). Emphasizing efficiency, we avoid the need for computationally expensive transformers. Leveraging CPC, our method captures long-term dependencies in the presence of time-varying confounders. Notably, recent models have disregarded the importance of invertible representation, compromising identification assumptions. To remedy this, we employ the InfoMax principle, maximizing a lower bound of mutual information between sequence data and its representation. Our method achieves state-of-the-art counterfactual estimation results using both synthetic and real-world data, marking the pioneering incorporation of Contrastive Predictive Encoding in causal inference.</summary></entry><entry><title type="html">Combining Experimental and Historical Data for Policy Evaluation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/CombiningExperimentalandHistoricalDataforPolicyEvaluation.html" rel="alternate" type="text/html" title="Combining Experimental and Historical Data for Policy Evaluation" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/CombiningExperimentalandHistoricalDataforPolicyEvaluation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/CombiningExperimentalandHistoricalDataforPolicyEvaluation.html">&lt;p&gt;This paper studies policy evaluation with multiple data sources, especially in scenarios that involve one experimental dataset with two arms, complemented by a historical dataset generated under a single control arm. We propose novel data integration methods that linearly integrate base policy value estimators constructed based on the experimental and historical data, with weights optimized to minimize the mean square error (MSE) of the resulting combined estimator. We further apply the pessimistic principle to obtain more robust estimators, and extend these developments to sequential decision making. Theoretically, we establish non-asymptotic error bounds for the MSEs of our proposed estimators, and derive their oracle, efficiency and robustness properties across a broad spectrum of reward shift scenarios. Numerical experiments and real-data-based analyses from a ridesharing company demonstrate the superior performance of the proposed estimators.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00317&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ting Li, Chengchun Shi, Qianglin Wen, Yang Sui, Yongli Qin, Chunbo Lai, Hongtu Zhu</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">This paper studies policy evaluation with multiple data sources, especially in scenarios that involve one experimental dataset with two arms, complemented by a historical dataset generated under a single control arm. We propose novel data integration methods that linearly integrate base policy value estimators constructed based on the experimental and historical data, with weights optimized to minimize the mean square error (MSE) of the resulting combined estimator. We further apply the pessimistic principle to obtain more robust estimators, and extend these developments to sequential decision making. Theoretically, we establish non-asymptotic error bounds for the MSEs of our proposed estimators, and derive their oracle, efficiency and robustness properties across a broad spectrum of reward shift scenarios. Numerical experiments and real-data-based analyses from a ridesharing company demonstrate the superior performance of the proposed estimators.</summary></entry><entry><title type="html">Comparison of Point Process Learning and its special case Takacs-Fiksel estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ComparisonofPointProcessLearninganditsspecialcaseTakacsFikselestimation.html" rel="alternate" type="text/html" title="Comparison of Point Process Learning and its special case Takacs-Fiksel estimation" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ComparisonofPointProcessLearninganditsspecialcaseTakacsFikselestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ComparisonofPointProcessLearninganditsspecialcaseTakacsFikselestimation.html">&lt;p&gt;Recently, Cronie et al. (2024) introduced the notion of cross-validation for point processes and a new statistical methodology called Point Process Learning (PPL). In PPL one splits a point process/pattern into a training and a validation set, and then predicts the latter from the former through a parametrised Papangelou conditional intensity. The model parameters are estimated by minimizing a point process prediction error; this notion was introduced as the second building block of PPL. It was shown that PPL outperforms the state-of-the-art in both kernel intensity estimation and estimation of the parameters of the Gibbs hard-core process. In the latter case, the state-of-the-art was represented by pseudolikelihood estimation. In this paper we study PPL in relation to Takacs-Fiksel estimation, of which pseudolikelihood is a special case. We show that Takacs-Fiksel estimation is a special case of PPL in the sense that PPL with a specific loss function asymptotically reduces to Takacs-Fiksel estimation if we let the cross-validation regime tend to leave-one-out cross-validation. Moreover, PPL involves a certain type of hyperparameter given by a weight function which ensures that the prediction errors have expectation zero if and only if we have the correct parametrisation. We show that the weight function takes an explicit but intractable form for general Gibbs models. Consequently, we propose different approaches to estimate the weight function in practice. In order to assess how the general PPL setup performs in relation to its special case Takacs-Fiksel estimation, we conduct a simulation study where we find that for common Gibbs models we can find loss functions and hyperparameters so that PPL typically outperforms Takacs-Fiksel estimation significantly in terms of mean square error. Here, the hyperparameters are the cross-validation parameters and the weight function estimate.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19523&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Julia Jansson, Ottmar Cronie</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Recently, Cronie et al. (2024) introduced the notion of cross-validation for point processes and a new statistical methodology called Point Process Learning (PPL). In PPL one splits a point process/pattern into a training and a validation set, and then predicts the latter from the former through a parametrised Papangelou conditional intensity. The model parameters are estimated by minimizing a point process prediction error; this notion was introduced as the second building block of PPL. It was shown that PPL outperforms the state-of-the-art in both kernel intensity estimation and estimation of the parameters of the Gibbs hard-core process. In the latter case, the state-of-the-art was represented by pseudolikelihood estimation. In this paper we study PPL in relation to Takacs-Fiksel estimation, of which pseudolikelihood is a special case. We show that Takacs-Fiksel estimation is a special case of PPL in the sense that PPL with a specific loss function asymptotically reduces to Takacs-Fiksel estimation if we let the cross-validation regime tend to leave-one-out cross-validation. Moreover, PPL involves a certain type of hyperparameter given by a weight function which ensures that the prediction errors have expectation zero if and only if we have the correct parametrisation. We show that the weight function takes an explicit but intractable form for general Gibbs models. Consequently, we propose different approaches to estimate the weight function in practice. In order to assess how the general PPL setup performs in relation to its special case Takacs-Fiksel estimation, we conduct a simulation study where we find that for common Gibbs models we can find loss functions and hyperparameters so that PPL typically outperforms Takacs-Fiksel estimation significantly in terms of mean square error. Here, the hyperparameters are the cross-validation parameters and the weight function estimate.</summary></entry><entry><title type="html">Composite Dyadic Models for Spatio-Temporal Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/CompositeDyadicModelsforSpatioTemporalData.html" rel="alternate" type="text/html" title="Composite Dyadic Models for Spatio-Temporal Data" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/CompositeDyadicModelsforSpatioTemporalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/CompositeDyadicModelsforSpatioTemporalData.html">&lt;p&gt;Mechanistic statistical models are commonly used to study the flow of biological processes. For example, in landscape genetics, the aim is to infer spatial mechanisms that govern gene flow in populations. Existing statistical approaches in landscape genetics do not account for temporal dependence in the data and may be computationally prohibitive. We infer mechanisms with a Bayesian hierarchical dyadic model that scales well with large data sets and that accounts for spatial and temporal dependence. We construct a fully-connected network comprising spatio-temporal data for the dyadic model and use normalized composite likelihoods to account for the dependence structure in space and time. We develop a dyadic model to account for physical mechanisms commonly found in physical-statistical models and apply our methods to ancient human DNA data to infer the mechanisms that affected human movement in Bronze Age Europe.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.01341&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael R Schwob, Mevin B Hooten, Vagheesh Narasimhan</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Mechanistic statistical models are commonly used to study the flow of biological processes. For example, in landscape genetics, the aim is to infer spatial mechanisms that govern gene flow in populations. Existing statistical approaches in landscape genetics do not account for temporal dependence in the data and may be computationally prohibitive. We infer mechanisms with a Bayesian hierarchical dyadic model that scales well with large data sets and that accounts for spatial and temporal dependence. We construct a fully-connected network comprising spatio-temporal data for the dyadic model and use normalized composite likelihoods to account for the dependence structure in space and time. We develop a dyadic model to account for physical mechanisms commonly found in physical-statistical models and apply our methods to ancient human DNA data to infer the mechanisms that affected human movement in Bronze Age Europe.</summary></entry><entry><title type="html">Convolutional Unscented Kalman Filter for Multi-Object Tracking with Outliers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ConvolutionalUnscentedKalmanFilterforMultiObjectTrackingwithOutliers.html" rel="alternate" type="text/html" title="Convolutional Unscented Kalman Filter for Multi-Object Tracking with Outliers" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ConvolutionalUnscentedKalmanFilterforMultiObjectTrackingwithOutliers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ConvolutionalUnscentedKalmanFilterforMultiObjectTrackingwithOutliers.html">&lt;p&gt;Multi-object tracking (MOT) is an essential technique for navigation in autonomous driving. In tracking-by-detection systems, biases, false positives, and misses, which are referred to as outliers, are inevitable due to complex traffic scenarios. Recent tracking methods are based on filtering algorithms that overlook these outliers, leading to reduced tracking accuracy or even loss of the objects trajectory. To handle this challenge, we adopt a probabilistic perspective, regarding the generation of outliers as misspecification between the actual distribution of measurement data and the nominal measurement model used for filtering. We further demonstrate that, by designing a convolutional operation, we can mitigate this misspecification. Incorporating this operation into the widely used unscented Kalman filter (UKF) in commonly adopted tracking algorithms, we derive a variant of the UKF that is robust to outliers, called the convolutional UKF (ConvUKF). We show that ConvUKF maintains the Gaussian conjugate property, thus allowing for real-time tracking. We also prove that ConvUKF has a bounded tracking error in the presence of outliers, which implies robust stability. The experimental results on the KITTI and nuScenes datasets show improved accuracy compared to representative baseline algorithms for MOT tasks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01380&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shiqi Liu, Wenhan Cao, Chang Liu, Tianyi Zhang, Shengbo Eben Li</name></author><category term="stat.AP" /><summary type="html">Multi-object tracking (MOT) is an essential technique for navigation in autonomous driving. In tracking-by-detection systems, biases, false positives, and misses, which are referred to as outliers, are inevitable due to complex traffic scenarios. Recent tracking methods are based on filtering algorithms that overlook these outliers, leading to reduced tracking accuracy or even loss of the objects trajectory. To handle this challenge, we adopt a probabilistic perspective, regarding the generation of outliers as misspecification between the actual distribution of measurement data and the nominal measurement model used for filtering. We further demonstrate that, by designing a convolutional operation, we can mitigate this misspecification. Incorporating this operation into the widely used unscented Kalman filter (UKF) in commonly adopted tracking algorithms, we derive a variant of the UKF that is robust to outliers, called the convolutional UKF (ConvUKF). We show that ConvUKF maintains the Gaussian conjugate property, thus allowing for real-time tracking. We also prove that ConvUKF has a bounded tracking error in the presence of outliers, which implies robust stability. The experimental results on the KITTI and nuScenes datasets show improved accuracy compared to representative baseline algorithms for MOT tasks.</summary></entry><entry><title type="html">DISCRET: Synthesizing Faithful Explanations For Treatment Effect Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/DISCRETSynthesizingFaithfulExplanationsForTreatmentEffectEstimation.html" rel="alternate" type="text/html" title="DISCRET: Synthesizing Faithful Explanations For Treatment Effect Estimation" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/DISCRETSynthesizingFaithfulExplanationsForTreatmentEffectEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/DISCRETSynthesizingFaithfulExplanationsForTreatmentEffectEstimation.html">&lt;p&gt;Designing faithful yet accurate AI models is challenging, particularly in the field of individual treatment effect estimation (ITE). ITE prediction models deployed in critical settings such as healthcare should ideally be (i) accurate, and (ii) provide faithful explanations. However, current solutions are inadequate: state-of-the-art black-box models do not supply explanations, post-hoc explainers for black-box models lack faithfulness guarantees, and self-interpretable models greatly compromise accuracy. To address these issues, we propose DISCRET, a self-interpretable ITE framework that synthesizes faithful, rule-based explanations for each sample. A key insight behind DISCRET is that explanations can serve dually as database queries to identify similar subgroups of samples. We provide a novel RL algorithm to efficiently synthesize these explanations from a large search space. We evaluate DISCRET on diverse tasks involving tabular, image, and text data. DISCRET outperforms the best self-interpretable models and has accuracy comparable to the best black-box models while providing faithful explanations. DISCRET is available at https://github.com/wuyinjun-1993/DISCRET-ICML2024.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00611&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yinjun Wu, Mayank Keoliya, Kan Chen, Neelay Velingker, Ziyang Li, Emily J Getzen, Qi Long, Mayur Naik, Ravi B Parikh, Eric Wong</name></author><category term="stat.ME" /><summary type="html">Designing faithful yet accurate AI models is challenging, particularly in the field of individual treatment effect estimation (ITE). ITE prediction models deployed in critical settings such as healthcare should ideally be (i) accurate, and (ii) provide faithful explanations. However, current solutions are inadequate: state-of-the-art black-box models do not supply explanations, post-hoc explainers for black-box models lack faithfulness guarantees, and self-interpretable models greatly compromise accuracy. To address these issues, we propose DISCRET, a self-interpretable ITE framework that synthesizes faithful, rule-based explanations for each sample. A key insight behind DISCRET is that explanations can serve dually as database queries to identify similar subgroups of samples. We provide a novel RL algorithm to efficiently synthesize these explanations from a large search space. We evaluate DISCRET on diverse tasks involving tabular, image, and text data. DISCRET outperforms the best self-interpretable models and has accuracy comparable to the best black-box models while providing faithful explanations. DISCRET is available at https://github.com/wuyinjun-1993/DISCRET-ICML2024.</summary></entry><entry><title type="html">Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation.html" rel="alternate" type="text/html" title="Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation.html">&lt;p&gt;There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using inverse probability weighting (IPW). However, due to their numerically unstable IPW weights, these methods suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator by weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Our experimental results show that by effectively correcting the weight values, our proposed method outperforms the existing ones, including traditional weighting schemes. Our code is available at https://github.com/ychika/DPSW.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17483&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yoichi Chikahara, Kansei Ushiyama</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using inverse probability weighting (IPW). However, due to their numerically unstable IPW weights, these methods suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator by weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Our experimental results show that by effectively correcting the weight values, our proposed method outperforms the existing ones, including traditional weighting schemes. Our code is available at https://github.com/ychika/DPSW.</summary></entry><entry><title type="html">Discovering an interpretable mathematical expression for a full wind-turbine wake with artificial intelligence enhanced symbolic regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Discoveringaninterpretablemathematicalexpressionforafullwindturbinewakewithartificialintelligenceenhancedsymbolicregression.html" rel="alternate" type="text/html" title="Discovering an interpretable mathematical expression for a full wind-turbine wake with artificial intelligence enhanced symbolic regression" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Discoveringaninterpretablemathematicalexpressionforafullwindturbinewakewithartificialintelligenceenhancedsymbolicregression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Discoveringaninterpretablemathematicalexpressionforafullwindturbinewakewithartificialintelligenceenhancedsymbolicregression.html">&lt;p&gt;The rapid expansion of wind power worldwide underscores the critical significance of engineering-focused analytical wake models in both the design and operation of wind farms. These theoretically-derived ana lytical wake models have limited predictive capabilities, particularly in the near-wake region close to the turbine rotor, due to assumptions that do not hold. Knowledge discovery methods can bridge these gaps by extracting insights, adjusting for theoretical assumptions, and developing accurate models for physical processes. In this study, we introduce a genetic symbolic regression (SR) algorithm to discover an interpretable mathematical expression for the mean velocity deficit throughout the wake, a previously unavailable insight. By incorporating a double Gaussian distribution into the SR algorithm as domain knowledge and designing a hierarchical equation structure, the search space is reduced, thus efficiently finding a concise, physically informed, and robust wake model. The proposed mathematical expression (equation) can predict the wake velocity deficit at any location in the full-wake region with high precision and stability. The model’s effectiveness and practicality are validated through experimental data and high-fidelity numerical simulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00695&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ding Wang, Yuntian Chen, Shiyi Chen</name></author><category term="stat.AP" /><summary type="html">The rapid expansion of wind power worldwide underscores the critical significance of engineering-focused analytical wake models in both the design and operation of wind farms. These theoretically-derived ana lytical wake models have limited predictive capabilities, particularly in the near-wake region close to the turbine rotor, due to assumptions that do not hold. Knowledge discovery methods can bridge these gaps by extracting insights, adjusting for theoretical assumptions, and developing accurate models for physical processes. In this study, we introduce a genetic symbolic regression (SR) algorithm to discover an interpretable mathematical expression for the mean velocity deficit throughout the wake, a previously unavailable insight. By incorporating a double Gaussian distribution into the SR algorithm as domain knowledge and designing a hierarchical equation structure, the search space is reduced, thus efficiently finding a concise, physically informed, and robust wake model. The proposed mathematical expression (equation) can predict the wake velocity deficit at any location in the full-wake region with high precision and stability. The model’s effectiveness and practicality are validated through experimental data and high-fidelity numerical simulations.</summary></entry><entry><title type="html">Distributed High-Dimensional Quantile Regression: Estimation Efficiency and Support Recovery</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/DistributedHighDimensionalQuantileRegressionEstimationEfficiencyandSupportRecovery.html" rel="alternate" type="text/html" title="Distributed High-Dimensional Quantile Regression: Estimation Efficiency and Support Recovery" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/DistributedHighDimensionalQuantileRegressionEstimationEfficiencyandSupportRecovery</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/DistributedHighDimensionalQuantileRegressionEstimationEfficiencyandSupportRecovery.html">&lt;p&gt;In this paper, we focus on distributed estimation and support recovery for high-dimensional linear quantile regression. Quantile regression is a popular alternative tool to the least squares regression for robustness against outliers and data heterogeneity. However, the non-smoothness of the check loss function poses big challenges to both computation and theory in the distributed setting. To tackle these problems, we transform the original quantile regression into the least-squares optimization. By applying a double-smoothing approach, we extend a previous Newton-type distributed approach without the restrictive independent assumption between the error term and covariates. An efficient algorithm is developed, which enjoys high computation and communication efficiency. Theoretically, the proposed distributed estimator achieves a near-oracle convergence rate and high support recovery accuracy after a constant number of iterations. Extensive experiments on synthetic examples and a real data application further demonstrate the effectiveness of the proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07552&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Caixing Wang, Ziliang Shen</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">In this paper, we focus on distributed estimation and support recovery for high-dimensional linear quantile regression. Quantile regression is a popular alternative tool to the least squares regression for robustness against outliers and data heterogeneity. However, the non-smoothness of the check loss function poses big challenges to both computation and theory in the distributed setting. To tackle these problems, we transform the original quantile regression into the least-squares optimization. By applying a double-smoothing approach, we extend a previous Newton-type distributed approach without the restrictive independent assumption between the error term and covariates. An efficient algorithm is developed, which enjoys high computation and communication efficiency. Theoretically, the proposed distributed estimator achieves a near-oracle convergence rate and high support recovery accuracy after a constant number of iterations. Extensive experiments on synthetic examples and a real data application further demonstrate the effectiveness of the proposed method.</summary></entry><entry><title type="html">Distributional Refinement Network: Distributional Forecasting via Deep Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/DistributionalRefinementNetworkDistributionalForecastingviaDeepLearning.html" rel="alternate" type="text/html" title="Distributional Refinement Network: Distributional Forecasting via Deep Learning" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/DistributionalRefinementNetworkDistributionalForecastingviaDeepLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/DistributionalRefinementNetworkDistributionalForecastingviaDeepLearning.html">&lt;p&gt;A key task in actuarial modelling involves modelling the distributional properties of losses. Classic (distributional) regression approaches like Generalized Linear Models (GLMs; Nelder and Wedderburn, 1972) are commonly used, but challenges remain in developing models that can (i) allow covariates to flexibly impact different aspects of the conditional distribution, (ii) integrate developments in machine learning and AI to maximise the predictive power while considering (i), and, (iii) maintain a level of interpretability in the model to enhance trust in the model and its outputs, which is often compromised in efforts pursuing (i) and (ii). We tackle this problem by proposing a Distributional Refinement Network (DRN), which combines an inherently interpretable baseline model (such as GLMs) with a flexible neural network-a modified Deep Distribution Regression (DDR; Li et al., 2019) method. Inspired by the Combined Actuarial Neural Network (CANN; Schelldorfer and W{&apos;‘u}thrich, 2019), our approach flexibly refines the entire baseline distribution. As a result, the DRN captures varying effects of features across all quantiles, improving predictive performance while maintaining adequate interpretability. Using both synthetic and real-world data, we demonstrate the DRN’s superior distributional forecasting capacity. The DRN has the potential to be a powerful distributional regression model in actuarial science and beyond.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00998&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Benjamin Avanzi, Eric Dong, Patrick J. Laub, Bernard Wong</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">A key task in actuarial modelling involves modelling the distributional properties of losses. Classic (distributional) regression approaches like Generalized Linear Models (GLMs; Nelder and Wedderburn, 1972) are commonly used, but challenges remain in developing models that can (i) allow covariates to flexibly impact different aspects of the conditional distribution, (ii) integrate developments in machine learning and AI to maximise the predictive power while considering (i), and, (iii) maintain a level of interpretability in the model to enhance trust in the model and its outputs, which is often compromised in efforts pursuing (i) and (ii). We tackle this problem by proposing a Distributional Refinement Network (DRN), which combines an inherently interpretable baseline model (such as GLMs) with a flexible neural network-a modified Deep Distribution Regression (DDR; Li et al., 2019) method. Inspired by the Combined Actuarial Neural Network (CANN; Schelldorfer and W{&apos;‘u}thrich, 2019), our approach flexibly refines the entire baseline distribution. As a result, the DRN captures varying effects of features across all quantiles, improving predictive performance while maintaining adequate interpretability. Using both synthetic and real-world data, we demonstrate the DRN’s superior distributional forecasting capacity. The DRN has the potential to be a powerful distributional regression model in actuarial science and beyond.</summary></entry><entry><title type="html">Efficient designs for multivariate crossover trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Efficientdesignsformultivariatecrossovertrials.html" rel="alternate" type="text/html" title="Efficient designs for multivariate crossover trials" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Efficientdesignsformultivariatecrossovertrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Efficientdesignsformultivariatecrossovertrials.html">&lt;p&gt;This article aims to study efficient/trace optimal designs for crossover trials with multiple responses recorded from each subject in the time periods. A multivariate fixed effects model is proposed with direct and carryover effects corresponding to the multiple responses. The corresponding error dispersion matrix is chosen to be either of the proportional or the generalized Markov covariance type, permitting the existence of direct and cross-correlations within and between the multiple responses. The corresponding information matrices for direct effects under the two types of dispersions are used to determine efficient designs. The efficiency of orthogonal array designs of Type $I$ and strength $2$ is investigated for a wide choice of covariance functions, namely, Mat($0.5$), Mat($1.5$) and Mat($\infty$). To motivate these multivariate crossover designs, a gene expression dataset in a $3 \times 3$ framework is utilized.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.04498&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shubham Niphadkar, Siuli Mukhopadhyay</name></author><category term="stat.ME" /><summary type="html">This article aims to study efficient/trace optimal designs for crossover trials with multiple responses recorded from each subject in the time periods. A multivariate fixed effects model is proposed with direct and carryover effects corresponding to the multiple responses. The corresponding error dispersion matrix is chosen to be either of the proportional or the generalized Markov covariance type, permitting the existence of direct and cross-correlations within and between the multiple responses. The corresponding information matrices for direct effects under the two types of dispersions are used to determine efficient designs. The efficiency of orthogonal array designs of Type $I$ and strength $2$ is investigated for a wide choice of covariance functions, namely, Mat($0.5$), Mat($1.5$) and Mat($\infty$). To motivate these multivariate crossover designs, a gene expression dataset in a $3 \times 3$ framework is utilized.</summary></entry><entry><title type="html">Enhancing Scalability in Bayesian Nonparametric Factor Analysis of Spatiotemporal Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/EnhancingScalabilityinBayesianNonparametricFactorAnalysisofSpatiotemporalData.html" rel="alternate" type="text/html" title="Enhancing Scalability in Bayesian Nonparametric Factor Analysis of Spatiotemporal Data" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/EnhancingScalabilityinBayesianNonparametricFactorAnalysisofSpatiotemporalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/EnhancingScalabilityinBayesianNonparametricFactorAnalysisofSpatiotemporalData.html">&lt;p&gt;This manuscript puts forward novel practicable spatiotemporal Bayesian factor analysis frameworks computationally feasible for moderate to large data. Our models exhibit significantly enhanced computational scalability and storage efficiency, deliver high overall modeling performances, and possess powerful inferential capabilities for adequately predicting outcomes at future time points or new spatial locations and satisfactorily clustering spatial locations into regions with similar temporal trajectories, a frequently encountered crucial task. We integrate on top of a baseline separable factor model with temporally dependent latent factors and spatially dependent factor loadings under a probit stick breaking process (PSBP) prior a new slice sampling algorithm that permits unknown varying numbers of spatial mixture components across all factors and guarantees them to be non-increasing through the MCMC iterations, thus considerably enhancing model flexibility, efficiency, and scalability. We further introduce a novel spatial latent nearest-neighbor Gaussian process (NNGP) prior and new sequential updating algorithms for the spatially varying latent variables in the PSBP prior, thereby attaining high spatial scalability. The markedly accelerated posterior sampling and spatial prediction as well as the great modeling and inferential performances of our models are substantiated by our simulation experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.05802&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yifan Cheng, Cheng Li</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">This manuscript puts forward novel practicable spatiotemporal Bayesian factor analysis frameworks computationally feasible for moderate to large data. Our models exhibit significantly enhanced computational scalability and storage efficiency, deliver high overall modeling performances, and possess powerful inferential capabilities for adequately predicting outcomes at future time points or new spatial locations and satisfactorily clustering spatial locations into regions with similar temporal trajectories, a frequently encountered crucial task. We integrate on top of a baseline separable factor model with temporally dependent latent factors and spatially dependent factor loadings under a probit stick breaking process (PSBP) prior a new slice sampling algorithm that permits unknown varying numbers of spatial mixture components across all factors and guarantees them to be non-increasing through the MCMC iterations, thus considerably enhancing model flexibility, efficiency, and scalability. We further introduce a novel spatial latent nearest-neighbor Gaussian process (NNGP) prior and new sequential updating algorithms for the spatially varying latent variables in the PSBP prior, thereby attaining high spatial scalability. The markedly accelerated posterior sampling and spatial prediction as well as the great modeling and inferential performances of our models are substantiated by our simulation experiments.</summary></entry><entry><title type="html">Ensemble distributional forecasting for insurance loss reserving</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Ensembledistributionalforecastingforinsurancelossreserving.html" rel="alternate" type="text/html" title="Ensemble distributional forecasting for insurance loss reserving" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Ensembledistributionalforecastingforinsurancelossreserving</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Ensembledistributionalforecastingforinsurancelossreserving.html">&lt;p&gt;Loss reserving generally focuses on identifying a single model that can generate superior predictive performance. However, different loss reserving models specialise in capturing different aspects of loss data. This is recognised in practice in the sense that results from different models are often considered, and sometimes combined. For instance, actuaries may take a weighted average of the prediction outcomes from various loss reserving models, often based on subjective assessments.
  In this paper, we propose a systematic framework to objectively combine (i.e. ensemble) multiple &lt;em&gt;stochastic&lt;/em&gt; loss reserving models such that the strengths offered by different models can be utilised effectively. Our framework contains two main innovations compared to existing literature and practice. Firstly, our criteria model combination considers the full distributional properties of the ensemble and not just the central estimate - which is of particular importance in the reserving context. Secondly, our framework is that it is tailored for the features inherent to reserving data. These include, for instance, accident, development, calendar, and claim maturity effects. Crucially, the relative importance and scarcity of data across accident periods renders the problem distinct from the traditional ensembling techniques in statistical learning.
  Our framework is illustrated with a complex synthetic dataset. In the results, the optimised ensemble outperforms both (i) traditional model selection strategies, and (ii) an equally weighted ensemble. In particular, the improvement occurs not only with central estimates but also relevant quantiles, such as the 75th percentile of reserves (typically of interest to both insurers and regulators). The framework developed in this paper can be implemented thanks to an R package, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ADLP&lt;/code&gt;, which is available from CRAN.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2206.08541&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Benjamin Avanzi, Yanfeng Li, Bernard Wong, Alan Xian</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Loss reserving generally focuses on identifying a single model that can generate superior predictive performance. However, different loss reserving models specialise in capturing different aspects of loss data. This is recognised in practice in the sense that results from different models are often considered, and sometimes combined. For instance, actuaries may take a weighted average of the prediction outcomes from various loss reserving models, often based on subjective assessments. In this paper, we propose a systematic framework to objectively combine (i.e. ensemble) multiple stochastic loss reserving models such that the strengths offered by different models can be utilised effectively. Our framework contains two main innovations compared to existing literature and practice. Firstly, our criteria model combination considers the full distributional properties of the ensemble and not just the central estimate - which is of particular importance in the reserving context. Secondly, our framework is that it is tailored for the features inherent to reserving data. These include, for instance, accident, development, calendar, and claim maturity effects. Crucially, the relative importance and scarcity of data across accident periods renders the problem distinct from the traditional ensembling techniques in statistical learning. Our framework is illustrated with a complex synthetic dataset. In the results, the optimised ensemble outperforms both (i) traditional model selection strategies, and (ii) an equally weighted ensemble. In particular, the improvement occurs not only with central estimates but also relevant quantiles, such as the 75th percentile of reserves (typically of interest to both insurers and regulators). The framework developed in this paper can be implemented thanks to an R package, ADLP, which is available from CRAN.</summary></entry><entry><title type="html">Estimating the Number of Street Vendors in New York City</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/EstimatingtheNumberofStreetVendorsinNewYorkCity.html" rel="alternate" type="text/html" title="Estimating the Number of Street Vendors in New York City" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/EstimatingtheNumberofStreetVendorsinNewYorkCity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/EstimatingtheNumberofStreetVendorsinNewYorkCity.html">&lt;p&gt;We estimate the number of street vendors in New York City. We first summarize the process by which vendors receive licenses and permits to legally operate in New York City. We then describe a survey that was administered by the Street Vendor Project while distributing Coronavirus relief aid to vendors operating in New York City both with and without a license or permit. Finally, we calculate the total number of vendors using ratio estimation. We find that approximately 23,000 street vendors operate in New York City: 20,500 mobile food vendors and 2,300 general merchandise vendors. One third are located in just six ZIP Codes: 11368 (16%), 11372 (3%), and 11354 (3%) in North and West Queens and 10036 (5%), 10019 (4%), and 10001 (3%) in the Chelsea and Clinton neighborhoods of Manhattan. We also provide a theoretical justification of our estimates based on the theory of point processes and a discussion of their accuracy and implications. In particular, our estimates suggest the American Community Survey fails to cover the majority of New York City street vendors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00527&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jonathan Auerbach</name></author><category term="stat.AP" /><summary type="html">We estimate the number of street vendors in New York City. We first summarize the process by which vendors receive licenses and permits to legally operate in New York City. We then describe a survey that was administered by the Street Vendor Project while distributing Coronavirus relief aid to vendors operating in New York City both with and without a license or permit. Finally, we calculate the total number of vendors using ratio estimation. We find that approximately 23,000 street vendors operate in New York City: 20,500 mobile food vendors and 2,300 general merchandise vendors. One third are located in just six ZIP Codes: 11368 (16%), 11372 (3%), and 11354 (3%) in North and West Queens and 10036 (5%), 10019 (4%), and 10001 (3%) in the Chelsea and Clinton neighborhoods of Manhattan. We also provide a theoretical justification of our estimates based on the theory of point processes and a discussion of their accuracy and implications. In particular, our estimates suggest the American Community Survey fails to cover the majority of New York City street vendors.</summary></entry><entry><title type="html">Examining properness in the external validation of survival models with squared and logarithmic losses</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Examiningpropernessintheexternalvalidationofsurvivalmodelswithsquaredandlogarithmiclosses.html" rel="alternate" type="text/html" title="Examining properness in the external validation of survival models with squared and logarithmic losses" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Examiningpropernessintheexternalvalidationofsurvivalmodelswithsquaredandlogarithmiclosses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Examiningpropernessintheexternalvalidationofsurvivalmodelswithsquaredandlogarithmiclosses.html">&lt;p&gt;Scoring rules promote rational and honest decision-making, which is becoming increasingly important for automated procedures in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto-ML&apos;. In this paper we survey common squared and logarithmic scoring rules for survival analysis and determine which losses are proper and improper. We prove that commonly utilised squared and logarithmic scoring rules that are claimed to be proper are in fact improper, such as the Integrated Survival Brier Score (ISBS). We further prove that under a strict set of assumptions a class of scoring rules is strictly proper for, what we term, &lt;/code&gt;approximate’ survival losses. Despite the difference in properness, experiments in simulated and real-world datasets show there is no major difference between improper and proper versions of the widely-used ISBS, ensuring that we can reasonably trust previous experiments utilizing the original score for evaluation purposes. We still advocate for the use of proper scoring rules, as even minor differences between losses can have important implications in automated processes such as model tuning. We hope our findings encourage further research into the properties of survival measures so that robust and honest evaluation of survival models can be achieved.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.05260&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Raphael Sonabend, John Zobolas, Philipp Kopper, Lukas Burk, Andreas Bender</name></author><category term="stat.AP," /><category term="stat.TH" /><summary type="html">Scoring rules promote rational and honest decision-making, which is becoming increasingly important for automated procedures in auto-ML&apos;. In this paper we survey common squared and logarithmic scoring rules for survival analysis and determine which losses are proper and improper. We prove that commonly utilised squared and logarithmic scoring rules that are claimed to be proper are in fact improper, such as the Integrated Survival Brier Score (ISBS). We further prove that under a strict set of assumptions a class of scoring rules is strictly proper for, what we term, approximate’ survival losses. Despite the difference in properness, experiments in simulated and real-world datasets show there is no major difference between improper and proper versions of the widely-used ISBS, ensuring that we can reasonably trust previous experiments utilizing the original score for evaluation purposes. We still advocate for the use of proper scoring rules, as even minor differences between losses can have important implications in automated processes such as model tuning. We hope our findings encourage further research into the properties of survival measures so that robust and honest evaluation of survival models can be achieved.</summary></entry><entry><title type="html">Far beyond day-ahead with econometric models for electricity price forecasting</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Farbeyonddayaheadwitheconometricmodelsforelectricitypriceforecasting.html" rel="alternate" type="text/html" title="Far beyond day-ahead with econometric models for electricity price forecasting" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Farbeyonddayaheadwitheconometricmodelsforelectricitypriceforecasting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Farbeyonddayaheadwitheconometricmodelsforelectricitypriceforecasting.html">&lt;p&gt;The surge in global energy prices during the recent energy crisis, which peaked in 2022, has intensified the need for mid-term to long-term forecasting for hedging and valuation purposes. This study analyzes the statistical predictability of power prices before, during, and after the energy crisis, using econometric models with an hourly resolution. To stabilize the model estimates, we define fundamentally derived coefficient bounds. We provide an in-depth analysis of the unit root behavior of the power price series, showing that the long-term stochastic trend is explained by the prices of commodities used as fuels for power generation: gas, coal, oil, and emission allowances (EUA). However, as the forecasting horizon increases, spurious effects become extremely relevant, leading to highly significant but economically meaningless results. To mitigate these spurious effects, we propose the “current” model: estimating the current same-day relationship between power prices and their regressors and projecting this relationship into the future. This flexible and interpretable method is applied to hourly German day-ahead power prices for forecasting horizons up to one year ahead, utilizing a combination of regularized regression methods and generalized additive models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00326&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Paul Ghelasi, Florian Ziel</name></author><category term="stat.AP" /><summary type="html">The surge in global energy prices during the recent energy crisis, which peaked in 2022, has intensified the need for mid-term to long-term forecasting for hedging and valuation purposes. This study analyzes the statistical predictability of power prices before, during, and after the energy crisis, using econometric models with an hourly resolution. To stabilize the model estimates, we define fundamentally derived coefficient bounds. We provide an in-depth analysis of the unit root behavior of the power price series, showing that the long-term stochastic trend is explained by the prices of commodities used as fuels for power generation: gas, coal, oil, and emission allowances (EUA). However, as the forecasting horizon increases, spurious effects become extremely relevant, leading to highly significant but economically meaningless results. To mitigate these spurious effects, we propose the “current” model: estimating the current same-day relationship between power prices and their regressors and projecting this relationship into the future. This flexible and interpretable method is applied to hourly German day-ahead power prices for forecasting horizons up to one year ahead, utilizing a combination of regularized regression methods and generalized additive models.</summary></entry><entry><title type="html">Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/FeatureAttributionwithNecessityandSufficiencyviaDualstagePerturbationTestforCausalExplanation.html" rel="alternate" type="text/html" title="Feature Attribution with Necessity and Sufficiency via Dual-stage Perturbation Test for Causal Explanation" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/FeatureAttributionwithNecessityandSufficiencyviaDualstagePerturbationTestforCausalExplanation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/FeatureAttributionwithNecessityandSufficiencyviaDualstagePerturbationTestforCausalExplanation.html">&lt;p&gt;We investigate the problem of explainability for machine learning models, focusing on Feature Attribution Methods (FAMs) that evaluate feature importance through perturbation tests. Despite their utility, FAMs struggle to distinguish the contributions of different features, when their prediction changes are similar after perturbation. To enhance FAMs’ discriminative power, we introduce Feature Attribution with Necessity and Sufficiency (FANS), which find a neighborhood of the input such that perturbing samples within this neighborhood have a high Probability of being Necessity and Sufficiency (PNS) cause for the change in predictions, and use this PNS as the importance of the feature. Specifically, FANS compute this PNS via a heuristic strategy for estimating the neighborhood and a perturbation test involving two stages (factual and interventional) for counterfactual reasoning. To generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution. We demonstrate that FANS outperforms existing attribution methods on six benchmarks. Please refer to the source code via \url{https://github.com/DMIRLAB-Group/FANS}.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.08845&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xuexin Chen, Ruichu Cai, Zhengting Huang, Yuxuan Zhu, Julien Horwood, Zhifeng Hao, Zijian Li, Jose Miguel Hernandez-Lobato</name></author><category term="stat.ME" /><summary type="html">We investigate the problem of explainability for machine learning models, focusing on Feature Attribution Methods (FAMs) that evaluate feature importance through perturbation tests. Despite their utility, FAMs struggle to distinguish the contributions of different features, when their prediction changes are similar after perturbation. To enhance FAMs’ discriminative power, we introduce Feature Attribution with Necessity and Sufficiency (FANS), which find a neighborhood of the input such that perturbing samples within this neighborhood have a high Probability of being Necessity and Sufficiency (PNS) cause for the change in predictions, and use this PNS as the importance of the feature. Specifically, FANS compute this PNS via a heuristic strategy for estimating the neighborhood and a perturbation test involving two stages (factual and interventional) for counterfactual reasoning. To generate counterfactual samples, we use a resampling-based approach on the observed samples to approximate the required conditional distribution. We demonstrate that FANS outperforms existing attribution methods on six benchmarks. Please refer to the source code via \url{https://github.com/DMIRLAB-Group/FANS}.</summary></entry><entry><title type="html">Generalized Posterior Calibration via Sequential Monte Carlo Sampler</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/GeneralizedPosteriorCalibrationviaSequentialMonteCarloSampler.html" rel="alternate" type="text/html" title="Generalized Posterior Calibration via Sequential Monte Carlo Sampler" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/GeneralizedPosteriorCalibrationviaSequentialMonteCarloSampler</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/GeneralizedPosteriorCalibrationviaSequentialMonteCarloSampler.html">&lt;p&gt;As the amount and complexity of available data increases, the need for robust statistical learning becomes more pressing. To enhance resilience against model misspecification, the generalized posterior inference method adjusts the likelihood term by exponentiating it with a learning rate, thereby fine-tuning the dispersion of the posterior distribution. This study proposes a computationally efficient strategy for selecting an appropriate learning rate. The proposed approach builds upon the generalized posterior calibration (GPC) algorithm, which is designed to select a learning rate that ensures nominal frequentist coverage. This algorithm, which evaluates the coverage probability using bootstrap samples, has high computational costs because of the repeated posterior simulations needed for bootstrap samples. To address this limitation, the study proposes an algorithm that combines elements of the GPC algorithm with the sequential Monte Carlo (SMC) sampler. By leveraging the similarity between the learning rate in generalized posterior inference and the inverse temperature in SMC sampling, the proposed algorithm efficiently calibrates the posterior distribution with a reduced computational cost. For demonstration, the proposed algorithm was applied to several statistical learning models and shown to be significantly faster than the original GPC.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.16528&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Masahiro Tanaka</name></author><category term="stat.CO" /><summary type="html">As the amount and complexity of available data increases, the need for robust statistical learning becomes more pressing. To enhance resilience against model misspecification, the generalized posterior inference method adjusts the likelihood term by exponentiating it with a learning rate, thereby fine-tuning the dispersion of the posterior distribution. This study proposes a computationally efficient strategy for selecting an appropriate learning rate. The proposed approach builds upon the generalized posterior calibration (GPC) algorithm, which is designed to select a learning rate that ensures nominal frequentist coverage. This algorithm, which evaluates the coverage probability using bootstrap samples, has high computational costs because of the repeated posterior simulations needed for bootstrap samples. To address this limitation, the study proposes an algorithm that combines elements of the GPC algorithm with the sequential Monte Carlo (SMC) sampler. By leveraging the similarity between the learning rate in generalized posterior inference and the inverse temperature in SMC sampling, the proposed algorithm efficiently calibrates the posterior distribution with a reduced computational cost. For demonstration, the proposed algorithm was applied to several statistical learning models and shown to be significantly faster than the original GPC.</summary></entry><entry><title type="html">Generalizing Orthogonalization for Models with Non-Linearities</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/GeneralizingOrthogonalizationforModelswithNonLinearities.html" rel="alternate" type="text/html" title="Generalizing Orthogonalization for Models with Non-Linearities" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/GeneralizingOrthogonalizationforModelswithNonLinearities</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/GeneralizingOrthogonalizationforModelswithNonLinearities.html">&lt;p&gt;The complexity of black-box algorithms can lead to various challenges, including the introduction of biases. These biases present immediate risks in the algorithms’ application. It was, for instance, shown that neural networks can deduce racial information solely from a patient’s X-ray scan, a task beyond the capability of medical experts. If this fact is not known to the medical expert, automatic decision-making based on this algorithm could lead to prescribing a treatment (purely) based on racial information. While current methodologies allow for the “orthogonalization” or “normalization” of neural networks with respect to such information, existing approaches are grounded in linear models. Our paper advances the discourse by introducing corrections for non-linearities such as ReLU activations. Our approach also encompasses scalar and tensor-valued predictions, facilitating its integration into neural network architectures. Through extensive experiments, we validate our method’s effectiveness in safeguarding sensitive data in generalized linear models, normalizing convolutional neural networks for metadata, and rectifying pre-existing embeddings for undesired attributes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02475&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David Rügamer, Chris Kolb, Tobias Weber, Lucas Kook, Thomas Nagler</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">The complexity of black-box algorithms can lead to various challenges, including the introduction of biases. These biases present immediate risks in the algorithms’ application. It was, for instance, shown that neural networks can deduce racial information solely from a patient’s X-ray scan, a task beyond the capability of medical experts. If this fact is not known to the medical expert, automatic decision-making based on this algorithm could lead to prescribing a treatment (purely) based on racial information. While current methodologies allow for the “orthogonalization” or “normalization” of neural networks with respect to such information, existing approaches are grounded in linear models. Our paper advances the discourse by introducing corrections for non-linearities such as ReLU activations. Our approach also encompasses scalar and tensor-valued predictions, facilitating its integration into neural network architectures. Through extensive experiments, we validate our method’s effectiveness in safeguarding sensitive data in generalized linear models, normalizing convolutional neural networks for metadata, and rectifying pre-existing embeddings for undesired attributes.</summary></entry><entry><title type="html">Graph Machine Learning based Doubly Robust Estimator for Network Causal Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/GraphMachineLearningbasedDoublyRobustEstimatorforNetworkCausalEffects.html" rel="alternate" type="text/html" title="Graph Machine Learning based Doubly Robust Estimator for Network Causal Effects" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/GraphMachineLearningbasedDoublyRobustEstimatorforNetworkCausalEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/GraphMachineLearningbasedDoublyRobustEstimatorforNetworkCausalEffects.html">&lt;p&gt;We address the challenge of inferring causal effects in social network data. This results in challenges due to interference – where a unit’s outcome is affected by neighbors’ treatments – and network-induced confounding factors. While there is extensive literature focusing on estimating causal effects in social network setups, a majority of them make prior assumptions about the form of network-induced confounding mechanisms. Such strong assumptions are rarely likely to hold especially in high-dimensional networks. We propose a novel methodology that combines graph machine learning approaches with the double machine learning framework to enable accurate and efficient estimation of direct and peer effects using a single observational social network. We demonstrate the semiparametric efficiency of our proposed estimator under mild regularity conditions, allowing for consistent uncertainty quantification. We demonstrate that our method is accurate, robust, and scalable via an extensive simulation study. We use our method to investigate the impact of Self-Help Group participation on financial risk tolerance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.11332&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Seyedeh Baharan Khatami, Harsh Parikh, Haowei Chen, Sudeepa Roy, Babak Salimi</name></author><category term="stat.ME" /><summary type="html">We address the challenge of inferring causal effects in social network data. This results in challenges due to interference – where a unit’s outcome is affected by neighbors’ treatments – and network-induced confounding factors. While there is extensive literature focusing on estimating causal effects in social network setups, a majority of them make prior assumptions about the form of network-induced confounding mechanisms. Such strong assumptions are rarely likely to hold especially in high-dimensional networks. We propose a novel methodology that combines graph machine learning approaches with the double machine learning framework to enable accurate and efficient estimation of direct and peer effects using a single observational social network. We demonstrate the semiparametric efficiency of our proposed estimator under mild regularity conditions, allowing for consistent uncertainty quantification. We demonstrate that our method is accurate, robust, and scalable via an extensive simulation study. We use our method to investigate the impact of Self-Help Group participation on financial risk tolerance.</summary></entry><entry><title type="html">High-energy Neutrino Source Cross-correlations with Nearest Neighbor Distributions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/HighenergyNeutrinoSourceCrosscorrelationswithNearestNeighborDistributions.html" rel="alternate" type="text/html" title="High-energy Neutrino Source Cross-correlations with Nearest Neighbor Distributions" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/HighenergyNeutrinoSourceCrosscorrelationswithNearestNeighborDistributions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/HighenergyNeutrinoSourceCrosscorrelationswithNearestNeighborDistributions.html">&lt;p&gt;The astrophysical origins of the majority of the IceCube neutrinos remain unknown. Effectively characterizing the spatial distribution of the neutrino samples and associating the events with astrophysical source catalogs can be challenging given the large atmospheric neutrino background and underlying non-Gaussian spatial features in the neutrino and source samples. In this paper, we investigate a framework for identifying and statistically evaluating the cross-correlations between IceCube data and an astrophysical source catalog based on the $k$-Nearest Neighbor Cumulative Distribution Functions ($k$NN-CDFs). We propose a maximum likelihood estimation procedure for inferring the true proportions of astrophysical neutrinos in the point-source data. We conduct a statistical power analysis of an associated likelihood ratio test with estimations of its sensitivity and discovery potential with synthetic neutrino data samples and a WISE-2MASS galaxy sample. We apply the method to IceCube’s public ten-year point-source data and find no statistically significant evidence for spatial cross-correlations with the selected galaxy sample. We discuss possible extensions to the current method and explore the method’s potential to identify the cross-correlation signals in data sets with different sample sizes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00796&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhuoyang Zhou, Jessi Cisewski-Kehe, Ke Fang, Arka Banerjee</name></author><category term="stat.AP" /><summary type="html">The astrophysical origins of the majority of the IceCube neutrinos remain unknown. Effectively characterizing the spatial distribution of the neutrino samples and associating the events with astrophysical source catalogs can be challenging given the large atmospheric neutrino background and underlying non-Gaussian spatial features in the neutrino and source samples. In this paper, we investigate a framework for identifying and statistically evaluating the cross-correlations between IceCube data and an astrophysical source catalog based on the $k$-Nearest Neighbor Cumulative Distribution Functions ($k$NN-CDFs). We propose a maximum likelihood estimation procedure for inferring the true proportions of astrophysical neutrinos in the point-source data. We conduct a statistical power analysis of an associated likelihood ratio test with estimations of its sensitivity and discovery potential with synthetic neutrino data samples and a WISE-2MASS galaxy sample. We apply the method to IceCube’s public ten-year point-source data and find no statistically significant evidence for spatial cross-correlations with the selected galaxy sample. We discuss possible extensions to the current method and explore the method’s potential to identify the cross-correlation signals in data sets with different sample sizes.</summary></entry><entry><title type="html">Individual claims reserving using the Aalen–Johansen estimator</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/IndividualclaimsreservingusingtheAalenJohansenestimator.html" rel="alternate" type="text/html" title="Individual claims reserving using the Aalen–Johansen estimator" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/IndividualclaimsreservingusingtheAalenJohansenestimator</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/IndividualclaimsreservingusingtheAalenJohansenestimator.html">&lt;p&gt;We propose an individual claims reserving model based on the conditional Aalen-Johansen estimator, as developed in Bladt and Furrer (2023b). In our approach, we formulate a multi-state problem, where the underlying variable is the individual claim size, rather than time. The states in this model represent development periods, and we estimate the cumulative density function of individual claim sizes using the conditional Aalen-Johansen method as transition probabilities to an absorbing state. Our methodology reinterprets the concept of multi-state models and offers a strategy for modeling the complete curve of individual claim sizes. To illustrate our approach, we apply our model to both simulated and real datasets. Having access to the entire dataset enables us to support the use of our approach by comparing the predicted total final cost with the actual amount, as well as evaluating it in terms of the continuously ranked probability score.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.07384&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Martin Bladt, Gabriele Pittarello</name></author><category term="stat.AP" /><summary type="html">We propose an individual claims reserving model based on the conditional Aalen-Johansen estimator, as developed in Bladt and Furrer (2023b). In our approach, we formulate a multi-state problem, where the underlying variable is the individual claim size, rather than time. The states in this model represent development periods, and we estimate the cumulative density function of individual claim sizes using the conditional Aalen-Johansen method as transition probabilities to an absorbing state. Our methodology reinterprets the concept of multi-state models and offers a strategy for modeling the complete curve of individual claim sizes. To illustrate our approach, we apply our model to both simulated and real datasets. Having access to the entire dataset enables us to support the use of our approach by comparing the predicted total final cost with the actual amount, as well as evaluating it in terms of the continuously ranked probability score.</summary></entry><entry><title type="html">Joint Modeling of Longitudinal Measurements and Time-to-event Outcomes Using BUGS</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/JointModelingofLongitudinalMeasurementsandTimetoeventOutcomesUsingBUGS.html" rel="alternate" type="text/html" title="Joint Modeling of Longitudinal Measurements and Time-to-event Outcomes Using BUGS" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/JointModelingofLongitudinalMeasurementsandTimetoeventOutcomesUsingBUGS</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/JointModelingofLongitudinalMeasurementsandTimetoeventOutcomesUsingBUGS.html">&lt;p&gt;The objective of this paper is to provide an introduction to the principles of Bayesian joint modeling of longitudinal measurements and time-to-event outcomes, as well as model implementation using the BUGS language syntax. This syntax can be executed directly using OpenBUGS or by utilizing convenient functions to invoke OpenBUGS and JAGS from R software. In this paper, all details of joint models are provided, ranging from simple to more advanced models. The presentation started with the joint modeling of a Gaussian longitudinal marker and time-to-event outcome. The implementation of the Bayesian paradigm of the model is reviewed. The strategies for simulating data from the JM are also discussed. A proportional hazard model with various forms of baseline hazards, along with the discussion of all possible association structures between the two sub-models are taken into consideration. The paper covers joint models with multivariate longitudinal measurements, zero-inflated longitudinal measurements, competing risks, and time-to-event with cure fraction. The models are illustrated by the analyses of several real data sets. All simulated and real data and code are available at \url{https://github.com/tbaghfalaki/JM-with-BUGS-and-JAGS}.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.07778&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Taban Baghfalaki, Mojtaba Ganjali, Antoine Barbieri, Reza Hashemi, Hélène Jacqmin-Gadda</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">The objective of this paper is to provide an introduction to the principles of Bayesian joint modeling of longitudinal measurements and time-to-event outcomes, as well as model implementation using the BUGS language syntax. This syntax can be executed directly using OpenBUGS or by utilizing convenient functions to invoke OpenBUGS and JAGS from R software. In this paper, all details of joint models are provided, ranging from simple to more advanced models. The presentation started with the joint modeling of a Gaussian longitudinal marker and time-to-event outcome. The implementation of the Bayesian paradigm of the model is reviewed. The strategies for simulating data from the JM are also discussed. A proportional hazard model with various forms of baseline hazards, along with the discussion of all possible association structures between the two sub-models are taken into consideration. The paper covers joint models with multivariate longitudinal measurements, zero-inflated longitudinal measurements, competing risks, and time-to-event with cure fraction. The models are illustrated by the analyses of several real data sets. All simulated and real data and code are available at \url{https://github.com/tbaghfalaki/JM-with-BUGS-and-JAGS}.</summary></entry><entry><title type="html">Kernel Debiased Plug-in Estimation: Simultaneous, Automated Debiasing without Influence Functions for Many Target Parameters</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/KernelDebiasedPluginEstimationSimultaneousAutomatedDebiasingwithoutInfluenceFunctionsforManyTargetParameters.html" rel="alternate" type="text/html" title="Kernel Debiased Plug-in Estimation: Simultaneous, Automated Debiasing without Influence Functions for Many Target Parameters" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/KernelDebiasedPluginEstimationSimultaneousAutomatedDebiasingwithoutInfluenceFunctionsforManyTargetParameters</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/KernelDebiasedPluginEstimationSimultaneousAutomatedDebiasingwithoutInfluenceFunctionsforManyTargetParameters.html">&lt;p&gt;When estimating target parameters in nonparametric models with nuisance parameters, substituting the unknown nuisances with nonparametric estimators can introduce ``plug-in bias.’’ Traditional methods addressing this suboptimal bias-variance trade-off rely on the \emph{influence function} (IF) of the target parameter. When estimating multiple target parameters, these methods require debiasing the nuisance parameter multiple times using the corresponding IFs, which poses analytical and computational challenges. In this work, we leverage the \emph{targeted maximum likelihood estimation} (TMLE) framework to propose a novel method named \emph{kernel debiased plug-in estimation} (KDPE). KDPE refines an initial estimate through regularized likelihood maximization steps, employing a nonparametric model based on \emph{reproducing kernel Hilbert spaces}. We show that KDPE: (i) simultaneously debiases \emph{all} pathwise differentiable target parameters that satisfy our regularity conditions, (ii) does not require the IF for implementation, and (iii) remains computationally tractable. We numerically illustrate the use of KDPE and validate our theoretical results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.08598&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Brian Cho, Yaroslav Mukhin, Kyra Gan, Ivana Malenica</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">When estimating target parameters in nonparametric models with nuisance parameters, substituting the unknown nuisances with nonparametric estimators can introduce ``plug-in bias.’’ Traditional methods addressing this suboptimal bias-variance trade-off rely on the \emph{influence function} (IF) of the target parameter. When estimating multiple target parameters, these methods require debiasing the nuisance parameter multiple times using the corresponding IFs, which poses analytical and computational challenges. In this work, we leverage the \emph{targeted maximum likelihood estimation} (TMLE) framework to propose a novel method named \emph{kernel debiased plug-in estimation} (KDPE). KDPE refines an initial estimate through regularized likelihood maximization steps, employing a nonparametric model based on \emph{reproducing kernel Hilbert spaces}. We show that KDPE: (i) simultaneously debiases \emph{all} pathwise differentiable target parameters that satisfy our regularity conditions, (ii) does not require the IF for implementation, and (iii) remains computationally tractable. We numerically illustrate the use of KDPE and validate our theoretical results.</summary></entry><entry><title type="html">LaLonde (1986) after Nearly Four Decades: Lessons Learned</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LaLonde1986afterNearlyFourDecadesLessonsLearned.html" rel="alternate" type="text/html" title="LaLonde (1986) after Nearly Four Decades: Lessons Learned" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LaLonde1986afterNearlyFourDecadesLessonsLearned</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LaLonde1986afterNearlyFourDecadesLessonsLearned.html">&lt;p&gt;In 1986, Robert LaLonde published an article that compared nonexperimental estimates to experimental benchmarks LaLonde (1986). He concluded that the nonexperimental methods at the time could not systematically replicate experimental benchmarks, casting doubt on the credibility of these methods. Following LaLonde’s critical assessment, there have been significant methodological advances and practical changes, including (i) an emphasis on estimators based on unconfoundedness, (ii) a focus on the importance of overlap in covariate distributions, (iii) the introduction of propensity score-based methods leading to doubly robust estimators, (iv) a greater emphasis on validation exercises to bolster research credibility, and (v) methods for estimating and exploiting treatment effect heterogeneity. To demonstrate the practical lessons from these advances, we reexamine the LaLonde data and the Imbens-Rubin-Sacerdote lottery data. We show that modern methods, when applied in contexts with significant covariate overlap, yield robust estimates for the adjusted differences between the treatment and control groups. However, this does not mean that these estimates are valid. To assess their credibility, validation exercises (such as placebo tests) are essential, whereas goodness of fit tests alone are inadequate. Our findings highlight the importance of closely examining the assignment process, carefully inspecting overlap, and conducting validation exercises when analyzing causal effects with nonexperimental data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00827&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Guido Imbens, Yiqing Xu</name></author><category term="stat.ME" /><summary type="html">In 1986, Robert LaLonde published an article that compared nonexperimental estimates to experimental benchmarks LaLonde (1986). He concluded that the nonexperimental methods at the time could not systematically replicate experimental benchmarks, casting doubt on the credibility of these methods. Following LaLonde’s critical assessment, there have been significant methodological advances and practical changes, including (i) an emphasis on estimators based on unconfoundedness, (ii) a focus on the importance of overlap in covariate distributions, (iii) the introduction of propensity score-based methods leading to doubly robust estimators, (iv) a greater emphasis on validation exercises to bolster research credibility, and (v) methods for estimating and exploiting treatment effect heterogeneity. To demonstrate the practical lessons from these advances, we reexamine the LaLonde data and the Imbens-Rubin-Sacerdote lottery data. We show that modern methods, when applied in contexts with significant covariate overlap, yield robust estimates for the adjusted differences between the treatment and control groups. However, this does not mean that these estimates are valid. To assess their credibility, validation exercises (such as placebo tests) are essential, whereas goodness of fit tests alone are inadequate. Our findings highlight the importance of closely examining the assignment process, carefully inspecting overlap, and conducting validation exercises when analyzing causal effects with nonexperimental data.</summary></entry><entry><title type="html">Learning Causal Abstractions of Linear Structural Causal Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LearningCausalAbstractionsofLinearStructuralCausalModels.html" rel="alternate" type="text/html" title="Learning Causal Abstractions of Linear Structural Causal Models" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LearningCausalAbstractionsofLinearStructuralCausalModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LearningCausalAbstractionsofLinearStructuralCausalModels.html">&lt;p&gt;The need for modelling causal knowledge at different levels of granularity arises in several settings. Causal Abstraction provides a framework for formalizing this problem by relating two Structural Causal Models at different levels of detail. Despite increasing interest in applying causal abstraction, e.g. in the interpretability of large machine learning models, the graphical and parametrical conditions under which a causal model can abstract another are not known. Furthermore, learning causal abstractions from data is still an open problem. In this work, we tackle both issues for linear causal models with linear abstraction functions. First, we characterize how the low-level coefficients and the abstraction function determine the high-level coefficients and how the high-level model constrains the causal ordering of low-level variables. Then, we apply our theoretical results to learn high-level and low-level causal models and their abstraction function from observational data. In particular, we introduce Abs-LiNGAM, a method that leverages the constraints induced by the learned high-level model and the abstraction function to speedup the recovery of the larger low-level model, under the assumption of non-Gaussian noise terms. In simulated settings, we show the effectiveness of learning causal abstractions from data and the potential of our method in improving scalability of causal discovery.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00394&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Riccardo Massidda, Sara Magliacane, Davide Bacciu</name></author><category term="stat.ME" /><summary type="html">The need for modelling causal knowledge at different levels of granularity arises in several settings. Causal Abstraction provides a framework for formalizing this problem by relating two Structural Causal Models at different levels of detail. Despite increasing interest in applying causal abstraction, e.g. in the interpretability of large machine learning models, the graphical and parametrical conditions under which a causal model can abstract another are not known. Furthermore, learning causal abstractions from data is still an open problem. In this work, we tackle both issues for linear causal models with linear abstraction functions. First, we characterize how the low-level coefficients and the abstraction function determine the high-level coefficients and how the high-level model constrains the causal ordering of low-level variables. Then, we apply our theoretical results to learn high-level and low-level causal models and their abstraction function from observational data. In particular, we introduce Abs-LiNGAM, a method that leverages the constraints induced by the learned high-level model and the abstraction function to speedup the recovery of the larger low-level model, under the assumption of non-Gaussian noise terms. In simulated settings, we show the effectiveness of learning causal abstractions from data and the potential of our method in improving scalability of causal discovery.</summary></entry><entry><title type="html">Likelihood Based Inference for ARMA Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LikelihoodBasedInferenceforARMAModels.html" rel="alternate" type="text/html" title="Likelihood Based Inference for ARMA Models" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LikelihoodBasedInferenceforARMAModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LikelihoodBasedInferenceforARMAModels.html">&lt;p&gt;Autoregressive moving average (ARMA) models are frequently used to analyze time series data. Despite the popularity of these models, likelihood-based inference for ARMA models has subtleties that have been previously identified but continue to cause difficulties in widely used data analysis strategies. We provide a summary of parameter estimation via maximum likelihood and discuss common pitfalls that may lead to sub-optimal parameter estimates. We propose a random initialization algorithm for parameter estimation that frequently yields higher likelihoods than traditional maximum likelihood estimation procedures. We then investigate the parameter uncertainty of maximum likelihood estimates, and propose the use of profile confidence intervals as a superior alternative to intervals derived from the Fisher’s information matrix. Through a series of simulation studies, we demonstrate the efficacy of our proposed algorithm and the improved nominal coverage of profile confidence intervals compared to the normal approximation based on Fisher’s Information.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.01198&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jesse Wheeler, Edward L. Ionides</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Autoregressive moving average (ARMA) models are frequently used to analyze time series data. Despite the popularity of these models, likelihood-based inference for ARMA models has subtleties that have been previously identified but continue to cause difficulties in widely used data analysis strategies. We provide a summary of parameter estimation via maximum likelihood and discuss common pitfalls that may lead to sub-optimal parameter estimates. We propose a random initialization algorithm for parameter estimation that frequently yields higher likelihoods than traditional maximum likelihood estimation procedures. We then investigate the parameter uncertainty of maximum likelihood estimates, and propose the use of profile confidence intervals as a superior alternative to intervals derived from the Fisher’s information matrix. Through a series of simulation studies, we demonstrate the efficacy of our proposed algorithm and the improved nominal coverage of profile confidence intervals compared to the normal approximation based on Fisher’s Information.</summary></entry><entry><title type="html">Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LocalDiscoverybyPartitioningPolynomialTimeCausalDiscoveryAroundExposureOutcomePairs.html" rel="alternate" type="text/html" title="Local Discovery by Partitioning: Polynomial-Time Causal Discovery Around Exposure-Outcome Pairs" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LocalDiscoverybyPartitioningPolynomialTimeCausalDiscoveryAroundExposureOutcomePairs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LocalDiscoverybyPartitioningPolynomialTimeCausalDiscoveryAroundExposureOutcomePairs.html">&lt;p&gt;Causal discovery is crucial for causal inference in observational studies, as it can enable the identification of valid adjustment sets (VAS) for unbiased effect estimation. However, global causal discovery is notoriously hard in the nonparametric setting, with exponential time and sample complexity in the worst case. To address this, we propose local discovery by partitioning (LDP): a local causal discovery method that is tailored for downstream inference tasks without requiring parametric and pretreatment assumptions. LDP is a constraint-based procedure that returns a VAS for an exposure-outcome pair under latent confounding, given sufficient conditions. The total number of independence tests performed is worst-case quadratic with respect to the cardinality of the variable set. Asymptotic theoretical guarantees are numerically validated on synthetic graphs. Adjustment sets from LDP yield less biased and more precise average treatment effect estimates than baseline discovery algorithms, with LDP outperforming on confounder recall, runtime, and test count for VAS discovery. Notably, LDP ran at least 1300x faster than baselines on a benchmark.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.17816&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jacqueline Maasch, Weishen Pan, Shantanu Gupta, Volodymyr Kuleshov, Kyra Gan, Fei Wang</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Causal discovery is crucial for causal inference in observational studies, as it can enable the identification of valid adjustment sets (VAS) for unbiased effect estimation. However, global causal discovery is notoriously hard in the nonparametric setting, with exponential time and sample complexity in the worst case. To address this, we propose local discovery by partitioning (LDP): a local causal discovery method that is tailored for downstream inference tasks without requiring parametric and pretreatment assumptions. LDP is a constraint-based procedure that returns a VAS for an exposure-outcome pair under latent confounding, given sufficient conditions. The total number of independence tests performed is worst-case quadratic with respect to the cardinality of the variable set. Asymptotic theoretical guarantees are numerically validated on synthetic graphs. Adjustment sets from LDP yield less biased and more precise average treatment effect estimates than baseline discovery algorithms, with LDP outperforming on confounder recall, runtime, and test count for VAS discovery. Notably, LDP ran at least 1300x faster than baselines on a benchmark.</summary></entry><entry><title type="html">Localized FDG loss in lung cancer lesions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LocalizedFDGlossinlungcancerlesions.html" rel="alternate" type="text/html" title="Localized FDG loss in lung cancer lesions" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LocalizedFDGlossinlungcancerlesions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LocalizedFDGlossinlungcancerlesions.html">&lt;p&gt;Purpose: Analysis of [18F]-Fluorodeoxyglucose (FDG) kinetics in cancer has been most often limited to the evaluation of the average uptake over relatively large volumes. Nevertheless, tumor lesion almost contains inflammatory infiltrates whose cells are characterized by a significant radioactivity washout due to the hydrolysis of FDG-6P catalyzed by glucose-6P phosphatase. The present study aimed to verify whether voxel-wise compartmental analysis of dynamic imaging can identify tumor regions characterized by tracer washout. Materials &amp;amp; Methods: The study included 11 patients with lung cancer submitted to PET/CT imaging for staging purposes. Tumor was defined by drawing a volume of interest loosely surrounding the lesion and considering all inside voxels with standardized uptake value (SUV) &amp;gt;40% of the maximum. After 20 minutes dynamic imaging centered on the heart, eight whole body scans were repeated. Six parametric maps were progressively generated by computing six regression lines that considered all eight frames, the last seven ones, and so on, up to the last three. Results: Progressively delaying the starting point of regression line computation identified a progressive increase in the prevalence of voxels with a negative slope. Conclusions: The voxel-wise parametric maps provided by compartmental analysis permits to identify a measurable volume characterized by radioactivity washout. The spatial localization of this pattern is compatible with the recognized preferential site of inflammatory infiltrates populating the tumor stroma and might improve the power of FDG imaging in monitoring the effectiveness of treatments aimed to empower the host immune response against the cancer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00382&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Davide Parodi, Edoardo Dighero, Giorgia Biddau, Francesca D&apos;Amico, Matteo Bauckneht, Cecilia Marini, Sara Garbarino, Cristina Campi, Michele Piana, Gianmario Sambuceti</name></author><category term="stat.AP" /><summary type="html">Purpose: Analysis of [18F]-Fluorodeoxyglucose (FDG) kinetics in cancer has been most often limited to the evaluation of the average uptake over relatively large volumes. Nevertheless, tumor lesion almost contains inflammatory infiltrates whose cells are characterized by a significant radioactivity washout due to the hydrolysis of FDG-6P catalyzed by glucose-6P phosphatase. The present study aimed to verify whether voxel-wise compartmental analysis of dynamic imaging can identify tumor regions characterized by tracer washout. Materials &amp;amp; Methods: The study included 11 patients with lung cancer submitted to PET/CT imaging for staging purposes. Tumor was defined by drawing a volume of interest loosely surrounding the lesion and considering all inside voxels with standardized uptake value (SUV) &amp;gt;40% of the maximum. After 20 minutes dynamic imaging centered on the heart, eight whole body scans were repeated. Six parametric maps were progressively generated by computing six regression lines that considered all eight frames, the last seven ones, and so on, up to the last three. Results: Progressively delaying the starting point of regression line computation identified a progressive increase in the prevalence of voxels with a negative slope. Conclusions: The voxel-wise parametric maps provided by compartmental analysis permits to identify a measurable volume characterized by radioactivity washout. The spatial localization of this pattern is compatible with the recognized preferential site of inflammatory infiltrates populating the tumor stroma and might improve the power of FDG imaging in monitoring the effectiveness of treatments aimed to empower the host immune response against the cancer.</summary></entry><entry><title type="html">Logistic Variational Bayes Revisited</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LogisticVariationalBayesRevisited.html" rel="alternate" type="text/html" title="Logistic Variational Bayes Revisited" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LogisticVariationalBayesRevisited</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/LogisticVariationalBayesRevisited.html">&lt;p&gt;Variational logistic regression is a popular method for approximate Bayesian inference seeing wide-spread use in many areas of machine learning including: Bayesian optimization, reinforcement learning and multi-instance learning to name a few. However, due to the intractability of the Evidence Lower Bound, authors have turned to the use of Monte Carlo, quadrature or bounds to perform inference, methods which are costly or give poor approximations to the true posterior.
  In this paper we introduce a new bound for the expectation of softplus function and subsequently show how this can be applied to variational logistic regression and Gaussian process classification. Unlike other bounds, our proposal does not rely on extending the variational family, or introducing additional parameters to ensure the bound is tight. In fact, we show that this bound is tighter than the state-of-the-art, and that the resulting variational posterior achieves state-of-the-art performance, whilst being significantly faster to compute than Monte-Carlo methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00713&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael Komodromos, Marina Evangelou, Sarah Filippi</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Variational logistic regression is a popular method for approximate Bayesian inference seeing wide-spread use in many areas of machine learning including: Bayesian optimization, reinforcement learning and multi-instance learning to name a few. However, due to the intractability of the Evidence Lower Bound, authors have turned to the use of Monte Carlo, quadrature or bounds to perform inference, methods which are costly or give poor approximations to the true posterior. In this paper we introduce a new bound for the expectation of softplus function and subsequently show how this can be applied to variational logistic regression and Gaussian process classification. Unlike other bounds, our proposal does not rely on extending the variational family, or introducing additional parameters to ensure the bound is tight. In fact, we show that this bound is tighter than the state-of-the-art, and that the resulting variational posterior achieves state-of-the-art performance, whilst being significantly faster to compute than Monte-Carlo methods.</summary></entry><entry><title type="html">MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MCGTAMetricConstrainedModelBasedClusteringusingGoodnessoffitTestswithAutocorrelations.html" rel="alternate" type="text/html" title="MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MCGTAMetricConstrainedModelBasedClusteringusingGoodnessoffitTestswithAutocorrelations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MCGTAMetricConstrainedModelBasedClusteringusingGoodnessoffitTestswithAutocorrelations.html">&lt;p&gt;A wide range of (multivariate) temporal (1D) and spatial (2D) data analysis tasks, such as grouping vehicle sensor trajectories, can be formulated as clustering with given metric constraints. Existing metric-constrained clustering algorithms overlook the rich correlation between feature similarity and metric distance, i.e., metric autocorrelation. The model-based variations of these clustering algorithms (e.g. TICC and STICC) achieve SOTA performance, yet suffer from computational instability and complexity by using a metric-constrained Expectation-Maximization procedure. In order to address these two problems, we propose a novel clustering algorithm, MC-GTA (Model-based Clustering via Goodness-of-fit Tests with Autocorrelations). Its objective is only composed of pairwise weighted sums of feature similarity terms (square Wasserstein-2 distance) and metric autocorrelation terms (a novel multivariate generalization of classic semivariogram). We show that MC-GTA is effectively minimizing the total hinge loss for intra-cluster observation pairs not passing goodness-of-fit tests, i.e., statistically not originating from the same distribution. Experiments on 1D/2D synthetic and real-world datasets demonstrate that MC-GTA successfully incorporates metric autocorrelation. It outperforms strong baselines by large margins (up to 14.3% in ARI and 32.1% in NMI) with faster and stabler optimization (&amp;gt;10x speedup).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18395&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhangyu Wang, Gengchen Mai, Krzysztof Janowicz, Ni Lao</name></author><category term="stat.AP" /><summary type="html">A wide range of (multivariate) temporal (1D) and spatial (2D) data analysis tasks, such as grouping vehicle sensor trajectories, can be formulated as clustering with given metric constraints. Existing metric-constrained clustering algorithms overlook the rich correlation between feature similarity and metric distance, i.e., metric autocorrelation. The model-based variations of these clustering algorithms (e.g. TICC and STICC) achieve SOTA performance, yet suffer from computational instability and complexity by using a metric-constrained Expectation-Maximization procedure. In order to address these two problems, we propose a novel clustering algorithm, MC-GTA (Model-based Clustering via Goodness-of-fit Tests with Autocorrelations). Its objective is only composed of pairwise weighted sums of feature similarity terms (square Wasserstein-2 distance) and metric autocorrelation terms (a novel multivariate generalization of classic semivariogram). We show that MC-GTA is effectively minimizing the total hinge loss for intra-cluster observation pairs not passing goodness-of-fit tests, i.e., statistically not originating from the same distribution. Experiments on 1D/2D synthetic and real-world datasets demonstrate that MC-GTA successfully incorporates metric autocorrelation. It outperforms strong baselines by large margins (up to 14.3% in ARI and 32.1% in NMI) with faster and stabler optimization (&amp;gt;10x speedup).</summary></entry><entry><title type="html">Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MachineLearningWhotoNudgeCausalvsPredictiveTargetinginaFieldExperimentonStudentFinancialAidRenewal.html" rel="alternate" type="text/html" title="Machine Learning Who to Nudge: Causal vs Predictive Targeting in a Field Experiment on Student Financial Aid Renewal" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MachineLearningWhotoNudgeCausalvsPredictiveTargetinginaFieldExperimentonStudentFinancialAidRenewal</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MachineLearningWhotoNudgeCausalvsPredictiveTargetinginaFieldExperimentonStudentFinancialAidRenewal.html">&lt;p&gt;In many settings, interventions may be more effective for some individuals than others, so that targeting interventions may be beneficial. We analyze the value of targeting in the context of a large-scale field experiment with over 53,000 college students, where the goal was to use “nudges” to encourage students to renew their financial-aid applications before a non-binding deadline. We begin with baseline approaches to targeting. First, we target based on a causal forest that estimates heterogeneous treatment effects and then assigns students to treatment according to those estimated to have the highest treatment effects. Next, we evaluate two alternative targeting policies, one targeting students with low predicted probability of renewing financial aid in the absence of the treatment, the other targeting those with high probability. The predicted baseline outcome is not the ideal criterion for targeting, nor is it a priori clear whether to prioritize low, high, or intermediate predicted probability. Nonetheless, targeting on low baseline outcomes is common in practice, for example because the relationship between individual characteristics and treatment effects is often difficult or impossible to estimate with historical data. We propose hybrid approaches that incorporate the strengths of both predictive approaches (accurate estimation) and causal approaches (correct criterion); we show that targeting intermediate baseline outcomes is most effective in our specific application, while targeting based on low baseline outcomes is detrimental. In one year of the experiment, nudging all students improved early filing by an average of 6.4 percentage points over a baseline average of 37% filing, and we estimate that targeting half of the students using our preferred policy attains around 75% of this benefit.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.08672&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Susan Athey, Niall Keleher, Jann Spiess</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">In many settings, interventions may be more effective for some individuals than others, so that targeting interventions may be beneficial. We analyze the value of targeting in the context of a large-scale field experiment with over 53,000 college students, where the goal was to use “nudges” to encourage students to renew their financial-aid applications before a non-binding deadline. We begin with baseline approaches to targeting. First, we target based on a causal forest that estimates heterogeneous treatment effects and then assigns students to treatment according to those estimated to have the highest treatment effects. Next, we evaluate two alternative targeting policies, one targeting students with low predicted probability of renewing financial aid in the absence of the treatment, the other targeting those with high probability. The predicted baseline outcome is not the ideal criterion for targeting, nor is it a priori clear whether to prioritize low, high, or intermediate predicted probability. Nonetheless, targeting on low baseline outcomes is common in practice, for example because the relationship between individual characteristics and treatment effects is often difficult or impossible to estimate with historical data. We propose hybrid approaches that incorporate the strengths of both predictive approaches (accurate estimation) and causal approaches (correct criterion); we show that targeting intermediate baseline outcomes is most effective in our specific application, while targeting based on low baseline outcomes is detrimental. In one year of the experiment, nudging all students improved early filing by an average of 6.4 percentage points over a baseline average of 37% filing, and we estimate that targeting half of the students using our preferred policy attains around 75% of this benefit.</summary></entry><entry><title type="html">Matrix-valued Factor Model with Time-varying Main Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MatrixvaluedFactorModelwithTimevaryingMainEffects.html" rel="alternate" type="text/html" title="Matrix-valued Factor Model with Time-varying Main Effects" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MatrixvaluedFactorModelwithTimevaryingMainEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MatrixvaluedFactorModelwithTimevaryingMainEffects.html">&lt;p&gt;We introduce the matrix-valued time-varying Main Effects Factor Model (MEFM). MEFM is a generalization to the traditional matrix-valued factor model (FM). We give rigorous definitions of MEFM and its identifications, and propose estimators for the time-varying grand mean, row and column main effects, and the row and column factor loading matrices for the common component. Rates of convergence for different estimators are spelt out, with asymptotic normality shown. The core rank estimator for the common component is also proposed, with consistency of the estimators presented. We propose a test for testing if FM is sufficient against the alternative that MEFM is necessary, and demonstrate the power of such a test in various simulation settings. We also demonstrate numerically the accuracy of our estimators in extended simulation experiments. A set of NYC Taxi traffic data is analysed and our test suggests that MEFM is indeed necessary for analysing the data against a traditional FM.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00128&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Clifford Lam, Zetai Cen</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We introduce the matrix-valued time-varying Main Effects Factor Model (MEFM). MEFM is a generalization to the traditional matrix-valued factor model (FM). We give rigorous definitions of MEFM and its identifications, and propose estimators for the time-varying grand mean, row and column main effects, and the row and column factor loading matrices for the common component. Rates of convergence for different estimators are spelt out, with asymptotic normality shown. The core rank estimator for the common component is also proposed, with consistency of the estimators presented. We propose a test for testing if FM is sufficient against the alternative that MEFM is necessary, and demonstrate the power of such a test in various simulation settings. We also demonstrate numerically the accuracy of our estimators in extended simulation experiments. A set of NYC Taxi traffic data is analysed and our test suggests that MEFM is indeed necessary for analysing the data against a traditional FM.</summary></entry><entry><title type="html">Measurement Error-Robust Causal Inference via Constructed Instrumental Variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MeasurementErrorRobustCausalInferenceviaConstructedInstrumentalVariables.html" rel="alternate" type="text/html" title="Measurement Error-Robust Causal Inference via Constructed Instrumental Variables" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MeasurementErrorRobustCausalInferenceviaConstructedInstrumentalVariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MeasurementErrorRobustCausalInferenceviaConstructedInstrumentalVariables.html">&lt;p&gt;Measurement error can often be harmful when estimating causal effects. Two scenarios in which this is the case are in the estimation of (a) the average treatment effect when confounders are measured with error and (b) the natural indirect effect when the exposure and/or confounders are measured with error. Methods adjusting for measurement error typically require external data or knowledge about the measurement error distribution. Here, we propose methodology not requiring any such information. Instead, we show that when the outcome regression is linear in the error-prone variables, consistent estimation of these causal effects can be recovered using constructed instrumental variables under certain conditions. These variables, which are functions of only the observed data, behave like instrumental variables for the error-prone variables. Using data from a study of the effects of prenatal exposure to heavy metals on growth and neurodevelopment in Bangladeshi mother-infant pairs, we apply our methodology to estimate (a) the effect of lead exposure on birth length while controlling for maternal protein intake, and (b) lead exposure’s role in mediating the effect of maternal protein intake on birth length. Protein intake is calculated from food journal entries, and is suspected to be highly prone to measurement error.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00940&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Caleb H. Miles, Linda Valeri, Brent Coull</name></author><category term="stat.ME" /><summary type="html">Measurement error can often be harmful when estimating causal effects. Two scenarios in which this is the case are in the estimation of (a) the average treatment effect when confounders are measured with error and (b) the natural indirect effect when the exposure and/or confounders are measured with error. Methods adjusting for measurement error typically require external data or knowledge about the measurement error distribution. Here, we propose methodology not requiring any such information. Instead, we show that when the outcome regression is linear in the error-prone variables, consistent estimation of these causal effects can be recovered using constructed instrumental variables under certain conditions. These variables, which are functions of only the observed data, behave like instrumental variables for the error-prone variables. Using data from a study of the effects of prenatal exposure to heavy metals on growth and neurodevelopment in Bangladeshi mother-infant pairs, we apply our methodology to estimate (a) the effect of lead exposure on birth length while controlling for maternal protein intake, and (b) lead exposure’s role in mediating the effect of maternal protein intake on birth length. Protein intake is calculated from food journal entries, and is suspected to be highly prone to measurement error.</summary></entry><entry><title type="html">Model-based Clustering of Zero-Inflated Single-Cell RNA Sequencing Data via the EM Algorithm</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ModelbasedClusteringofZeroInflatedSingleCellRNASequencingDataviatheEMAlgorithm.html" rel="alternate" type="text/html" title="Model-based Clustering of Zero-Inflated Single-Cell RNA Sequencing Data via the EM Algorithm" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ModelbasedClusteringofZeroInflatedSingleCellRNASequencingDataviatheEMAlgorithm</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ModelbasedClusteringofZeroInflatedSingleCellRNASequencingDataviatheEMAlgorithm.html">&lt;p&gt;Biological cells can be distinguished by their phenotype or at the molecular level, based on their genome, epigenome, and transcriptome. This paper focuses on the transcriptome, which encompasses all the RNA transcripts in a given cell population, indicating the genes being expressed at a given time. We consider single-cell RNA sequencing data and develop a novel model-based clustering method to group cells based on their transcriptome profiles. Our clustering approach takes into account the presence of zero inflation in the data, which can occur due to genuine biological zeros or technological noise. The proposed model for clustering involves a mixture of zero-inflated Poisson or zero-inflated negative binomial distributions, and parameter estimation is carried out using the EM algorithm. We evaluate the performance of our proposed methodology through simulation studies and analyses of publicly available datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00245&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zahra AghahosseinaliShirazi, Pedro A. Rangel, Camila P. E. de Souza</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Biological cells can be distinguished by their phenotype or at the molecular level, based on their genome, epigenome, and transcriptome. This paper focuses on the transcriptome, which encompasses all the RNA transcripts in a given cell population, indicating the genes being expressed at a given time. We consider single-cell RNA sequencing data and develop a novel model-based clustering method to group cells based on their transcriptome profiles. Our clustering approach takes into account the presence of zero inflation in the data, which can occur due to genuine biological zeros or technological noise. The proposed model for clustering involves a mixture of zero-inflated Poisson or zero-inflated negative binomial distributions, and parameter estimation is carried out using the EM algorithm. We evaluate the performance of our proposed methodology through simulation studies and analyses of publicly available datasets.</summary></entry><entry><title type="html">Multiple Comparison Procedures for Simultaneous Inference in Functional MANOVA</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MultipleComparisonProceduresforSimultaneousInferenceinFunctionalMANOVA.html" rel="alternate" type="text/html" title="Multiple Comparison Procedures for Simultaneous Inference in Functional MANOVA" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MultipleComparisonProceduresforSimultaneousInferenceinFunctionalMANOVA</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MultipleComparisonProceduresforSimultaneousInferenceinFunctionalMANOVA.html">&lt;p&gt;Functional data analysis is becoming increasingly popular to study data from real-valued random functions. Nevertheless, there is a lack of multiple testing procedures for such data. These are particularly important in factorial designs to compare different groups or to infer factor effects. We propose a new class of testing procedures for arbitrary linear hypotheses in general factorial designs with functional data. Our methods allow global as well as multiple inference of both, univariate and multivariate mean functions without assuming particular error distributions nor homoscedasticity. That is, we allow for different structures of the covariance functions between groups. To this end, we use point-wise quadratic-form-type test functions that take potential heteroscedasticity into account. Taking the supremum over each test function, we define a class of local test statistics. We analyse their (joint) asymptotic behaviour and propose a resampling approach to approximate the limit distributions. The resulting global and multiple testing procedures are asymptotic valid under weak conditions and applicable in general functional MANOVA settings. We evaluate their small-sample performance in extensive simulations and finally illustrate their applicability by analysing a multivariate functional air pollution data set.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01242&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Merle Munko, Marc Ditzhaus, Markus Pauly, {\L}ukasz Smaga</name></author><category term="stat.ME" /><summary type="html">Functional data analysis is becoming increasingly popular to study data from real-valued random functions. Nevertheless, there is a lack of multiple testing procedures for such data. These are particularly important in factorial designs to compare different groups or to infer factor effects. We propose a new class of testing procedures for arbitrary linear hypotheses in general factorial designs with functional data. Our methods allow global as well as multiple inference of both, univariate and multivariate mean functions without assuming particular error distributions nor homoscedasticity. That is, we allow for different structures of the covariance functions between groups. To this end, we use point-wise quadratic-form-type test functions that take potential heteroscedasticity into account. Taking the supremum over each test function, we define a class of local test statistics. We analyse their (joint) asymptotic behaviour and propose a resampling approach to approximate the limit distributions. The resulting global and multiple testing procedures are asymptotic valid under weak conditions and applicable in general functional MANOVA settings. We evaluate their small-sample performance in extensive simulations and finally illustrate their applicability by analysing a multivariate functional air pollution data set.</summary></entry><entry><title type="html">Multiply-Robust Causal Change Attribution</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MultiplyRobustCausalChangeAttribution.html" rel="alternate" type="text/html" title="Multiply-Robust Causal Change Attribution" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MultiplyRobustCausalChangeAttribution</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MultiplyRobustCausalChangeAttribution.html">&lt;p&gt;Comparing two samples of data, we observe a change in the distribution of an outcome variable. In the presence of multiple explanatory variables, how much of the change can be explained by each possible cause? We develop a new estimation strategy that, given a causal model, combines regression and re-weighting methods to quantify the contribution of each causal mechanism. Our proposed methodology is multiply robust, meaning that it still recovers the target parameter under partial misspecification. We prove that our estimator is consistent and asymptotically normal. Moreover, it can be incorporated into existing frameworks for causal attribution, such as Shapley values, which will inherit the consistency and large-sample distribution properties. Our method demonstrates excellent performance in Monte Carlo simulations, and we show its usefulness in an empirical application. Our method is implemented as part of the Python library DoWhy (arXiv:2011.04216, arXiv:2206.06821).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.08839&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Victor Quintas-Martinez, Mohammad Taha Bahadori, Eduardo Santiago, Jeff Mu, Dominik Janzing, David Heckerman</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Comparing two samples of data, we observe a change in the distribution of an outcome variable. In the presence of multiple explanatory variables, how much of the change can be explained by each possible cause? We develop a new estimation strategy that, given a causal model, combines regression and re-weighting methods to quantify the contribution of each causal mechanism. Our proposed methodology is multiply robust, meaning that it still recovers the target parameter under partial misspecification. We prove that our estimator is consistent and asymptotically normal. Moreover, it can be incorporated into existing frameworks for causal attribution, such as Shapley values, which will inherit the consistency and large-sample distribution properties. Our method demonstrates excellent performance in Monte Carlo simulations, and we show its usefulness in an empirical application. Our method is implemented as part of the Python library DoWhy (arXiv:2011.04216, arXiv:2206.06821).</summary></entry><entry><title type="html">Multivariate Matérn Models – A Spectral Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MultivariateMat%C3%A9rnModelsASpectralApproach.html" rel="alternate" type="text/html" title="Multivariate Matérn Models – A Spectral Approach" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MultivariateMat%C3%A9rnModelsASpectralApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/MultivariateMat%C3%A9rnModelsASpectralApproach.html">&lt;p&gt;The classical Mat&apos;ern model has been a staple in spatial statistics. Novel data-rich applications in environmental and physical sciences, however, call for new, flexible vector-valued spatial and space-time models. Therefore, the extension of the classical Mat&apos;ern model has been a problem of active theoretical and methodological interest. In this paper, we offer a new perspective to extending the Mat&apos;ern covariance model to the vector-valued setting. We adopt a spectral, stochastic integral approach, which allows us to address challenging issues on the validity of the covariance structure and at the same time to obtain new, flexible, and interpretable models. In particular, our multivariate extensions of the Mat&apos;ern model allow for asymmetric covariance structures. Moreover, the spectral approach provides an essentially complete flexibility in modeling the local structure of the process. We establish closed-form representations of the cross-covariances when available, compare them with existing models, simulate Gaussian instances of these new processes, and demonstrate estimation of the model’s parameters through maximum likelihood. An application of the new class of multivariate Mat&apos;ern models to environmental data indicate their success in capturing inherent covariance-asymmetry phenomena.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.02584&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Drew Yarger, Stilian Stoev, Tailen Hsing</name></author><category term="stat.ME," /><category term="stat.OT," /><category term="stat.TH" /><summary type="html">The classical Mat&apos;ern model has been a staple in spatial statistics. Novel data-rich applications in environmental and physical sciences, however, call for new, flexible vector-valued spatial and space-time models. Therefore, the extension of the classical Mat&apos;ern model has been a problem of active theoretical and methodological interest. In this paper, we offer a new perspective to extending the Mat&apos;ern covariance model to the vector-valued setting. We adopt a spectral, stochastic integral approach, which allows us to address challenging issues on the validity of the covariance structure and at the same time to obtain new, flexible, and interpretable models. In particular, our multivariate extensions of the Mat&apos;ern model allow for asymmetric covariance structures. Moreover, the spectral approach provides an essentially complete flexibility in modeling the local structure of the process. We establish closed-form representations of the cross-covariances when available, compare them with existing models, simulate Gaussian instances of these new processes, and demonstrate estimation of the model’s parameters through maximum likelihood. An application of the new class of multivariate Mat&apos;ern models to environmental data indicate their success in capturing inherent covariance-asymmetry phenomena.</summary></entry><entry><title type="html">Not feeling the buzz: Correction study of mispricing and inefficiency in online sportsbooks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/NotfeelingthebuzzCorrectionstudyofmispricingandinefficiencyinonlinesportsbooks.html" rel="alternate" type="text/html" title="Not feeling the buzz: Correction study of mispricing and inefficiency in online sportsbooks" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/NotfeelingthebuzzCorrectionstudyofmispricingandinefficiencyinonlinesportsbooks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/NotfeelingthebuzzCorrectionstudyofmispricingandinefficiencyinonlinesportsbooks.html">&lt;p&gt;We present a replication and correction of a recent article (Ramirez, P., Reade, J.J., Singleton, C., Betting on a buzz: Mispricing and inefficiency in online sportsbooks, International Journal of Forecasting, 39:3, 2023, pp. 1413-1423, doi: 10.1016/j.ijforecast.2022.07.011). RRS measure profile page views on Wikipedia to generate a “buzz factor” metric for tennis players and show that it can be used to form a profitable gambling strategy by predicting bookmaker mispricing. Here, we use the same dataset as RRS to reproduce their results exactly, thus confirming the robustness of their mispricing claim. However, we discover that the published betting results are significantly affected by a single bet (the “Hercog” bet), which returns substantial outlier profits based on erroneously long odds. When this data quality issue is resolved, the majority of reported profits disappear and only one strategy, which bets on “competitive” matches, remains significantly profitable in the original out-of-sample period. While one profitable strategy offers weaker support than the original study, it still provides an indication that market inefficiencies may exist, as originally claimed by RRS. As an extension, we continue backtesting after 2020 on a cleaned dataset. Results show that (a) the “competitive” strategy generates no further profits, potentially suggesting markets have become more efficient, and (b) model coefficients estimated over this more recent period are no longer reliable predictors of bookmaker mispricing. We present this work as a case study demonstrating the importance of replication studies in sports forecasting, and the necessity to clean data. We open-source release comprehensive datasets and code.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.01740&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lawrence Clegg, John Cartlidge</name></author><category term="stat.AP" /><summary type="html">We present a replication and correction of a recent article (Ramirez, P., Reade, J.J., Singleton, C., Betting on a buzz: Mispricing and inefficiency in online sportsbooks, International Journal of Forecasting, 39:3, 2023, pp. 1413-1423, doi: 10.1016/j.ijforecast.2022.07.011). RRS measure profile page views on Wikipedia to generate a “buzz factor” metric for tennis players and show that it can be used to form a profitable gambling strategy by predicting bookmaker mispricing. Here, we use the same dataset as RRS to reproduce their results exactly, thus confirming the robustness of their mispricing claim. However, we discover that the published betting results are significantly affected by a single bet (the “Hercog” bet), which returns substantial outlier profits based on erroneously long odds. When this data quality issue is resolved, the majority of reported profits disappear and only one strategy, which bets on “competitive” matches, remains significantly profitable in the original out-of-sample period. While one profitable strategy offers weaker support than the original study, it still provides an indication that market inefficiencies may exist, as originally claimed by RRS. As an extension, we continue backtesting after 2020 on a cleaned dataset. Results show that (a) the “competitive” strategy generates no further profits, potentially suggesting markets have become more efficient, and (b) model coefficients estimated over this more recent period are no longer reliable predictors of bookmaker mispricing. We present this work as a case study demonstrating the importance of replication studies in sports forecasting, and the necessity to clean data. We open-source release comprehensive datasets and code.</summary></entry><entry><title type="html">On Weighted Orthogonal Learners for Heterogeneous Treatment Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/OnWeightedOrthogonalLearnersforHeterogeneousTreatmentEffects.html" rel="alternate" type="text/html" title="On Weighted Orthogonal Learners for Heterogeneous Treatment Effects" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/OnWeightedOrthogonalLearnersforHeterogeneousTreatmentEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/OnWeightedOrthogonalLearnersforHeterogeneousTreatmentEffects.html">&lt;p&gt;Motivated by applications in personalized medicine and individualized policymaking, there is a growing interest in techniques for quantifying treatment effect heterogeneity in terms of the conditional average treatment effect (CATE). Some of the most prominent methods for CATE estimation developed in recent years are T-Learner, DR-Learner and R-Learner. The latter two were designed to improve on the former by being Neyman-orthogonal. However, the relations between them remain unclear, and likewise the literature remains vague on whether these learners converge to a useful quantity or (functional) estimand when the underlying optimization procedure is restricted to a class of functions that does not include the CATE. In this article, we provide insight into these questions by discussing DR-Learner and R-Learner as special cases of a general class of weighted Neyman-orthogonal learners for the CATE, for which we moreover derive oracle bounds. Our results shed light on how one may construct Neyman-orthogonal learners with desirable properties, on when DR-Learner may be preferred over R-Learner (and vice versa), and on novel learners that may sometimes be preferable to either of these. Theoretical findings are confirmed using results from simulation studies on synthetic data, as well as an application in critical care medicine.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.12687&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Pawel Morzywolek, Johan Decruyenaere, Stijn Vansteelandt</name></author><category term="stat.ME" /><summary type="html">Motivated by applications in personalized medicine and individualized policymaking, there is a growing interest in techniques for quantifying treatment effect heterogeneity in terms of the conditional average treatment effect (CATE). Some of the most prominent methods for CATE estimation developed in recent years are T-Learner, DR-Learner and R-Learner. The latter two were designed to improve on the former by being Neyman-orthogonal. However, the relations between them remain unclear, and likewise the literature remains vague on whether these learners converge to a useful quantity or (functional) estimand when the underlying optimization procedure is restricted to a class of functions that does not include the CATE. In this article, we provide insight into these questions by discussing DR-Learner and R-Learner as special cases of a general class of weighted Neyman-orthogonal learners for the CATE, for which we moreover derive oracle bounds. Our results shed light on how one may construct Neyman-orthogonal learners with desirable properties, on when DR-Learner may be preferred over R-Learner (and vice versa), and on novel learners that may sometimes be preferable to either of these. Theoretical findings are confirmed using results from simulation studies on synthetic data, as well as an application in critical care medicine.</summary></entry><entry><title type="html">On the Addams family of discrete frailty distributions for modelling multivariate case I interval-censored data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/OntheAddamsfamilyofdiscretefrailtydistributionsformodellingmultivariatecaseIintervalcensoreddata.html" rel="alternate" type="text/html" title="On the Addams family of discrete frailty distributions for modelling multivariate case I interval-censored data" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/OntheAddamsfamilyofdiscretefrailtydistributionsformodellingmultivariatecaseIintervalcensoreddata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/OntheAddamsfamilyofdiscretefrailtydistributionsformodellingmultivariatecaseIintervalcensoreddata.html">&lt;p&gt;Random effect models for time-to-event data, also known as frailty models, provide a conceptually appealing way of quantifying association between survival times and of representing heterogeneities resulting from factors which may be difficult or impossible to measure. In the literature, the random effect is usually assumed to have a continuous distribution. However, in some areas of application, discrete frailty distributions may be more appropriate. The present paper is about the implementation and interpretation of the Addams family of discrete frailty distributions. We propose methods of estimation for this family of densities in the context of shared frailty models for the hazard rates for case I interval-censored data. Our optimization framework allows for stratification of random effect distributions by covariates. We highlight interpretational advantages of the Addams family of discrete frailty distributions and the K-point distribution as compared to other frailty distributions. A unique feature of the Addams family and the K-point distribution is that the support of the frailty distribution depends on its parameters. This feature is best exploited by imposing a model on the distributional parameters, resulting in a model with non-homogeneous covariate effects that can be analysed using standard measures such as the hazard ratio. Our methods are illustrated with applications to multivariate case I interval-censored infection data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00804&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maximilian Bardo, Niel Hens, Steffen Unkel</name></author><category term="stat.ME" /><summary type="html">Random effect models for time-to-event data, also known as frailty models, provide a conceptually appealing way of quantifying association between survival times and of representing heterogeneities resulting from factors which may be difficult or impossible to measure. In the literature, the random effect is usually assumed to have a continuous distribution. However, in some areas of application, discrete frailty distributions may be more appropriate. The present paper is about the implementation and interpretation of the Addams family of discrete frailty distributions. We propose methods of estimation for this family of densities in the context of shared frailty models for the hazard rates for case I interval-censored data. Our optimization framework allows for stratification of random effect distributions by covariates. We highlight interpretational advantages of the Addams family of discrete frailty distributions and the K-point distribution as compared to other frailty distributions. A unique feature of the Addams family and the K-point distribution is that the support of the frailty distribution depends on its parameters. This feature is best exploited by imposing a model on the distributional parameters, resulting in a model with non-homogeneous covariate effects that can be analysed using standard measures such as the hazard ratio. Our methods are illustrated with applications to multivariate case I interval-censored infection data.</summary></entry><entry><title type="html">On the modelling and prediction of high-dimensional functional time series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Onthemodellingandpredictionofhighdimensionalfunctionaltimeseries.html" rel="alternate" type="text/html" title="On the modelling and prediction of high-dimensional functional time series" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Onthemodellingandpredictionofhighdimensionalfunctionaltimeseries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Onthemodellingandpredictionofhighdimensionalfunctionaltimeseries.html">&lt;p&gt;We propose a two-step procedure to model and predict high-dimensional functional time series, where the number of function-valued time series $p$ is large in relation to the length of time series $n$. Our first step performs an eigenanalysis of a positive definite matrix, which leads to a one-to-one linear transformation for the original high-dimensional functional time series, and the transformed curve series can be segmented into several groups such that any two subseries from any two different groups are uncorrelated both contemporaneously and serially. Consequently in our second step those groups are handled separately without the information loss on the overall linear dynamic structure. The second step is devoted to establishing a finite-dimensional dynamical structure for all the transformed functional time series within each group. Furthermore the finite-dimensional structure is represented by that of a vector time series. Modelling and forecasting for the original high-dimensional functional time series are realized via those for the vector time series in all the groups. We investigate the theoretical properties of our proposed methods, and illustrate the finite-sample performance through both extensive simulation and two real datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00700&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jinyuan Chang, Qin Fang, Xinghao Qiao, Qiwei Yao</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We propose a two-step procedure to model and predict high-dimensional functional time series, where the number of function-valued time series $p$ is large in relation to the length of time series $n$. Our first step performs an eigenanalysis of a positive definite matrix, which leads to a one-to-one linear transformation for the original high-dimensional functional time series, and the transformed curve series can be segmented into several groups such that any two subseries from any two different groups are uncorrelated both contemporaneously and serially. Consequently in our second step those groups are handled separately without the information loss on the overall linear dynamic structure. The second step is devoted to establishing a finite-dimensional dynamical structure for all the transformed functional time series within each group. Furthermore the finite-dimensional structure is represented by that of a vector time series. Modelling and forecasting for the original high-dimensional functional time series are realized via those for the vector time series in all the groups. We investigate the theoretical properties of our proposed methods, and illustrate the finite-sample performance through both extensive simulation and two real datasets.</summary></entry><entry><title type="html">Parameterizations for Gradient-based Markov Chain Monte Carlo on the Stiefel Manifold: A Comparative Study</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ParameterizationsforGradientbasedMarkovChainMonteCarloontheStiefelManifoldAComparativeStudy.html" rel="alternate" type="text/html" title="Parameterizations for Gradient-based Markov Chain Monte Carlo on the Stiefel Manifold: A Comparative Study" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ParameterizationsforGradientbasedMarkovChainMonteCarloontheStiefelManifoldAComparativeStudy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ParameterizationsforGradientbasedMarkovChainMonteCarloontheStiefelManifoldAComparativeStudy.html">&lt;p&gt;Orthogonal matrices play an important role in probability and statistics, particularly in high-dimensional statistical models. Parameterizing these models using orthogonal matrices facilitates dimension reduction and parameter identification. However, establishing the theoretical validity of statistical inference in these models from a frequentist perspective is challenging, leading to a preference for Bayesian approaches because of their ability to offer consistent uncertainty quantification. Markov chain Monte Carlo methods are commonly used for numerical approximation of posterior distributions, and sampling on the Stiefel manifold, which comprises orthogonal matrices, poses significant difficulties. While various strategies have been proposed for this purpose, gradient-based Markov chain Monte Carlo with parameterizations is the most efficient. However, a comprehensive comparison of these parameterizations is lacking in the existing literature. This study aims to address this gap by evaluating numerical efficiency of the four alternative parameterizations of orthogonal matrices under equivalent conditions. The evaluation was conducted for four problems. The results suggest that polar expansion parameterization is the most efficient, particularly for the high-dimensional and complex problems. However, all parameterizations exhibit limitations in significantly high-dimensional or difficult tasks, emphasizing the need for further advancements in sampling methods for orthogonal matrices.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.07434&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Masahiro Tanaka</name></author><category term="stat.CO" /><summary type="html">Orthogonal matrices play an important role in probability and statistics, particularly in high-dimensional statistical models. Parameterizing these models using orthogonal matrices facilitates dimension reduction and parameter identification. However, establishing the theoretical validity of statistical inference in these models from a frequentist perspective is challenging, leading to a preference for Bayesian approaches because of their ability to offer consistent uncertainty quantification. Markov chain Monte Carlo methods are commonly used for numerical approximation of posterior distributions, and sampling on the Stiefel manifold, which comprises orthogonal matrices, poses significant difficulties. While various strategies have been proposed for this purpose, gradient-based Markov chain Monte Carlo with parameterizations is the most efficient. However, a comprehensive comparison of these parameterizations is lacking in the existing literature. This study aims to address this gap by evaluating numerical efficiency of the four alternative parameterizations of orthogonal matrices under equivalent conditions. The evaluation was conducted for four problems. The results suggest that polar expansion parameterization is the most efficient, particularly for the high-dimensional and complex problems. However, all parameterizations exhibit limitations in significantly high-dimensional or difficult tasks, emphasizing the need for further advancements in sampling methods for orthogonal matrices.</summary></entry><entry><title type="html">Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/PeekingwithPEAKSequentialNonparametricCompositeHypothesisTestsforMeansofMultipleDataStreams.html" rel="alternate" type="text/html" title="Peeking with PEAK: Sequential, Nonparametric Composite Hypothesis Tests for Means of Multiple Data Streams" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/PeekingwithPEAKSequentialNonparametricCompositeHypothesisTestsforMeansofMultipleDataStreams</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/PeekingwithPEAKSequentialNonparametricCompositeHypothesisTestsforMeansofMultipleDataStreams.html">&lt;p&gt;We propose a novel nonparametric sequential test for composite hypotheses for means of multiple data streams. Our proposed method, \emph{peeking with expectation-based averaged capital} (PEAK), builds upon the testing-by-betting framework and provides a non-asymptotic $\alpha$-level test across any stopping time. Our contributions are two-fold: (1) we propose a novel betting scheme and provide theoretical guarantees on type-I error control, power, and asymptotic growth rate/$e$-power in the setting of a single data stream; (2) we introduce PEAK, a generalization of this betting scheme to multiple streams, that (i) avoids using wasteful union bounds via averaging, (ii) is a test of power one under mild regularity conditions on the sampling scheme of the streams, and (iii) reduces computational overhead when applying the testing-as-betting approaches for pure-exploration bandit problems. We illustrate the practical benefits of PEAK using both synthetic and real-world HeartSteps datasets. Our experiments show that PEAK provides up to an 85\% reduction in the number of samples before stopping compared to existing stopping rules for pure-exploration bandit problems, and matches the performance of state-of-the-art sequential tests while improving upon computational complexity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.06122&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Brian Cho, Kyra Gan, Nathan Kallus</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We propose a novel nonparametric sequential test for composite hypotheses for means of multiple data streams. Our proposed method, \emph{peeking with expectation-based averaged capital} (PEAK), builds upon the testing-by-betting framework and provides a non-asymptotic $\alpha$-level test across any stopping time. Our contributions are two-fold: (1) we propose a novel betting scheme and provide theoretical guarantees on type-I error control, power, and asymptotic growth rate/$e$-power in the setting of a single data stream; (2) we introduce PEAK, a generalization of this betting scheme to multiple streams, that (i) avoids using wasteful union bounds via averaging, (ii) is a test of power one under mild regularity conditions on the sampling scheme of the streams, and (iii) reduces computational overhead when applying the testing-as-betting approaches for pure-exploration bandit problems. We illustrate the practical benefits of PEAK using both synthetic and real-world HeartSteps datasets. Our experiments show that PEAK provides up to an 85\% reduction in the number of samples before stopping compared to existing stopping rules for pure-exploration bandit problems, and matches the performance of state-of-the-art sequential tests while improving upon computational complexity.</summary></entry><entry><title type="html">Planning for Gold: Sample Splitting for Valid Powerful Design of Observational Studies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/PlanningforGoldSampleSplittingforValidPowerfulDesignofObservationalStudies.html" rel="alternate" type="text/html" title="Planning for Gold: Sample Splitting for Valid Powerful Design of Observational Studies" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/PlanningforGoldSampleSplittingforValidPowerfulDesignofObservationalStudies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/PlanningforGoldSampleSplittingforValidPowerfulDesignofObservationalStudies.html">&lt;p&gt;Observational studies are valuable tools for inferring causal effects in the absence of controlled experiments. However, these studies may be biased due to the presence of some relevant, unmeasured set of covariates. The design of an observational study has a prominent effect on its sensitivity to hidden biases, and the best design may not be apparent without examining the data. One approach to facilitate a data-inspired design is to split the sample into a planning sample for choosing the design and an analysis sample for making inferences. We devise a powerful and flexible method for selecting outcomes in the planning sample when an unknown number of outcomes are affected by the treatment. We investigate the theoretical properties of our method and conduct extensive simulations that demonstrate pronounced benefits, especially at higher levels of allowance for unmeasured confounding. Finally, we demonstrate our method in an observational study of the multi-dimensional impacts of a devastating flood in Bangladesh.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00866&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>William Bekerman, Abhinandan Dalal, Carlo del Ninno, Dylan S. Small</name></author><category term="stat.ME" /><summary type="html">Observational studies are valuable tools for inferring causal effects in the absence of controlled experiments. However, these studies may be biased due to the presence of some relevant, unmeasured set of covariates. The design of an observational study has a prominent effect on its sensitivity to hidden biases, and the best design may not be apparent without examining the data. One approach to facilitate a data-inspired design is to split the sample into a planning sample for choosing the design and an analysis sample for making inferences. We devise a powerful and flexible method for selecting outcomes in the planning sample when an unknown number of outcomes are affected by the treatment. We investigate the theoretical properties of our method and conduct extensive simulations that demonstrate pronounced benefits, especially at higher levels of allowance for unmeasured confounding. Finally, we demonstrate our method in an observational study of the multi-dimensional impacts of a devastating flood in Bangladesh.</summary></entry><entry><title type="html">Profiled Transfer Learning for High Dimensional Linear Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ProfiledTransferLearningforHighDimensionalLinearModel.html" rel="alternate" type="text/html" title="Profiled Transfer Learning for High Dimensional Linear Model" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ProfiledTransferLearningforHighDimensionalLinearModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ProfiledTransferLearningforHighDimensionalLinearModel.html">&lt;p&gt;We develop here a novel transfer learning methodology called Profiled Transfer Learning (PTL). The method is based on the \textit{approximate-linear} assumption between the source and target parameters. Compared with the commonly assumed \textit{vanishing-difference} assumption and \textit{low-rank} assumption in the literature, the \textit{approximate-linear} assumption is more flexible and less stringent. Specifically, the PTL estimator is constructed by two major steps. Firstly, we regress the response on the transferred feature, leading to the profiled responses. Subsequently, we learn the regression relationship between profiled responses and the covariates on the target data. The final estimator is then assembled based on the \textit{approximate-linear} relationship. To theoretically support the PTL estimator, we derive the non-asymptotic upper bound and minimax lower bound. We find that the PTL estimator is minimax optimal under appropriate regularity conditions. Extensive simulation studies are presented to demonstrate the finite sample performance of the new method. A real data example about sentence prediction is also presented with very encouraging results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00701&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ziqian Lin, Junlong Zhao, Fang Wang, Hansheng Wang</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We develop here a novel transfer learning methodology called Profiled Transfer Learning (PTL). The method is based on the \textit{approximate-linear} assumption between the source and target parameters. Compared with the commonly assumed \textit{vanishing-difference} assumption and \textit{low-rank} assumption in the literature, the \textit{approximate-linear} assumption is more flexible and less stringent. Specifically, the PTL estimator is constructed by two major steps. Firstly, we regress the response on the transferred feature, leading to the profiled responses. Subsequently, we learn the regression relationship between profiled responses and the covariates on the target data. The final estimator is then assembled based on the \textit{approximate-linear} relationship. To theoretically support the PTL estimator, we derive the non-asymptotic upper bound and minimax lower bound. We find that the PTL estimator is minimax optimal under appropriate regularity conditions. Extensive simulation studies are presented to demonstrate the finite sample performance of the new method. A real data example about sentence prediction is also presented with very encouraging results.</summary></entry><entry><title type="html">Provably Scalable Black-Box Variational Inference with Structured Variational Families</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ProvablyScalableBlackBoxVariationalInferencewithStructuredVariationalFamilies.html" rel="alternate" type="text/html" title="Provably Scalable Black-Box Variational Inference with Structured Variational Families" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ProvablyScalableBlackBoxVariationalInferencewithStructuredVariationalFamilies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ProvablyScalableBlackBoxVariationalInferencewithStructuredVariationalFamilies.html">&lt;p&gt;Variational families with full-rank covariance approximations are known not to work well in black-box variational inference (BBVI), both empirically and theoretically. In fact, recent computational complexity results for BBVI have established that full-rank variational families scale poorly with the dimensionality of the problem compared to e.g. mean-field families. This is particularly critical to hierarchical Bayesian models with local variables; their dimensionality increases with the size of the datasets. Consequently, one gets an iteration complexity with an explicit (\mathcal{O}(N^2)) dependence on the dataset size (N). In this paper, we explore a theoretical middle ground between mean-field variational families and full-rank families: structured variational families. We rigorously prove that certain scale matrix structures can achieve a better iteration complexity of (\mathcal{O}\left(N\right)), implying better scaling with respect to (N). We empirically verify our theoretical results on large-scale hierarchical models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.10989&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joohwan Ko, Kyurae Kim, Woo Chang Kim, Jacob R. Gardner</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Variational families with full-rank covariance approximations are known not to work well in black-box variational inference (BBVI), both empirically and theoretically. In fact, recent computational complexity results for BBVI have established that full-rank variational families scale poorly with the dimensionality of the problem compared to e.g. mean-field families. This is particularly critical to hierarchical Bayesian models with local variables; their dimensionality increases with the size of the datasets. Consequently, one gets an iteration complexity with an explicit (\mathcal{O}(N^2)) dependence on the dataset size (N). In this paper, we explore a theoretical middle ground between mean-field variational families and full-rank families: structured variational families. We rigorously prove that certain scale matrix structures can achieve a better iteration complexity of (\mathcal{O}\left(N\right)), implying better scaling with respect to (N). We empirically verify our theoretical results on large-scale hierarchical models.</summary></entry><entry><title type="html">Risk Set Matched Difference-in-Differences for the Analysis of Effect Modification in an Observational Study on the Impact of Gun Violence on Health Outcomes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/RiskSetMatchedDifferenceinDifferencesfortheAnalysisofEffectModificationinanObservationalStudyontheImpactofGunViolenceonHealthOutcomes.html" rel="alternate" type="text/html" title="Risk Set Matched Difference-in-Differences for the Analysis of Effect Modification in an Observational Study on the Impact of Gun Violence on Health Outcomes" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/RiskSetMatchedDifferenceinDifferencesfortheAnalysisofEffectModificationinanObservationalStudyontheImpactofGunViolenceonHealthOutcomes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/RiskSetMatchedDifferenceinDifferencesfortheAnalysisofEffectModificationinanObservationalStudyontheImpactofGunViolenceonHealthOutcomes.html">&lt;p&gt;Gun violence is a major source of injury and death in the United States. However, relatively little is known about the effects of firearm injuries on survivors and their family members and how these effects vary across subpopulations. To study these questions and, more generally, to address a gap in the causal inference literature, we present a framework for the study of effect modification or heterogeneous treatment effects in difference-in-differences designs. We implement a new matching technique, which combines profile matching and risk set matching, to (i) preserve the time alignment of covariates, exposure, and outcomes, avoiding pitfalls of other common approaches for difference-in-differences, and (ii) explicitly control biases due to imbalances in observed covariates in subgroups discovered from the data. Our case study shows significant and persistent effects of nonfatal firearm injuries on several health outcomes for those injured and on the mental health of their family members. Sensitivity analyses reveal that these results are moderately robust to unmeasured confounding bias. Finally, while the effects for those injured vary largely by the severity of the injury and its documented intent, for families, effects are strongest for those whose relative’s injury is documented as resulting from an assault, self-harm, or law enforcement intervention.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.04143&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Eric R. Cohn, Zirui Song, Jose R. Zubizarreta</name></author><category term="stat.AP" /><summary type="html">Gun violence is a major source of injury and death in the United States. However, relatively little is known about the effects of firearm injuries on survivors and their family members and how these effects vary across subpopulations. To study these questions and, more generally, to address a gap in the causal inference literature, we present a framework for the study of effect modification or heterogeneous treatment effects in difference-in-differences designs. We implement a new matching technique, which combines profile matching and risk set matching, to (i) preserve the time alignment of covariates, exposure, and outcomes, avoiding pitfalls of other common approaches for difference-in-differences, and (ii) explicitly control biases due to imbalances in observed covariates in subgroups discovered from the data. Our case study shows significant and persistent effects of nonfatal firearm injuries on several health outcomes for those injured and on the mental health of their family members. Sensitivity analyses reveal that these results are moderately robust to unmeasured confounding bias. Finally, while the effects for those injured vary largely by the severity of the injury and its documented intent, for families, effects are strongest for those whose relative’s injury is documented as resulting from an assault, self-harm, or law enforcement intervention.</summary></entry><entry><title type="html">SMC Is All You Need: Parallel Strong Scaling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/SMCIsAllYouNeedParallelStrongScaling.html" rel="alternate" type="text/html" title="SMC Is All You Need: Parallel Strong Scaling" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/SMCIsAllYouNeedParallelStrongScaling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/SMCIsAllYouNeedParallelStrongScaling.html">&lt;p&gt;The Bayesian posterior distribution can only be evaluated up-to a constant of proportionality, which makes simulation and consistent estimation challenging. Classical consistent Bayesian methods such as sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC) have unbounded time complexity requirements. We develop a fully parallel sequential Monte Carlo (pSMC) method which provably delivers parallel strong scaling, i.e. the time complexity (and per-node memory) remains bounded if the number of asynchronous processes is allowed to grow. More precisely, the pSMC has a theoretical convergence rate of Mean Square Error (MSE)$ = O(1/NP)$, where $N$ denotes the number of communicating samples in each processor and $P$ denotes the number of processors. In particular, for suitably-large problem-dependent $N$, as $P \rightarrow \infty$ the method converges to infinitesimal accuracy MSE$=O(\varepsilon^2)$ with a fixed finite time-complexity Cost$=O(1)$ and with no efficiency leakage, i.e. computational complexity Cost$=O(\varepsilon^{-2})$. A number of Bayesian inference problems are taken into consideration to compare the pSMC and MCMC methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.06173&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinzhu Liang, Joseph M. Lukens, Sanjaya Lohani, Brian T. Kirby, Thomas A. Searles, Kody J. H. Law</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">The Bayesian posterior distribution can only be evaluated up-to a constant of proportionality, which makes simulation and consistent estimation challenging. Classical consistent Bayesian methods such as sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC) have unbounded time complexity requirements. We develop a fully parallel sequential Monte Carlo (pSMC) method which provably delivers parallel strong scaling, i.e. the time complexity (and per-node memory) remains bounded if the number of asynchronous processes is allowed to grow. More precisely, the pSMC has a theoretical convergence rate of Mean Square Error (MSE)$ = O(1/NP)$, where $N$ denotes the number of communicating samples in each processor and $P$ denotes the number of processors. In particular, for suitably-large problem-dependent $N$, as $P \rightarrow \infty$ the method converges to infinitesimal accuracy MSE$=O(\varepsilon^2)$ with a fixed finite time-complexity Cost$=O(1)$ and with no efficiency leakage, i.e. computational complexity Cost$=O(\varepsilon^{-2})$. A number of Bayesian inference problems are taken into consideration to compare the pSMC and MCMC methods.</summary></entry><entry><title type="html">Sequential FDR and pFDR control under arbitrary dependence, with application to pharmacovigilance database monitoring</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/SequentialFDRandpFDRcontrolunderarbitrarydependencewithapplicationtopharmacovigilancedatabasemonitoring.html" rel="alternate" type="text/html" title="Sequential FDR and pFDR control under arbitrary dependence, with application to pharmacovigilance database monitoring" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/SequentialFDRandpFDRcontrolunderarbitrarydependencewithapplicationtopharmacovigilancedatabasemonitoring</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/SequentialFDRandpFDRcontrolunderarbitrarydependencewithapplicationtopharmacovigilancedatabasemonitoring.html">&lt;p&gt;We propose sequential multiple testing procedures which control the false discover rate (FDR) or the positive false discovery rate (pFDR) under arbitrary dependence between the data streams. This is accomplished by “optimizing” an upper bound on these error metrics for a class of step down sequential testing procedures. Both open-ended and truncated versions of these sequential procedures are given, both being able to control both the type~I multiple testing metric (FDR or pFDR) at specified levels, and the former being able to control both the type I and type II (e.g., FDR and the false nondiscovery rate, FNR). In simulation studies, these procedures provide 45-65% savings in average sample size over their fixed-sample competitors. We illustrate our procedures on drug data from the United Kingdom’s Yellow Card Pharmacovigilance Database.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01218&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael Hankin, Jay Bartroff</name></author><category term="stat.ME" /><summary type="html">We propose sequential multiple testing procedures which control the false discover rate (FDR) or the positive false discovery rate (pFDR) under arbitrary dependence between the data streams. This is accomplished by “optimizing” an upper bound on these error metrics for a class of step down sequential testing procedures. Both open-ended and truncated versions of these sequential procedures are given, both being able to control both the type~I multiple testing metric (FDR or pFDR) at specified levels, and the former being able to control both the type I and type II (e.g., FDR and the false nondiscovery rate, FNR). In simulation studies, these procedures provide 45-65% savings in average sample size over their fixed-sample competitors. We illustrate our procedures on drug data from the United Kingdom’s Yellow Card Pharmacovigilance Database.</summary></entry><entry><title type="html">Skew-symmetric schemes for stochastic differential equations with non-Lipschitz drift: an unadjusted Barker algorithm</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/SkewsymmetricschemesforstochasticdifferentialequationswithnonLipschitzdriftanunadjustedBarkeralgorithm.html" rel="alternate" type="text/html" title="Skew-symmetric schemes for stochastic differential equations with non-Lipschitz drift: an unadjusted Barker algorithm" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/SkewsymmetricschemesforstochasticdifferentialequationswithnonLipschitzdriftanunadjustedBarkeralgorithm</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/SkewsymmetricschemesforstochasticdifferentialequationswithnonLipschitzdriftanunadjustedBarkeralgorithm.html">&lt;p&gt;We propose a new simple and explicit numerical scheme for time-homogeneous stochastic differential equations. The scheme is based on sampling increments at each time step from a skew-symmetric probability distribution, with the level of skewness determined by the drift and volatility of the underlying process. We show that as the step-size decreases the scheme converges weakly to the diffusion of interest. We then consider the problem of simulating from the limiting distribution of an ergodic diffusion process using the numerical scheme with a fixed step-size. We establish conditions under which the numerical scheme converges to equilibrium at a geometric rate, and quantify the bias between the equilibrium distributions of the scheme and of the true diffusion process. Notably, our results do not require a global Lipschitz assumption on the drift, in contrast to those required for the Euler–Maruyama scheme for long-time simulation at fixed step-sizes. Our weak convergence result relies on an extension of the theory of Milstein \&amp;amp; Tretyakov to stochastic differential equations with non-Lipschitz drift, which could also be of independent interest. We support our theoretical results with numerical simulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14373&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Samuel Livingstone, Nikolas Nüsken, Giorgos Vasdekis, Rui-Yang Zhang</name></author><category term="stat.CO" /><summary type="html">We propose a new simple and explicit numerical scheme for time-homogeneous stochastic differential equations. The scheme is based on sampling increments at each time step from a skew-symmetric probability distribution, with the level of skewness determined by the drift and volatility of the underlying process. We show that as the step-size decreases the scheme converges weakly to the diffusion of interest. We then consider the problem of simulating from the limiting distribution of an ergodic diffusion process using the numerical scheme with a fixed step-size. We establish conditions under which the numerical scheme converges to equilibrium at a geometric rate, and quantify the bias between the equilibrium distributions of the scheme and of the true diffusion process. Notably, our results do not require a global Lipschitz assumption on the drift, in contrast to those required for the Euler–Maruyama scheme for long-time simulation at fixed step-sizes. Our weak convergence result relies on an extension of the theory of Milstein \&amp;amp; Tretyakov to stochastic differential equations with non-Lipschitz drift, which could also be of independent interest. We support our theoretical results with numerical simulations.</summary></entry><entry><title type="html">Spectral Extraction of Distinctive Latent Variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/SpectralExtractionofDistinctiveLatentVariables.html" rel="alternate" type="text/html" title="Spectral Extraction of Distinctive Latent Variables" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/SpectralExtractionofDistinctiveLatentVariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/SpectralExtractionofDistinctiveLatentVariables.html">&lt;p&gt;Multimodal datasets contain observations generated by multiple types of sensors. Most works to date focus on uncovering latent structures in the data that appear in all modalities. However, important aspects of the data may appear in only one modality due to the differences between the sensors. Uncovering modality-specific attributes may provide insights into the sources of the variability of the data. For example, certain clusters may appear in the analysis of genetics but not in epigenetic markers. Another example is hyper-spectral satellite imaging, where various atmospheric and ground phenomena are detectable using different parts of the spectrum. In this paper, we address the problem of uncovering latent structures that are unique to a single modality. Our approach is based on computing a graph representation of datasets from two modalities and analyzing the differences between their connectivity patterns. We provide an asymptotic analysis of the convergence of our approach based on a product manifold model. To evaluate the performance of our method, we test its ability to uncover latent structures in multiple types of artificial and real datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.18741&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shira Yoffe, Amit Moscovich, Ariel Jaffe</name></author><category term="stat.ME" /><summary type="html">Multimodal datasets contain observations generated by multiple types of sensors. Most works to date focus on uncovering latent structures in the data that appear in all modalities. However, important aspects of the data may appear in only one modality due to the differences between the sensors. Uncovering modality-specific attributes may provide insights into the sources of the variability of the data. For example, certain clusters may appear in the analysis of genetics but not in epigenetic markers. Another example is hyper-spectral satellite imaging, where various atmospheric and ground phenomena are detectable using different parts of the spectrum. In this paper, we address the problem of uncovering latent structures that are unique to a single modality. Our approach is based on computing a graph representation of datasets from two modalities and analyzing the differences between their connectivity patterns. We provide an asymptotic analysis of the convergence of our approach based on a product manifold model. To evaluate the performance of our method, we test its ability to uncover latent structures in multiple types of artificial and real datasets.</summary></entry><entry><title type="html">Structural Health Monitoring with Functional Data: Two Case Studies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/StructuralHealthMonitoringwithFunctionalDataTwoCaseStudies.html" rel="alternate" type="text/html" title="Structural Health Monitoring with Functional Data: Two Case Studies" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/StructuralHealthMonitoringwithFunctionalDataTwoCaseStudies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/StructuralHealthMonitoringwithFunctionalDataTwoCaseStudies.html">&lt;p&gt;Structural Health Monitoring (SHM) is increasingly used in civil engineering. One of its main purposes is to detect and assess changes in infrastructure conditions to reduce possible maintenance downtime and increase safety. Ideally, this process should be automated and implemented in real-time. Recent advances in sensor technology facilitate data collection and process automation, resulting in massive data streams. Functional data analysis (FDA) can be used to model and aggregate the data obtained transparently and interpretably. In two real-world case studies of bridges in Germany and Belgium, this paper demonstrates how a function-on-function regression approach, combined with profile monitoring, can be applied to SHM data to adjust sensor/system outputs for environmental-induced variation and detect changes in construction. Specifically, we consider the R package \texttt{funcharts} and discuss some challenges when using this software on real-world SHM data. For instance, we show that pre-smoothing of the data can improve and extend its usability.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01262&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Philipp Wittenberg, Sven Knoth, Jan Gertheiss</name></author><category term="stat.AP" /><summary type="html">Structural Health Monitoring (SHM) is increasingly used in civil engineering. One of its main purposes is to detect and assess changes in infrastructure conditions to reduce possible maintenance downtime and increase safety. Ideally, this process should be automated and implemented in real-time. Recent advances in sensor technology facilitate data collection and process automation, resulting in massive data streams. Functional data analysis (FDA) can be used to model and aggregate the data obtained transparently and interpretably. In two real-world case studies of bridges in Germany and Belgium, this paper demonstrates how a function-on-function regression approach, combined with profile monitoring, can be applied to SHM data to adjust sensor/system outputs for environmental-induced variation and detect changes in construction. Specifically, we consider the R package \texttt{funcharts} and discuss some challenges when using this software on real-world SHM data. For instance, we show that pre-smoothing of the data can improve and extend its usability.</summary></entry><entry><title type="html">Testing for Stationary or Persistent Coefficient Randomness in Predictive Regressions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TestingforStationaryorPersistentCoefficientRandomnessinPredictiveRegressions.html" rel="alternate" type="text/html" title="Testing for Stationary or Persistent Coefficient Randomness in Predictive Regressions" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TestingforStationaryorPersistentCoefficientRandomnessinPredictiveRegressions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TestingforStationaryorPersistentCoefficientRandomnessinPredictiveRegressions.html">&lt;p&gt;This study considers tests for coefficient randomness in predictive regressions. Our focus is on how tests for coefficient randomness are influenced by the persistence of random coefficient. We show that when the random coefficient is stationary, or I(0), Nyblom’s (1989) LM test loses its optimality (in terms of power), which is established against the alternative of integrated, or I(1), random coefficient. We demonstrate this by constructing a test that is more powerful than the LM test when the random coefficient is stationary, although the test is dominated in terms of power by the LM test when the random coefficient is integrated. The power comparison is made under the sequence of local alternatives that approaches the null hypothesis at different rates depending on the persistence of the random coefficient and which test is considered. We revisit an earlier empirical research and apply the tests considered in this study to the U.S. stock returns data. The result mostly reverses the earlier finding.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.04926&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mikihito Nishi</name></author><category term="stat.ME" /><summary type="html">This study considers tests for coefficient randomness in predictive regressions. Our focus is on how tests for coefficient randomness are influenced by the persistence of random coefficient. We show that when the random coefficient is stationary, or I(0), Nyblom’s (1989) LM test loses its optimality (in terms of power), which is established against the alternative of integrated, or I(1), random coefficient. We demonstrate this by constructing a test that is more powerful than the LM test when the random coefficient is stationary, although the test is dominated in terms of power by the LM test when the random coefficient is integrated. The power comparison is made under the sequence of local alternatives that approaches the null hypothesis at different rates depending on the persistence of the random coefficient and which test is considered. We revisit an earlier empirical research and apply the tests considered in this study to the U.S. stock returns data. The result mostly reverses the earlier finding.</summary></entry><entry><title type="html">Testing for the extent of instability in nearly unstable processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Testingfortheextentofinstabilityinnearlyunstableprocesses.html" rel="alternate" type="text/html" title="Testing for the extent of instability in nearly unstable processes" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Testingfortheextentofinstabilityinnearlyunstableprocesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/Testingfortheextentofinstabilityinnearlyunstableprocesses.html">&lt;p&gt;This paper deals with unit root issues in time series analysis. It has been known for a long time that unit root tests may be flawed when a series although stationary has a root close to unity. That motivated recent papers dedicated to autoregressive processes where the bridge between stability and instability is expressed by means of time-varying coefficients. The process we consider has a companion matrix $A_{n}$ with spectral radius $\rho(A_{n}) &amp;lt; 1$ satisfying $\rho(A_{n}) \rightarrow 1$, a situation described as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nearly-unstable&apos;. The question we investigate is: given an observed path supposed to come from a nearly-unstable process, is it possible to test for the &lt;/code&gt;extent of instability’, i.e. to test how close we are to the unit root? In this regard, we develop a strategy to evaluate $\alpha$ and to test for $\mathcal{H}&lt;em&gt;0 : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\alpha = \alpha_0&quot;$ against $\mathcal{H}_1 :&lt;/code&gt;\alpha &amp;gt; \alpha_0”$ when $\rho(A&lt;/em&gt;{n})$ lies in an inner $O(n^{-\alpha})$-neighborhood of the unity, for some $0 &amp;lt; \alpha &amp;lt; 1$. Empirical evidence is given about the advantages of the flexibility induced by such a procedure compared to the common unit root tests. We also build a symmetric procedure for the usually left out situation where the dominant root lies around $-1$.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.13444&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marie Badreau, Frédéric Proïa</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This paper deals with unit root issues in time series analysis. It has been known for a long time that unit root tests may be flawed when a series although stationary has a root close to unity. That motivated recent papers dedicated to autoregressive processes where the bridge between stability and instability is expressed by means of time-varying coefficients. The process we consider has a companion matrix $A_{n}$ with spectral radius $\rho(A_{n}) &amp;lt; 1$ satisfying $\rho(A_{n}) \rightarrow 1$, a situation described as nearly-unstable&apos;. The question we investigate is: given an observed path supposed to come from a nearly-unstable process, is it possible to test for the extent of instability’, i.e. to test how close we are to the unit root? In this regard, we develop a strategy to evaluate $\alpha$ and to test for $\mathcal{H}0 : \alpha = \alpha_0&quot;$ against $\mathcal{H}_1 :\alpha &amp;gt; \alpha_0”$ when $\rho(A{n})$ lies in an inner $O(n^{-\alpha})$-neighborhood of the unity, for some $0 &amp;lt; \alpha &amp;lt; 1$. Empirical evidence is given about the advantages of the flexibility induced by such a procedure compared to the common unit root tests. We also build a symmetric procedure for the usually left out situation where the dominant root lies around $-1$.</summary></entry><entry><title type="html">The ARR2 prior: flexible predictive prior definition for Bayesian auto-regressions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TheARR2priorflexiblepredictivepriordefinitionforBayesianautoregressions.html" rel="alternate" type="text/html" title="The ARR2 prior: flexible predictive prior definition for Bayesian auto-regressions" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TheARR2priorflexiblepredictivepriordefinitionforBayesianautoregressions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TheARR2priorflexiblepredictivepriordefinitionforBayesianautoregressions.html">&lt;p&gt;We present the ARR2 prior, a joint prior over the auto-regressive components in Bayesian time-series models and their induced $R^2$. Compared to other priors designed for times-series models, the ARR2 prior allows for flexible and intuitive shrinkage. We derive the prior for pure auto-regressive models, and extend it to auto-regressive models with exogenous inputs, and state-space models. Through both simulations and real-world modelling exercises, we demonstrate the efficacy of the ARR2 prior in improving sparse and reliable inference, while showing greater inference quality and predictive performance than other shrinkage priors. An open-source implementation of the prior is provided.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19920&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David Kohns, Noa Kallioinen, Yann McLatchie, Aki Vehtari</name></author><category term="stat.CO" /><summary type="html">We present the ARR2 prior, a joint prior over the auto-regressive components in Bayesian time-series models and their induced $R^2$. Compared to other priors designed for times-series models, the ARR2 prior allows for flexible and intuitive shrinkage. We derive the prior for pure auto-regressive models, and extend it to auto-regressive models with exogenous inputs, and state-space models. Through both simulations and real-world modelling exercises, we demonstrate the efficacy of the ARR2 prior in improving sparse and reliable inference, while showing greater inference quality and predictive performance than other shrinkage priors. An open-source implementation of the prior is provided.</summary></entry><entry><title type="html">The Firefighter Algorithm: A Hybrid Metaheuristic for Optimization Problems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TheFirefighterAlgorithmAHybridMetaheuristicforOptimizationProblems.html" rel="alternate" type="text/html" title="The Firefighter Algorithm: A Hybrid Metaheuristic for Optimization Problems" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TheFirefighterAlgorithmAHybridMetaheuristicforOptimizationProblems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TheFirefighterAlgorithmAHybridMetaheuristicforOptimizationProblems.html">&lt;p&gt;This paper presents the Firefighter Optimization (FFO) algorithm as a new hybrid metaheuristic for optimization problems. This algorithm stems inspiration from the collaborative strategies often deployed by firefighters in firefighting activities. To evaluate the performance of FFO, extensive experiments were conducted, wherein the FFO was examined against 13 commonly used optimization algorithms, namely, the Ant Colony Optimization (ACO), Bat Algorithm (BA), Biogeography-Based Optimization (BBO), Flower Pollination Algorithm (FPA), Genetic Algorithm (GA), Grey Wolf Optimizer (GWO), Harmony Search (HS), Particle Swarm Optimization (PSO), Simulated Annealing (SA), Tabu Search (TS), and Whale Optimization Algorithm (WOA), and across 24 benchmark functions of various dimensions and complexities. The results demonstrate that FFO achieves comparative performance and, in some scenarios, outperforms commonly adopted optimization algorithms in terms of the obtained fitness, time taken for exaction, and research space covered per unit of time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00528&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>M. Z. Naser, A. Z. Naser</name></author><category term="stat.AP" /><summary type="html">This paper presents the Firefighter Optimization (FFO) algorithm as a new hybrid metaheuristic for optimization problems. This algorithm stems inspiration from the collaborative strategies often deployed by firefighters in firefighting activities. To evaluate the performance of FFO, extensive experiments were conducted, wherein the FFO was examined against 13 commonly used optimization algorithms, namely, the Ant Colony Optimization (ACO), Bat Algorithm (BA), Biogeography-Based Optimization (BBO), Flower Pollination Algorithm (FPA), Genetic Algorithm (GA), Grey Wolf Optimizer (GWO), Harmony Search (HS), Particle Swarm Optimization (PSO), Simulated Annealing (SA), Tabu Search (TS), and Whale Optimization Algorithm (WOA), and across 24 benchmark functions of various dimensions and complexities. The results demonstrate that FFO achieves comparative performance and, in some scenarios, outperforms commonly adopted optimization algorithms in terms of the obtained fitness, time taken for exaction, and research space covered per unit of time.</summary></entry><entry><title type="html">The Multi-Range Theory of Translation Quality Measurement: MQM scoring models and Statistical Quality Control</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TheMultiRangeTheoryofTranslationQualityMeasurementMQMscoringmodelsandStatisticalQualityControl.html" rel="alternate" type="text/html" title="The Multi-Range Theory of Translation Quality Measurement: MQM scoring models and Statistical Quality Control" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TheMultiRangeTheoryofTranslationQualityMeasurementMQMscoringmodelsandStatisticalQualityControl</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TheMultiRangeTheoryofTranslationQualityMeasurementMQMscoringmodelsandStatisticalQualityControl.html">&lt;p&gt;The year 2024 marks the 10th anniversary of the Multidimensional Quality Metrics (MQM) framework for analytic translation quality evaluation. The MQM error typology has been widely used by practitioners in the translation and localization industry and has served as the basis for many derivative projects. The annual Conference on Machine Translation (WMT) shared tasks on both human and automatic translation quality evaluations used the MQM error typology.
  The metric stands on two pillars: error typology and the scoring model. The scoring model calculates the quality score from annotation data, detailing how to convert error type and severity counts into numeric scores to determine if the content meets specifications. Previously, only the raw scoring model had been published. This April, the MQM Council published the Linear Calibrated Scoring Model, officially presented herein, along with the Non-Linear Scoring Model, which had not been published before.
  This paper details the latest MQM developments and presents a universal approach to translation quality measurement across three sample size ranges. It also explains why Statistical Quality Control should be used for very small sample sizes, starting from a single sentence.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16969&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arle Lommel, Serge Gladkoff, Alan Melby, Sue Ellen Wright, Ingemar Strandvik, Katerina Gasova, Angelika Vaasa, Andy Benzo, Romina Marazzato Sparano, Monica Foresi, Johani Innis, Lifeng Han, Goran Nenadic</name></author><category term="stat.AP" /><summary type="html">The year 2024 marks the 10th anniversary of the Multidimensional Quality Metrics (MQM) framework for analytic translation quality evaluation. The MQM error typology has been widely used by practitioners in the translation and localization industry and has served as the basis for many derivative projects. The annual Conference on Machine Translation (WMT) shared tasks on both human and automatic translation quality evaluations used the MQM error typology. The metric stands on two pillars: error typology and the scoring model. The scoring model calculates the quality score from annotation data, detailing how to convert error type and severity counts into numeric scores to determine if the content meets specifications. Previously, only the raw scoring model had been published. This April, the MQM Council published the Linear Calibrated Scoring Model, officially presented herein, along with the Non-Linear Scoring Model, which had not been published before. This paper details the latest MQM developments and presents a universal approach to translation quality measurement across three sample size ranges. It also explains why Statistical Quality Control should be used for very small sample sizes, starting from a single sentence.</summary></entry><entry><title type="html">The Topology and Geometry of Neural Representations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TheTopologyandGeometryofNeuralRepresentations.html" rel="alternate" type="text/html" title="The Topology and Geometry of Neural Representations" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TheTopologyandGeometryofNeuralRepresentations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TheTopologyandGeometryofNeuralRepresentations.html">&lt;p&gt;A central question for neuroscience is how to characterize brain representations of perceptual and cognitive content. An ideal characterization should distinguish different functional regions with robustness to noise and idiosyncrasies of individual brains that do not correspond to computational differences. Previous studies have characterized brain representations by their representational geometry, which is defined by the representational dissimilarity matrix (RDM), a summary statistic that abstracts from the roles of individual neurons (or responses channels) and characterizes the discriminability of stimuli. Here we explore a further step of abstraction: from the geometry to the topology of brain representations. We propose topological representational similarity analysis (tRSA), an extension of representational similarity analysis (RSA) that uses a family of geo-topological summary statistics that generalizes the RDM to characterize the topology while de-emphasizing the geometry. We evaluate this new family of statistics in terms of the sensitivity and specificity for model selection using both simulations and fMRI data. In the simulations, the ground truth is a data-generating layer representation in a neural network model and the models are the same and other layers in different model instances (trained from different random seeds). In fMRI, the ground truth is a visual area and the models are the same and other areas measured in different subjects. Results show that topology-sensitive characterizations of population codes are robust to noise and interindividual variability and maintain excellent sensitivity to the unique representational signatures of different neural network layers and brain regions. These methods enable researchers to calibrate comparisons among representations in brains and models to be sensitive to the geometry, the topology, or a combination of both.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.11028&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Baihan Lin, Nikolaus Kriegeskorte</name></author><category term="stat.ME" /><summary type="html">A central question for neuroscience is how to characterize brain representations of perceptual and cognitive content. An ideal characterization should distinguish different functional regions with robustness to noise and idiosyncrasies of individual brains that do not correspond to computational differences. Previous studies have characterized brain representations by their representational geometry, which is defined by the representational dissimilarity matrix (RDM), a summary statistic that abstracts from the roles of individual neurons (or responses channels) and characterizes the discriminability of stimuli. Here we explore a further step of abstraction: from the geometry to the topology of brain representations. We propose topological representational similarity analysis (tRSA), an extension of representational similarity analysis (RSA) that uses a family of geo-topological summary statistics that generalizes the RDM to characterize the topology while de-emphasizing the geometry. We evaluate this new family of statistics in terms of the sensitivity and specificity for model selection using both simulations and fMRI data. In the simulations, the ground truth is a data-generating layer representation in a neural network model and the models are the same and other layers in different model instances (trained from different random seeds). In fMRI, the ground truth is a visual area and the models are the same and other areas measured in different subjects. Results show that topology-sensitive characterizations of population codes are robust to noise and interindividual variability and maintain excellent sensitivity to the unique representational signatures of different neural network layers and brain regions. These methods enable researchers to calibrate comparisons among representations in brains and models to be sensitive to the geometry, the topology, or a combination of both.</summary></entry><entry><title type="html">Towards Data-Conditional Simulation for ABC Inference in Stochastic Differential Equations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TowardsDataConditionalSimulationforABCInferenceinStochasticDifferentialEquations.html" rel="alternate" type="text/html" title="Towards Data-Conditional Simulation for ABC Inference in Stochastic Differential Equations" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TowardsDataConditionalSimulationforABCInferenceinStochasticDifferentialEquations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/TowardsDataConditionalSimulationforABCInferenceinStochasticDifferentialEquations.html">&lt;p&gt;We develop a Bayesian inference method for discretely-observed stochastic differential equations (SDEs). Inference is challenging for most SDEs, due to the analytical intractability of the likelihood function. Nevertheless, forward simulation via numerical methods is straightforward, motivating the use of approximate Bayesian computation (ABC). We propose a conditional simulation scheme for SDEs that is based on lookahead strategies for sequential Monte Carlo (SMC) and particle smoothing using backward simulation. This leads to the simulation of trajectories that are consistent with the observed trajectory, thereby increasing the ABC acceptance rate. We additionally employ an invariant neural network, previously developed for Markov processes, to learn the summary statistics function required in ABC. The neural network is incrementally retrained by exploiting an ABC-SMC sampler, which provides new training data at each round. Since the SDEs simulation scheme differs from standard forward simulation, we propose a suitable importance sampling correction, which has the added advantage of guiding the parameters towards regions of high posterior density, especially in the first ABC-SMC round. Our approach achieves accurate inference and is about three times faster than standard (forward-only) ABC-SMC. We illustrate our method in five simulation studies, including three examples from the Chan-Karaolyi-Longstaff-Sanders SDE family, a stochastic bi-stable model (Schl{&quot;o}gl) that is notoriously challenging for ABC methods, and a two dimensional biochemical reaction network.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.10329&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Petar Jovanovski, Andrew Golightly, Umberto Picchini</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">We develop a Bayesian inference method for discretely-observed stochastic differential equations (SDEs). Inference is challenging for most SDEs, due to the analytical intractability of the likelihood function. Nevertheless, forward simulation via numerical methods is straightforward, motivating the use of approximate Bayesian computation (ABC). We propose a conditional simulation scheme for SDEs that is based on lookahead strategies for sequential Monte Carlo (SMC) and particle smoothing using backward simulation. This leads to the simulation of trajectories that are consistent with the observed trajectory, thereby increasing the ABC acceptance rate. We additionally employ an invariant neural network, previously developed for Markov processes, to learn the summary statistics function required in ABC. The neural network is incrementally retrained by exploiting an ABC-SMC sampler, which provides new training data at each round. Since the SDEs simulation scheme differs from standard forward simulation, we propose a suitable importance sampling correction, which has the added advantage of guiding the parameters towards regions of high posterior density, especially in the first ABC-SMC round. Our approach achieves accurate inference and is about three times faster than standard (forward-only) ABC-SMC. We illustrate our method in five simulation studies, including three examples from the Chan-Karaolyi-Longstaff-Sanders SDE family, a stochastic bi-stable model (Schl{&quot;o}gl) that is notoriously challenging for ABC methods, and a two dimensional biochemical reaction network.</summary></entry><entry><title type="html">Uplift Modeling Under Limited Supervision</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/UpliftModelingUnderLimitedSupervision.html" rel="alternate" type="text/html" title="Uplift Modeling Under Limited Supervision" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/UpliftModelingUnderLimitedSupervision</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/UpliftModelingUnderLimitedSupervision.html">&lt;p&gt;Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of the training set in settings with extremely low experimental budget. The framework is flexible since each step can be used separately with other models or treatment policies. The experiments on real large-scale networks indicate a clear advantage of our methodology over the state of the art, which in many cases performs close to random, underlining the need for models that can generalize with limited supervision to reduce experimental risks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.19289&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>George Panagopoulos, Daniele Malitesta, Fragkiskos D. Malliaros, Jun Pang</name></author><category term="stat.ME" /><summary type="html">Estimating causal effects in e-commerce tends to involve costly treatment assignments which can be impractical in large-scale settings. Leveraging machine learning to predict such treatment effects without actual intervention is a standard practice to diminish the risk. However, existing methods for treatment effect prediction tend to rely on training sets of substantial size, which are built from real experiments and are thus inherently risky to create. In this work we propose a graph neural network to diminish the required training set size, relying on graphs that are common in e-commerce data. Specifically, we view the problem as node regression with a restricted number of labeled instances, develop a two-model neural architecture akin to previous causal effect estimators, and test varying message-passing layers for encoding. Furthermore, as an extra step, we combine the model with an acquisition function to guide the creation of the training set in settings with extremely low experimental budget. The framework is flexible since each step can be used separately with other models or treatment policies. The experiments on real large-scale networks indicate a clear advantage of our methodology over the state of the art, which in many cases performs close to random, underlining the need for models that can generalize with limited supervision to reduce experimental risks.</summary></entry><entry><title type="html">W-kernel and essential subspace for frequentist evaluation of Bayesian estimators</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/WkernelandessentialsubspaceforfrequentistevaluationofBayesianestimators.html" rel="alternate" type="text/html" title="W-kernel and essential subspace for frequentist evaluation of Bayesian estimators" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/WkernelandessentialsubspaceforfrequentistevaluationofBayesianestimators</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/WkernelandessentialsubspaceforfrequentistevaluationofBayesianestimators.html">&lt;p&gt;The posterior covariance matrix W defined by the log-likelihood of each observation plays important roles both in the sensitivity analysis and frequentist evaluation of the Bayesian estimators. This study is focused on the matrix W and its principal space; we term the latter as an essential subspace. Projections to the essential subspace realize dimensional reduction in the sensitivity analysis and frequentist evaluation. A key tool for treating frequentist properties is the recently proposed Bayesian infinitesimal jackknife approximation(Giordano and Broderick (2023)). The matrix W can be interpreted as a reproducing kernel and is denoted as W-kernel. Using W-kernel, the essential subspace is expressed as a principal space given by the kernel principal component analysis. A relation to the Fisher kernel and neural tangent kernel is established, which elucidates the connection to the classical asymptotic theory. We also discuss a type of Bayesian-frequentist duality, naturally appeared from the kernel framework. Two applications are discussed: the selection of a representative set of observations and dimensional reduction in the approximate bootstrap. In the former, incomplete Cholesky decomposition is introduced as an efficient method for computing the essential subspace. In the latter, different implementations of the approximate bootstrap for posterior means are compared.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.13017&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yukito Iba</name></author><category term="stat.ME," /><category term="cond-mat.stat-mech," /><category term="stat.ML" /><summary type="html">The posterior covariance matrix W defined by the log-likelihood of each observation plays important roles both in the sensitivity analysis and frequentist evaluation of the Bayesian estimators. This study is focused on the matrix W and its principal space; we term the latter as an essential subspace. Projections to the essential subspace realize dimensional reduction in the sensitivity analysis and frequentist evaluation. A key tool for treating frequentist properties is the recently proposed Bayesian infinitesimal jackknife approximation(Giordano and Broderick (2023)). The matrix W can be interpreted as a reproducing kernel and is denoted as W-kernel. Using W-kernel, the essential subspace is expressed as a principal space given by the kernel principal component analysis. A relation to the Fisher kernel and neural tangent kernel is established, which elucidates the connection to the classical asymptotic theory. We also discuss a type of Bayesian-frequentist duality, naturally appeared from the kernel framework. Two applications are discussed: the selection of a representative set of observations and dimensional reduction in the approximate bootstrap. In the former, incomplete Cholesky decomposition is introduced as an efficient method for computing the essential subspace. In the latter, different implementations of the approximate bootstrap for posterior means are compared.</summary></entry><entry><title type="html">Zero Inflation as a Missing Data Problem: a Proxy-based Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ZeroInflationasaMissingDataProblemaProxybasedApproach.html" rel="alternate" type="text/html" title="Zero Inflation as a Missing Data Problem: a Proxy-based Approach" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ZeroInflationasaMissingDataProblemaProxybasedApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/ZeroInflationasaMissingDataProblemaProxybasedApproach.html">&lt;p&gt;A common type of zero-inflated data has certain true values incorrectly replaced by zeros due to data recording conventions (rare outcomes assumed to be absent) or details of data recording equipment (e.g. artificial zeros in gene expression data).
  Existing methods for zero-inflated data either fit the observed data likelihood via parametric mixture models that explicitly represent excess zeros, or aim to replace excess zeros by imputed values. If the goal of the analysis relies on knowing true data realizations, a particular challenge with zero-inflated data is identifiability, since it is difficult to correctly determine which observed zeros are real and which are inflated.
  This paper views zero-inflated data as a general type of missing data problem, where the observability indicator for a potentially censored variable is itself unobserved whenever a zero is recorded. We show that, without additional assumptions, target parameters involving a zero-inflated variable are not identified. However, if a proxy of the missingness indicator is observed, a modification of the effect restoration approach of Kuroki and Pearl allows identification and estimation, given the proxy-indicator relationship is known.
  If this relationship is unknown, our approach yields a partial identification strategy for sensitivity analysis. Specifically, we show that only certain proxy-indicator relationships are compatible with the observed data distribution. We give an analytic bound for this relationship in cases with a categorical outcome, which is sharp in certain models. For more complex cases, sharp numerical bounds may be computed using methods in Duarte et al.[2023].
  We illustrate our method via simulation studies and a data application on central line-associated bloodstream infections (CLABSIs).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00549&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Trung Phung, Jaron J. R. Lee, Opeyemi Oladapo-Shittu, Eili Y. Klein, Ayse Pinar Gurses, Susan M. Hannum, Kimberly Weems, Jill A. Marsteller, Sara E. Cosgrove, Sara C. Keller, Ilya Shpitser</name></author><category term="stat.ME" /><summary type="html">A common type of zero-inflated data has certain true values incorrectly replaced by zeros due to data recording conventions (rare outcomes assumed to be absent) or details of data recording equipment (e.g. artificial zeros in gene expression data). Existing methods for zero-inflated data either fit the observed data likelihood via parametric mixture models that explicitly represent excess zeros, or aim to replace excess zeros by imputed values. If the goal of the analysis relies on knowing true data realizations, a particular challenge with zero-inflated data is identifiability, since it is difficult to correctly determine which observed zeros are real and which are inflated. This paper views zero-inflated data as a general type of missing data problem, where the observability indicator for a potentially censored variable is itself unobserved whenever a zero is recorded. We show that, without additional assumptions, target parameters involving a zero-inflated variable are not identified. However, if a proxy of the missingness indicator is observed, a modification of the effect restoration approach of Kuroki and Pearl allows identification and estimation, given the proxy-indicator relationship is known. If this relationship is unknown, our approach yields a partial identification strategy for sensitivity analysis. Specifically, we show that only certain proxy-indicator relationships are compatible with the observed data distribution. We give an analytic bound for this relationship in cases with a categorical outcome, which is sharp in certain models. For more complex cases, sharp numerical bounds may be computed using methods in Duarte et al.[2023]. We illustrate our method via simulation studies and a data application on central line-associated bloodstream infections (CLABSIs).</summary></entry><entry><title type="html">animal2vec and MeerKAT: A self-supervised transformer for rare-event raw audio input and a large-scale reference dataset for bioacoustics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/animal2vecandMeerKATAselfsupervisedtransformerforrareeventrawaudioinputandalargescalereferencedatasetforbioacoustics.html" rel="alternate" type="text/html" title="animal2vec and MeerKAT: A self-supervised transformer for rare-event raw audio input and a large-scale reference dataset for bioacoustics" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/animal2vecandMeerKATAselfsupervisedtransformerforrareeventrawaudioinputandalargescalereferencedatasetforbioacoustics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/animal2vecandMeerKATAselfsupervisedtransformerforrareeventrawaudioinputandalargescalereferencedatasetforbioacoustics.html">&lt;p&gt;Bioacoustic research provides invaluable insights into the behavior, ecology, and conservation of animals. Most bioacoustic datasets consist of long recordings where events of interest, such as vocalizations, are exceedingly rare. Analyzing these datasets poses a monumental challenge to researchers, where deep learning techniques have emerged as a standard method. Their adaptation remains challenging, focusing on models conceived for computer vision, where the audio waveforms are engineered into spectrographic representations for training and inference. We improve the current state of deep learning in bioacoustics in two ways: First, we present the animal2vec framework: a fully interpretable transformer model and self-supervised training scheme tailored for sparse and unbalanced bioacoustic data. Second, we openly publish MeerKAT: Meerkat Kalahari Audio Transcripts, a large-scale dataset containing audio collected via biologgers deployed on free-ranging meerkats with a length of over 1068h, of which 184h have twelve time-resolved vocalization-type classes, each with ms-resolution, making it the largest publicly-available labeled dataset on terrestrial mammals. Further, we benchmark animal2vec against the NIPS4Bplus birdsong dataset. We report new state-of-the-art results on both datasets and evaluate the few-shot capabilities of animal2vec of labeled training data. Finally, we perform ablation studies to highlight the differences between our architecture and a vanilla transformer baseline for human-produced sounds. animal2vec allows researchers to classify massive amounts of sparse bioacoustic data even with little ground truth information available. In addition, the MeerKAT dataset is the first large-scale, millisecond-resolution corpus for benchmarking bioacoustic models in the pretrain/finetune paradigm. We believe this sets the stage for a new reference point for bioacoustics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.01253&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Julian C. Schäfer-Zimmermann, Vlad Demartsev, Baptiste Averly, Kiran Dhanjal-Adams, Mathieu Duteil, Gabriella Gall, Marius Fai{\ss}, Lily Johnson-Ulrich, Dan Stowell, Marta B. Manser, Marie A. Roch, Ariana Strandburg-Peshkin</name></author><category term="stat.AP" /><summary type="html">Bioacoustic research provides invaluable insights into the behavior, ecology, and conservation of animals. Most bioacoustic datasets consist of long recordings where events of interest, such as vocalizations, are exceedingly rare. Analyzing these datasets poses a monumental challenge to researchers, where deep learning techniques have emerged as a standard method. Their adaptation remains challenging, focusing on models conceived for computer vision, where the audio waveforms are engineered into spectrographic representations for training and inference. We improve the current state of deep learning in bioacoustics in two ways: First, we present the animal2vec framework: a fully interpretable transformer model and self-supervised training scheme tailored for sparse and unbalanced bioacoustic data. Second, we openly publish MeerKAT: Meerkat Kalahari Audio Transcripts, a large-scale dataset containing audio collected via biologgers deployed on free-ranging meerkats with a length of over 1068h, of which 184h have twelve time-resolved vocalization-type classes, each with ms-resolution, making it the largest publicly-available labeled dataset on terrestrial mammals. Further, we benchmark animal2vec against the NIPS4Bplus birdsong dataset. We report new state-of-the-art results on both datasets and evaluate the few-shot capabilities of animal2vec of labeled training data. Finally, we perform ablation studies to highlight the differences between our architecture and a vanilla transformer baseline for human-produced sounds. animal2vec allows researchers to classify massive amounts of sparse bioacoustic data even with little ground truth information available. In addition, the MeerKAT dataset is the first large-scale, millisecond-resolution corpus for benchmarking bioacoustic models in the pretrain/finetune paradigm. We believe this sets the stage for a new reference point for bioacoustics.</summary></entry><entry><title type="html">mdendro: An R package for extended agglomerative hierarchical clustering</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/mdendroAnRpackageforextendedagglomerativehierarchicalclustering.html" rel="alternate" type="text/html" title="mdendro: An R package for extended agglomerative hierarchical clustering" /><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/mdendroAnRpackageforextendedagglomerativehierarchicalclustering</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/mdendroAnRpackageforextendedagglomerativehierarchicalclustering.html">&lt;p&gt;“mdendro” is an R package that provides a comprehensive collection of linkage methods for agglomerative hierarchical clustering on a matrix of proximity data (distances or similarities), returning a multifurcated dendrogram or multidendrogram. Multidendrograms can group more than two clusters at the same time, solving the nonuniqueness problem that arises when there are ties in the data. This problem causes that different binary dendrograms are possible depending both on the order of the input data and on the criterion used to break ties. Weighted and unweighted versions of the most common linkage methods are included in the package, which also implements two parametric linkage methods. In addition, package “mdendro” provides five descriptive measures to analyze the resulting dendrograms: cophenetic correlation coefficient, space distortion ratio, agglomeration coefficient, chaining coefficient and tree balance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.13333&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alberto Fernández, Sergio Gómez</name></author><category term="stat.CO" /><summary type="html">“mdendro” is an R package that provides a comprehensive collection of linkage methods for agglomerative hierarchical clustering on a matrix of proximity data (distances or similarities), returning a multifurcated dendrogram or multidendrogram. Multidendrograms can group more than two clusters at the same time, solving the nonuniqueness problem that arises when there are ties in the data. This problem causes that different binary dendrograms are possible depending both on the order of the input data and on the criterion used to break ties. Weighted and unweighted versions of the most common linkage methods are included in the package, which also implements two parametric linkage methods. In addition, package “mdendro” provides five descriptive measures to analyze the resulting dendrograms: cophenetic correlation coefficient, space distortion ratio, agglomeration coefficient, chaining coefficient and tree balance.</summary></entry></feed>