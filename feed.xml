<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-28T07:14:36+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A First Course in Monte Carlo Methods</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AFirstCourseinMonteCarloMethods.html" rel="alternate" type="text/html" title="A First Course in Monte Carlo Methods" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AFirstCourseinMonteCarloMethods</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AFirstCourseinMonteCarloMethods.html">&lt;p&gt;This is a concise mathematical introduction to Monte Carlo methods, a rich family of algorithms with far-reaching applications in science and engineering. Monte Carlo methods are an exciting subject for mathematical statisticians and computational and applied mathematicians: the design and analysis of modern algorithms are rooted in a broad mathematical toolbox that includes ergodic theory of Markov chains, Hamiltonian dynamical systems, transport maps, stochastic differential equations, information theory, optimization, Riemannian geometry, and gradient flows, among many others. These lecture notes celebrate the breadth of mathematical ideas that have led to tangible advancements in Monte Carlo methods and their applications. To accommodate a diverse audience, the level of mathematical rigor varies from chapter to chapter, giving only an intuitive treatment to the most technically demanding subjects. The aim is not to be comprehensive or encyclopedic, but rather to illustrate some key principles in the design and analysis of Monte Carlo methods through a carefully-crafted choice of topics that emphasizes timeless over timely ideas. Algorithms are presented in a way that is conducive to conceptual understanding and mathematical analysis – clarity and intuition are favored over state-of-the-art implementations that are harder to comprehend or rely on ad-hoc heuristics. To help readers navigate the expansive landscape of Monte Carlo methods, each algorithm is accompanied by a summary of its pros and cons, and by a discussion of the type of problems for which they are most useful. The presentation is self-contained, and therefore adequate for self-guided learning or as a teaching resource. Each chapter contains a section with bibliographic remarks that will be useful for those interested in conducting research on Monte Carlo methods and their applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16359&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Daniel Sanz-Alonso, Omar Al-Ghattas</name></author><category term="stat.CO" /><summary type="html">This is a concise mathematical introduction to Monte Carlo methods, a rich family of algorithms with far-reaching applications in science and engineering. Monte Carlo methods are an exciting subject for mathematical statisticians and computational and applied mathematicians: the design and analysis of modern algorithms are rooted in a broad mathematical toolbox that includes ergodic theory of Markov chains, Hamiltonian dynamical systems, transport maps, stochastic differential equations, information theory, optimization, Riemannian geometry, and gradient flows, among many others. These lecture notes celebrate the breadth of mathematical ideas that have led to tangible advancements in Monte Carlo methods and their applications. To accommodate a diverse audience, the level of mathematical rigor varies from chapter to chapter, giving only an intuitive treatment to the most technically demanding subjects. The aim is not to be comprehensive or encyclopedic, but rather to illustrate some key principles in the design and analysis of Monte Carlo methods through a carefully-crafted choice of topics that emphasizes timeless over timely ideas. Algorithms are presented in a way that is conducive to conceptual understanding and mathematical analysis – clarity and intuition are favored over state-of-the-art implementations that are harder to comprehend or rely on ad-hoc heuristics. To help readers navigate the expansive landscape of Monte Carlo methods, each algorithm is accompanied by a summary of its pros and cons, and by a discussion of the type of problems for which they are most useful. The presentation is self-contained, and therefore adequate for self-guided learning or as a teaching resource. Each chapter contains a section with bibliographic remarks that will be useful for those interested in conducting research on Monte Carlo methods and their applications.</summary></entry><entry><title type="html">A Systematic Bias of Machine Learning Regression Models and Its Correction: an Application to Imaging-based Brain Age Prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ASystematicBiasofMachineLearningRegressionModelsandItsCorrectionanApplicationtoImagingbasedBrainAgePrediction.html" rel="alternate" type="text/html" title="A Systematic Bias of Machine Learning Regression Models and Its Correction: an Application to Imaging-based Brain Age Prediction" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ASystematicBiasofMachineLearningRegressionModelsandItsCorrectionanApplicationtoImagingbasedBrainAgePrediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ASystematicBiasofMachineLearningRegressionModelsandItsCorrectionanApplicationtoImagingbasedBrainAgePrediction.html">&lt;p&gt;Machine learning models for continuous outcomes often yield systematically biased predictions, particularly for values that largely deviate from the mean. Specifically, predictions for large-valued outcomes tend to be negatively biased, while those for small-valued outcomes are positively biased. We refer to this linear central tendency warped bias as the “systematic bias of machine learning regression”. In this paper, we first demonstrate that this issue persists across various machine learning models, and then delve into its theoretical underpinnings. We propose a general constrained optimization approach designed to correct this bias and develop a computationally efficient algorithm to implement our method. Our simulation results indicate that our correction method effectively eliminates the bias from the predicted outcomes. We apply the proposed approach to the prediction of brain age using neuroimaging data. In comparison to competing machine learning models, our method effectively addresses the longstanding issue of “systematic bias of machine learning regression” in neuroimaging-based brain age calculation, yielding unbiased predictions of brain age.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15950&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hwiyoung Lee, Shuo Chen</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Machine learning models for continuous outcomes often yield systematically biased predictions, particularly for values that largely deviate from the mean. Specifically, predictions for large-valued outcomes tend to be negatively biased, while those for small-valued outcomes are positively biased. We refer to this linear central tendency warped bias as the “systematic bias of machine learning regression”. In this paper, we first demonstrate that this issue persists across various machine learning models, and then delve into its theoretical underpinnings. We propose a general constrained optimization approach designed to correct this bias and develop a computationally efficient algorithm to implement our method. Our simulation results indicate that our correction method effectively eliminates the bias from the predicted outcomes. We apply the proposed approach to the prediction of brain age using neuroimaging data. In comparison to competing machine learning models, our method effectively addresses the longstanding issue of “systematic bias of machine learning regression” in neuroimaging-based brain age calculation, yielding unbiased predictions of brain age.</summary></entry><entry><title type="html">Acquiring Better Load Estimates by Combining Anomaly and Change-point Detection in Power Grid Time-series Measurements</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AcquiringBetterLoadEstimatesbyCombiningAnomalyandChangepointDetectioninPowerGridTimeseriesMeasurements.html" rel="alternate" type="text/html" title="Acquiring Better Load Estimates by Combining Anomaly and Change-point Detection in Power Grid Time-series Measurements" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AcquiringBetterLoadEstimatesbyCombiningAnomalyandChangepointDetectioninPowerGridTimeseriesMeasurements</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AcquiringBetterLoadEstimatesbyCombiningAnomalyandChangepointDetectioninPowerGridTimeseriesMeasurements.html">&lt;p&gt;In this paper we present novel methodology for automatic anomaly and switch event filtering to improve load estimation in power grid systems. By leveraging unsupervised methods with supervised optimization, our approach prioritizes interpretability while ensuring robust and generalizable performance on unseen data. Through experimentation, a combination of binary segmentation for change point detection and statistical process control for anomaly detection emerges as the most effective strategy, specifically when ensembled in a novel sequential manner. Results indicate the clear wasted potential when filtering is not applied. The automatic load estimation is also fairly accurate, with approximately 90% of estimates falling within a 10% error margin, with only a single significant failure in both the minimum and maximum load estimates across 60 measurements in the test set. Our methodology’s interpretability makes it particularly suitable for critical infrastructure planning, thereby enhancing decision-making processes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16164&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roel Bouman, Linda Schmeitz, Luco Buise, Jacco Heres, Yuliya Shapovalova, Tom heskes</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">In this paper we present novel methodology for automatic anomaly and switch event filtering to improve load estimation in power grid systems. By leveraging unsupervised methods with supervised optimization, our approach prioritizes interpretability while ensuring robust and generalizable performance on unseen data. Through experimentation, a combination of binary segmentation for change point detection and statistical process control for anomaly detection emerges as the most effective strategy, specifically when ensembled in a novel sequential manner. Results indicate the clear wasted potential when filtering is not applied. The automatic load estimation is also fairly accurate, with approximately 90% of estimates falling within a 10% error margin, with only a single significant failure in both the minimum and maximum load estimates across 60 measurements in the test set. Our methodology’s interpretability makes it particularly suitable for critical infrastructure planning, thereby enhancing decision-making processes.</summary></entry><entry><title type="html">A flexible model for correlated count data, with application to multi-condition differential expression analyses of single-cell RNA sequencing data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AflexiblemodelforcorrelatedcountdatawithapplicationtomulticonditiondifferentialexpressionanalysesofsinglecellRNAsequencingdata.html" rel="alternate" type="text/html" title="A flexible model for correlated count data, with application to multi-condition differential expression analyses of single-cell RNA sequencing data" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AflexiblemodelforcorrelatedcountdatawithapplicationtomulticonditiondifferentialexpressionanalysesofsinglecellRNAsequencingdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AflexiblemodelforcorrelatedcountdatawithapplicationtomulticonditiondifferentialexpressionanalysesofsinglecellRNAsequencingdata.html">&lt;p&gt;Detecting differences in gene expression is an important part of single-cell RNA sequencing experiments, and many statistical methods have been developed for this aim. Most differential expression analyses focus on comparing expression between two groups (e.g., treatment vs. control). But there is increasing interest in multi-condition differential expression analyses in which expression is measured in many conditions, and the aim is to accurately detect and estimate expression differences in all conditions. We show that directly modeling single-cell RNA-seq counts in all conditions simultaneously, while also inferring how expression differences are shared across conditions, leads to greatly improved performance for detecting and estimating expression differences compared to existing methods. We illustrate the potential of this new approach by analyzing data from a single-cell experiment studying the effects of cytokine stimulation on gene expression. We call our new method “Poisson multivariate adaptive shrinkage”, and it is implemented in an R package available online at https://github.com/stephenslab/poisson.mash.alpha.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.00697&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yusha Liu, Peter Carbonetto, Michihiro Takahama, Adam Gruenbaum, Dongyue Xie, Nicolas Chevrier, Matthew Stephens</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Detecting differences in gene expression is an important part of single-cell RNA sequencing experiments, and many statistical methods have been developed for this aim. Most differential expression analyses focus on comparing expression between two groups (e.g., treatment vs. control). But there is increasing interest in multi-condition differential expression analyses in which expression is measured in many conditions, and the aim is to accurately detect and estimate expression differences in all conditions. We show that directly modeling single-cell RNA-seq counts in all conditions simultaneously, while also inferring how expression differences are shared across conditions, leads to greatly improved performance for detecting and estimating expression differences compared to existing methods. We illustrate the potential of this new approach by analyzing data from a single-cell experiment studying the effects of cytokine stimulation on gene expression. We call our new method “Poisson multivariate adaptive shrinkage”, and it is implemented in an R package available online at https://github.com/stephenslab/poisson.mash.alpha.</summary></entry><entry><title type="html">A joint model for (un)bounded longitudinal markers, competing risks, and recurrent events using patient registry data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Ajointmodelforunboundedlongitudinalmarkerscompetingrisksandrecurrenteventsusingpatientregistrydata.html" rel="alternate" type="text/html" title="A joint model for (un)bounded longitudinal markers, competing risks, and recurrent events using patient registry data" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Ajointmodelforunboundedlongitudinalmarkerscompetingrisksandrecurrenteventsusingpatientregistrydata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Ajointmodelforunboundedlongitudinalmarkerscompetingrisksandrecurrenteventsusingpatientregistrydata.html">&lt;p&gt;Joint models for longitudinal and survival data have become a popular framework for studying the association between repeatedly measured biomarkers and clinical events. Nevertheless, addressing complex survival data structures, especially handling both recurrent and competing event times within a single model, remains a challenge. This causes important information to be disregarded. Moreover, existing frameworks rely on a Gaussian distribution for continuous markers, which may be unsuitable for bounded biomarkers, resulting in biased estimates of associations. To address these limitations, we propose a Bayesian shared-parameter joint model that simultaneously accommodates multiple (possibly bounded) longitudinal markers, a recurrent event process, and competing risks. We use the beta distribution to model responses bounded within any interval (a,b) without sacrificing the interpretability of the association. The model offers various forms of association, discontinuous risk intervals, and both gap and calendar timescales. A simulation study shows that it outperforms simpler joint models. We utilize the US Cystic Fibrosis Foundation Patient Registry to study the associations between changes in lung function and body mass index, and the risk of recurrent pulmonary exacerbations, while accounting for the competing risks of death and lung transplantation. Our efficient implementation allows fast fitting of the model despite its complexity and the large sample size from this patient registry. Our comprehensive approach provides new insights into cystic fibrosis disease progression by quantifying the relationship between the most important clinical markers and events more precisely than has been possible before. The model implementation is available in the R package JMbayes2.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16492&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Pedro Miranda Afonso, Dimitris Rizopoulos, Anushka K. Palipana, Emrah Gecili, Cole Brokamp, John P. Clancy, Rhonda D. Szczesniak, Eleni-Rosalina Andrinopoulou</name></author><category term="stat.ME" /><summary type="html">Joint models for longitudinal and survival data have become a popular framework for studying the association between repeatedly measured biomarkers and clinical events. Nevertheless, addressing complex survival data structures, especially handling both recurrent and competing event times within a single model, remains a challenge. This causes important information to be disregarded. Moreover, existing frameworks rely on a Gaussian distribution for continuous markers, which may be unsuitable for bounded biomarkers, resulting in biased estimates of associations. To address these limitations, we propose a Bayesian shared-parameter joint model that simultaneously accommodates multiple (possibly bounded) longitudinal markers, a recurrent event process, and competing risks. We use the beta distribution to model responses bounded within any interval (a,b) without sacrificing the interpretability of the association. The model offers various forms of association, discontinuous risk intervals, and both gap and calendar timescales. A simulation study shows that it outperforms simpler joint models. We utilize the US Cystic Fibrosis Foundation Patient Registry to study the associations between changes in lung function and body mass index, and the risk of recurrent pulmonary exacerbations, while accounting for the competing risks of death and lung transplantation. Our efficient implementation allows fast fitting of the model despite its complexity and the large sample size from this patient registry. Our comprehensive approach provides new insights into cystic fibrosis disease progression by quantifying the relationship between the most important clinical markers and events more precisely than has been possible before. The model implementation is available in the R package JMbayes2.</summary></entry><entry><title type="html">A maximin optimal approach for sampling designs in two-phase studies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Amaximinoptimalapproachforsamplingdesignsintwophasestudies.html" rel="alternate" type="text/html" title="A maximin optimal approach for sampling designs in two-phase studies" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Amaximinoptimalapproachforsamplingdesignsintwophasestudies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Amaximinoptimalapproachforsamplingdesignsintwophasestudies.html">&lt;p&gt;Data collection costs can vary widely across variables in data science tasks. Two-phase designs are often employed to save data collection costs. In two-phase studies, inexpensive variables are collected for all subjects in the first phase, and expensive variables are measured for a subset of subjects in the second phase based on a predetermined sampling rule. The estimation efficiency under two-phase designs relies heavily on the sampling rule. Existing literature primarily focuses on designing sampling rules for estimating a scalar parameter in some parametric models or specific estimating problems. However, real-world scenarios are usually model-unknown and involve two-phase designs for model-free estimation of a scalar or multi-dimensional parameter. This paper proposes a maximin criterion to design an optimal sampling rule based on semiparametric efficiency bounds. The proposed method is model-free and applicable to general estimating problems. The resulting sampling rule can minimize the semiparametric efficiency bound when the parameter is scalar and improve the bound for every component when the parameter is multi-dimensional. Simulation studies demonstrate that the proposed designs reduce the variance of the resulting estimator in various settings. The implementation of the proposed design is illustrated in a real data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.10596&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ruoyu Wang, Qihua Wang, Wang Miao</name></author><category term="stat.ME" /><summary type="html">Data collection costs can vary widely across variables in data science tasks. Two-phase designs are often employed to save data collection costs. In two-phase studies, inexpensive variables are collected for all subjects in the first phase, and expensive variables are measured for a subset of subjects in the second phase based on a predetermined sampling rule. The estimation efficiency under two-phase designs relies heavily on the sampling rule. Existing literature primarily focuses on designing sampling rules for estimating a scalar parameter in some parametric models or specific estimating problems. However, real-world scenarios are usually model-unknown and involve two-phase designs for model-free estimation of a scalar or multi-dimensional parameter. This paper proposes a maximin criterion to design an optimal sampling rule based on semiparametric efficiency bounds. The proposed method is model-free and applicable to general estimating problems. The resulting sampling rule can minimize the semiparametric efficiency bound when the parameter is scalar and improve the bound for every component when the parameter is multi-dimensional. Simulation studies demonstrate that the proposed designs reduce the variance of the resulting estimator in various settings. The implementation of the proposed design is illustrated in a real data analysis.</summary></entry><entry><title type="html">Analysis of Broken Randomized Experiments by Principal Stratification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AnalysisofBrokenRandomizedExperimentsbyPrincipalStratification.html" rel="alternate" type="text/html" title="Analysis of Broken Randomized Experiments by Principal Stratification" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AnalysisofBrokenRandomizedExperimentsbyPrincipalStratification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AnalysisofBrokenRandomizedExperimentsbyPrincipalStratification.html">&lt;p&gt;Although randomized controlled trials have long been regarded as the ``gold standard’’ for evaluating treatment effects, there is no natural prevention from post-treatment events. For example, non-compliance makes the actual treatment different from the assigned treatment, truncation-by-death renders the outcome undefined or ill-defined, and missingness prevents the outcomes from being measured. In this paper, we develop a statistical analysis framework using principal stratification to investigate the treatment effect in broken randomized experiments. The average treatment effect in compliers and always-survivors is adopted as the target causal estimand. We establish the asymptotic property for the estimator. We apply the framework to study the effect of training on earnings in the Job Corps Study and find that the training program does not have an effect on employment but possibly have an effect on improving the earnings after employment.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16780&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Qinqing Liu, Xiang Peng, Tao Zhang, Yuhao Deng</name></author><category term="stat.ME" /><summary type="html">Although randomized controlled trials have long been regarded as the ``gold standard’’ for evaluating treatment effects, there is no natural prevention from post-treatment events. For example, non-compliance makes the actual treatment different from the assigned treatment, truncation-by-death renders the outcome undefined or ill-defined, and missingness prevents the outcomes from being measured. In this paper, we develop a statistical analysis framework using principal stratification to investigate the treatment effect in broken randomized experiments. The average treatment effect in compliers and always-survivors is adopted as the target causal estimand. We establish the asymptotic property for the estimator. We apply the framework to study the effect of training on earnings in the Job Corps Study and find that the training program does not have an effect on employment but possibly have an effect on improving the earnings after employment.</summary></entry><entry><title type="html">Analyzing the Impact of Climate Change With Major Emphasis on Pollution: A Comparative Study of ML and Statistical Models in Time Series Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AnalyzingtheImpactofClimateChangeWithMajorEmphasisonPollutionAComparativeStudyofMLandStatisticalModelsinTimeSeriesData.html" rel="alternate" type="text/html" title="Analyzing the Impact of Climate Change With Major Emphasis on Pollution: A Comparative Study of ML and Statistical Models in Time Series Data" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AnalyzingtheImpactofClimateChangeWithMajorEmphasisonPollutionAComparativeStudyofMLandStatisticalModelsinTimeSeriesData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AnalyzingtheImpactofClimateChangeWithMajorEmphasisonPollutionAComparativeStudyofMLandStatisticalModelsinTimeSeriesData.html">&lt;p&gt;Industrial operations have grown exponentially over the last century, driving advancements in energy utilization through vehicles and machinery.This growth has significant environmental implications, necessitating the use of sophisticated technology to monitor and analyze climate data.The surge in industrial activities presents a complex challenge in forecasting its diverse environmental impacts, which vary greatly across different regions.Aim to understand these dynamics more deeply to predict and mitigate the environmental impacts of industrial activities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15835&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anurag Mishra, Ronen Gold, Sanjeev Vijayakumar</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Industrial operations have grown exponentially over the last century, driving advancements in energy utilization through vehicles and machinery.This growth has significant environmental implications, necessitating the use of sophisticated technology to monitor and analyze climate data.The surge in industrial activities presents a complex challenge in forecasting its diverse environmental impacts, which vary greatly across different regions.Aim to understand these dynamics more deeply to predict and mitigate the environmental impacts of industrial activities.</summary></entry><entry><title type="html">Assessing uncertainty in Gaussian mixtures-based entropy estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AssessinguncertaintyinGaussianmixturesbasedentropyestimation.html" rel="alternate" type="text/html" title="Assessing uncertainty in Gaussian mixtures-based entropy estimation" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AssessinguncertaintyinGaussianmixturesbasedentropyestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AssessinguncertaintyinGaussianmixturesbasedentropyestimation.html">&lt;p&gt;Entropy estimation plays a crucial role in various fields, such as information theory, statistical data science, and machine learning. However, traditional entropy estimation methods often struggle with complex data distributions. Mixture-based estimation of entropy has been recently proposed and gained attention due to its ease of use and accuracy. This paper presents a novel approach to quantify the uncertainty associated with this mixture-based entropy estimation method using weighted likelihood bootstrap. Unlike standard methods, our approach leverages the underlying mixture structure by assigning random weights to observations in a weighted likelihood bootstrap procedure, leading to more accurate uncertainty estimation. The generation of weights is also investigated, leading to the proposal of using weights obtained from a Dirichlet distribution with parameter $\alpha = 0.8137$ instead of the usual $\alpha = 1$. Furthermore, the use of centered percentile intervals emerges as the preferred choice to ensure empirical coverage close to the nominal level. Extensive simulation studies comparing different resampling strategies are presented and results discussed. The proposed approach is illustrated by analyzing the log-returns of daily Gold prices at COMEX for the years 2014–2022, and the Net Rating scores, an advanced statistic used in basketball analytics, for NBA teams with reference to the 2022/23 regular season.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17265&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luca Scrucca</name></author><category term="stat.ME" /><summary type="html">Entropy estimation plays a crucial role in various fields, such as information theory, statistical data science, and machine learning. However, traditional entropy estimation methods often struggle with complex data distributions. Mixture-based estimation of entropy has been recently proposed and gained attention due to its ease of use and accuracy. This paper presents a novel approach to quantify the uncertainty associated with this mixture-based entropy estimation method using weighted likelihood bootstrap. Unlike standard methods, our approach leverages the underlying mixture structure by assigning random weights to observations in a weighted likelihood bootstrap procedure, leading to more accurate uncertainty estimation. The generation of weights is also investigated, leading to the proposal of using weights obtained from a Dirichlet distribution with parameter $\alpha = 0.8137$ instead of the usual $\alpha = 1$. Furthermore, the use of centered percentile intervals emerges as the preferred choice to ensure empirical coverage close to the nominal level. Extensive simulation studies comparing different resampling strategies are presented and results discussed. The proposed approach is illustrated by analyzing the log-returns of daily Gold prices at COMEX for the years 2014–2022, and the Net Rating scores, an advanced statistic used in basketball analytics, for NBA teams with reference to the 2022/23 regular season.</summary></entry><entry><title type="html">Automating the Selection of Proxy Variables of Unmeasured Confounders</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AutomatingtheSelectionofProxyVariablesofUnmeasuredConfounders.html" rel="alternate" type="text/html" title="Automating the Selection of Proxy Variables of Unmeasured Confounders" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AutomatingtheSelectionofProxyVariablesofUnmeasuredConfounders</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/AutomatingtheSelectionofProxyVariablesofUnmeasuredConfounders.html">&lt;p&gt;Recently, interest has grown in the use of proxy variables of unobserved confounding for inferring the causal effect in the presence of unmeasured confounders from observational data. One difficulty inhibiting the practical use is finding valid proxy variables of unobserved confounding to a target causal effect of interest. These proxy variables are typically justified by background knowledge. In this paper, we investigate the estimation of causal effects among multiple treatments and a single outcome, all of which are affected by unmeasured confounders, within a linear causal model, without prior knowledge of the validity of proxy variables. To be more specific, we first extend the existing proxy variable estimator, originally addressing a single unmeasured confounder, to accommodate scenarios where multiple unmeasured confounders exist between the treatments and the outcome. Subsequently, we present two different sets of precise identifiability conditions for selecting valid proxy variables of unmeasured confounders, based on the second-order statistics and higher-order statistics of the data, respectively. Moreover, we propose two data-driven methods for the selection of proxy variables and for the unbiased estimation of causal effects. Theoretical analysis demonstrates the correctness of our proposed algorithms. Experimental results on both synthetic and real-world data show the effectiveness of the proposed approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16130&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Feng Xie, Zhengming Chen, Shanshan Luo, Wang Miao, Ruichu Cai, Zhi Geng</name></author><category term="stat.ME" /><summary type="html">Recently, interest has grown in the use of proxy variables of unobserved confounding for inferring the causal effect in the presence of unmeasured confounders from observational data. One difficulty inhibiting the practical use is finding valid proxy variables of unobserved confounding to a target causal effect of interest. These proxy variables are typically justified by background knowledge. In this paper, we investigate the estimation of causal effects among multiple treatments and a single outcome, all of which are affected by unmeasured confounders, within a linear causal model, without prior knowledge of the validity of proxy variables. To be more specific, we first extend the existing proxy variable estimator, originally addressing a single unmeasured confounder, to accommodate scenarios where multiple unmeasured confounders exist between the treatments and the outcome. Subsequently, we present two different sets of precise identifiability conditions for selecting valid proxy variables of unmeasured confounders, based on the second-order statistics and higher-order statistics of the data, respectively. Moreover, we propose two data-driven methods for the selection of proxy variables and for the unbiased estimation of causal effects. Theoretical analysis demonstrates the correctness of our proposed algorithms. Experimental results on both synthetic and real-world data show the effectiveness of the proposed approach.</summary></entry><entry><title type="html">Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/BayesianSafePolicyLearningwithChanceConstrainedOptimizationApplicationtoMilitarySecurityAssessmentduringtheVietnamWar.html" rel="alternate" type="text/html" title="Bayesian Safe Policy Learning with Chance Constrained Optimization: Application to Military Security Assessment during the Vietnam War" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/BayesianSafePolicyLearningwithChanceConstrainedOptimizationApplicationtoMilitarySecurityAssessmentduringtheVietnamWar</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/BayesianSafePolicyLearningwithChanceConstrainedOptimizationApplicationtoMilitarySecurityAssessmentduringtheVietnamWar.html">&lt;p&gt;Algorithmic decisions and recommendations are used in many high-stakes decision-making settings such as criminal justice, medicine, and public policy. We investigate whether it would have been possible to improve a security assessment algorithm employed during the Vietnam War, using outcomes measured immediately after its introduction in late 1969. This empirical application raises several methodological challenges that frequently arise in high-stakes algorithmic decision-making. First, before implementing a new algorithm, it is essential to characterize and control the risk of yielding worse outcomes than the existing algorithm. Second, the existing algorithm is deterministic, and learning a new algorithm requires transparent extrapolation. Third, the existing algorithm involves discrete decision tables that are difficult to optimize over.
  To address these challenges, we introduce the Average Conditional Risk (ACRisk), which first quantifies the risk that a new algorithmic policy leads to worse outcomes for subgroups of individual units and then averages this over the distribution of subgroups. We also propose a Bayesian policy learning framework that maximizes the posterior expected value while controlling the posterior expected ACRisk. This framework separates the estimation of heterogeneous treatment effects from policy optimization, enabling flexible estimation of effects and optimization over complex policy classes. We characterize the resulting chance-constrained optimization problem as a constrained linear programming problem. Our analysis shows that compared to the actual algorithm used during the Vietnam War, the learned algorithm assesses most regions as more secure and emphasizes economic and political factors over military factors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.08840&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zeyang Jia, Eli Ben-Michael, Kosuke Imai</name></author><category term="stat.AP" /><summary type="html">Algorithmic decisions and recommendations are used in many high-stakes decision-making settings such as criminal justice, medicine, and public policy. We investigate whether it would have been possible to improve a security assessment algorithm employed during the Vietnam War, using outcomes measured immediately after its introduction in late 1969. This empirical application raises several methodological challenges that frequently arise in high-stakes algorithmic decision-making. First, before implementing a new algorithm, it is essential to characterize and control the risk of yielding worse outcomes than the existing algorithm. Second, the existing algorithm is deterministic, and learning a new algorithm requires transparent extrapolation. Third, the existing algorithm involves discrete decision tables that are difficult to optimize over. To address these challenges, we introduce the Average Conditional Risk (ACRisk), which first quantifies the risk that a new algorithmic policy leads to worse outcomes for subgroups of individual units and then averages this over the distribution of subgroups. We also propose a Bayesian policy learning framework that maximizes the posterior expected value while controlling the posterior expected ACRisk. This framework separates the estimation of heterogeneous treatment effects from policy optimization, enabling flexible estimation of effects and optimization over complex policy classes. We characterize the resulting chance-constrained optimization problem as a constrained linear programming problem. Our analysis shows that compared to the actual algorithm used during the Vietnam War, the learned algorithm assesses most regions as more secure and emphasizes economic and political factors over military factors.</summary></entry><entry><title type="html">Causal Temporal Regime Structure Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/CausalTemporalRegimeStructureLearning.html" rel="alternate" type="text/html" title="Causal Temporal Regime Structure Learning" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/CausalTemporalRegimeStructureLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/CausalTemporalRegimeStructureLearning.html">&lt;p&gt;We address the challenge of structure learning from multivariate time series that are characterized by a sequence of different, unknown regimes. We introduce a new optimization-based method (CASTOR), that concurrently learns the Directed Acyclic Graph (DAG) for each regime and determine the number of regimes along with their sequential arrangement. Through the optimization of a score function via an expectation maximization (EM) algorithm, CASTOR alternates between learning the regime indices (Expectation step) and inferring causal relationships in each regime (Maximization step). We further prove the identifiability of regimes and DAGs within the CASTOR framework. We conduct extensive experiments and show that our method consistently outperforms causal discovery models across various settings (linear and nonlinear causal relationships) and datasets (synthetic and real data).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.01412&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Abdellah Rahmani, Pascal Frossard</name></author><category term="stat.ME" /><summary type="html">We address the challenge of structure learning from multivariate time series that are characterized by a sequence of different, unknown regimes. We introduce a new optimization-based method (CASTOR), that concurrently learns the Directed Acyclic Graph (DAG) for each regime and determine the number of regimes along with their sequential arrangement. Through the optimization of a score function via an expectation maximization (EM) algorithm, CASTOR alternates between learning the regime indices (Expectation step) and inferring causal relationships in each regime (Maximization step). We further prove the identifiability of regimes and DAGs within the CASTOR framework. We conduct extensive experiments and show that our method consistently outperforms causal discovery models across various settings (linear and nonlinear causal relationships) and datasets (synthetic and real data).</summary></entry><entry><title type="html">Chauhan Weighted Trajectory Analysis reduces sample size requirements and expedites time-to-efficacy signals in advanced cancer clinical trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ChauhanWeightedTrajectoryAnalysisreducessamplesizerequirementsandexpeditestimetoefficacysignalsinadvancedcancerclinicaltrials.html" rel="alternate" type="text/html" title="Chauhan Weighted Trajectory Analysis reduces sample size requirements and expedites time-to-efficacy signals in advanced cancer clinical trials" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ChauhanWeightedTrajectoryAnalysisreducessamplesizerequirementsandexpeditestimetoefficacysignalsinadvancedcancerclinicaltrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ChauhanWeightedTrajectoryAnalysisreducessamplesizerequirementsandexpeditestimetoefficacysignalsinadvancedcancerclinicaltrials.html">&lt;p&gt;As Kaplan-Meier (KM) analysis is limited to single unidirectional endpoints, most advanced cancer randomized clinical trials (RCTs) are powered for either progression free survival (PFS) or overall survival (OS). This discards efficacy information carried by partial responses, complete responses, and stable disease that frequently precede progressive disease and death. Chauhan Weighted Trajectory Analysis (CWTA) is a generalization of KM that simultaneously assesses multiple rank-ordered endpoints. We hypothesized that CWTA could use this efficacy information to reduce sample size requirements and expedite efficacy signals in advanced cancer trials. We performed 100-fold and 1000-fold simulations of solid tumour systemic therapy RCTs with health statuses rank ordered from complete response (Stage 0) to death (Stage 4). At increments of sample size and hazard ratio, we compared KM PFS and OS with CWTA for (i) sample size requirements to achieve a power of 0.8 and (ii) time-to-first significant efficacy signal. CWTA consistently demonstrated greater power, and reduced sample size requirements by 18% to 35% compared to KM PFS and 14% to 20% compared to KM OS. CWTA also expedited time-to-efficacy signals 2- to 6-fold. CWTA, by incorporating all efficacy signals in the cancer treatment trajectory, provides clinically relevant reduction in required sample size and meaningfully expedites the efficacy signals of cancer treatments compared to KM PFS and KM OS. Using CWTA rather than KM as the primary trial outcome has the potential to meaningfully reduce the numbers of patients, trial duration, and costs to evaluate therapies in advanced cancer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02529&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Utkarsh Chauhan, Daylen Mackey, John R. Mackey</name></author><category term="stat.ME" /><summary type="html">As Kaplan-Meier (KM) analysis is limited to single unidirectional endpoints, most advanced cancer randomized clinical trials (RCTs) are powered for either progression free survival (PFS) or overall survival (OS). This discards efficacy information carried by partial responses, complete responses, and stable disease that frequently precede progressive disease and death. Chauhan Weighted Trajectory Analysis (CWTA) is a generalization of KM that simultaneously assesses multiple rank-ordered endpoints. We hypothesized that CWTA could use this efficacy information to reduce sample size requirements and expedite efficacy signals in advanced cancer trials. We performed 100-fold and 1000-fold simulations of solid tumour systemic therapy RCTs with health statuses rank ordered from complete response (Stage 0) to death (Stage 4). At increments of sample size and hazard ratio, we compared KM PFS and OS with CWTA for (i) sample size requirements to achieve a power of 0.8 and (ii) time-to-first significant efficacy signal. CWTA consistently demonstrated greater power, and reduced sample size requirements by 18% to 35% compared to KM PFS and 14% to 20% compared to KM OS. CWTA also expedited time-to-efficacy signals 2- to 6-fold. CWTA, by incorporating all efficacy signals in the cancer treatment trajectory, provides clinically relevant reduction in required sample size and meaningfully expedites the efficacy signals of cancer treatments compared to KM PFS and KM OS. Using CWTA rather than KM as the primary trial outcome has the potential to meaningfully reduce the numbers of patients, trial duration, and costs to evaluate therapies in advanced cancer.</summary></entry><entry><title type="html">Combining straight-line and map-based distances to investigate the connection between proximity to healthy foods and disease</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Combiningstraightlineandmapbaseddistancestoinvestigatetheconnectionbetweenproximitytohealthyfoodsanddisease.html" rel="alternate" type="text/html" title="Combining straight-line and map-based distances to investigate the connection between proximity to healthy foods and disease" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Combiningstraightlineandmapbaseddistancestoinvestigatetheconnectionbetweenproximitytohealthyfoodsanddisease</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Combiningstraightlineandmapbaseddistancestoinvestigatetheconnectionbetweenproximitytohealthyfoodsanddisease.html">&lt;p&gt;Healthy foods are essential for a healthy life, but accessing healthy food can be more challenging for some people than others. This disparity in food access may lead to disparities in well-being, potentially with disproportionate rates of diseases in communities that face more challenges in accessing healthy food (i.e., low-access communities). Identifying low-access, high-risk communities for targeted interventions is a public health priority, but current methods to quantify food access rely on distance measures that are either computationally simple (like the length of the shortest straight-line route) or accurate (like the length of the shortest map-based driving route), but not both. We propose a multiple imputation approach to combine these distance measures, allowing researchers to harness the computational ease of one with the accuracy of the other. The approach incorporates straight-line distances for all neighborhoods and map-based distances for just a subset, offering comparable estimates to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gold standard&apos;&apos; model using map-based distances for all neighborhoods and improved efficiency over the&lt;/code&gt;complete case’’ model using map-based distances for just the subset. Through the adoption of a measurement error framework, information from the straight-line distances can be leveraged to compute informative placeholders (i.e., impute) for any neighborhoods without map-based distances. Using simulations and data for the Piedmont Triad region of North Carolina, we quantify and compare the associations between various health outcomes (diabetes and obesity) and neighborhood-level proximity to healthy foods. The imputation procedure also makes it possible to predict the full landscape of food access in an area without requiring map-based measurements for all neighborhoods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16385&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sarah C. Lotspeich, Ashley E. Mullan, Lucy D&apos;Agostino McGowan, Staci A. Hepler</name></author><category term="stat.AP" /><summary type="html">Healthy foods are essential for a healthy life, but accessing healthy food can be more challenging for some people than others. This disparity in food access may lead to disparities in well-being, potentially with disproportionate rates of diseases in communities that face more challenges in accessing healthy food (i.e., low-access communities). Identifying low-access, high-risk communities for targeted interventions is a public health priority, but current methods to quantify food access rely on distance measures that are either computationally simple (like the length of the shortest straight-line route) or accurate (like the length of the shortest map-based driving route), but not both. We propose a multiple imputation approach to combine these distance measures, allowing researchers to harness the computational ease of one with the accuracy of the other. The approach incorporates straight-line distances for all neighborhoods and map-based distances for just a subset, offering comparable estimates to the gold standard&apos;&apos; model using map-based distances for all neighborhoods and improved efficiency over thecomplete case’’ model using map-based distances for just the subset. Through the adoption of a measurement error framework, information from the straight-line distances can be leveraged to compute informative placeholders (i.e., impute) for any neighborhoods without map-based distances. Using simulations and data for the Piedmont Triad region of North Carolina, we quantify and compare the associations between various health outcomes (diabetes and obesity) and neighborhood-level proximity to healthy foods. The imputation procedure also makes it possible to predict the full landscape of food access in an area without requiring map-based measurements for all neighborhoods.</summary></entry><entry><title type="html">Conformal Robust Control of Linear Systems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ConformalRobustControlofLinearSystems.html" rel="alternate" type="text/html" title="Conformal Robust Control of Linear Systems" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ConformalRobustControlofLinearSystems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ConformalRobustControlofLinearSystems.html">&lt;p&gt;End-to-end engineering design pipelines, in which designs are evaluated using concurrently defined optimal controllers, are becoming increasingly common in practice. To discover designs that perform well even under the misspecification of system dynamics, such end-to-end pipelines have now begun evaluating designs with a robust control objective in place of the nominal optimal control setup. Current approaches of specifying such robust control subproblems, however, rely on hand specification of perturbations anticipated to be present upon deployment or margin methods that ignore problem structure, resulting in a lack of theoretical guarantees and overly conservative empirical performance. We, instead, propose a novel methodology for LQR systems that leverages conformal prediction to specify such uncertainty regions in a data-driven fashion. Such regions have distribution-free coverage guarantees on the true system dynamics, in turn allowing for a probabilistic characterization of the regret of the resulting robust controller. We then demonstrate that such a controller can be efficiently produced via a novel policy gradient method that has convergence guarantees. We finally demonstrate the superior empirical performance of our method over alternate robust control specifications in a collection of engineering control systems, specifically for airfoils and a load-positioning system.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16250&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yash Patel, Sahana Rayan, Ambuj Tewari</name></author><category term="stat.ME" /><summary type="html">End-to-end engineering design pipelines, in which designs are evaluated using concurrently defined optimal controllers, are becoming increasingly common in practice. To discover designs that perform well even under the misspecification of system dynamics, such end-to-end pipelines have now begun evaluating designs with a robust control objective in place of the nominal optimal control setup. Current approaches of specifying such robust control subproblems, however, rely on hand specification of perturbations anticipated to be present upon deployment or margin methods that ignore problem structure, resulting in a lack of theoretical guarantees and overly conservative empirical performance. We, instead, propose a novel methodology for LQR systems that leverages conformal prediction to specify such uncertainty regions in a data-driven fashion. Such regions have distribution-free coverage guarantees on the true system dynamics, in turn allowing for a probabilistic characterization of the regret of the resulting robust controller. We then demonstrate that such a controller can be efficiently produced via a novel policy gradient method that has convergence guarantees. We finally demonstrate the superior empirical performance of our method over alternate robust control specifications in a collection of engineering control systems, specifically for airfoils and a load-positioning system.</summary></entry><entry><title type="html">Conformalized Late Fusion Multi-View Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ConformalizedLateFusionMultiViewLearning.html" rel="alternate" type="text/html" title="Conformalized Late Fusion Multi-View Learning" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ConformalizedLateFusionMultiViewLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ConformalizedLateFusionMultiViewLearning.html">&lt;p&gt;Uncertainty quantification for multi-view learning is motivated by the increasing use of multi-view data in scientific problems. A common variant of multi-view learning is late fusion: train separate predictors on individual views and combine them after single-view predictions are available. Existing methods for uncertainty quantification for late fusion often rely on undesirable distributional assumptions for validity. Conformal prediction is one approach that avoids such distributional assumptions. However, naively applying conformal prediction to late-stage fusion pipelines often produces overly conservative and uninformative prediction regions, limiting its downstream utility. We propose a novel methodology, Multi-View Conformal Prediction (MVCP), where conformal prediction is instead performed separately on the single-view predictors and only fused subsequently. Our framework extends the standard scalar formulation of a score function to a multivariate score that produces more efficient downstream prediction regions in both classification and regression settings. We then demonstrate that such improvements can be realized in methods built atop conformalized regressors, specifically in robust predict-then-optimize pipelines.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16246&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Eduardo Ochoa Rivera, Yash Patel, Ambuj Tewari</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Uncertainty quantification for multi-view learning is motivated by the increasing use of multi-view data in scientific problems. A common variant of multi-view learning is late fusion: train separate predictors on individual views and combine them after single-view predictions are available. Existing methods for uncertainty quantification for late fusion often rely on undesirable distributional assumptions for validity. Conformal prediction is one approach that avoids such distributional assumptions. However, naively applying conformal prediction to late-stage fusion pipelines often produces overly conservative and uninformative prediction regions, limiting its downstream utility. We propose a novel methodology, Multi-View Conformal Prediction (MVCP), where conformal prediction is instead performed separately on the single-view predictors and only fused subsequently. Our framework extends the standard scalar formulation of a score function to a multivariate score that produces more efficient downstream prediction regions in both classification and regression settings. We then demonstrate that such improvements can be realized in methods built atop conformalized regressors, specifically in robust predict-then-optimize pipelines.</summary></entry><entry><title type="html">Contextual Linear Optimization with Bandit Feedback</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ContextualLinearOptimizationwithBanditFeedback.html" rel="alternate" type="text/html" title="Contextual Linear Optimization with Bandit Feedback" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ContextualLinearOptimizationwithBanditFeedback</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ContextualLinearOptimizationwithBanditFeedback.html">&lt;p&gt;Contextual linear optimization (CLO) uses predictive observations to reduce uncertainty in random cost coefficients and thereby improve average-cost performance. An example is a stochastic shortest path with random edge costs (e.g., traffic) and predictive features (e.g., lagged traffic, weather). Existing work on CLO assumes the data has fully observed cost coefficient vectors, but in many applications, we can only see the realized cost of a historical decision, that is, just one projection of the random cost coefficient vector, to which we refer as bandit feedback. We study a class of algorithms for CLO with bandit feedback, which we term induced empirical risk minimization (IERM), where we fit a predictive model to directly optimize the downstream performance of the policy it induces. We show a fast-rate regret bound for IERM that allows for misspecified model classes and flexible choices of the optimization estimate, and we develop computationally tractable surrogate losses. A byproduct of our theory of independent interest is fast-rate regret bound for IERM with full feedback and misspecified policy class. We compare the performance of different modeling choices numerically using a stochastic shortest path example and provide practical insights from the empirical results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16564&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yichun Hu, Nathan Kallus, Xiaojie Mao, Yanchen Wu</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Contextual linear optimization (CLO) uses predictive observations to reduce uncertainty in random cost coefficients and thereby improve average-cost performance. An example is a stochastic shortest path with random edge costs (e.g., traffic) and predictive features (e.g., lagged traffic, weather). Existing work on CLO assumes the data has fully observed cost coefficient vectors, but in many applications, we can only see the realized cost of a historical decision, that is, just one projection of the random cost coefficient vector, to which we refer as bandit feedback. We study a class of algorithms for CLO with bandit feedback, which we term induced empirical risk minimization (IERM), where we fit a predictive model to directly optimize the downstream performance of the policy it induces. We show a fast-rate regret bound for IERM that allows for misspecified model classes and flexible choices of the optimization estimate, and we develop computationally tractable surrogate losses. A byproduct of our theory of independent interest is fast-rate regret bound for IERM with full feedback and misspecified policy class. We compare the performance of different modeling choices numerically using a stochastic shortest path example and provide practical insights from the empirical results.</summary></entry><entry><title type="html">Continual Release of Differentially Private Synthetic Data from Longitudinal Data Collections</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ContinualReleaseofDifferentiallyPrivateSyntheticDatafromLongitudinalDataCollections.html" rel="alternate" type="text/html" title="Continual Release of Differentially Private Synthetic Data from Longitudinal Data Collections" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ContinualReleaseofDifferentiallyPrivateSyntheticDatafromLongitudinalDataCollections</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ContinualReleaseofDifferentiallyPrivateSyntheticDatafromLongitudinalDataCollections.html">&lt;p&gt;Motivated by privacy concerns in long-term longitudinal studies in medical and social science research, we study the problem of continually releasing differentially private synthetic data from longitudinal data collections. We introduce a model where, in every time step, each individual reports a new data element, and the goal of the synthesizer is to incrementally update a synthetic dataset in a consistent way to capture a rich class of statistical properties. We give continual synthetic data generation algorithms that preserve two basic types of queries: fixed time window queries and cumulative time queries. We show nearly tight upper bounds on the error rates of these algorithms and demonstrate their empirical performance on realistically sized datasets from the U.S. Census Bureau’s Survey of Income and Program Participation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.07884&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mark Bun, Marco Gaboardi, Marcel Neunhoeffer, Wanrong Zhang</name></author><category term="stat.AP" /><summary type="html">Motivated by privacy concerns in long-term longitudinal studies in medical and social science research, we study the problem of continually releasing differentially private synthetic data from longitudinal data collections. We introduce a model where, in every time step, each individual reports a new data element, and the goal of the synthesizer is to incrementally update a synthetic dataset in a consistent way to capture a rich class of statistical properties. We give continual synthetic data generation algorithms that preserve two basic types of queries: fixed time window queries and cumulative time queries. We show nearly tight upper bounds on the error rates of these algorithms and demonstrate their empirical performance on realistically sized datasets from the U.S. Census Bureau’s Survey of Income and Program Participation.</summary></entry><entry><title type="html">Continuous Tensor Relaxation for Finding Diverse Solutions in Combinatorial Optimization Problems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ContinuousTensorRelaxationforFindingDiverseSolutionsinCombinatorialOptimizationProblems.html" rel="alternate" type="text/html" title="Continuous Tensor Relaxation for Finding Diverse Solutions in Combinatorial Optimization Problems" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ContinuousTensorRelaxationforFindingDiverseSolutionsinCombinatorialOptimizationProblems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ContinuousTensorRelaxationforFindingDiverseSolutionsinCombinatorialOptimizationProblems.html">&lt;p&gt;Finding the best solution is a common objective in combinatorial optimization (CO). In practice, directly handling constraints is often challenging, incorporating them into the objective function as the penalties. However, balancing these penalties to achieve the desired solution is time-consuming. Additionally, formulated objective functions and constraints often only approximate real-world scenarios, where the optimal solution is not necessarily the best solution for the original real-world problem. One solution is to obtain (i) penalty-diversified solutions with varying penalty strengths for the former issue and (ii) variation-diversified solutions with different characteristics for the latter issue. Users can then post-select the desired solution from these diverse solutions. However, efficiently finding these diverse solutions is more difficult than identifying one. This study introduces Continual Tensor Relaxation Annealing (CTRA) for unsupervised-learning (UL)-based CO solvers, a computationally efficient framework for finding these diverse solutions in a single training run. The key idea is to leverage representation learning capability to automatically and efficiently learn common representations and parallelization. Numerical experiments show that CTRA enables UL-based solvers to find these diverse solutions much faster than repeatedly running existing UL-based solvers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.02190&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuma Ichikawa, Hiroaki Iwashita</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Finding the best solution is a common objective in combinatorial optimization (CO). In practice, directly handling constraints is often challenging, incorporating them into the objective function as the penalties. However, balancing these penalties to achieve the desired solution is time-consuming. Additionally, formulated objective functions and constraints often only approximate real-world scenarios, where the optimal solution is not necessarily the best solution for the original real-world problem. One solution is to obtain (i) penalty-diversified solutions with varying penalty strengths for the former issue and (ii) variation-diversified solutions with different characteristics for the latter issue. Users can then post-select the desired solution from these diverse solutions. However, efficiently finding these diverse solutions is more difficult than identifying one. This study introduces Continual Tensor Relaxation Annealing (CTRA) for unsupervised-learning (UL)-based CO solvers, a computationally efficient framework for finding these diverse solutions in a single training run. The key idea is to leverage representation learning capability to automatically and efficiently learn common representations and parallelization. Numerical experiments show that CTRA enables UL-based solvers to find these diverse solutions much faster than repeatedly running existing UL-based solvers.</summary></entry><entry><title type="html">Controlling Continuous Relaxation for Combinatorial Optimization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ControllingContinuousRelaxationforCombinatorialOptimization.html" rel="alternate" type="text/html" title="Controlling Continuous Relaxation for Combinatorial Optimization" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ControllingContinuousRelaxationforCombinatorialOptimization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ControllingContinuousRelaxationforCombinatorialOptimization.html">&lt;p&gt;Unsupervised learning (UL)-based solvers for combinatorial optimization (CO) train a neural network whose output provides a soft solution by directly optimizing the CO objective using a continuous relaxation strategy. These solvers offer several advantages over traditional methods and other learning-based methods, particularly for large-scale CO problems. However, UL-based solvers face two practical issues: (I) an optimization issue where UL-based solvers are easily trapped at local optima, and (II) a rounding issue where UL-based solvers require artificial post-learning rounding from the continuous space back to the original discrete space, undermining the robustness of the results. This study proposes a Continuous Relaxation Annealing (CRA) strategy, an effective rounding-free learning method for UL-based solvers. CRA introduces a penalty term that dynamically shifts from prioritizing continuous solutions, effectively smoothing the non-convexity of the objective function, to enforcing discreteness, eliminating the artificial rounding. Experimental results demonstrate that CRA significantly enhances the performance of UL-based solvers, outperforming existing UL-based solvers and greedy algorithms in complex CO problems. It also effectively eliminates the artificial rounding and accelerates the learning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.16965&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuma Ichikawa</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Unsupervised learning (UL)-based solvers for combinatorial optimization (CO) train a neural network whose output provides a soft solution by directly optimizing the CO objective using a continuous relaxation strategy. These solvers offer several advantages over traditional methods and other learning-based methods, particularly for large-scale CO problems. However, UL-based solvers face two practical issues: (I) an optimization issue where UL-based solvers are easily trapped at local optima, and (II) a rounding issue where UL-based solvers require artificial post-learning rounding from the continuous space back to the original discrete space, undermining the robustness of the results. This study proposes a Continuous Relaxation Annealing (CRA) strategy, an effective rounding-free learning method for UL-based solvers. CRA introduces a penalty term that dynamically shifts from prioritizing continuous solutions, effectively smoothing the non-convexity of the objective function, to enforcing discreteness, eliminating the artificial rounding. Experimental results demonstrate that CRA significantly enhances the performance of UL-based solvers, outperforming existing UL-based solvers and greedy algorithms in complex CO problems. It also effectively eliminates the artificial rounding and accelerates the learning.</summary></entry><entry><title type="html">Data-adaptive exposure thresholds for the Horvitz-Thompson estimator of the Average Treatment Effect in experiments with network interference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/DataadaptiveexposurethresholdsfortheHorvitzThompsonestimatoroftheAverageTreatmentEffectinexperimentswithnetworkinterference.html" rel="alternate" type="text/html" title="Data-adaptive exposure thresholds for the Horvitz-Thompson estimator of the Average Treatment Effect in experiments with network interference" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/DataadaptiveexposurethresholdsfortheHorvitzThompsonestimatoroftheAverageTreatmentEffectinexperimentswithnetworkinterference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/DataadaptiveexposurethresholdsfortheHorvitzThompsonestimatoroftheAverageTreatmentEffectinexperimentswithnetworkinterference.html">&lt;p&gt;Randomized controlled trials often suffer from interference, a violation of the Stable Unit Treatment Values Assumption (SUTVA) in which a unit’s treatment assignment affects the outcomes of its neighbors. This interference causes bias in naive estimators of the average treatment effect (ATE). A popular method to achieve unbiasedness is to pair the Horvitz-Thompson estimator of the ATE with a known exposure mapping: a function that identifies which units in a given randomization are not subject to interference. For example, an exposure mapping can specify that any unit with at least $h$-fraction of its neighbors having the same treatment status does not experience interference. However, this threshold $h$ is difficult to elicit from domain experts, and a misspecified threshold can induce bias. In this work, we propose a data-adaptive method to select the “$h$”-fraction threshold that minimizes the mean squared error of the Hortvitz-Thompson estimator. Our method estimates the bias and variance of the Horvitz-Thompson estimator under different thresholds using a linear dose-response model of the potential outcomes. We present simulations illustrating that our method improves upon non-adaptive choices of the threshold. We further illustrate the performance of our estimator by running experiments on a publicly-available Amazon product similarity graph. Furthermore, we demonstrate that our method is robust to deviations from the linear potential outcomes model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15887&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vydhourie Thiyageswaran, Tyler McCormick, Jennifer Brennan</name></author><category term="stat.ME" /><summary type="html">Randomized controlled trials often suffer from interference, a violation of the Stable Unit Treatment Values Assumption (SUTVA) in which a unit’s treatment assignment affects the outcomes of its neighbors. This interference causes bias in naive estimators of the average treatment effect (ATE). A popular method to achieve unbiasedness is to pair the Horvitz-Thompson estimator of the ATE with a known exposure mapping: a function that identifies which units in a given randomization are not subject to interference. For example, an exposure mapping can specify that any unit with at least $h$-fraction of its neighbors having the same treatment status does not experience interference. However, this threshold $h$ is difficult to elicit from domain experts, and a misspecified threshold can induce bias. In this work, we propose a data-adaptive method to select the “$h$”-fraction threshold that minimizes the mean squared error of the Hortvitz-Thompson estimator. Our method estimates the bias and variance of the Horvitz-Thompson estimator under different thresholds using a linear dose-response model of the potential outcomes. We present simulations illustrating that our method improves upon non-adaptive choices of the threshold. We further illustrate the performance of our estimator by running experiments on a publicly-available Amazon product similarity graph. Furthermore, we demonstrate that our method is robust to deviations from the linear potential outcomes model.</summary></entry><entry><title type="html">Debiased Distribution Compression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/DebiasedDistributionCompression.html" rel="alternate" type="text/html" title="Debiased Distribution Compression" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/DebiasedDistributionCompression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/DebiasedDistributionCompression.html">&lt;p&gt;Modern compression methods can summarize a target distribution $\mathbb{P}$ more succinctly than i.i.d. sampling but require access to a low-bias input sequence like a Markov chain converging quickly to $\mathbb{P}$. We introduce a new suite of compression methods suitable for compression with biased input sequences. Given $n$ points targeting the wrong distribution and quadratic time, Stein kernel thinning (SKT) returns $\sqrt{n}$ equal-weighted points with $\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\mathbb{P}$. For larger-scale compression tasks, low-rank SKT achieves the same feat in sub-quadratic time using an adaptive low-rank debiasing procedure that may be of independent interest. For downstream tasks that support simplex or constant-preserving weights, Stein recombination and Stein Cholesky achieve even greater parsimony, matching the guarantees of SKT with as few as $\text{poly-log}(n)$ weighted points. Underlying these advances are new guarantees for the quality of simplex-weighted coresets, the spectral decay of kernel matrices, and the covering numbers of Stein kernel Hilbert spaces. In our experiments, our techniques provide succinct and accurate posterior summaries while overcoming biases due to burn-in, approximate Markov chain Monte Carlo, and tempering.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.12290&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lingxiao Li, Raaz Dwivedi, Lester Mackey</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Modern compression methods can summarize a target distribution $\mathbb{P}$ more succinctly than i.i.d. sampling but require access to a low-bias input sequence like a Markov chain converging quickly to $\mathbb{P}$. We introduce a new suite of compression methods suitable for compression with biased input sequences. Given $n$ points targeting the wrong distribution and quadratic time, Stein kernel thinning (SKT) returns $\sqrt{n}$ equal-weighted points with $\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\mathbb{P}$. For larger-scale compression tasks, low-rank SKT achieves the same feat in sub-quadratic time using an adaptive low-rank debiasing procedure that may be of independent interest. For downstream tasks that support simplex or constant-preserving weights, Stein recombination and Stein Cholesky achieve even greater parsimony, matching the guarantees of SKT with as few as $\text{poly-log}(n)$ weighted points. Underlying these advances are new guarantees for the quality of simplex-weighted coresets, the spectral decay of kernel matrices, and the covering numbers of Stein kernel Hilbert spaces. In our experiments, our techniques provide succinct and accurate posterior summaries while overcoming biases due to burn-in, approximate Markov chain Monte Carlo, and tempering.</summary></entry><entry><title type="html">Decoding Social Sentiment in DAO: A Comparative Analysis of Blockchain Governance Communities</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/DecodingSocialSentimentinDAOAComparativeAnalysisofBlockchainGovernanceCommunities.html" rel="alternate" type="text/html" title="Decoding Social Sentiment in DAO: A Comparative Analysis of Blockchain Governance Communities" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/DecodingSocialSentimentinDAOAComparativeAnalysisofBlockchainGovernanceCommunities</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/DecodingSocialSentimentinDAOAComparativeAnalysisofBlockchainGovernanceCommunities.html">&lt;p&gt;Blockchain technology is leading a revolutionary transformation across diverse industries, with effective governance being critical for the success and sustainability of blockchain projects. Community forums, pivotal in engaging decentralized autonomous organizations (DAOs), significantly impact blockchain governance decisions. Concurrently, Natural Language Processing (NLP), particularly sentiment analysis, provides powerful insights from textual data. While prior research has explored the potential of NLP tools in social media sentiment analysis, there is a gap in understanding the sentiment landscape of blockchain governance communities. The evolving discourse and sentiment dynamics on the forums of top DAOs remain largely unknown. This paper delves deep into the evolving discourse and sentiment dynamics on the public forums of leading DeFi projects: Aave, Uniswap, Curve DAO, Yearn.finance, Merit Circle, and Balancer, focusing primarily on discussions related to governance issues. Our study shows that participants in decentralized communities generally express positive sentiments during Discord discussions. Furthermore, there is a potential interaction between discussion intensity and sentiment dynamics; higher discussion volume may contribute to a more stable sentiment from code analysis. The insights gained from this study are valuable for decision-makers in blockchain governance, underscoring the pivotal role of sentiment analysis in interpreting community emotions and its evolving impact on the landscape of blockchain governance. This research significantly contributes to the interdisciplinary exploration of the intersection of blockchain and society, specifically emphasizing the decentralized blockchain governance ecosystem. We provide our data and code for replicability as open access on GitHub.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.14676&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yutong Quan, Xintong Wu, Wanlin Deng, Luyao Zhang</name></author><category term="stat.AP" /><summary type="html">Blockchain technology is leading a revolutionary transformation across diverse industries, with effective governance being critical for the success and sustainability of blockchain projects. Community forums, pivotal in engaging decentralized autonomous organizations (DAOs), significantly impact blockchain governance decisions. Concurrently, Natural Language Processing (NLP), particularly sentiment analysis, provides powerful insights from textual data. While prior research has explored the potential of NLP tools in social media sentiment analysis, there is a gap in understanding the sentiment landscape of blockchain governance communities. The evolving discourse and sentiment dynamics on the forums of top DAOs remain largely unknown. This paper delves deep into the evolving discourse and sentiment dynamics on the public forums of leading DeFi projects: Aave, Uniswap, Curve DAO, Yearn.finance, Merit Circle, and Balancer, focusing primarily on discussions related to governance issues. Our study shows that participants in decentralized communities generally express positive sentiments during Discord discussions. Furthermore, there is a potential interaction between discussion intensity and sentiment dynamics; higher discussion volume may contribute to a more stable sentiment from code analysis. The insights gained from this study are valuable for decision-makers in blockchain governance, underscoring the pivotal role of sentiment analysis in interpreting community emotions and its evolving impact on the landscape of blockchain governance. This research significantly contributes to the interdisciplinary exploration of the intersection of blockchain and society, specifically emphasizing the decentralized blockchain governance ecosystem. We provide our data and code for replicability as open access on GitHub.</summary></entry><entry><title type="html">Diffusive Gibbs Sampling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/DiffusiveGibbsSampling.html" rel="alternate" type="text/html" title="Diffusive Gibbs Sampling" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/DiffusiveGibbsSampling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/DiffusiveGibbsSampling.html">&lt;p&gt;The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. A novel Metropolis-within-Gibbs scheme is proposed to enhance mixing in the denoising sampling step. DiGS exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering, attaining substantially improved performance across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.03008&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wenlin Chen, Mingtian Zhang, Brooks Paige, José Miguel Hernández-Lobato, David Barber</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. A novel Metropolis-within-Gibbs scheme is proposed to enhance mixing in the denoising sampling step. DiGS exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering, attaining substantially improved performance across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.</summary></entry><entry><title type="html">Dynamic survival analysis: modelling the hazard function via ordinary differential equations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Dynamicsurvivalanalysismodellingthehazardfunctionviaordinarydifferentialequations.html" rel="alternate" type="text/html" title="Dynamic survival analysis: modelling the hazard function via ordinary differential equations" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Dynamicsurvivalanalysismodellingthehazardfunctionviaordinarydifferentialequations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Dynamicsurvivalanalysismodellingthehazardfunctionviaordinarydifferentialequations.html">&lt;p&gt;The hazard function represents one of the main quantities of interest in the analysis of survival data. We propose a general approach for parametrically modelling the dynamics of the hazard function using systems of autonomous ordinary differential equations (ODEs). This modelling approach can be used to provide qualitative and quantitative analyses of the evolution of the hazard function over time. Our proposal capitalises on the extensive literature of ODEs which, in particular, allow for establishing basic rules or laws on the dynamics of the hazard function via the use of autonomous ODEs. We show how to implement the proposed modelling framework in cases where there is an analytic solution to the system of ODEs or where an ODE solver is required to obtain a numerical solution. We focus on the use of a Bayesian modelling approach, but the proposed methodology can also be coupled with maximum likelihood estimation. A simulation study is presented to illustrate the performance of these models and the interplay of sample size and censoring. Two case studies using real data are presented to illustrate the use of the proposed approach and to highlight the interpretability of the corresponding models. We conclude with a discussion on potential extensions of our work and strategies to include covariates into our framework. Although we focus on examples on Medical Statistics, the proposed framework is applicable in any context where the interest lies on estimating and interpreting the dynamics hazard function.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.05205&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>J. A. Christen, F. J. Rubio</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The hazard function represents one of the main quantities of interest in the analysis of survival data. We propose a general approach for parametrically modelling the dynamics of the hazard function using systems of autonomous ordinary differential equations (ODEs). This modelling approach can be used to provide qualitative and quantitative analyses of the evolution of the hazard function over time. Our proposal capitalises on the extensive literature of ODEs which, in particular, allow for establishing basic rules or laws on the dynamics of the hazard function via the use of autonomous ODEs. We show how to implement the proposed modelling framework in cases where there is an analytic solution to the system of ODEs or where an ODE solver is required to obtain a numerical solution. We focus on the use of a Bayesian modelling approach, but the proposed methodology can also be coupled with maximum likelihood estimation. A simulation study is presented to illustrate the performance of these models and the interplay of sample size and censoring. Two case studies using real data are presented to illustrate the use of the proposed approach and to highlight the interpretability of the corresponding models. We conclude with a discussion on potential extensions of our work and strategies to include covariates into our framework. Although we focus on examples on Medical Statistics, the proposed framework is applicable in any context where the interest lies on estimating and interpreting the dynamics hazard function.</summary></entry><entry><title type="html">Efficient Algorithms for the Sensitivities of the Pearson Correlation Coefficient and Its Statistical Significance to Online Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/EfficientAlgorithmsfortheSensitivitiesofthePearsonCorrelationCoefficientandItsStatisticalSignificancetoOnlineData.html" rel="alternate" type="text/html" title="Efficient Algorithms for the Sensitivities of the Pearson Correlation Coefficient and Its Statistical Significance to Online Data" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/EfficientAlgorithmsfortheSensitivitiesofthePearsonCorrelationCoefficientandItsStatisticalSignificancetoOnlineData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/EfficientAlgorithmsfortheSensitivitiesofthePearsonCorrelationCoefficientandItsStatisticalSignificancetoOnlineData.html">&lt;p&gt;Reliably measuring the collinearity of bivariate data is crucial in statistics, particularly for time-series analysis or ongoing studies in which incoming observations can significantly impact current collinearity estimates. Leveraging identities from Welford’s online algorithm for sample variance, we develop a rigorous theoretical framework for analyzing the maximal change to the Pearson correlation coefficient and its p-value that can be induced by additional data. Further, we show that the resulting optimization problems yield elegant closed-form solutions that can be accurately computed by linear- and constant-time algorithms. Our work not only creates new theoretical avenues for robust correlation measures, but also has broad practical implications for disciplines that span econometrics, operations research, clinical trials, climatology, differential privacy, and bioinformatics. Software implementations of our algorithms in Cython-wrapped C are made available at https://github.com/marc-harary/sensitivity for reproducibility, practical deployment, and future theoretical development.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14686&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marc Harary</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Reliably measuring the collinearity of bivariate data is crucial in statistics, particularly for time-series analysis or ongoing studies in which incoming observations can significantly impact current collinearity estimates. Leveraging identities from Welford’s online algorithm for sample variance, we develop a rigorous theoretical framework for analyzing the maximal change to the Pearson correlation coefficient and its p-value that can be induced by additional data. Further, we show that the resulting optimization problems yield elegant closed-form solutions that can be accurately computed by linear- and constant-time algorithms. Our work not only creates new theoretical avenues for robust correlation measures, but also has broad practical implications for disciplines that span econometrics, operations research, clinical trials, climatology, differential privacy, and bioinformatics. Software implementations of our algorithms in Cython-wrapped C are made available at https://github.com/marc-harary/sensitivity for reproducibility, practical deployment, and future theoretical development.</summary></entry><entry><title type="html">Efficient mid-term forecasting of hourly electricity load using generalized additive models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Efficientmidtermforecastingofhourlyelectricityloadusinggeneralizedadditivemodels.html" rel="alternate" type="text/html" title="Efficient mid-term forecasting of hourly electricity load using generalized additive models" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Efficientmidtermforecastingofhourlyelectricityloadusinggeneralizedadditivemodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Efficientmidtermforecastingofhourlyelectricityloadusinggeneralizedadditivemodels.html">&lt;p&gt;Accurate mid-term (weeks to one year) hourly electricity load forecasts are essential for strategic decision-making in power plant operation, ensuring supply security and grid stability, and energy trading. While numerous models effectively predict short-term (hours to a few days) hourly load, mid-term forecasting solutions remain scarce. In mid-term load forecasting, besides daily, weekly, and annual seasonal and autoregressive effects, capturing weather and holiday effects, as well as socio-economic non-stationarities in the data, poses significant modeling challenges. To address these challenges, we propose a novel forecasting method using Generalized Additive Models (GAMs) built from interpretable P-splines and enhanced with autoregressive post-processing. This model uses smoothed temperatures, Error-Trend-Seasonal (ETS) modeled non-stationary states, a nuanced representation of holiday effects with weekday variations, and seasonal information as input. The proposed model is evaluated on load data from 24 European countries. This analysis demonstrates that the model not only has significantly enhanced forecasting accuracy compared to state-of-the-art methods but also offers valuable insights into the influence of individual components on predicted load, given its full interpretability. Achieving performance akin to day-ahead TSO forecasts in fast computation times of a few seconds for several years of hourly data underscores the model’s potential for practical application in the power system industry.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17070&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Monika Zimmermann, Florian Ziel</name></author><category term="stat.AP" /><summary type="html">Accurate mid-term (weeks to one year) hourly electricity load forecasts are essential for strategic decision-making in power plant operation, ensuring supply security and grid stability, and energy trading. While numerous models effectively predict short-term (hours to a few days) hourly load, mid-term forecasting solutions remain scarce. In mid-term load forecasting, besides daily, weekly, and annual seasonal and autoregressive effects, capturing weather and holiday effects, as well as socio-economic non-stationarities in the data, poses significant modeling challenges. To address these challenges, we propose a novel forecasting method using Generalized Additive Models (GAMs) built from interpretable P-splines and enhanced with autoregressive post-processing. This model uses smoothed temperatures, Error-Trend-Seasonal (ETS) modeled non-stationary states, a nuanced representation of holiday effects with weekday variations, and seasonal information as input. The proposed model is evaluated on load data from 24 European countries. This analysis demonstrates that the model not only has significantly enhanced forecasting accuracy compared to state-of-the-art methods but also offers valuable insights into the influence of individual components on predicted load, given its full interpretability. Achieving performance akin to day-ahead TSO forecasts in fast computation times of a few seconds for several years of hourly data underscores the model’s potential for practical application in the power system industry.</summary></entry><entry><title type="html">Estimating Dyadic Treatment Effects with Unknown Confounders</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/EstimatingDyadicTreatmentEffectswithUnknownConfounders.html" rel="alternate" type="text/html" title="Estimating Dyadic Treatment Effects with Unknown Confounders" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/EstimatingDyadicTreatmentEffectswithUnknownConfounders</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/EstimatingDyadicTreatmentEffectswithUnknownConfounders.html">&lt;p&gt;This paper proposes a statistical inference method for assessing treatment effects with dyadic data. Under the assumption that the treatments follow an exchangeable distribution, our approach allows for the presence of any unobserved confounding factors that potentially cause endogeneity of treatment choice without requiring additional information other than the treatments and outcomes. Building on the literature of graphon estimation in network data analysis, we propose a neighborhood kernel smoothing method for estimating dyadic average treatment effects. We also develop a permutation inference method for testing the sharp null hypothesis. Under certain regularity conditions, we derive the rate of convergence of the proposed estimator and demonstrate the size control property of our test. We apply our method to international trade data to assess the impact of free trade agreements on bilateral trade flows.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16547&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tadao Hoshino, Takahide Yanagi</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">This paper proposes a statistical inference method for assessing treatment effects with dyadic data. Under the assumption that the treatments follow an exchangeable distribution, our approach allows for the presence of any unobserved confounding factors that potentially cause endogeneity of treatment choice without requiring additional information other than the treatments and outcomes. Building on the literature of graphon estimation in network data analysis, we propose a neighborhood kernel smoothing method for estimating dyadic average treatment effects. We also develop a permutation inference method for testing the sharp null hypothesis. Under certain regularity conditions, we derive the rate of convergence of the proposed estimator and demonstrate the size control property of our test. We apply our method to international trade data to assess the impact of free trade agreements on bilateral trade flows.</summary></entry><entry><title type="html">Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/EstimationofconditionalaveragetreatmenteffectsondistributeddataAprivacypreservingapproach.html" rel="alternate" type="text/html" title="Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/EstimationofconditionalaveragetreatmenteffectsondistributeddataAprivacypreservingapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/EstimationofconditionalaveragetreatmenteffectsondistributeddataAprivacypreservingapproach.html">&lt;p&gt;Estimation of conditional average treatment effects (CATEs) is an important topic in sciences. CATEs can be estimated with high accuracy if distributed data across multiple parties can be centralized. However, it is difficult to aggregate such data owing to privacy concerns. To address this issue, we proposed data collaboration double machine learning, a method that can estimate CATE models with privacy preservation of distributed data, and evaluated the method through simulations. Our contributions are summarized in the following three points. First, our method enables estimation and testing of semi-parametric CATE models without iterative communication on distributed data. Semi-parametric CATE models enable estimation and testing that is more robust to model mis-specification than parametric models. Second, our method enables collaborative estimation between multiple time points and different parties. Third, our method performed equally or better than other methods in simulations using synthetic, semi-synthetic and real-world datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.02672&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuji Kawamata, Ryoki Motai, Yukihiko Okada, Akira Imakura, Tetsuya Sakurai</name></author><category term="stat.ME" /><summary type="html">Estimation of conditional average treatment effects (CATEs) is an important topic in sciences. CATEs can be estimated with high accuracy if distributed data across multiple parties can be centralized. However, it is difficult to aggregate such data owing to privacy concerns. To address this issue, we proposed data collaboration double machine learning, a method that can estimate CATE models with privacy preservation of distributed data, and evaluated the method through simulations. Our contributions are summarized in the following three points. First, our method enables estimation and testing of semi-parametric CATE models without iterative communication on distributed data. Semi-parametric CATE models enable estimation and testing that is more robust to model mis-specification than parametric models. Second, our method enables collaborative estimation between multiple time points and different parties. Third, our method performed equally or better than other methods in simulations using synthetic, semi-synthetic and real-world datasets.</summary></entry><entry><title type="html">Estimators for multivariate allometric regression model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Estimatorsformultivariateallometricregressionmodel.html" rel="alternate" type="text/html" title="Estimators for multivariate allometric regression model" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Estimatorsformultivariateallometricregressionmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Estimatorsformultivariateallometricregressionmodel.html">&lt;p&gt;In a regression model with multiple response variables and multiple explanatory variables, if the difference of the mean vectors of the response variables for different values of explanatory variables is always in the direction of the first principal eigenvector of the covariance matrix of the response variables, then it is called a multivariate allometric regression model. This paper studies the estimation of the first principal eigenvector in the multivariate allometric regression model. A class of estimators that includes conventional estimators is proposed based on weighted sum-of-squares matrices of regression sum-of-squares matrix and residual sum-of-squares matrix. We establish an upper bound of the mean squared error of the estimators contained in this class, and the weight value minimizing the upper bound is derived. Sufficient conditions for the consistency of the estimators are discussed in weak identifiability regimes under which the difference of the largest and second largest eigenvalues of the covariance matrix decays asymptotically and in ``large $p$, large $n$” regimes, where $p$ is the number of response variables and $n$ is the sample size. Several numerical results are also presented.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.11219&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Koji Tsukuda, Shun Matsuura</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In a regression model with multiple response variables and multiple explanatory variables, if the difference of the mean vectors of the response variables for different values of explanatory variables is always in the direction of the first principal eigenvector of the covariance matrix of the response variables, then it is called a multivariate allometric regression model. This paper studies the estimation of the first principal eigenvector in the multivariate allometric regression model. A class of estimators that includes conventional estimators is proposed based on weighted sum-of-squares matrices of regression sum-of-squares matrix and residual sum-of-squares matrix. We establish an upper bound of the mean squared error of the estimators contained in this class, and the weight value minimizing the upper bound is derived. Sufficient conditions for the consistency of the estimators are discussed in weak identifiability regimes under which the difference of the largest and second largest eigenvalues of the covariance matrix decays asymptotically and in ``large $p$, large $n$” regimes, where $p$ is the number of response variables and $n$ is the sample size. Several numerical results are also presented.</summary></entry><entry><title type="html">Exact phylodynamic likelihood via structured Markov genealogy processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ExactphylodynamiclikelihoodviastructuredMarkovgenealogyprocesses.html" rel="alternate" type="text/html" title="Exact phylodynamic likelihood via structured Markov genealogy processes" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ExactphylodynamiclikelihoodviastructuredMarkovgenealogyprocesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ExactphylodynamiclikelihoodviastructuredMarkovgenealogyprocesses.html">&lt;p&gt;We consider genealogies arising from a Markov population process in which individuals are categorized into a discrete collection of compartments, with the requirement that individuals within the same compartment are statistically exchangeable. When equipped with a sampling process, each such population process induces a time-evolving tree-valued process defined as the genealogy of all sampled individuals. We provide a construction of this genealogy process and derive exact expressions for the likelihood of an observed genealogy in terms of filter equations. These filter equations can be numerically solved using standard Monte Carlo integration methods. Thus, we obtain statistically efficient likelihood-based inference for essentially arbitrary compartment models based on an observed genealogy of individuals sampled from the population.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17032&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aaron A. King, Qianying Lin, Edward L. Ionides</name></author><category term="stat.AP" /><summary type="html">We consider genealogies arising from a Markov population process in which individuals are categorized into a discrete collection of compartments, with the requirement that individuals within the same compartment are statistically exchangeable. When equipped with a sampling process, each such population process induces a time-evolving tree-valued process defined as the genealogy of all sampled individuals. We provide a construction of this genealogy process and derive exact expressions for the likelihood of an observed genealogy in terms of filter equations. These filter equations can be numerically solved using standard Monte Carlo integration methods. Thus, we obtain statistically efficient likelihood-based inference for essentially arbitrary compartment models based on an observed genealogy of individuals sampled from the population.</summary></entry><entry><title type="html">Extremal correlation coefficient for functional data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Extremalcorrelationcoefficientforfunctionaldata.html" rel="alternate" type="text/html" title="Extremal correlation coefficient for functional data" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Extremalcorrelationcoefficientforfunctionaldata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Extremalcorrelationcoefficientforfunctionaldata.html">&lt;p&gt;We propose a coefficient that measures dependence in paired samples of functions. It has properties similar to the Pearson correlation, but differs in significant ways: 1) it is designed to measure dependence between curves, 2) it focuses only on extreme curves. The new coefficient is derived within the framework of regular variation in Banach spaces. A consistent estimator is proposed and justified by an asymptotic analysis and a simulation study. The usefulness of the new coefficient is illustrated on financial and and climate functional data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17318&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mihyun Kim, Piotr Kokoszka</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We propose a coefficient that measures dependence in paired samples of functions. It has properties similar to the Pearson correlation, but differs in significant ways: 1) it is designed to measure dependence between curves, 2) it focuses only on extreme curves. The new coefficient is derived within the framework of regular variation in Banach spaces. A consistent estimator is proposed and justified by an asymptotic analysis and a simulation study. The usefulness of the new coefficient is illustrated on financial and and climate functional data.</summary></entry><entry><title type="html">False Discovery Rate Control for Confounder Selection Using Mirror Statistics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FalseDiscoveryRateControlforConfounderSelectionUsingMirrorStatistics.html" rel="alternate" type="text/html" title="False Discovery Rate Control for Confounder Selection Using Mirror Statistics" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FalseDiscoveryRateControlforConfounderSelectionUsingMirrorStatistics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FalseDiscoveryRateControlforConfounderSelectionUsingMirrorStatistics.html">&lt;p&gt;While data-driven confounder selection requires careful consideration, it is frequently employed in observational studies. Widely recognized criteria for confounder selection include the minimal-set approach, which involves selecting variables relevant to both treatment and outcome, and the union-set approach, which involves selecting variables associated with either treatment or outcome. These approaches are often implemented using heuristics and off-the-shelf statistical methods, where the degree of uncertainty may not be clear. In this paper, we focus on the false discovery rate (FDR) to measure uncertainty in confounder selection. We define the FDR specific to confounder selection and propose methods based on the mirror statistic, a recently developed approach for FDR control that does not rely on p-values. The proposed methods are p-value-free and require only the assumption of some symmetry in the distribution of the mirror statistic. It can be combined with sparse estimation and other methods that involve difficulties in deriving p-values. The properties of the proposed methods are investigated through exhaustive numerical experiments. Particularly in high-dimensional data scenarios, the proposed methods effectively control FDR and perform better than the p-value-based methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.18904&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kazuharu Harada, Masataka Taguri</name></author><category term="stat.ME" /><summary type="html">While data-driven confounder selection requires careful consideration, it is frequently employed in observational studies. Widely recognized criteria for confounder selection include the minimal-set approach, which involves selecting variables relevant to both treatment and outcome, and the union-set approach, which involves selecting variables associated with either treatment or outcome. These approaches are often implemented using heuristics and off-the-shelf statistical methods, where the degree of uncertainty may not be clear. In this paper, we focus on the false discovery rate (FDR) to measure uncertainty in confounder selection. We define the FDR specific to confounder selection and propose methods based on the mirror statistic, a recently developed approach for FDR control that does not rely on p-values. The proposed methods are p-value-free and require only the assumption of some symmetry in the distribution of the mirror statistic. It can be combined with sparse estimation and other methods that involve difficulties in deriving p-values. The properties of the proposed methods are investigated through exhaustive numerical experiments. Particularly in high-dimensional data scenarios, the proposed methods effectively control FDR and perform better than the p-value-based methods.</summary></entry><entry><title type="html">Fast Emulation and Modular Calibration for Simulators with Functional Response</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FastEmulationandModularCalibrationforSimulatorswithFunctionalResponse.html" rel="alternate" type="text/html" title="Fast Emulation and Modular Calibration for Simulators with Functional Response" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FastEmulationandModularCalibrationforSimulatorswithFunctionalResponse</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FastEmulationandModularCalibrationforSimulatorswithFunctionalResponse.html">&lt;p&gt;Scalable surrogate models enable efficient emulation of computer models (or simulators), particularly when dealing with large ensembles of runs. While Gaussian Process (GP) models are commonly employed for emulation, they face limitations in scaling to truly large datasets. Furthermore, when dealing with dense functional output, such as spatial or time-series data, additional complexities arise, requiring careful handling to ensure fast emulation. This work presents a highly scalable emulator for functional data, building upon the works of Kennedy and O’Hagan (2001) and Higdon et al. (2008), while incorporating the local approximate Gaussian Process framework proposed by Gramacy and Apley (2015). The emulator utilizes global GP lengthscale parameter estimates to scale the input space, leading to a substantial improvement in prediction speed. We demonstrate that our fast approximation-based emulator can serve as a viable alternative to the methods outlined in Higdon et al. (2008) for functional response, while drastically reducing computational costs. The proposed emulator is applied to quickly calibrate the multiphysics continuum hydrodynamics simulator FLAG with a large ensemble of 20000 runs. The methods presented are implemented in the R package FlaGP.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16298&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Grant Hutchings, Derek Bingham, Earl Lawrence</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Scalable surrogate models enable efficient emulation of computer models (or simulators), particularly when dealing with large ensembles of runs. While Gaussian Process (GP) models are commonly employed for emulation, they face limitations in scaling to truly large datasets. Furthermore, when dealing with dense functional output, such as spatial or time-series data, additional complexities arise, requiring careful handling to ensure fast emulation. This work presents a highly scalable emulator for functional data, building upon the works of Kennedy and O’Hagan (2001) and Higdon et al. (2008), while incorporating the local approximate Gaussian Process framework proposed by Gramacy and Apley (2015). The emulator utilizes global GP lengthscale parameter estimates to scale the input space, leading to a substantial improvement in prediction speed. We demonstrate that our fast approximation-based emulator can serve as a viable alternative to the methods outlined in Higdon et al. (2008) for functional response, while drastically reducing computational costs. The proposed emulator is applied to quickly calibrate the multiphysics continuum hydrodynamics simulator FLAG with a large ensemble of 20000 runs. The methods presented are implemented in the R package FlaGP.</summary></entry><entry><title type="html">Fast Power Curve Approximation for Posterior Analyses</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FastPowerCurveApproximationforPosteriorAnalyses.html" rel="alternate" type="text/html" title="Fast Power Curve Approximation for Posterior Analyses" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FastPowerCurveApproximationforPosteriorAnalyses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FastPowerCurveApproximationforPosteriorAnalyses.html">&lt;p&gt;Bayesian hypothesis tests leverage posterior probabilities, Bayes factors, or credible intervals to inform data-driven decision making. We propose a framework for power curve approximation with such hypothesis tests. We present a fast approach to explore the approximate sampling distribution of posterior probabilities when the conditions for the Bernstein-von Mises theorem are satisfied. We extend that approach to consider segments of such sampling distributions in a targeted manner for each sample size explored. These sampling distribution segments are used to construct power curves for various types of posterior analyses. Our resulting method for power curve approximation is orders of magnitude faster than conventional power curve estimation for Bayesian hypothesis tests. We also prove the consistency of the corresponding power estimates and sample size recommendations under certain conditions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.12427&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luke Hagar, Nathaniel T. Stevens</name></author><category term="stat.ME" /><summary type="html">Bayesian hypothesis tests leverage posterior probabilities, Bayes factors, or credible intervals to inform data-driven decision making. We propose a framework for power curve approximation with such hypothesis tests. We present a fast approach to explore the approximate sampling distribution of posterior probabilities when the conditions for the Bernstein-von Mises theorem are satisfied. We extend that approach to consider segments of such sampling distributions in a targeted manner for each sample size explored. These sampling distribution segments are used to construct power curves for various types of posterior analyses. Our resulting method for power curve approximation is orders of magnitude faster than conventional power curve estimation for Bayesian hypothesis tests. We also prove the consistency of the corresponding power estimates and sample size recommendations under certain conditions.</summary></entry><entry><title type="html">Federated Learning for Non-factorizable Models using Deep Generative Prior Approximations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FederatedLearningforNonfactorizableModelsusingDeepGenerativePriorApproximations.html" rel="alternate" type="text/html" title="Federated Learning for Non-factorizable Models using Deep Generative Prior Approximations" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FederatedLearningforNonfactorizableModelsusingDeepGenerativePriorApproximations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FederatedLearningforNonfactorizableModelsusingDeepGenerativePriorApproximations.html">&lt;p&gt;Federated learning (FL) allows for collaborative model training across decentralized clients while preserving privacy by avoiding data sharing. However, current FL methods assume conditional independence between client models, limiting the use of priors that capture dependence, such as Gaussian processes (GPs). We introduce the Structured Independence via deep Generative Model Approximation (SIGMA) prior which enables FL for non-factorizable models across clients, expanding the applicability of FL to fields such as spatial statistics, epidemiology, environmental science, and other domains where modeling dependencies is crucial. The SIGMA prior is a pre-trained deep generative model that approximates the desired prior and induces a specified conditional independence structure in the latent variables, creating an approximate model suitable for FL settings. We demonstrate the SIGMA prior’s effectiveness on synthetic data and showcase its utility in a real-world example of FL for spatial data, using a conditional autoregressive prior to model spatial dependence across Australia. Our work enables new FL applications in domains where modeling dependent data is essential for accurate predictions and decision-making.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16055&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Conor Hassan, Joshua J Bon, Elizaveta Semenova, Antonietta Mira, Kerrie Mengersen</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Federated learning (FL) allows for collaborative model training across decentralized clients while preserving privacy by avoiding data sharing. However, current FL methods assume conditional independence between client models, limiting the use of priors that capture dependence, such as Gaussian processes (GPs). We introduce the Structured Independence via deep Generative Model Approximation (SIGMA) prior which enables FL for non-factorizable models across clients, expanding the applicability of FL to fields such as spatial statistics, epidemiology, environmental science, and other domains where modeling dependencies is crucial. The SIGMA prior is a pre-trained deep generative model that approximates the desired prior and induces a specified conditional independence structure in the latent variables, creating an approximate model suitable for FL settings. We demonstrate the SIGMA prior’s effectiveness on synthetic data and showcase its utility in a real-world example of FL for spatial data, using a conditional autoregressive prior to model spatial dependence across Australia. Our work enables new FL applications in domains where modeling dependent data is essential for accurate predictions and decision-making.</summary></entry><entry><title type="html">Flexible Modeling of Demographic Transition Processes with a Bayesian Hierarchical B-splines Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FlexibleModelingofDemographicTransitionProcesseswithaBayesianHierarchicalBsplinesModel.html" rel="alternate" type="text/html" title="Flexible Modeling of Demographic Transition Processes with a Bayesian Hierarchical B-splines Model" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FlexibleModelingofDemographicTransitionProcesseswithaBayesianHierarchicalBsplinesModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/FlexibleModelingofDemographicTransitionProcesseswithaBayesianHierarchicalBsplinesModel.html">&lt;p&gt;Several demographic and health indicators, including the total fertility rate (TFR) and modern contraceptive use rate (mCPR), evolve similarly over time, characterized by a transition between stable states. Existing approaches for estimation or projection of transitions in multiple populations have successfully used parametric functions to capture the relation between the rate of change of an indicator and its level. However, incorrect parametric forms may result in bias or incorrect coverage in long-term projections. We propose a new class of models to capture demographic transitions in multiple populations. Our proposal, the B-spline Transition Model (BTM), models the relationship between the rate of change of an indicator and its level using B-splines, allowing for data-adaptive estimation of transition functions. Bayesian hierarchical models are used to share information on the transition function between populations. We apply the BTM to estimate and project country-level TFR and mCPR and compare the results against those from extant parametric models. For TFR, BTM projections have generally lower error than the comparison model. For mCPR, while results are comparable between BTM and a parametric approach, the B-spline model generally improves out-of-sample predictions. The case studies suggest that the BTM may be considered for demographic applications&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.09694&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Herbert Susmann, Leontine Alkema</name></author><category term="stat.ME" /><summary type="html">Several demographic and health indicators, including the total fertility rate (TFR) and modern contraceptive use rate (mCPR), evolve similarly over time, characterized by a transition between stable states. Existing approaches for estimation or projection of transitions in multiple populations have successfully used parametric functions to capture the relation between the rate of change of an indicator and its level. However, incorrect parametric forms may result in bias or incorrect coverage in long-term projections. We propose a new class of models to capture demographic transitions in multiple populations. Our proposal, the B-spline Transition Model (BTM), models the relationship between the rate of change of an indicator and its level using B-splines, allowing for data-adaptive estimation of transition functions. Bayesian hierarchical models are used to share information on the transition function between populations. We apply the BTM to estimate and project country-level TFR and mCPR and compare the results against those from extant parametric models. For TFR, BTM projections have generally lower error than the comparison model. For mCPR, while results are comparable between BTM and a parametric approach, the B-spline model generally improves out-of-sample predictions. The case studies suggest that the BTM may be considered for demographic applications</summary></entry><entry><title type="html">Gaussian Mixture Model with Rare Events</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/GaussianMixtureModelwithRareEvents.html" rel="alternate" type="text/html" title="Gaussian Mixture Model with Rare Events" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/GaussianMixtureModelwithRareEvents</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/GaussianMixtureModelwithRareEvents.html">&lt;p&gt;We study here a Gaussian Mixture Model (GMM) with rare events data. In this case, the commonly used Expectation-Maximization (EM) algorithm exhibits extremely slow numerical convergence rate. To theoretically understand this phenomenon, we formulate the numerical convergence problem of the EM algorithm with rare events data as a problem about a contraction operator. Theoretical analysis reveals that the spectral radius of the contraction operator in this case could be arbitrarily close to 1 asymptotically. This theoretical finding explains the empirical slow numerical convergence of the EM algorithm with rare events data. To overcome this challenge, a Mixed EM (MEM) algorithm is developed, which utilizes the information provided by partially labeled data. As compared with the standard EM algorithm, the key feature of the MEM algorithm is that it requires additionally labeled data. We find that MEM algorithm significantly improves the numerical convergence rate as compared with the standard EM algorithm. The finite sample performance of the proposed method is illustrated by both simulation studies and a real-world dataset of Swedish traffic signs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16859&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xuetong Li, Jing Zhou, Hansheng Wang</name></author><category term="stat.ME" /><summary type="html">We study here a Gaussian Mixture Model (GMM) with rare events data. In this case, the commonly used Expectation-Maximization (EM) algorithm exhibits extremely slow numerical convergence rate. To theoretically understand this phenomenon, we formulate the numerical convergence problem of the EM algorithm with rare events data as a problem about a contraction operator. Theoretical analysis reveals that the spectral radius of the contraction operator in this case could be arbitrarily close to 1 asymptotically. This theoretical finding explains the empirical slow numerical convergence of the EM algorithm with rare events data. To overcome this challenge, a Mixed EM (MEM) algorithm is developed, which utilizes the information provided by partially labeled data. As compared with the standard EM algorithm, the key feature of the MEM algorithm is that it requires additionally labeled data. We find that MEM algorithm significantly improves the numerical convergence rate as compared with the standard EM algorithm. The finite sample performance of the proposed method is illustrated by both simulation studies and a real-world dataset of Swedish traffic signs.</summary></entry><entry><title type="html">Hidden Markov modelling of spatio-temporal dynamics of measles in 1750-1850 Finland</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/HiddenMarkovmodellingofspatiotemporaldynamicsofmeaslesin17501850Finland.html" rel="alternate" type="text/html" title="Hidden Markov modelling of spatio-temporal dynamics of measles in 1750-1850 Finland" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/HiddenMarkovmodellingofspatiotemporaldynamicsofmeaslesin17501850Finland</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/HiddenMarkovmodellingofspatiotemporaldynamicsofmeaslesin17501850Finland.html">&lt;p&gt;Real world spatio-temporal datasets, and phenomena related to them, are often challenging to visualise or gain a general overview of. In order to summarise information encompassed in such data, we combine two well known statistical modelling methods. To account for the spatial dimension, we use the intrinsic modification of the conditional autoregression, and incorporate it with the hidden Markov model, allowing the spatial patterns to vary over time. We apply our method into parish register data considering deaths caused by measles in Finland in 1750-1850, and gain novel insight of previously undiscovered infection dynamics. Five distinctive, reoccurring states describing spatially and temporally differing infection burden and potential routes of spread are identified. We also find that there is a change in the occurrences of the most typical spatial patterns circa 1812, possibly due to changes in communication routes after major administrative transformations in Finland.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16885&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tiia-Maria Pasanen, Jouni Helske, Tarmo Ketola</name></author><category term="stat.ME" /><summary type="html">Real world spatio-temporal datasets, and phenomena related to them, are often challenging to visualise or gain a general overview of. In order to summarise information encompassed in such data, we combine two well known statistical modelling methods. To account for the spatial dimension, we use the intrinsic modification of the conditional autoregression, and incorporate it with the hidden Markov model, allowing the spatial patterns to vary over time. We apply our method into parish register data considering deaths caused by measles in Finland in 1750-1850, and gain novel insight of previously undiscovered infection dynamics. Five distinctive, reoccurring states describing spatially and temporally differing infection burden and potential routes of spread are identified. We also find that there is a change in the occurrences of the most typical spatial patterns circa 1812, possibly due to changes in communication routes after major administrative transformations in Finland.</summary></entry><entry><title type="html">Hierarchical Bayesian inference for community detection and connectivity of functional brain networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/HierarchicalBayesianinferenceforcommunitydetectionandconnectivityoffunctionalbrainnetworks.html" rel="alternate" type="text/html" title="Hierarchical Bayesian inference for community detection and connectivity of functional brain networks" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/HierarchicalBayesianinferenceforcommunitydetectionandconnectivityoffunctionalbrainnetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/HierarchicalBayesianinferenceforcommunitydetectionandconnectivityoffunctionalbrainnetworks.html">&lt;p&gt;Many functional magnetic resonance imaging (fMRI) studies rely on estimates of hierarchically organised brain networks whose segregation and integration reflect the dynamic transitions of latent cognitive states. However, most existing methods for estimating the community structure of networks from both individual and group-level analysis neglect the variability between subjects and lack validation. In this paper, we develop a new multilayer community detection method based on Bayesian latent block modelling. The method can robustly detect the group-level community structure of weighted functional networks that give rise to hidden brain states with an unknown number of communities and retain the variability of individual networks. For validation, we propose a new community structure-based multivariate Gaussian generative model to simulate synthetic signal. Our result shows that the inferred community memberships using hierarchical Bayesian analysis are consistent with the predefined node labels in the generative model. The method is also tested using real working memory task-fMRI data of 100 unrelated healthy subjects from the Human Connectome Project. The results show distinctive community structure patterns between 2-back, 0-back, and fixation conditions, which may reflect cognitive and behavioural states under working memory task conditions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.07386&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lingbin Bian, Nizhuan Wang, Leonardo Novelli, Jonathan Keith, Adeel Razi</name></author><category term="stat.AP" /><summary type="html">Many functional magnetic resonance imaging (fMRI) studies rely on estimates of hierarchically organised brain networks whose segregation and integration reflect the dynamic transitions of latent cognitive states. However, most existing methods for estimating the community structure of networks from both individual and group-level analysis neglect the variability between subjects and lack validation. In this paper, we develop a new multilayer community detection method based on Bayesian latent block modelling. The method can robustly detect the group-level community structure of weighted functional networks that give rise to hidden brain states with an unknown number of communities and retain the variability of individual networks. For validation, we propose a new community structure-based multivariate Gaussian generative model to simulate synthetic signal. Our result shows that the inferred community memberships using hierarchical Bayesian analysis are consistent with the predefined node labels in the generative model. The method is also tested using real working memory task-fMRI data of 100 unrelated healthy subjects from the Human Connectome Project. The results show distinctive community structure patterns between 2-back, 0-back, and fixation conditions, which may reflect cognitive and behavioural states under working memory task conditions.</summary></entry><entry><title type="html">Human-in-the-loop: Towards Label Embeddings for Measuring Classification Difficulty</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/HumanintheloopTowardsLabelEmbeddingsforMeasuringClassificationDifficulty.html" rel="alternate" type="text/html" title="Human-in-the-loop: Towards Label Embeddings for Measuring Classification Difficulty" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/HumanintheloopTowardsLabelEmbeddingsforMeasuringClassificationDifficulty</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/HumanintheloopTowardsLabelEmbeddingsforMeasuringClassificationDifficulty.html">&lt;p&gt;Uncertainty in machine learning models is a timely and vast field of research. In supervised learning, uncertainty can already occur in the first stage of the training process, the annotation phase. This scenario is particularly evident when some instances cannot be definitively classified. In other words, there is inevitable ambiguity in the annotation step and hence, not necessarily a “ground truth” associated with each instance. The main idea of this work is to drop the assumption of a ground truth label and instead embed the annotations into a multidimensional space. This embedding is derived from the empirical distribution of annotations in a Bayesian setup, modeled via a Dirichlet-Multinomial framework. We estimate the model parameters and posteriors using a stochastic Expectation Maximization algorithm with Markov Chain Monte Carlo steps. The methods developed in this paper readily extend to various situations where multiple annotators independently label instances. To showcase the generality of the proposed approach, we apply our approach to three benchmark datasets for image classification and Natural Language Inference. Besides the embeddings, we can investigate the resulting correlation matrices, which reflect the semantic similarities of the original classes very well for all three exemplary datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.08874&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Katharina Hechinger, Christoph Koller, Xiao Xiang Zhu, Göran Kauermann</name></author><category term="stat.AP" /><summary type="html">Uncertainty in machine learning models is a timely and vast field of research. In supervised learning, uncertainty can already occur in the first stage of the training process, the annotation phase. This scenario is particularly evident when some instances cannot be definitively classified. In other words, there is inevitable ambiguity in the annotation step and hence, not necessarily a “ground truth” associated with each instance. The main idea of this work is to drop the assumption of a ground truth label and instead embed the annotations into a multidimensional space. This embedding is derived from the empirical distribution of annotations in a Bayesian setup, modeled via a Dirichlet-Multinomial framework. We estimate the model parameters and posteriors using a stochastic Expectation Maximization algorithm with Markov Chain Monte Carlo steps. The methods developed in this paper readily extend to various situations where multiple annotators independently label instances. To showcase the generality of the proposed approach, we apply our approach to three benchmark datasets for image classification and Natural Language Inference. Besides the embeddings, we can investigate the resulting correlation matrices, which reflect the semantic similarities of the original classes very well for all three exemplary datasets.</summary></entry><entry><title type="html">Impacts of Innovation School System in Korea: A Latent Space Item Response Model with Neyman-Scott Point Process</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ImpactsofInnovationSchoolSysteminKoreaALatentSpaceItemResponseModelwithNeymanScottPointProcess.html" rel="alternate" type="text/html" title="Impacts of Innovation School System in Korea: A Latent Space Item Response Model with Neyman-Scott Point Process" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ImpactsofInnovationSchoolSysteminKoreaALatentSpaceItemResponseModelwithNeymanScottPointProcess</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ImpactsofInnovationSchoolSysteminKoreaALatentSpaceItemResponseModelwithNeymanScottPointProcess.html">&lt;p&gt;South Korea’s educational system has faced criticism for its lack of focus on critical thinking and creativity, resulting in high levels of stress and anxiety among students. As part of the government’s effort to improve the educational system, the innovation school system was introduced in 2009, which aims to develop students’ creativity as well as their non-cognitive skills. To better understand the differences between innovation and regular school systems in South Korea, we propose a novel method that combines the latent space item response model (LSIRM) with the Neyman-Scott (NS) point process model. Our method accounts for the heterogeneity of items and students, captures relationships between respondents and items, and identifies item and student clusters that can provide a comprehensive understanding of students’ behaviors/perceptions on non-cognitive outcomes. Our analysis reveals that students in the innovation school system show a higher sense of citizenship, while those in the regular school system tend to associate confidence in appearance with social ability. We compare our model with exploratory item factor analysis in terms of item clustering and find that our approach provides a more detailed and automated analysis. A comparison with exploratory item factor analysis highlights our method’s advantages in terms of uncertainty quantification of the clustering process and more detailed and nuanced clustering results. Our method is made available to an existing R package, lsirm12pl.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.02106&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Seorim Yi, Minkyu Kim, Jaewoo Park, Minjeong Jeon, Ick Hoon Jin</name></author><category term="stat.AP" /><summary type="html">South Korea’s educational system has faced criticism for its lack of focus on critical thinking and creativity, resulting in high levels of stress and anxiety among students. As part of the government’s effort to improve the educational system, the innovation school system was introduced in 2009, which aims to develop students’ creativity as well as their non-cognitive skills. To better understand the differences between innovation and regular school systems in South Korea, we propose a novel method that combines the latent space item response model (LSIRM) with the Neyman-Scott (NS) point process model. Our method accounts for the heterogeneity of items and students, captures relationships between respondents and items, and identifies item and student clusters that can provide a comprehensive understanding of students’ behaviors/perceptions on non-cognitive outcomes. Our analysis reveals that students in the innovation school system show a higher sense of citizenship, while those in the regular school system tend to associate confidence in appearance with social ability. We compare our model with exploratory item factor analysis in terms of item clustering and find that our approach provides a more detailed and automated analysis. A comparison with exploratory item factor analysis highlights our method’s advantages in terms of uncertainty quantification of the clustering process and more detailed and nuanced clustering results. Our method is made available to an existing R package, lsirm12pl.</summary></entry><entry><title type="html">IncomeSCM: From tabular data set to time-series simulator and causal estimation benchmark</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/IncomeSCMFromtabulardatasettotimeseriessimulatorandcausalestimationbenchmark.html" rel="alternate" type="text/html" title="IncomeSCM: From tabular data set to time-series simulator and causal estimation benchmark" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/IncomeSCMFromtabulardatasettotimeseriessimulatorandcausalestimationbenchmark</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/IncomeSCMFromtabulardatasettotimeseriessimulatorandcausalestimationbenchmark.html">&lt;p&gt;Evaluating observational estimators of causal effects demands information that is rarely available: unconfounded interventions and outcomes from the population of interest, created either by randomization or adjustment. As a result, it is customary to fall back on simulators when creating benchmark tasks. Simulators offer great control but are often too simplistic to make challenging tasks, either because they are hand-designed and lack the nuances of real-world data, or because they are fit to observational data without structural constraints. In this work, we propose a general, repeatable strategy for turning observational data into sequential structural causal models and challenging estimation tasks by following two simple principles: 1) fitting real-world data where possible, and 2) creating complexity by composing simple, hand-designed mechanisms. We implement these ideas in a highly configurable software package and apply it to the well-known Adult income data set to construct the IncomeSCM simulator. From this, we devise multiple estimation tasks and sample data sets to compare established estimators of causal effects. The tasks present a suitable challenge, with effect estimates varying greatly in quality between methods, despite similar performance in the modeling of factual outcomes, highlighting the need for dedicated causal estimators and model selection criteria.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16069&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fredrik D. Johansson</name></author><category term="stat.ME" /><summary type="html">Evaluating observational estimators of causal effects demands information that is rarely available: unconfounded interventions and outcomes from the population of interest, created either by randomization or adjustment. As a result, it is customary to fall back on simulators when creating benchmark tasks. Simulators offer great control but are often too simplistic to make challenging tasks, either because they are hand-designed and lack the nuances of real-world data, or because they are fit to observational data without structural constraints. In this work, we propose a general, repeatable strategy for turning observational data into sequential structural causal models and challenging estimation tasks by following two simple principles: 1) fitting real-world data where possible, and 2) creating complexity by composing simple, hand-designed mechanisms. We implement these ideas in a highly configurable software package and apply it to the well-known Adult income data set to construct the IncomeSCM simulator. From this, we devise multiple estimation tasks and sample data sets to compare established estimators of causal effects. The tasks present a suitable challenge, with effect estimates varying greatly in quality between methods, despite similar performance in the modeling of factual outcomes, highlighting the need for dedicated causal estimators and model selection criteria.</summary></entry><entry><title type="html">Inference for Cumulative Incidences and Treatment Effects in Randomized Controlled Trials with Time-to-Event Outcomes under ICH E9 (R1)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/InferenceforCumulativeIncidencesandTreatmentEffectsinRandomizedControlledTrialswithTimetoEventOutcomesunderICHE9R1.html" rel="alternate" type="text/html" title="Inference for Cumulative Incidences and Treatment Effects in Randomized Controlled Trials with Time-to-Event Outcomes under ICH E9 (R1)" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/InferenceforCumulativeIncidencesandTreatmentEffectsinRandomizedControlledTrialswithTimetoEventOutcomesunderICHE9R1</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/InferenceforCumulativeIncidencesandTreatmentEffectsinRandomizedControlledTrialswithTimetoEventOutcomesunderICHE9R1.html">&lt;p&gt;In randomized controlled trials (RCT) with time-to-event outcomes, intercurrent events occur as semi-competing/competing events, and they could affect the hazard of outcomes or render outcomes ill-defined. Although five strategies have been proposed in ICH E9 (R1) addendum to address intercurrent events in RCT, they did not readily extend to the context of time-to-event data for studying causal effects. In this study, we show how to define, estimate, and infer the time-dependent cumulative incidence of outcome events in such contexts for obtaining causal interpretations. Specifically, we derive the mathematical forms of the scientific objective (i.e., causal estimands) under the five strategies and clarify the required data structure to identify these causal estimands. Furthermore, we summarize estimation and inference methods for these causal estimands by adopting methodologies in survival analysis, including analytic formulas for asymptotic analysis and hypothesis testing. We illustrate our methods with the LEADER Trial on investigating the effect of liraglutide on cardiovascular outcomes. Studies of multiple endpoints and combining strategies to address multiple intercurrent events can help practitioners understand treatment effects more comprehensively.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.14684&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuhao Deng, Shasha Han, Xiao-Hua Zhou</name></author><category term="stat.ME" /><summary type="html">In randomized controlled trials (RCT) with time-to-event outcomes, intercurrent events occur as semi-competing/competing events, and they could affect the hazard of outcomes or render outcomes ill-defined. Although five strategies have been proposed in ICH E9 (R1) addendum to address intercurrent events in RCT, they did not readily extend to the context of time-to-event data for studying causal effects. In this study, we show how to define, estimate, and infer the time-dependent cumulative incidence of outcome events in such contexts for obtaining causal interpretations. Specifically, we derive the mathematical forms of the scientific objective (i.e., causal estimands) under the five strategies and clarify the required data structure to identify these causal estimands. Furthermore, we summarize estimation and inference methods for these causal estimands by adopting methodologies in survival analysis, including analytic formulas for asymptotic analysis and hypothesis testing. We illustrate our methods with the LEADER Trial on investigating the effect of liraglutide on cardiovascular outcomes. Studies of multiple endpoints and combining strategies to address multiple intercurrent events can help practitioners understand treatment effects more comprehensively.</summary></entry><entry><title type="html">Inference for Optimal Linear Treatment Regimes in Personalized Decision-making</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/InferenceforOptimalLinearTreatmentRegimesinPersonalizedDecisionmaking.html" rel="alternate" type="text/html" title="Inference for Optimal Linear Treatment Regimes in Personalized Decision-making" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/InferenceforOptimalLinearTreatmentRegimesinPersonalizedDecisionmaking</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/InferenceforOptimalLinearTreatmentRegimesinPersonalizedDecisionmaking.html">&lt;p&gt;Personalized decision-making, tailored to individual characteristics, is gaining significant attention. The optimal treatment regime aims to provide the best-expected outcome in the entire population, known as the value function. One approach to determine this optimal regime is by maximizing the Augmented Inverse Probability Weighting (AIPW) estimator of the value function. However, the derived treatment regime can be intricate and nonlinear, limiting their use. For clarity and interoperability, we emphasize linear regimes and determine the optimal linear regime by optimizing the AIPW estimator within set constraints.
  While the AIPW estimator offers a viable path to estimating the optimal regime, current methodologies predominantly focus on its asymptotic distribution, leaving a gap in studying the linear regime itself. However, there are many benefits to understanding the regime, as pinpointing significant covariates can enhance treatment effects and provide future clinical guidance. In this paper, we explore the asymptotic distribution of the estimated linear regime. Our results show that the parameter associated with the linear regime follows a cube-root convergence to a non-normal limiting distribution characterized by the maximizer of a centered Gaussian process with a quadratic drift. When making inferences for the estimated linear regimes with cube-root convergence in practical scenarios, the standard nonparametric bootstrap is invalid. As a solution, we facilitate the Cattaneo et al. (2020) bootstrap technique to provide a consistent distributional approximation for the estimated linear regimes, validated further through simulations and real-world data applications from the eICU Collaborative Research Database.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16161&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuwen Cheng, Shu Yang</name></author><category term="stat.ME" /><summary type="html">Personalized decision-making, tailored to individual characteristics, is gaining significant attention. The optimal treatment regime aims to provide the best-expected outcome in the entire population, known as the value function. One approach to determine this optimal regime is by maximizing the Augmented Inverse Probability Weighting (AIPW) estimator of the value function. However, the derived treatment regime can be intricate and nonlinear, limiting their use. For clarity and interoperability, we emphasize linear regimes and determine the optimal linear regime by optimizing the AIPW estimator within set constraints. While the AIPW estimator offers a viable path to estimating the optimal regime, current methodologies predominantly focus on its asymptotic distribution, leaving a gap in studying the linear regime itself. However, there are many benefits to understanding the regime, as pinpointing significant covariates can enhance treatment effects and provide future clinical guidance. In this paper, we explore the asymptotic distribution of the estimated linear regime. Our results show that the parameter associated with the linear regime follows a cube-root convergence to a non-normal limiting distribution characterized by the maximizer of a centered Gaussian process with a quadratic drift. When making inferences for the estimated linear regimes with cube-root convergence in practical scenarios, the standard nonparametric bootstrap is invalid. As a solution, we facilitate the Cattaneo et al. (2020) bootstrap technique to provide a consistent distributional approximation for the estimated linear regimes, validated further through simulations and real-world data applications from the eICU Collaborative Research Database.</summary></entry><entry><title type="html">Latent Energy-Based Odyssey: Black-Box Optimization via Expanded Exploration in the Energy-Based Latent Space</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/LatentEnergyBasedOdysseyBlackBoxOptimizationviaExpandedExplorationintheEnergyBasedLatentSpace.html" rel="alternate" type="text/html" title="Latent Energy-Based Odyssey: Black-Box Optimization via Expanded Exploration in the Energy-Based Latent Space" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/LatentEnergyBasedOdysseyBlackBoxOptimizationviaExpandedExplorationintheEnergyBasedLatentSpace</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/LatentEnergyBasedOdysseyBlackBoxOptimizationviaExpandedExplorationintheEnergyBasedLatentSpace.html">&lt;p&gt;Offline Black-Box Optimization (BBO) aims at optimizing a black-box function using the knowledge from a pre-collected offline dataset of function values and corresponding input designs. However, the high-dimensional and highly-multimodal input design space of black-box function pose inherent challenges for most existing methods that model and operate directly upon input designs. These issues include but are not limited to high sample complexity, which relates to inaccurate approximation of black-box function; and insufficient coverage and exploration of input design modes, which leads to suboptimal proposal of new input designs. In this work, we consider finding a latent space that serves as a compressed yet accurate representation of the design-value joint space, enabling effective latent exploration of high-value input design modes. To this end, we formulate an learnable energy-based latent space, and propose Noise-intensified Telescoping density-Ratio Estimation (NTRE) scheme for variational learning of an accurate latent space model without costly Markov Chain Monte Carlo. The optimization process is then exploration of high-value designs guided by the learned energy-based model in the latent space, formulated as gradient-based sampling from a latent-variable-parameterized inverse model. We show that our particular parameterization encourages expanded exploration around high-value design modes, motivated by inversion thinking of a fundamental result of conditional covariance matrix typically used for variance reduction. We observe that our method, backed by an accurately learned informative latent space and an expanding-exploration model design, yields significant improvements over strong previous methods on both synthetic and real world datasets such as the design-bench suite.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16730&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Peiyu Yu, Dinghuai Zhang, Hengzhi He, Xiaojian Ma, Ruiyao Miao, Yifan Lu, Yasi Zhang, Deqian Kong, Ruiqi Gao, Jianwen Xie, Guang Cheng, Ying Nian Wu</name></author><category term="stat.AP" /><summary type="html">Offline Black-Box Optimization (BBO) aims at optimizing a black-box function using the knowledge from a pre-collected offline dataset of function values and corresponding input designs. However, the high-dimensional and highly-multimodal input design space of black-box function pose inherent challenges for most existing methods that model and operate directly upon input designs. These issues include but are not limited to high sample complexity, which relates to inaccurate approximation of black-box function; and insufficient coverage and exploration of input design modes, which leads to suboptimal proposal of new input designs. In this work, we consider finding a latent space that serves as a compressed yet accurate representation of the design-value joint space, enabling effective latent exploration of high-value input design modes. To this end, we formulate an learnable energy-based latent space, and propose Noise-intensified Telescoping density-Ratio Estimation (NTRE) scheme for variational learning of an accurate latent space model without costly Markov Chain Monte Carlo. The optimization process is then exploration of high-value designs guided by the learned energy-based model in the latent space, formulated as gradient-based sampling from a latent-variable-parameterized inverse model. We show that our particular parameterization encourages expanded exploration around high-value design modes, motivated by inversion thinking of a fundamental result of conditional covariance matrix typically used for variance reduction. We observe that our method, backed by an accurately learned informative latent space and an expanding-exploration model design, yields significant improvements over strong previous methods on both synthetic and real world datasets such as the design-bench suite.</summary></entry><entry><title type="html">Localised Natural Causal Learning Algorithms for Weak Consistency Conditions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/LocalisedNaturalCausalLearningAlgorithmsforWeakConsistencyConditions.html" rel="alternate" type="text/html" title="Localised Natural Causal Learning Algorithms for Weak Consistency Conditions" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/LocalisedNaturalCausalLearningAlgorithmsforWeakConsistencyConditions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/LocalisedNaturalCausalLearningAlgorithmsforWeakConsistencyConditions.html">&lt;p&gt;By relaxing conditions for natural structure learning algorithms, a family of constraint-based algorithms containing all exact structure learning algorithms under the faithfulness assumption, we define localised natural structure learning algorithms (LoNS). We also provide a set of necessary and sufficient assumptions for consistency of LoNS, which can be thought of as a strict relaxation of the restricted faithfulness assumption. We provide a practical LoNS algorithm that runs in exponential time, which is then compared with related existing structure learning algorithms, namely PC/SGS and the relatively recent sparsest permutation algorithm. Simulation studies are also provided.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.14775&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kai Z Teh, Kayvan Sadeghi, Terry Soo</name></author><category term="stat.ME" /><summary type="html">By relaxing conditions for natural structure learning algorithms, a family of constraint-based algorithms containing all exact structure learning algorithms under the faithfulness assumption, we define localised natural structure learning algorithms (LoNS). We also provide a set of necessary and sufficient assumptions for consistency of LoNS, which can be thought of as a strict relaxation of the restricted faithfulness assumption. We provide a practical LoNS algorithm that runs in exponential time, which is then compared with related existing structure learning algorithms, namely PC/SGS and the relatively recent sparsest permutation algorithm. Simulation studies are also provided.</summary></entry><entry><title type="html">Long Story Short: Omitted Variable Bias in Causal Machine Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/LongStoryShortOmittedVariableBiasinCausalMachineLearning.html" rel="alternate" type="text/html" title="Long Story Short: Omitted Variable Bias in Causal Machine Learning" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/LongStoryShortOmittedVariableBiasinCausalMachineLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/LongStoryShortOmittedVariableBiasinCausalMachineLearning.html">&lt;p&gt;We develop a general theory of omitted variable bias for a wide range of common causal parameters, including (but not limited to) averages of potential outcomes, average treatment effects, average causal derivatives, and policy effects from covariate shifts. Our theory applies to nonparametric models, while naturally allowing for (semi-)parametric restrictions (such as partial linearity) when such assumptions are made. We show how simple plausibility judgments on the maximum explanatory power of omitted variables are sufficient to bound the magnitude of the bias, thus facilitating sensitivity analysis in otherwise complex, nonlinear models. Finally, we provide flexible and efficient statistical inference methods for the bounds, which can leverage modern machine learning algorithms for estimation. These results allow empirical researchers to perform sensitivity analyses in a flexible class of machine-learned causal models using very simple, and interpretable, tools. We demonstrate the utility of our approach with two empirical examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.13398&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Victor Chernozhukov, Carlos Cinelli, Whitney Newey, Amit Sharma, Vasilis Syrgkanis</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We develop a general theory of omitted variable bias for a wide range of common causal parameters, including (but not limited to) averages of potential outcomes, average treatment effects, average causal derivatives, and policy effects from covariate shifts. Our theory applies to nonparametric models, while naturally allowing for (semi-)parametric restrictions (such as partial linearity) when such assumptions are made. We show how simple plausibility judgments on the maximum explanatory power of omitted variables are sufficient to bound the magnitude of the bias, thus facilitating sensitivity analysis in otherwise complex, nonlinear models. Finally, we provide flexible and efficient statistical inference methods for the bounds, which can leverage modern machine learning algorithms for estimation. These results allow empirical researchers to perform sensitivity analyses in a flexible class of machine-learned causal models using very simple, and interpretable, tools. We demonstrate the utility of our approach with two empirical examples.</summary></entry><entry><title type="html">Lost in the Shuffle: Testing Power in the Presence of Errorful Network Vertex Labels</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/LostintheShuffleTestingPowerinthePresenceofErrorfulNetworkVertexLabels.html" rel="alternate" type="text/html" title="Lost in the Shuffle: Testing Power in the Presence of Errorful Network Vertex Labels" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/LostintheShuffleTestingPowerinthePresenceofErrorfulNetworkVertexLabels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/LostintheShuffleTestingPowerinthePresenceofErrorfulNetworkVertexLabels.html">&lt;p&gt;Two-sample network hypothesis testing is an important inference task with applications across diverse fields such as medicine, neuroscience, and sociology. Many of these testing methodologies operate under the implicit assumption that the vertex correspondence across networks is a priori known. This assumption is often untrue, and the power of the subsequent test can degrade when there are misaligned/label-shuffled vertices across networks. This power loss due to shuffling is theoretically explored in the context of random dot product and stochastic block model networks for a pair of hypothesis tests based on Frobenius norm differences between estimated edge probability matrices or between adjacency matrices. The loss in testing power is further reinforced by numerous simulations and experiments, both in the stochastic block model and in the random dot product graph model, where the power loss across multiple recently proposed tests in the literature is considered. Lastly, the impact that shuffling can have in real-data testing is demonstrated in a pair of examples from neuroscience and from social network analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.08638&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ayushi Saxena, Vince Lyzinski</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Two-sample network hypothesis testing is an important inference task with applications across diverse fields such as medicine, neuroscience, and sociology. Many of these testing methodologies operate under the implicit assumption that the vertex correspondence across networks is a priori known. This assumption is often untrue, and the power of the subsequent test can degrade when there are misaligned/label-shuffled vertices across networks. This power loss due to shuffling is theoretically explored in the context of random dot product and stochastic block model networks for a pair of hypothesis tests based on Frobenius norm differences between estimated edge probability matrices or between adjacency matrices. The loss in testing power is further reinforced by numerous simulations and experiments, both in the stochastic block model and in the random dot product graph model, where the power loss across multiple recently proposed tests in the literature is considered. Lastly, the impact that shuffling can have in real-data testing is demonstrated in a pair of examples from neuroscience and from social network analysis.</summary></entry><entry><title type="html">Mean-Field Microcanonical Gradient Descent</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/MeanFieldMicrocanonicalGradientDescent.html" rel="alternate" type="text/html" title="Mean-Field Microcanonical Gradient Descent" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/MeanFieldMicrocanonicalGradientDescent</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/MeanFieldMicrocanonicalGradientDescent.html">&lt;p&gt;Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension. It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent. We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent. As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit. We study these models in the context of financial time series, illustrating the improvements on both synthetic and real data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.08362&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marcus Häggbom, Morten Karlsmark, Joakim Andén</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Microcanonical gradient descent is a sampling procedure for energy-based models allowing for efficient sampling of distributions in high dimension. It works by transporting samples from a high-entropy distribution, such as Gaussian white noise, to a low-energy region using gradient descent. We put this model in the framework of normalizing flows, showing how it can often overfit by losing an unnecessary amount of entropy in the descent. As a remedy, we propose a mean-field microcanonical gradient descent that samples several weakly coupled data points simultaneously, allowing for better control of the entropy loss while paying little in terms of likelihood fit. We study these models in the context of financial time series, illustrating the improvements on both synthetic and real data.</summary></entry><entry><title type="html">Mixture Matrix-valued Autoregressive Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/MixtureMatrixvaluedAutoregressiveModel.html" rel="alternate" type="text/html" title="Mixture Matrix-valued Autoregressive Model" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/MixtureMatrixvaluedAutoregressiveModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/MixtureMatrixvaluedAutoregressiveModel.html">&lt;p&gt;Time series of matrix-valued data are increasingly available in various areas including economics, finance, social science, etc. These data may shed light on the inter-dynamical relationships between two sets of attributes, for instance countries and economic indices. The matrix autoregressive (MAR) model provides a parsimonious approach for analyzing such data. However, the MAR model, being a linear model with parametric constraints, cannot capture the nonlinear patterns in the data, such as regime shifts in the dynamics. We propose a mixture matrix autoregressive (MMAR) model for analyzing potential regime shifts in the dynamics between two attributes, for instance, due to recession vs. blooming, or quiet period vs. pandemic. We propose an EM algorithm for maximum likelihood estimation. We derive some theoretical properties of the proposed method including consistency and asymptotic distribution, and illustrate its performance via simulations and real applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.06098&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fei Wu, Kung-Sik Chan</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Time series of matrix-valued data are increasingly available in various areas including economics, finance, social science, etc. These data may shed light on the inter-dynamical relationships between two sets of attributes, for instance countries and economic indices. The matrix autoregressive (MAR) model provides a parsimonious approach for analyzing such data. However, the MAR model, being a linear model with parametric constraints, cannot capture the nonlinear patterns in the data, such as regime shifts in the dynamics. We propose a mixture matrix autoregressive (MMAR) model for analyzing potential regime shifts in the dynamics between two attributes, for instance, due to recession vs. blooming, or quiet period vs. pandemic. We propose an EM algorithm for maximum likelihood estimation. We derive some theoretical properties of the proposed method including consistency and asymptotic distribution, and illustrate its performance via simulations and real applications.</summary></entry><entry><title type="html">Modeling the Evolutionary Trends in Corporate ESG Reporting: A Study based on Knowledge Management Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ModelingtheEvolutionaryTrendsinCorporateESGReportingAStudybasedonKnowledgeManagementModel.html" rel="alternate" type="text/html" title="Modeling the Evolutionary Trends in Corporate ESG Reporting: A Study based on Knowledge Management Model" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ModelingtheEvolutionaryTrendsinCorporateESGReportingAStudybasedonKnowledgeManagementModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ModelingtheEvolutionaryTrendsinCorporateESGReportingAStudybasedonKnowledgeManagementModel.html">&lt;p&gt;Environmental, social, and governance (ESG) reports are globally recognized as a keystone in sustainable enterprise development. However, current literature has not concluded the development of topics and trends in ESG contexts in the twenty-first century. Therefore, We selected 1114 ESG reports from firms in the technology industry to analyze the evolutionary trends of ESG topics by text mining. We discovered the homogenization effect towards low environmental, medium governance, and high social features in the evolution. We also designed a strategic framework to look closer into the dynamic changes of firms’ within-industry scores and across-domain importances. We found that companies are gradually converging towards the third quadrant, which indicates that firms contribute less to industrial outstanding and professional distinctiveness in ESG reporting. Firms choose to imitate ESG reports from each other to mitigate uncertainty and enhance behavioral legitimacy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.07001&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ziyuan Xia, Anchen Sun, Xiaodong Cai, Saixing Zeng</name></author><category term="stat.AP" /><summary type="html">Environmental, social, and governance (ESG) reports are globally recognized as a keystone in sustainable enterprise development. However, current literature has not concluded the development of topics and trends in ESG contexts in the twenty-first century. Therefore, We selected 1114 ESG reports from firms in the technology industry to analyze the evolutionary trends of ESG topics by text mining. We discovered the homogenization effect towards low environmental, medium governance, and high social features in the evolution. We also designed a strategic framework to look closer into the dynamic changes of firms’ within-industry scores and across-domain importances. We found that companies are gradually converging towards the third quadrant, which indicates that firms contribute less to industrial outstanding and professional distinctiveness in ESG reporting. Firms choose to imitate ESG reports from each other to mitigate uncertainty and enhance behavioral legitimacy.</summary></entry><entry><title type="html">Modelling between- and within-season trajectories in elite athletic performance data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Modellingbetweenandwithinseasontrajectoriesineliteathleticperformancedata.html" rel="alternate" type="text/html" title="Modelling between- and within-season trajectories in elite athletic performance data" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Modellingbetweenandwithinseasontrajectoriesineliteathleticperformancedata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Modellingbetweenandwithinseasontrajectoriesineliteathleticperformancedata.html">&lt;p&gt;Athletic performance follows a typical pattern of improvement and decline during a career. This pattern is also often observed within-seasons as athlete aims for their performance to peak at key events such as the Olympic Games or World Championships. A Bayesian hierarchical model is developed to analyse the evolution of athletic sporting performance throughout an athlete’s career and separate these effects whilst allowing for confounding factors such as environmental conditions. Our model works in continuous time and estimates both the average performance level of the population, $g(t)$, at age $t$ and how each $i$-th athlete differs from the average $f_i(t)$. We further decompose $f_i(t)$ into changes from season-to-season, termed the between-season performance trajectory, and within-season performance trajectories which are modelled by a constrained Bernstein polynomial. Hence, the specific focus of this project is to identify the differences in performance that exist both between and within-seasons for each athlete. For the implementation of the model an adaptive Metropolis-within-Gibbs algorithm is used. An illustration of algorithm’s performance on 100 metres and 200 metres freestyle swimming in both female and male athletes is presented.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17214&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>M. Spyropoulou, J. G. Hopker, J. E. Griffin</name></author><category term="stat.AP" /><summary type="html">Athletic performance follows a typical pattern of improvement and decline during a career. This pattern is also often observed within-seasons as athlete aims for their performance to peak at key events such as the Olympic Games or World Championships. A Bayesian hierarchical model is developed to analyse the evolution of athletic sporting performance throughout an athlete’s career and separate these effects whilst allowing for confounding factors such as environmental conditions. Our model works in continuous time and estimates both the average performance level of the population, $g(t)$, at age $t$ and how each $i$-th athlete differs from the average $f_i(t)$. We further decompose $f_i(t)$ into changes from season-to-season, termed the between-season performance trajectory, and within-season performance trajectories which are modelled by a constrained Bernstein polynomial. Hence, the specific focus of this project is to identify the differences in performance that exist both between and within-seasons for each athlete. For the implementation of the model an adaptive Metropolis-within-Gibbs algorithm is used. An illustration of algorithm’s performance on 100 metres and 200 metres freestyle swimming in both female and male athletes is presented.</summary></entry><entry><title type="html">Multicalibration for Censored Survival Data: Towards Universal Adaptability in Predictive Modeling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/MulticalibrationforCensoredSurvivalDataTowardsUniversalAdaptabilityinPredictiveModeling.html" rel="alternate" type="text/html" title="Multicalibration for Censored Survival Data: Towards Universal Adaptability in Predictive Modeling" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/MulticalibrationforCensoredSurvivalDataTowardsUniversalAdaptabilityinPredictiveModeling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/MulticalibrationforCensoredSurvivalDataTowardsUniversalAdaptabilityinPredictiveModeling.html">&lt;p&gt;Traditional statistical and machine learning methods assume identical distribution for the training and test data sets. This assumption, however, is often violated in real applications, particularly in health care research, where the training data~(source) may underrepresent specific subpopulations in the testing or target domain. Such disparities, coupled with censored observations, present significant challenges for investigators aiming to make predictions for those minority groups. This paper focuses on target-independent learning under covariate shift, where we study multicalibration for survival probability and restricted mean survival time, and propose a black-box post-processing boosting algorithm designed for censored survival data. Our algorithm, leveraging the pseudo observations, yields a multicalibrated predictor competitive with propensity scoring regarding predictions on the unlabeled target domain, not just overall but across diverse subpopulations. Our theoretical analysis for pseudo observations relies on functional delta method and $p$-variational norm. We further investigate the algorithm’s sample complexity and convergence properties, as well as the multicalibration guarantee for post-processed predictors. Our theoretical insights reveal the link between multicalibration and universal adaptability, suggesting that our calibrated function performs comparably to, if not better than, the inverse propensity score weighting estimator. The performance of our proposed methods is corroborated through extensive numerical simulations and a real-world case study focusing on prediction of cardiovascular disease risk in two large prospective cohort studies. These empirical results confirm its potential as a powerful tool for predictive analysis with censored outcomes in diverse and shifting populations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15948&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hanxuan Ye, Hongzhe Li</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Traditional statistical and machine learning methods assume identical distribution for the training and test data sets. This assumption, however, is often violated in real applications, particularly in health care research, where the training data~(source) may underrepresent specific subpopulations in the testing or target domain. Such disparities, coupled with censored observations, present significant challenges for investigators aiming to make predictions for those minority groups. This paper focuses on target-independent learning under covariate shift, where we study multicalibration for survival probability and restricted mean survival time, and propose a black-box post-processing boosting algorithm designed for censored survival data. Our algorithm, leveraging the pseudo observations, yields a multicalibrated predictor competitive with propensity scoring regarding predictions on the unlabeled target domain, not just overall but across diverse subpopulations. Our theoretical analysis for pseudo observations relies on functional delta method and $p$-variational norm. We further investigate the algorithm’s sample complexity and convergence properties, as well as the multicalibration guarantee for post-processed predictors. Our theoretical insights reveal the link between multicalibration and universal adaptability, suggesting that our calibrated function performs comparably to, if not better than, the inverse propensity score weighting estimator. The performance of our proposed methods is corroborated through extensive numerical simulations and a real-world case study focusing on prediction of cardiovascular disease risk in two large prospective cohort studies. These empirical results confirm its potential as a powerful tool for predictive analysis with censored outcomes in diverse and shifting populations.</summary></entry><entry><title type="html">Multiple imputation of missing covariates when using the Fine-Gray model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/MultipleimputationofmissingcovariateswhenusingtheFineGraymodel.html" rel="alternate" type="text/html" title="Multiple imputation of missing covariates when using the Fine-Gray model" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/MultipleimputationofmissingcovariateswhenusingtheFineGraymodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/MultipleimputationofmissingcovariateswhenusingtheFineGraymodel.html">&lt;p&gt;The Fine-Gray model for the subdistribution hazard is commonly used for estimating associations between covariates and competing risks outcomes. When there are missing values in the covariates included in a given model, researchers may wish to multiply impute them. Assuming interest lies in estimating the risk of only one of the competing events, this paper develops a substantive-model-compatible multiple imputation approach that exploits the parallels between the Fine-Gray model and the standard (single-event) Cox model. In the presence of right-censoring, this involves first imputing the potential censoring times for those failing from competing events, and thereafter imputing the missing covariates by leveraging methodology previously developed for the Cox model in the setting without competing risks. In a simulation study, we compared the proposed approach to alternative methods, such as imputing compatibly with cause-specific Cox models. The proposed method performed well (in terms of estimation of both subdistribution log hazard ratios and cumulative incidences) when data were generated assuming proportional subdistribution hazards, and performed satisfactorily when this assumption was not satisfied. The gain in efficiency compared to a complete-case analysis was demonstrated in both the simulation study and in an applied data example on competing outcomes following an allogeneic stem cell transplantation. For individual-specific cumulative incidence estimation, assuming proportionality on the correct scale at the analysis phase appears to be more important than correctly specifying the imputation procedure used to impute the missing covariates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16602&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Edouard F. Bonneville, Jan Beyersmann, Ruth H. Keogh, Jonathan W. Bartlett, Tim P. Morris, Nicola Polverelli, Liesbeth C. de Wreede, Hein Putter</name></author><category term="stat.ME" /><summary type="html">The Fine-Gray model for the subdistribution hazard is commonly used for estimating associations between covariates and competing risks outcomes. When there are missing values in the covariates included in a given model, researchers may wish to multiply impute them. Assuming interest lies in estimating the risk of only one of the competing events, this paper develops a substantive-model-compatible multiple imputation approach that exploits the parallels between the Fine-Gray model and the standard (single-event) Cox model. In the presence of right-censoring, this involves first imputing the potential censoring times for those failing from competing events, and thereafter imputing the missing covariates by leveraging methodology previously developed for the Cox model in the setting without competing risks. In a simulation study, we compared the proposed approach to alternative methods, such as imputing compatibly with cause-specific Cox models. The proposed method performed well (in terms of estimation of both subdistribution log hazard ratios and cumulative incidences) when data were generated assuming proportional subdistribution hazards, and performed satisfactorily when this assumption was not satisfied. The gain in efficiency compared to a complete-case analysis was demonstrated in both the simulation study and in an applied data example on competing outcomes following an allogeneic stem cell transplantation. For individual-specific cumulative incidence estimation, assuming proportionality on the correct scale at the analysis phase appears to be more important than correctly specifying the imputation procedure used to impute the missing covariates.</summary></entry><entry><title type="html">Multi-task learning via robust regularized clustering with non-convex group penalties</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Multitasklearningviarobustregularizedclusteringwithnonconvexgrouppenalties.html" rel="alternate" type="text/html" title="Multi-task learning via robust regularized clustering with non-convex group penalties" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Multitasklearningviarobustregularizedclusteringwithnonconvexgrouppenalties</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Multitasklearningviarobustregularizedclusteringwithnonconvexgrouppenalties.html">&lt;p&gt;Multi-task learning (MTL) aims to improve estimation and prediction performance by sharing common information among related tasks. One natural assumption in MTL is that tasks are classified into clusters based on their characteristics. However, existing MTL methods based on this assumption often ignore outlier tasks that have large task-specific components or no relation to other tasks. To address this issue, we propose a novel MTL method called Multi-Task Learning via Robust Regularized Clustering (MTLRRC). MTLRRC incorporates robust regularization terms inspired by robust convex clustering, which is further extended to handle non-convex and group-sparse penalties. The extension allows MTLRRC to simultaneously perform robust task clustering and outlier task detection. The connection between the extended robust clustering and the multivariate M-estimator is also established. This provides an interpretation of the robustness of MTLRRC against outlier tasks. An efficient algorithm based on a modified alternating direction method of multipliers is developed for the estimation of the parameters. The effectiveness of MTLRRC is demonstrated through simulation studies and application to real data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.03250&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Akira Okazaki, Shuichi Kawano</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Multi-task learning (MTL) aims to improve estimation and prediction performance by sharing common information among related tasks. One natural assumption in MTL is that tasks are classified into clusters based on their characteristics. However, existing MTL methods based on this assumption often ignore outlier tasks that have large task-specific components or no relation to other tasks. To address this issue, we propose a novel MTL method called Multi-Task Learning via Robust Regularized Clustering (MTLRRC). MTLRRC incorporates robust regularization terms inspired by robust convex clustering, which is further extended to handle non-convex and group-sparse penalties. The extension allows MTLRRC to simultaneously perform robust task clustering and outlier task detection. The connection between the extended robust clustering and the multivariate M-estimator is also established. This provides an interpretation of the robustness of MTLRRC against outlier tasks. An efficient algorithm based on a modified alternating direction method of multipliers is developed for the estimation of the parameters. The effectiveness of MTLRRC is demonstrated through simulation studies and application to real data.</summary></entry><entry><title type="html">Neural networks for geospatial data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Neuralnetworksforgeospatialdata.html" rel="alternate" type="text/html" title="Neural networks for geospatial data" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Neuralnetworksforgeospatialdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Neuralnetworksforgeospatialdata.html">&lt;p&gt;Analysis of geospatial data has traditionally been model-based, with a mean model, customarily specified as a linear regression on the covariates, and a covariance model, encoding the spatial dependence. We relax the strong assumption of linearity and propose embedding neural networks directly within the traditional geostatistical models to accommodate non-linear mean functions while retaining all other advantages including use of Gaussian Processes to explicitly model the spatial covariance, enabling inference on the covariate effect through the mean and on the spatial dependence through the covariance, and offering predictions at new locations via kriging. We propose NN-GLS, a new neural network estimation algorithm for the non-linear mean in GP models that explicitly accounts for the spatial covariance through generalized least squares (GLS), the same loss used in the linear case. We show that NN-GLS admits a representation as a special type of graph neural network (GNN). This connection facilitates use of standard neural network computational techniques for irregular geospatial data, enabling novel and scalable mini-batching, backpropagation, and kriging schemes. Theoretically, we show that NN-GLS will be consistent for irregularly observed spatially correlated data processes. We also provide a finite sample concentration rate, which quantifies the need to accurately model the spatial covariance in neural networks for dependent data. To our knowledge, these are the first large-sample results for any neural network algorithm for irregular spatial data. We demonstrate the methodology through simulated and real datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.09157&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wentao Zhan, Abhirup Datta</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Analysis of geospatial data has traditionally been model-based, with a mean model, customarily specified as a linear regression on the covariates, and a covariance model, encoding the spatial dependence. We relax the strong assumption of linearity and propose embedding neural networks directly within the traditional geostatistical models to accommodate non-linear mean functions while retaining all other advantages including use of Gaussian Processes to explicitly model the spatial covariance, enabling inference on the covariate effect through the mean and on the spatial dependence through the covariance, and offering predictions at new locations via kriging. We propose NN-GLS, a new neural network estimation algorithm for the non-linear mean in GP models that explicitly accounts for the spatial covariance through generalized least squares (GLS), the same loss used in the linear case. We show that NN-GLS admits a representation as a special type of graph neural network (GNN). This connection facilitates use of standard neural network computational techniques for irregular geospatial data, enabling novel and scalable mini-batching, backpropagation, and kriging schemes. Theoretically, we show that NN-GLS will be consistent for irregularly observed spatially correlated data processes. We also provide a finite sample concentration rate, which quantifies the need to accurately model the spatial covariance in neural networks for dependent data. To our knowledge, these are the first large-sample results for any neural network algorithm for irregular spatial data. We demonstrate the methodology through simulated and real datasets.</summary></entry><entry><title type="html">Novel closed-form point estimators for a weighted exponential family derived from likelihood equations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Novelclosedformpointestimatorsforaweightedexponentialfamilyderivedfromlikelihoodequations.html" rel="alternate" type="text/html" title="Novel closed-form point estimators for a weighted exponential family derived from likelihood equations" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Novelclosedformpointestimatorsforaweightedexponentialfamilyderivedfromlikelihoodequations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Novelclosedformpointestimatorsforaweightedexponentialfamilyderivedfromlikelihoodequations.html">&lt;p&gt;In this paper, we propose and investigate closed-form point estimators for a weighted exponential family. We also develop a bias-reduced version of these proposed closed-form estimators through bootstrap methods. Estimators are assessed using a Monte Carlo simulation, revealing favorable results for the proposed bootstrap bias-reduced estimators.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16192&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roberto Vila, Eduardo Nakano, Helton Saulo</name></author><category term="stat.ME" /><summary type="html">In this paper, we propose and investigate closed-form point estimators for a weighted exponential family. We also develop a bias-reduced version of these proposed closed-form estimators through bootstrap methods. Estimators are assessed using a Monte Carlo simulation, revealing favorable results for the proposed bootstrap bias-reduced estimators.</summary></entry><entry><title type="html">On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/OntheAlgorithmicBiasofAligningLargeLanguageModelswithRLHFPreferenceCollapseandMatchingRegularization.html" rel="alternate" type="text/html" title="On the Algorithmic Bias of Aligning Large Language Models with RLHF: Preference Collapse and Matching Regularization" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/OntheAlgorithmicBiasofAligningLargeLanguageModelswithRLHFPreferenceCollapseandMatchingRegularization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/OntheAlgorithmicBiasofAligningLargeLanguageModelswithRLHFPreferenceCollapseandMatchingRegularization.html">&lt;p&gt;Accurately aligning large language models (LLMs) with human preferences is crucial for informing fair, economically sound, and statistically efficient decision-making processes. However, we argue that reinforcement learning from human feedback (RLHF) – the predominant approach for aligning LLMs with human preferences through a reward model – suffers from an inherent algorithmic bias due to its Kullback–Leibler-based regularization in optimization. In extreme cases, this bias could lead to a phenomenon we term preference collapse, where minority preferences are virtually disregarded. To mitigate this algorithmic bias, we introduce preference matching (PM) RLHF, a novel approach that provably aligns LLMs with the preference distribution of the reward model under the Bradley–Terry–Luce/Plackett–Luce model. Central to our approach is a PM regularizer that takes the form of the negative logarithm of the LLM’s policy probability distribution over responses, which helps the LLM balance response diversification and reward maximization. Notably, we obtain this regularizer by solving an ordinary differential equation that is necessary for the PM property. For practical implementation, we introduce a conditional variant of PM RLHF that is tailored to natural language generation. Finally, we empirically validate the effectiveness of conditional PM RLHF through experiments on the OPT-1.3B and Llama-2-7B models, demonstrating a 29% to 41% improvement in alignment with human preferences, as measured by a certain metric, compared to standard RLHF.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16455&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiancong Xiao, Ziniu Li, Xingyu Xie, Emily Getzen, Cong Fang, Qi Long, Weijie J. Su</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Accurately aligning large language models (LLMs) with human preferences is crucial for informing fair, economically sound, and statistically efficient decision-making processes. However, we argue that reinforcement learning from human feedback (RLHF) – the predominant approach for aligning LLMs with human preferences through a reward model – suffers from an inherent algorithmic bias due to its Kullback–Leibler-based regularization in optimization. In extreme cases, this bias could lead to a phenomenon we term preference collapse, where minority preferences are virtually disregarded. To mitigate this algorithmic bias, we introduce preference matching (PM) RLHF, a novel approach that provably aligns LLMs with the preference distribution of the reward model under the Bradley–Terry–Luce/Plackett–Luce model. Central to our approach is a PM regularizer that takes the form of the negative logarithm of the LLM’s policy probability distribution over responses, which helps the LLM balance response diversification and reward maximization. Notably, we obtain this regularizer by solving an ordinary differential equation that is necessary for the PM property. For practical implementation, we introduce a conditional variant of PM RLHF that is tailored to natural language generation. Finally, we empirically validate the effectiveness of conditional PM RLHF through experiments on the OPT-1.3B and Llama-2-7B models, demonstrating a 29% to 41% improvement in alignment with human preferences, as measured by a certain metric, compared to standard RLHF.</summary></entry><entry><title type="html">On the PM2.5 – Mortality Relationship: A Bayesian Dynamic Model for Spatio-Temporal Confounding</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/OnthePM25MortalityRelationshipABayesianDynamicModelforSpatioTemporalConfounding.html" rel="alternate" type="text/html" title="On the PM2.5 – Mortality Relationship: A Bayesian Dynamic Model for Spatio-Temporal Confounding" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/OnthePM25MortalityRelationshipABayesianDynamicModelforSpatioTemporalConfounding</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/OnthePM25MortalityRelationshipABayesianDynamicModelforSpatioTemporalConfounding.html">&lt;p&gt;Spatial confounding, often regarded as a major concern in epidemiological studies, relates to the difficulty of recovering the effect of an exposure on an outcome when these variables are associated with unobserved factors. This issue is particularly challenging in spatio-temporal analyses, where it has been less explored so far. To study the effects of air pollution on mortality in Italy, we argue that a model that simultaneously accounts for spatio-temporal confounding and for the non-linear form of the effect of interest is needed. To this end, we propose a Bayesian dynamic generalized linear model, which allows for a non-linear association and for a decomposition of the exposure effect into two components. This decomposition accommodates associations with the outcome at fine and coarse temporal and spatial scales of variation. These features, when combined, allow reducing the spatio-temporal confounding bias and recovering the true shape of the association, as demonstrated through simulation studies. The results from the real-data application indicate that the exposure effect seems to have different magnitudes in different seasons, with peaks in the summer. We hypothesize that this could be due to possible interactions between the exposure variable with air temperature and unmeasured confounders.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16106&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carlo Zaccardi, Pasquale Valentini, Luigi Ippoliti, Alexandra M. Schmidt</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Spatial confounding, often regarded as a major concern in epidemiological studies, relates to the difficulty of recovering the effect of an exposure on an outcome when these variables are associated with unobserved factors. This issue is particularly challenging in spatio-temporal analyses, where it has been less explored so far. To study the effects of air pollution on mortality in Italy, we argue that a model that simultaneously accounts for spatio-temporal confounding and for the non-linear form of the effect of interest is needed. To this end, we propose a Bayesian dynamic generalized linear model, which allows for a non-linear association and for a decomposition of the exposure effect into two components. This decomposition accommodates associations with the outcome at fine and coarse temporal and spatial scales of variation. These features, when combined, allow reducing the spatio-temporal confounding bias and recovering the true shape of the association, as demonstrated through simulation studies. The results from the real-data application indicate that the exposure effect seems to have different magnitudes in different seasons, with peaks in the summer. We hypothesize that this could be due to possible interactions between the exposure variable with air temperature and unmeasured confounders.</summary></entry><entry><title type="html">Outcome-Driven Dynamic Refugee Assignment with Allocation Balancing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/OutcomeDrivenDynamicRefugeeAssignmentwithAllocationBalancing.html" rel="alternate" type="text/html" title="Outcome-Driven Dynamic Refugee Assignment with Allocation Balancing" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/OutcomeDrivenDynamicRefugeeAssignmentwithAllocationBalancing</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/OutcomeDrivenDynamicRefugeeAssignmentwithAllocationBalancing.html">&lt;p&gt;This study proposes two new dynamic assignment algorithms to match refugees and asylum seekers to geographic localities within a host country. The first, currently implemented in a multi-year randomized control trial in Switzerland, seeks to maximize the average predicted employment level (or any measured outcome of interest) of refugees through a minimum-discord online assignment algorithm. The performance of this algorithm is tested on real refugee resettlement data from both the US and Switzerland, where we find that it is able to achieve near-optimal expected employment compared to the hindsight-optimal solution, and is able to improve upon the status quo procedure by 40-50%. However, pure outcome maximization can result in a periodically imbalanced allocation to the localities over time, leading to implementation difficulties and an undesirable workflow for resettlement resources and agents. To address these problems, the second algorithm balances the goal of improving refugee outcomes with the desire for an even allocation over time. We find that this algorithm can achieve near-perfect balance over time with only a small loss in expected employment compared to the employment-maximizing algorithm. In addition, the allocation balancing algorithm offers a number of ancillary benefits compared to pure outcome maximization, including robustness to unknown arrival flows and greater exploration.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2007.03069&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kirk Bansak, Elisabeth Paulson</name></author><category term="stat.ME" /><summary type="html">This study proposes two new dynamic assignment algorithms to match refugees and asylum seekers to geographic localities within a host country. The first, currently implemented in a multi-year randomized control trial in Switzerland, seeks to maximize the average predicted employment level (or any measured outcome of interest) of refugees through a minimum-discord online assignment algorithm. The performance of this algorithm is tested on real refugee resettlement data from both the US and Switzerland, where we find that it is able to achieve near-optimal expected employment compared to the hindsight-optimal solution, and is able to improve upon the status quo procedure by 40-50%. However, pure outcome maximization can result in a periodically imbalanced allocation to the localities over time, leading to implementation difficulties and an undesirable workflow for resettlement resources and agents. To address these problems, the second algorithm balances the goal of improving refugee outcomes with the desire for an even allocation over time. We find that this algorithm can achieve near-perfect balance over time with only a small loss in expected employment compared to the employment-maximizing algorithm. In addition, the allocation balancing algorithm offers a number of ancillary benefits compared to pure outcome maximization, including robustness to unknown arrival flows and greater exploration.</summary></entry><entry><title type="html">Pairwise likelihood estimation and limited information goodness-of-fit test statistics for binary factor analysis models under complex survey sampling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Pairwiselikelihoodestimationandlimitedinformationgoodnessoffitteststatisticsforbinaryfactoranalysismodelsundercomplexsurveysampling.html" rel="alternate" type="text/html" title="Pairwise likelihood estimation and limited information goodness-of-fit test statistics for binary factor analysis models under complex survey sampling" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Pairwiselikelihoodestimationandlimitedinformationgoodnessoffitteststatisticsforbinaryfactoranalysismodelsundercomplexsurveysampling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Pairwiselikelihoodestimationandlimitedinformationgoodnessoffitteststatisticsforbinaryfactoranalysismodelsundercomplexsurveysampling.html">&lt;p&gt;This paper discusses estimation and limited information goodness-of-fit test statistics in factor models for binary data using pairwise likelihood estimation and sampling weights. The paper extends the applicability of pairwise likelihood estimation for factor models with binary data to accommodate complex sampling designs. Additionally, it introduces two key limited information test statistics: the Pearson chi-squared test and the Wald test. To enhance computational efficiency, the paper introduces modifications to both test statistics. The performance of the estimation and the proposed test statistics under simple random sampling and unequal probability sampling is evaluated using simulated data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.02543&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Haziq Jamil, Irini Moustaki, Chris Skinner</name></author><category term="stat.ME" /><summary type="html">This paper discusses estimation and limited information goodness-of-fit test statistics in factor models for binary data using pairwise likelihood estimation and sampling weights. The paper extends the applicability of pairwise likelihood estimation for factor models with binary data to accommodate complex sampling designs. Additionally, it introduces two key limited information test statistics: the Pearson chi-squared test and the Wald test. To enhance computational efficiency, the paper introduces modifications to both test statistics. The performance of the estimation and the proposed test statistics under simple random sampling and unequal probability sampling is evaluated using simulated data.</summary></entry><entry><title type="html">Quasi-Likelihood Analysis for Student-Lévy Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/QuasiLikelihoodAnalysisforStudentL%C3%A9vyRegression.html" rel="alternate" type="text/html" title="Quasi-Likelihood Analysis for Student-Lévy Regression" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/QuasiLikelihoodAnalysisforStudentL%C3%A9vyRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/QuasiLikelihoodAnalysisforStudentL%C3%A9vyRegression.html">&lt;p&gt;We consider the quasi-likelihood analysis for a linear regression model driven by a Student-t L&apos;{e}vy process with constant scale and arbitrary degrees of freedom. The model is observed at high frequency over an extending period, under which we can quantify how the sampling frequency affects estimation accuracy. In that setting, joint estimation of trend, scale, and degrees of freedom is a non-trivial problem. The bottleneck is that the Student-t distribution is not closed under convolution, making it difficult to estimate all the parameters fully based on the high-frequency time scale. To efficiently deal with the intricate nature from both theoretical and computational points of view, we propose a two-step quasi-likelihood analysis: first, we make use of the Cauchy quasi-likelihood for estimating the regression-coefficient vector and the scale parameter; then, we construct the sequence of the unit-period cumulative residuals to estimate the remaining degrees of freedom. In particular, using full data in the first step causes a problem stemming from the small-time Cauchy approximation, showing the need for data thinning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.16790&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hiroki Masuda, Lorenzo Mercuri, Yuma Uehara</name></author><category term="stat.AP," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">We consider the quasi-likelihood analysis for a linear regression model driven by a Student-t L&apos;{e}vy process with constant scale and arbitrary degrees of freedom. The model is observed at high frequency over an extending period, under which we can quantify how the sampling frequency affects estimation accuracy. In that setting, joint estimation of trend, scale, and degrees of freedom is a non-trivial problem. The bottleneck is that the Student-t distribution is not closed under convolution, making it difficult to estimate all the parameters fully based on the high-frequency time scale. To efficiently deal with the intricate nature from both theoretical and computational points of view, we propose a two-step quasi-likelihood analysis: first, we make use of the Cauchy quasi-likelihood for estimating the regression-coefficient vector and the scale parameter; then, we construct the sequence of the unit-period cumulative residuals to estimate the remaining degrees of freedom. In particular, using full data in the first step causes a problem stemming from the small-time Cauchy approximation, showing the need for data thinning.</summary></entry><entry><title type="html">RealTCD: Temporal Causal Discovery from Interventional Data with Large Language Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/RealTCDTemporalCausalDiscoveryfromInterventionalDatawithLargeLanguageModel.html" rel="alternate" type="text/html" title="RealTCD: Temporal Causal Discovery from Interventional Data with Large Language Model" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/RealTCDTemporalCausalDiscoveryfromInterventionalDatawithLargeLanguageModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/RealTCDTemporalCausalDiscoveryfromInterventionalDatawithLargeLanguageModel.html">&lt;p&gt;In the field of Artificial Intelligence for Information Technology Operations, causal discovery is pivotal for operation and maintenance of graph construction, facilitating downstream industrial tasks such as root cause analysis. Temporal causal discovery, as an emerging method, aims to identify temporal causal relationships between variables directly from observations by utilizing interventional data. However, existing methods mainly focus on synthetic datasets with heavy reliance on intervention targets and ignore the textual information hidden in real-world systems, failing to conduct causal discovery for real industrial scenarios. To tackle this problem, in this paper we propose to investigate temporal causal discovery in industrial scenarios, which faces two critical challenges: 1) how to discover causal relationships without the interventional targets that are costly to obtain in practice, and 2) how to discover causal relations via leveraging the textual information in systems which can be complex yet abundant in industrial contexts. To address these challenges, we propose the RealTCD framework, which is able to leverage domain knowledge to discover temporal causal relationships without interventional targets. Specifically, we first develop a score-based temporal causal discovery method capable of discovering causal relations for root cause analysis without relying on interventional targets through strategic masking and regularization. Furthermore, by employing Large Language Models (LLMs) to handle texts and integrate domain knowledge, we introduce LLM-guided meta-initialization to extract the meta-knowledge from textual information hidden in systems to boost the quality of discovery. We conduct extensive experiments on simulation and real-world datasets to show the superiority of our proposed RealTCD framework over existing baselines in discovering temporal causal structures.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.14786&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Peiwen Li, Xin Wang, Zeyang Zhang, Yuan Meng, Fang Shen, Yue Li, Jialong Wang, Yang Li, Wenweu Zhu</name></author><category term="stat.ME" /><summary type="html">In the field of Artificial Intelligence for Information Technology Operations, causal discovery is pivotal for operation and maintenance of graph construction, facilitating downstream industrial tasks such as root cause analysis. Temporal causal discovery, as an emerging method, aims to identify temporal causal relationships between variables directly from observations by utilizing interventional data. However, existing methods mainly focus on synthetic datasets with heavy reliance on intervention targets and ignore the textual information hidden in real-world systems, failing to conduct causal discovery for real industrial scenarios. To tackle this problem, in this paper we propose to investigate temporal causal discovery in industrial scenarios, which faces two critical challenges: 1) how to discover causal relationships without the interventional targets that are costly to obtain in practice, and 2) how to discover causal relations via leveraging the textual information in systems which can be complex yet abundant in industrial contexts. To address these challenges, we propose the RealTCD framework, which is able to leverage domain knowledge to discover temporal causal relationships without interventional targets. Specifically, we first develop a score-based temporal causal discovery method capable of discovering causal relations for root cause analysis without relying on interventional targets through strategic masking and regularization. Furthermore, by employing Large Language Models (LLMs) to handle texts and integrate domain knowledge, we introduce LLM-guided meta-initialization to extract the meta-knowledge from textual information hidden in systems to boost the quality of discovery. We conduct extensive experiments on simulation and real-world datasets to show the superiority of our proposed RealTCD framework over existing baselines in discovering temporal causal structures.</summary></entry><entry><title type="html">Robust Reproducible Network Exploration</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/RobustReproducibleNetworkExploration.html" rel="alternate" type="text/html" title="Robust Reproducible Network Exploration" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/RobustReproducibleNetworkExploration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/RobustReproducibleNetworkExploration.html">&lt;p&gt;We propose a novel method of network detection that is robust against any complex dependence structure. Our goal is to conduct exploratory network detection, meaning that we attempt to detect a network composed of ``connectable’’ edges that are worth investigating in detail for further modelling or precise network analysis. For a reproducible network detection, we pursuit high power while controlling the false discovery rate (FDR). In particular, we formalize the problem as a multiple testing, and propose p-variables that are used in the Benjamini-Hochberg procedure. We show that the proposed method controls the FDR under arbitrary dependence structure with any sample size, and has asymptotic power one. The validity is also confirmed by simulations and a real data example.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17117&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Masaki Toyoda, Yoshimasa Uematsu</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We propose a novel method of network detection that is robust against any complex dependence structure. Our goal is to conduct exploratory network detection, meaning that we attempt to detect a network composed of ``connectable’’ edges that are worth investigating in detail for further modelling or precise network analysis. For a reproducible network detection, we pursuit high power while controlling the false discovery rate (FDR). In particular, we formalize the problem as a multiple testing, and propose p-variables that are used in the Benjamini-Hochberg procedure. We show that the proposed method controls the FDR under arbitrary dependence structure with any sample size, and has asymptotic power one. The validity is also confirmed by simulations and a real data example.</summary></entry><entry><title type="html">Selecting the number of components in PCA via random signflips</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SelectingthenumberofcomponentsinPCAviarandomsignflips.html" rel="alternate" type="text/html" title="Selecting the number of components in PCA via random signflips" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SelectingthenumberofcomponentsinPCAviarandomsignflips</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SelectingthenumberofcomponentsinPCAviarandomsignflips.html">&lt;p&gt;Principal component analysis (PCA) is a foundational tool in modern data analysis, and a crucial step in PCA is selecting the number of components to keep. However, classical selection methods (e.g., scree plots, parallel analysis, etc.) lack statistical guarantees in the increasingly common setting of large-dimensional data with heterogeneous noise, i.e., where each entry may have a different noise variance. Moreover, it turns out that these methods, which are highly effective for homogeneous noise, can fail dramatically for data with heterogeneous noise. This paper proposes a new method called signflip parallel analysis (FlipPA) for the setting of approximately symmetric noise: it compares the data singular values to those of “empirical null” matrices generated by flipping the sign of each entry randomly with probability one-half. We develop a rigorous theory for FlipPA, showing that it has nonasymptotic type I error control and that it consistently selects the correct rank for signals rising above the noise floor in the large-dimensional limit (even when the noise is heterogeneous). We also rigorously explain why classical permutation-based parallel analysis degrades under heterogeneous noise. Finally, we illustrate that FlipPA compares favorably to state-of-the art methods via numerical simulations and an illustration on data coming from astronomy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2012.02985&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David Hong, Yue Sheng, Edgar Dobriban</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Principal component analysis (PCA) is a foundational tool in modern data analysis, and a crucial step in PCA is selecting the number of components to keep. However, classical selection methods (e.g., scree plots, parallel analysis, etc.) lack statistical guarantees in the increasingly common setting of large-dimensional data with heterogeneous noise, i.e., where each entry may have a different noise variance. Moreover, it turns out that these methods, which are highly effective for homogeneous noise, can fail dramatically for data with heterogeneous noise. This paper proposes a new method called signflip parallel analysis (FlipPA) for the setting of approximately symmetric noise: it compares the data singular values to those of “empirical null” matrices generated by flipping the sign of each entry randomly with probability one-half. We develop a rigorous theory for FlipPA, showing that it has nonasymptotic type I error control and that it consistently selects the correct rank for signals rising above the noise floor in the large-dimensional limit (even when the noise is heterogeneous). We also rigorously explain why classical permutation-based parallel analysis degrades under heterogeneous noise. Finally, we illustrate that FlipPA compares favorably to state-of-the art methods via numerical simulations and an illustration on data coming from astronomy.</summary></entry><entry><title type="html">Selective inference for multiple pairs of clusters after K-means clustering</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SelectiveinferenceformultiplepairsofclustersafterKmeansclustering.html" rel="alternate" type="text/html" title="Selective inference for multiple pairs of clusters after K-means clustering" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SelectiveinferenceformultiplepairsofclustersafterKmeansclustering</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SelectiveinferenceformultiplepairsofclustersafterKmeansclustering.html">&lt;p&gt;If the same data is used for both clustering and for testing a null hypothesis that is formulated in terms of the estimated clusters, then the traditional hypothesis testing framework often fails to control the Type I error. Gao et al. [2022] and Chen and Witten [2023] provide selective inference frameworks for testing if a pair of estimated clusters indeed stem from underlying differences, for the case where hierarchical clustering and K-means clustering, respectively, are used to define the clusters. In applications, however, it is often of interest to test for multiple pairs of clusters. In our work, we extend the pairwise test of Chen and Witten [2023] to a test for multiple pairs of clusters, where the cluster assignments are produced by K-means clustering. We further develop an analogous test for the setting where the variance is unknown, building on the work of Yun and Barber [2023] that extends Gao et al. [2022]’s pairwise test to the case of unknown variance. For both known and unknown variance settings, we present methods that address certain forms of data-dependence in the choice of pairs of clusters to test for. We show that our proposed tests control the Type I error, both theoretically and empirically, and provide a numerical study of their empirical powers under various settings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16379&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Youngjoo Yun, Yinqiu He</name></author><category term="stat.ME" /><summary type="html">If the same data is used for both clustering and for testing a null hypothesis that is formulated in terms of the estimated clusters, then the traditional hypothesis testing framework often fails to control the Type I error. Gao et al. [2022] and Chen and Witten [2023] provide selective inference frameworks for testing if a pair of estimated clusters indeed stem from underlying differences, for the case where hierarchical clustering and K-means clustering, respectively, are used to define the clusters. In applications, however, it is often of interest to test for multiple pairs of clusters. In our work, we extend the pairwise test of Chen and Witten [2023] to a test for multiple pairs of clusters, where the cluster assignments are produced by K-means clustering. We further develop an analogous test for the setting where the variance is unknown, building on the work of Yun and Barber [2023] that extends Gao et al. [2022]’s pairwise test to the case of unknown variance. For both known and unknown variance settings, we present methods that address certain forms of data-dependence in the choice of pairs of clusters to test for. We show that our proposed tests control the Type I error, both theoretically and empirically, and provide a numerical study of their empirical powers under various settings.</summary></entry><entry><title type="html">Sensitivity Analysis for Attributable Effects in Case$^2$ Studies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SensitivityAnalysisforAttributableEffectsinCase2Studies.html" rel="alternate" type="text/html" title="Sensitivity Analysis for Attributable Effects in Case$^2$ Studies" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SensitivityAnalysisforAttributableEffectsinCase2Studies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SensitivityAnalysisforAttributableEffectsinCase2Studies.html">&lt;p&gt;The case$^2$ study, also referred to as the case-case study design, is a valuable approach for conducting inference for treatment effects. Unlike traditional case-control studies, the case$^2$ design compares treatment in two types of cases with the same disease. A key quantity of interest is the attributable effect, which is the number of cases of disease among treated units which are caused by the treatment. Two key assumptions that are usually made for making inferences about the attributable effect in case$^2$ studies are 1.) treatment does not cause the second type of case, and 2.) the treatment does not alter an individual’s case type. However, these assumptions are not realistic in many real-data applications. In this article, we present a sensitivity analysis framework to scrutinize the impact of deviations from these assumptions on obtained results. We also include sensitivity analyses related to the assumption of unmeasured confounding, recognizing the potential bias introduced by unobserved covariates. The proposed methodology is exemplified through an investigation into whether having violent behavior in the last year of life increases suicide risk via 1993 National Mortality Followback Survey dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16046&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kan Chen, Ting Ye, Dylan S. Small</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The case$^2$ study, also referred to as the case-case study design, is a valuable approach for conducting inference for treatment effects. Unlike traditional case-control studies, the case$^2$ design compares treatment in two types of cases with the same disease. A key quantity of interest is the attributable effect, which is the number of cases of disease among treated units which are caused by the treatment. Two key assumptions that are usually made for making inferences about the attributable effect in case$^2$ studies are 1.) treatment does not cause the second type of case, and 2.) the treatment does not alter an individual’s case type. However, these assumptions are not realistic in many real-data applications. In this article, we present a sensitivity analysis framework to scrutinize the impact of deviations from these assumptions on obtained results. We also include sensitivity analyses related to the assumption of unmeasured confounding, recognizing the potential bias introduced by unobserved covariates. The proposed methodology is exemplified through an investigation into whether having violent behavior in the last year of life increases suicide risk via 1993 National Mortality Followback Survey dataset.</summary></entry><entry><title type="html">Single-seed generation of Brownian paths and integrals for adaptive and high order SDE solvers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SingleseedgenerationofBrownianpathsandintegralsforadaptiveandhighorderSDEsolvers.html" rel="alternate" type="text/html" title="Single-seed generation of Brownian paths and integrals for adaptive and high order SDE solvers" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SingleseedgenerationofBrownianpathsandintegralsforadaptiveandhighorderSDEsolvers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SingleseedgenerationofBrownianpathsandintegralsforadaptiveandhighorderSDEsolvers.html">&lt;p&gt;Despite the success of adaptive time-stepping in ODE simulation, it has so far seen few applications for Stochastic Differential Equations (SDEs). To simulate SDEs adaptively, methods such as the Virtual Brownian Tree (VBT) have been developed, which can generate Brownian motion (BM) non-chronologically. However, in most applications, knowing only the values of Brownian motion is not enough to achieve a high order of convergence; for that, we must compute time-integrals of BM such as $\int_s^t W_r \, dr$. With the aim of using high order SDE solvers adaptively, we extend the VBT to generate these integrals of BM in addition to the Brownian increments. A JAX-based implementation of our construction is included in the popular Diffrax library (https://github.com/patrick-kidger/diffrax).
  Since the entire Brownian path produced by VBT is uniquely determined by a single PRNG seed, previously generated samples need not be stored, which results in a constant memory footprint and enables experiment repeatability and strong error estimation. Based on binary search, the VBT’s time complexity is logarithmic in the tolerance parameter $\varepsilon$. Unlike the original VBT algorithm, which was only precise at some dyadic times, we prove that our construction exactly matches the joint distribution of the Brownian motion and its time integrals at any query times, provided they are at least $\varepsilon$ apart.
  We present two applications of adaptive high order solvers enabled by our new VBT. Using adaptive solvers to simulate a high-volatility CIR model, we achieve more than twice the convergence order of constant stepping. We apply an adaptive third order underdamped or kinetic Langevin solver to an MCMC problem, where our approach outperforms the No U-Turn Sampler, while using only a tenth of its function evaluations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06464&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andraž Jelinčič, James Foster, Patrick Kidger</name></author><category term="stat.CO" /><summary type="html">Despite the success of adaptive time-stepping in ODE simulation, it has so far seen few applications for Stochastic Differential Equations (SDEs). To simulate SDEs adaptively, methods such as the Virtual Brownian Tree (VBT) have been developed, which can generate Brownian motion (BM) non-chronologically. However, in most applications, knowing only the values of Brownian motion is not enough to achieve a high order of convergence; for that, we must compute time-integrals of BM such as $\int_s^t W_r \, dr$. With the aim of using high order SDE solvers adaptively, we extend the VBT to generate these integrals of BM in addition to the Brownian increments. A JAX-based implementation of our construction is included in the popular Diffrax library (https://github.com/patrick-kidger/diffrax). Since the entire Brownian path produced by VBT is uniquely determined by a single PRNG seed, previously generated samples need not be stored, which results in a constant memory footprint and enables experiment repeatability and strong error estimation. Based on binary search, the VBT’s time complexity is logarithmic in the tolerance parameter $\varepsilon$. Unlike the original VBT algorithm, which was only precise at some dyadic times, we prove that our construction exactly matches the joint distribution of the Brownian motion and its time integrals at any query times, provided they are at least $\varepsilon$ apart. We present two applications of adaptive high order solvers enabled by our new VBT. Using adaptive solvers to simulate a high-volatility CIR model, we achieve more than twice the convergence order of constant stepping. We apply an adaptive third order underdamped or kinetic Langevin solver to an MCMC problem, where our approach outperforms the No U-Turn Sampler, while using only a tenth of its function evaluations.</summary></entry><entry><title type="html">Subgradient Langevin Methods for Sampling from Non-smooth Potentials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SubgradientLangevinMethodsforSamplingfromNonsmoothPotentials.html" rel="alternate" type="text/html" title="Subgradient Langevin Methods for Sampling from Non-smooth Potentials" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SubgradientLangevinMethodsforSamplingfromNonsmoothPotentials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/SubgradientLangevinMethodsforSamplingfromNonsmoothPotentials.html">&lt;p&gt;This paper is concerned with sampling from probability distributions $\pi$ on $\mathbb{R}^d$ admitting a density of the form $\pi(x) \propto e^{-U(x)}$, where $U(x)=F(x)+G(Kx)$ with $K$ being a linear operator and $G$ being non-differentiable. Two different methods are proposed, both employing a subgradient step with respect to $G\circ K$, but, depending on the regularity of $F$, either an explicit or an implicit gradient step with respect to $F$ can be implemented. For both methods, non-asymptotic convergence proofs are provided, with improved convergence results for more regular $F$. Further, numerical experiments are conducted for simple 2D examples, illustrating the convergence rates, and for examples of Bayesian imaging, showing the practical feasibility of the proposed methods for high dimensional data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.01417&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andreas Habring, Martin Holler, Thomas Pock</name></author><category term="stat.CO" /><summary type="html">This paper is concerned with sampling from probability distributions $\pi$ on $\mathbb{R}^d$ admitting a density of the form $\pi(x) \propto e^{-U(x)}$, where $U(x)=F(x)+G(Kx)$ with $K$ being a linear operator and $G$ being non-differentiable. Two different methods are proposed, both employing a subgradient step with respect to $G\circ K$, but, depending on the regularity of $F$, either an explicit or an implicit gradient step with respect to $F$ can be implemented. For both methods, non-asymptotic convergence proofs are provided, with improved convergence results for more regular $F$. Further, numerical experiments are conducted for simple 2D examples, illustrating the convergence rates, and for examples of Bayesian imaging, showing the practical feasibility of the proposed methods for high dimensional data.</summary></entry><entry><title type="html">Sufficient dimension reduction for regression with metric space-valued responses</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Sufficientdimensionreductionforregressionwithmetricspacevaluedresponses.html" rel="alternate" type="text/html" title="Sufficient dimension reduction for regression with metric space-valued responses" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Sufficientdimensionreductionforregressionwithmetricspacevaluedresponses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Sufficientdimensionreductionforregressionwithmetricspacevaluedresponses.html">&lt;p&gt;Data visualization and dimension reduction for regression between a general metric space-valued response and Euclidean predictors is proposed. Current Fr&apos;ech&apos;et dimension reduction methods require that the response metric space be continuously embeddable into a Hilbert space, which imposes restriction on the type of metric and kernel choice. We relax this assumption by proposing a Euclidean embedding technique which avoids the use of kernels. Under this framework, classical dimension reduction methods such as ordinary least squares and sliced inverse regression are extended. An extensive simulation experiment demonstrates the superior performance of the proposed method on synthetic data compared to existing methods where applicable. The real data analysis of factors influencing the distribution of COVID-19 transmission in the U.S. and the association between BMI and structural brain connectivity of healthy individuals are also investigated.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.12402&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Abdul-Nasah Soale, Yuexiao Dong</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Data visualization and dimension reduction for regression between a general metric space-valued response and Euclidean predictors is proposed. Current Fr&apos;ech&apos;et dimension reduction methods require that the response metric space be continuously embeddable into a Hilbert space, which imposes restriction on the type of metric and kernel choice. We relax this assumption by proposing a Euclidean embedding technique which avoids the use of kernels. Under this framework, classical dimension reduction methods such as ordinary least squares and sliced inverse regression are extended. An extensive simulation experiment demonstrates the superior performance of the proposed method on synthetic data compared to existing methods where applicable. The real data analysis of factors influencing the distribution of COVID-19 transmission in the U.S. and the association between BMI and structural brain connectivity of healthy individuals are also investigated.</summary></entry><entry><title type="html">The Multi-Range Theory of Translation Quality Measurement: MQM scoring models and Statistical Quality Control</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TheMultiRangeTheoryofTranslationQualityMeasurementMQMscoringmodelsandStatisticalQualityControl.html" rel="alternate" type="text/html" title="The Multi-Range Theory of Translation Quality Measurement: MQM scoring models and Statistical Quality Control" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TheMultiRangeTheoryofTranslationQualityMeasurementMQMscoringmodelsandStatisticalQualityControl</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TheMultiRangeTheoryofTranslationQualityMeasurementMQMscoringmodelsandStatisticalQualityControl.html">&lt;p&gt;The year 2024 marks the 10th anniversary of the Multidimensional Quality Metrics (MQM) framework for analytic translation quality evaluation. The MQM error typology has been widely used by practitioners in the translation and localization industry and has served as the basis for many derivative projects. The annual Conference on Machine Translation (WMT) shared tasks on both human and automatic translation quality evaluations used the MQM error typology.
  The metric stands on two pillars: error typology and the scoring model. The scoring model calculates the quality score from annotation data, detailing how to convert error type and severity counts into numeric scores to determine if the content meets specifications. Previously, only the raw scoring model had been published. This April, the MQM Council published the Linear Calibrated Scoring Model, officially presented herein, along with the Non-Linear Scoring Model, which had not been published before.
  This paper details the latest MQM developments and presents a universal approach to translation quality measurement across three sample size ranges. It also explains why Statistical Quality Control should be used for very small sample sizes, starting from a single sentence.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16969&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arle Lommel, Serge Gladkoff, Alan Melby, Sue Ellen Wright, Ingemar Strandvik, Katerina Gasova, Angelika Vaasa, Andy Benzo, Romina Marazzato Sparano, Monica Faresi, Johani Innis, Lifeng Han, Goran Nenadic</name></author><category term="stat.AP" /><summary type="html">The year 2024 marks the 10th anniversary of the Multidimensional Quality Metrics (MQM) framework for analytic translation quality evaluation. The MQM error typology has been widely used by practitioners in the translation and localization industry and has served as the basis for many derivative projects. The annual Conference on Machine Translation (WMT) shared tasks on both human and automatic translation quality evaluations used the MQM error typology. The metric stands on two pillars: error typology and the scoring model. The scoring model calculates the quality score from annotation data, detailing how to convert error type and severity counts into numeric scores to determine if the content meets specifications. Previously, only the raw scoring model had been published. This April, the MQM Council published the Linear Calibrated Scoring Model, officially presented herein, along with the Non-Linear Scoring Model, which had not been published before. This paper details the latest MQM developments and presents a universal approach to translation quality measurement across three sample size ranges. It also explains why Statistical Quality Control should be used for very small sample sizes, starting from a single sentence.</summary></entry><entry><title type="html">The Multivariate Bernoulli detector: Change point estimation in discrete survival analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TheMultivariateBernoullidetectorChangepointestimationindiscretesurvivalanalysis.html" rel="alternate" type="text/html" title="The Multivariate Bernoulli detector: Change point estimation in discrete survival analysis" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TheMultivariateBernoullidetectorChangepointestimationindiscretesurvivalanalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TheMultivariateBernoullidetectorChangepointestimationindiscretesurvivalanalysis.html">&lt;p&gt;Time-to-event data are often recorded on a discrete scale with multiple, competing risks as potential causes for the event. In this context, application of continuous survival analysis methods with a single risk suffer from biased estimation. Therefore, we propose the Multivariate Bernoulli detector for competing risks with discrete times involving a multivariate change point model on the cause-specific baseline hazards. Through the prior on the number of change points and their location, we impose dependence between change points across risks, as well as allowing for data-driven learning of their number. Then, conditionally on these change points, a Multivariate Bernoulli prior is used to infer which risks are involved. Focus of posterior inference is cause-specific hazard rates and dependence across risks. Such dependence is often present due to subject-specific changes across time that affect all risks. Full posterior inference is performed through a tailored local-global Markov chain Monte Carlo (MCMC) algorithm, which exploits a data augmentation trick and MCMC updates from non-conjugate Bayesian nonparametric methods. We illustrate our model in simulations and on ICU data, comparing its performance with existing approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.10583&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Willem van den Boom, Maria De Iorio, Fang Qian, Alessandra Guglielmi</name></author><category term="stat.ME" /><summary type="html">Time-to-event data are often recorded on a discrete scale with multiple, competing risks as potential causes for the event. In this context, application of continuous survival analysis methods with a single risk suffer from biased estimation. Therefore, we propose the Multivariate Bernoulli detector for competing risks with discrete times involving a multivariate change point model on the cause-specific baseline hazards. Through the prior on the number of change points and their location, we impose dependence between change points across risks, as well as allowing for data-driven learning of their number. Then, conditionally on these change points, a Multivariate Bernoulli prior is used to infer which risks are involved. Focus of posterior inference is cause-specific hazard rates and dependence across risks. Such dependence is often present due to subject-specific changes across time that affect all risks. Full posterior inference is performed through a tailored local-global Markov chain Monte Carlo (MCMC) algorithm, which exploits a data augmentation trick and MCMC updates from non-conjugate Bayesian nonparametric methods. We illustrate our model in simulations and on ICU data, comparing its performance with existing approaches.</summary></entry><entry><title type="html">The Probability of Improved Prediction: a new concept in statistical inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TheProbabilityofImprovedPredictionanewconceptinstatisticalinference.html" rel="alternate" type="text/html" title="The Probability of Improved Prediction: a new concept in statistical inference" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TheProbabilityofImprovedPredictionanewconceptinstatisticalinference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TheProbabilityofImprovedPredictionanewconceptinstatisticalinference.html">&lt;p&gt;In an attempt to provide an answer to the increasing criticism against p-values and to bridge the gap between statistical inference and prediction modelling, we introduce the probability of improved prediction (PIP). In general, the PIP is a probabilistic measure for comparing two competing models. Three versions of the PIP and several estimators are introduced and the relationships between them, p-values and the mean squared error are investigated. The performance of the estimators is assessed in a simulation study. An application shows how the PIP can support p-values to strengthen the conclusions or possibly point at issues with e.g. replicability.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17064&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Olivier Thas, Stijn Jaspers</name></author><category term="stat.ME" /><summary type="html">In an attempt to provide an answer to the increasing criticism against p-values and to bridge the gap between statistical inference and prediction modelling, we introduce the probability of improved prediction (PIP). In general, the PIP is a probabilistic measure for comparing two competing models. Three versions of the PIP and several estimators are introduced and the relationships between them, p-values and the mean squared error are investigated. The performance of the estimators is assessed in a simulation study. An application shows how the PIP can support p-values to strengthen the conclusions or possibly point at issues with e.g. replicability.</summary></entry><entry><title type="html">Theoretical guarantees for lifted samplers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Theoreticalguaranteesforliftedsamplers.html" rel="alternate" type="text/html" title="Theoretical guarantees for lifted samplers" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Theoreticalguaranteesforliftedsamplers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Theoreticalguaranteesforliftedsamplers.html">&lt;p&gt;Lifted samplers form a class of Markov chain Monte Carlo methods which has drawn a lot attention in recent years due to superior performance in challenging Bayesian applications. A canonical example of such sampler is the one that is derived from a random walk Metropolis algorithm for a totally-ordered state space such as the integers or the real numbers. The lifted sampler is derived by splitting into two the proposal distribution: one part in the increasing direction, and the other part in the decreasing direction. It keeps following a direction, until a rejection, upon which it flips the direction. In terms of asymptotic variances, it outperforms the random walk Metropolis algorithm, regardless of the target distribution, at no additional computational cost. Other studies show, however, that beyond this simple case, lifted samplers do not always outperform their Metropolis counterparts. In this paper, we leverage the celebrated work of Tierney (1998) to provide an analysis in a general framework encompassing a broad class of lifted samplers. Our finding is that, essentially, the asymptotic variances cannot increase by a factor of more than 2, regardless of the target distribution, the way the directions are induced, and the type of algorithm from which the lifted sampler is derived (be it a Metropolis–Hastings algorithm, a reversible jump algorithm, etc.). This result indicates that, while there is potentially a lot to gain from lifting a sampler, there is not much to lose.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15952&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Philippe Gagnon, Florian Maire</name></author><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Lifted samplers form a class of Markov chain Monte Carlo methods which has drawn a lot attention in recent years due to superior performance in challenging Bayesian applications. A canonical example of such sampler is the one that is derived from a random walk Metropolis algorithm for a totally-ordered state space such as the integers or the real numbers. The lifted sampler is derived by splitting into two the proposal distribution: one part in the increasing direction, and the other part in the decreasing direction. It keeps following a direction, until a rejection, upon which it flips the direction. In terms of asymptotic variances, it outperforms the random walk Metropolis algorithm, regardless of the target distribution, at no additional computational cost. Other studies show, however, that beyond this simple case, lifted samplers do not always outperform their Metropolis counterparts. In this paper, we leverage the celebrated work of Tierney (1998) to provide an analysis in a general framework encompassing a broad class of lifted samplers. Our finding is that, essentially, the asymptotic variances cannot increase by a factor of more than 2, regardless of the target distribution, the way the directions are induced, and the type of algorithm from which the lifted sampler is derived (be it a Metropolis–Hastings algorithm, a reversible jump algorithm, etc.). This result indicates that, while there is potentially a lot to gain from lifting a sampler, there is not much to lose.</summary></entry><entry><title type="html">The state learner – a super learner for right-censored data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Thestatelearnerasuperlearnerforrightcensoreddata.html" rel="alternate" type="text/html" title="The state learner – a super learner for right-censored data" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Thestatelearnerasuperlearnerforrightcensoreddata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/Thestatelearnerasuperlearnerforrightcensoreddata.html">&lt;p&gt;In survival analysis, prediction models are needed as stand-alone tools and in applications of causal inference to estimate nuisance parameters. The super learner is a machine learning algorithm which combines a library of prediction models into a meta learner based on cross-validated loss. In right-censored data, the choice of the loss function and the estimation of the expected loss need careful consideration. We introduce the state learner, a new super learner for survival analysis, which simultaneously evaluates libraries of prediction models for the event of interest and the censoring distribution. The state learner can be applied to all types of survival models, works in the presence of competing risks, and does not require a single pre-specified estimator of the conditional censoring distribution. We establish an oracle inequality for the state learner and investigate its performance through numerical experiments. We illustrate the application of the state learner with prostate cancer data, as a stand-alone prediction tool, and, for causal inference, as a way to estimate the nuisance parameter models of a smooth statistical functional.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.17259&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anders Munch, Thomas A. Gerds</name></author><category term="stat.ME" /><summary type="html">In survival analysis, prediction models are needed as stand-alone tools and in applications of causal inference to estimate nuisance parameters. The super learner is a machine learning algorithm which combines a library of prediction models into a meta learner based on cross-validated loss. In right-censored data, the choice of the loss function and the estimation of the expected loss need careful consideration. We introduce the state learner, a new super learner for survival analysis, which simultaneously evaluates libraries of prediction models for the event of interest and the censoring distribution. The state learner can be applied to all types of survival models, works in the presence of competing risks, and does not require a single pre-specified estimator of the conditional censoring distribution. We establish an oracle inequality for the state learner and investigate its performance through numerical experiments. We illustrate the application of the state learner with prostate cancer data, as a stand-alone prediction tool, and, for causal inference, as a way to estimate the nuisance parameter models of a smooth statistical functional.</summary></entry><entry><title type="html">Towards Generalizing Inferences from Trials to Target Populations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TowardsGeneralizingInferencesfromTrialstoTargetPopulations.html" rel="alternate" type="text/html" title="Towards Generalizing Inferences from Trials to Target Populations" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TowardsGeneralizingInferencesfromTrialstoTargetPopulations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TowardsGeneralizingInferencesfromTrialstoTargetPopulations.html">&lt;p&gt;Randomized Controlled Trials (RCTs) are pivotal in generating internally valid estimates with minimal assumptions, serving as a cornerstone for researchers dedicated to advancing causal inference methods. However, extending these findings beyond the experimental cohort to achieve externally valid estimates is crucial for broader scientific inquiry. This paper delves into the forefront of addressing these external validity challenges, encapsulating the essence of a multidisciplinary workshop held at the Institute for Computational and Experimental Research in Mathematics (ICERM), Brown University, in Fall 2023. The workshop congregated experts from diverse fields including social science, medicine, public health, statistics, computer science, and education, to tackle the unique obstacles each discipline faces in extrapolating experimental findings. Our study presents three key contributions: we integrate ongoing efforts, highlighting methodological synergies across fields; provide an exhaustive review of generalizability and transportability based on the workshop’s discourse; and identify persistent hurdles while suggesting avenues for future research. By doing so, this paper aims to enhance the collective understanding of the generalizability and transportability of causal effects, fostering cross-disciplinary collaboration and offering valuable insights for researchers working on refining and applying causal inference methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.17042&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Melody Y Huang, Harsh Parikh</name></author><category term="stat.ME" /><summary type="html">Randomized Controlled Trials (RCTs) are pivotal in generating internally valid estimates with minimal assumptions, serving as a cornerstone for researchers dedicated to advancing causal inference methods. However, extending these findings beyond the experimental cohort to achieve externally valid estimates is crucial for broader scientific inquiry. This paper delves into the forefront of addressing these external validity challenges, encapsulating the essence of a multidisciplinary workshop held at the Institute for Computational and Experimental Research in Mathematics (ICERM), Brown University, in Fall 2023. The workshop congregated experts from diverse fields including social science, medicine, public health, statistics, computer science, and education, to tackle the unique obstacles each discipline faces in extrapolating experimental findings. Our study presents three key contributions: we integrate ongoing efforts, highlighting methodological synergies across fields; provide an exhaustive review of generalizability and transportability based on the workshop’s discourse; and identify persistent hurdles while suggesting avenues for future research. By doing so, this paper aims to enhance the collective understanding of the generalizability and transportability of causal effects, fostering cross-disciplinary collaboration and offering valuable insights for researchers working on refining and applying causal inference methods.</summary></entry><entry><title type="html">Transfer Learning Under High-Dimensional Graph Convolutional Regression Model for Node Classification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TransferLearningUnderHighDimensionalGraphConvolutionalRegressionModelforNodeClassification.html" rel="alternate" type="text/html" title="Transfer Learning Under High-Dimensional Graph Convolutional Regression Model for Node Classification" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TransferLearningUnderHighDimensionalGraphConvolutionalRegressionModelforNodeClassification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/TransferLearningUnderHighDimensionalGraphConvolutionalRegressionModelforNodeClassification.html">&lt;p&gt;Node classification is a fundamental task, but obtaining node classification labels can be challenging and expensive in many real-world scenarios. Transfer learning has emerged as a promising solution to address this challenge by leveraging knowledge from source domains to enhance learning in a target domain. Existing transfer learning methods for node classification primarily focus on integrating Graph Convolutional Networks (GCNs) with various transfer learning techniques. While these approaches have shown promising results, they often suffer from a lack of theoretical guarantees, restrictive conditions, and high sensitivity to hyperparameter choices. To overcome these limitations, we propose a Graph Convolutional Multinomial Logistic Regression (GCR) model and a transfer learning method based on the GCR model, called Trans-GCR. We provide theoretical guarantees of the estimate obtained under GCR model in high-dimensional settings. Moreover, Trans-GCR demonstrates superior empirical performance, has a low computational cost, and requires fewer hyperparameters than existing methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16672&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiachen Chen, Danyang Huang, Liyuan Wang, Kathryn L. Lunetta, Debarghya Mukherjee, Huimin Cheng</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Node classification is a fundamental task, but obtaining node classification labels can be challenging and expensive in many real-world scenarios. Transfer learning has emerged as a promising solution to address this challenge by leveraging knowledge from source domains to enhance learning in a target domain. Existing transfer learning methods for node classification primarily focus on integrating Graph Convolutional Networks (GCNs) with various transfer learning techniques. While these approaches have shown promising results, they often suffer from a lack of theoretical guarantees, restrictive conditions, and high sensitivity to hyperparameter choices. To overcome these limitations, we propose a Graph Convolutional Multinomial Logistic Regression (GCR) model and a transfer learning method based on the GCR model, called Trans-GCR. We provide theoretical guarantees of the estimate obtained under GCR model in high-dimensional settings. Moreover, Trans-GCR demonstrates superior empirical performance, has a low computational cost, and requires fewer hyperparameters than existing methods.</summary></entry><entry><title type="html">Uncertainty Learning for High-dimensional Mean-variance Portfolio</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/UncertaintyLearningforHighdimensionalMeanvariancePortfolio.html" rel="alternate" type="text/html" title="Uncertainty Learning for High-dimensional Mean-variance Portfolio" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/UncertaintyLearningforHighdimensionalMeanvariancePortfolio</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/UncertaintyLearningforHighdimensionalMeanvariancePortfolio.html">&lt;p&gt;Accounting for uncertainty in Data quality is important for accurate statistical inference. We aim to an optimal conservative allocation for a large universe of assets in mean-variance portfolio (MVP), which is the worst choice within uncertainty in data distribution. Unlike the low dimensional MVP studied in Blanchet et al. (2022, Management Science), the large number of assets raises a challenging problem in quantifying the uncertainty, due to the big deviation of the sample covariance matrix from the population version. To overcome this difficulty, we propose a data-adaptive method to quantify the uncertainty with the help of a factor structure. Monte-Carlo Simulation is conducted to show the superiority of our method in high-dimensional cases, that, avoiding the over-conservative results in Blanchet et al. (2022), our allocation is closer to the oracle version in terms of risk minimization and expected portfolio return controlling.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16989&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Han Lin Shang, Ruike Wu, Yanrong Yang</name></author><category term="stat.ME" /><summary type="html">Accounting for uncertainty in Data quality is important for accurate statistical inference. We aim to an optimal conservative allocation for a large universe of assets in mean-variance portfolio (MVP), which is the worst choice within uncertainty in data distribution. Unlike the low dimensional MVP studied in Blanchet et al. (2022, Management Science), the large number of assets raises a challenging problem in quantifying the uncertainty, due to the big deviation of the sample covariance matrix from the population version. To overcome this difficulty, we propose a data-adaptive method to quantify the uncertainty with the help of a factor structure. Monte-Carlo Simulation is conducted to show the superiority of our method in high-dimensional cases, that, avoiding the over-conservative results in Blanchet et al. (2022), our allocation is closer to the oracle version in terms of risk minimization and expected portfolio return controlling.</summary></entry><entry><title type="html">W-kernel and essential subspace for frequencist evaluation of Bayesian estimators</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/WkernelandessentialsubspaceforfrequencistevaluationofBayesianestimators.html" rel="alternate" type="text/html" title="W-kernel and essential subspace for frequencist evaluation of Bayesian estimators" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/WkernelandessentialsubspaceforfrequencistevaluationofBayesianestimators</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/WkernelandessentialsubspaceforfrequencistevaluationofBayesianestimators.html">&lt;p&gt;The posterior covariance matrix W defined by the log-likelihood of each observation plays important roles both in the sensitivity analysis and frequencist evaluation of the Bayesian estimators. This study is focused on the matrix W and its principal space; we term the latter as an essential subspace. A key tool for treating frequencist properties is the recently proposed Bayesian infinitesimal jackknife approximation (Giordano and Broderick (2023)). The matrix W can be interpreted as a reproducing kernel and is denoted as W-kernel. Using W-kernel, the essential subspace is expressed as a principal space given by the kernel principal component analysis. A relation to the Fisher kernel and neural tangent kernel is established, which elucidates the connection to the classical asymptotic theory. We also discuss a type of Bayesian-frequencist duality, which is naturally appeared from the kernel framework. Finally, two applications are discussed: the selection of a representative set of observations and dimensional reduction in the approximate bootstrap. In the former, incomplete Cholesky decomposition is introduced as an efficient method for computing the essential subspace. In the latter, different implementations of the approximate bootstrap for posterior means are compared.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.13017&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yukito Iba</name></author><category term="stat.ME," /><category term="cond-mat.stat-mech," /><category term="stat.ML" /><summary type="html">The posterior covariance matrix W defined by the log-likelihood of each observation plays important roles both in the sensitivity analysis and frequencist evaluation of the Bayesian estimators. This study is focused on the matrix W and its principal space; we term the latter as an essential subspace. A key tool for treating frequencist properties is the recently proposed Bayesian infinitesimal jackknife approximation (Giordano and Broderick (2023)). The matrix W can be interpreted as a reproducing kernel and is denoted as W-kernel. Using W-kernel, the essential subspace is expressed as a principal space given by the kernel principal component analysis. A relation to the Fisher kernel and neural tangent kernel is established, which elucidates the connection to the classical asymptotic theory. We also discuss a type of Bayesian-frequencist duality, which is naturally appeared from the kernel framework. Finally, two applications are discussed: the selection of a representative set of observations and dimensional reduction in the approximate bootstrap. In the former, incomplete Cholesky decomposition is introduced as an efficient method for computing the essential subspace. In the latter, different implementations of the approximate bootstrap for posterior means are compared.</summary></entry><entry><title type="html">Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ZerothOrderSamplingMethodsforNonLogConcaveDistributionsAlleviatingMetastabilitybyDenoisingDiffusion.html" rel="alternate" type="text/html" title="Zeroth-Order Sampling Methods for Non-Log-Concave Distributions: Alleviating Metastability by Denoising Diffusion" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ZerothOrderSamplingMethodsforNonLogConcaveDistributionsAlleviatingMetastabilitybyDenoisingDiffusion</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/ZerothOrderSamplingMethodsforNonLogConcaveDistributionsAlleviatingMetastabilitybyDenoisingDiffusion.html">&lt;p&gt;This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Diffusion Monte Carlo (DMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit still suffering from the curse of dimensionality. Consequently, for low dimensional distributions, ZOD-MC is a very efficient sampler, with performance exceeding latest samplers, including also-denoising-diffusion-based RDMC and RS-DMC. Last, we experimentally demonstrate the insensitivity of ZOD-MC to increasingly higher barriers between modes or discontinuity in non-convex potential.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.17886&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ye He, Kevin Rojas, Molei Tao</name></author><category term="stat.ML," /><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This paper considers the problem of sampling from non-logconcave distribution, based on queries of its unnormalized density. It first describes a framework, Diffusion Monte Carlo (DMC), based on the simulation of a denoising diffusion process with its score function approximated by a generic Monte Carlo estimator. DMC is an oracle-based meta-algorithm, where its oracle is the assumed access to samples that generate a Monte Carlo score estimator. Then we provide an implementation of this oracle, based on rejection sampling, and this turns DMC into a true algorithm, termed Zeroth-Order Diffusion Monte Carlo (ZOD-MC). We provide convergence analyses by first constructing a general framework, i.e. a performance guarantee for DMC, without assuming the target distribution to be log-concave or satisfying any isoperimetric inequality. Then we prove that ZOD-MC admits an inverse polynomial dependence on the desired sampling accuracy, albeit still suffering from the curse of dimensionality. Consequently, for low dimensional distributions, ZOD-MC is a very efficient sampler, with performance exceeding latest samplers, including also-denoising-diffusion-based RDMC and RS-DMC. Last, we experimentally demonstrate the insensitivity of ZOD-MC to increasingly higher barriers between modes or discontinuity in non-convex potential.</summary></entry><entry><title type="html">dynamite: An R Package for Dynamic Multivariate Panel Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/dynamiteAnRPackageforDynamicMultivariatePanelModels.html" rel="alternate" type="text/html" title="dynamite: An R Package for Dynamic Multivariate Panel Models" /><published>2024-05-28T00:00:00+00:00</published><updated>2024-05-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/dynamiteAnRPackageforDynamicMultivariatePanelModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/28/dynamiteAnRPackageforDynamicMultivariatePanelModels.html">&lt;p&gt;dynamite is an R package for Bayesian inference of intensive panel (time series) data comprising multiple measurements per multiple individuals measured in time. The package supports joint modeling of multiple response variables, time-varying and time-invariant effects, a wide range of discrete and continuous distributions, group-specific random effects, latent factors, and customization of prior distributions of the model parameters. Models in the package are defined via a user-friendly formula interface, and estimation of the posterior distribution of the model parameters takes advantage of state-of-the-art Markov chain Monte Carlo methods. The package enables efficient computation of both individual-level and summarized predictions and offers a comprehensive suite of tools for visualization and model diagnostics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.01607&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Santtu Tikka, Jouni Helske</name></author><category term="stat.ME" /><summary type="html">dynamite is an R package for Bayesian inference of intensive panel (time series) data comprising multiple measurements per multiple individuals measured in time. The package supports joint modeling of multiple response variables, time-varying and time-invariant effects, a wide range of discrete and continuous distributions, group-specific random effects, latent factors, and customization of prior distributions of the model parameters. Models in the package are defined via a user-friendly formula interface, and estimation of the posterior distribution of the model parameters takes advantage of state-of-the-art Markov chain Monte Carlo methods. The package enables efficient computation of both individual-level and summarized predictions and offers a comprehensive suite of tools for visualization and model diagnostics.</summary></entry></feed>