<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-01T07:16:57+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Bayesian Hybrid Design with Borrowing from Historical Study</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/ABayesianHybridDesignwithBorrowingfromHistoricalStudy.html" rel="alternate" type="text/html" title="A Bayesian Hybrid Design with Borrowing from Historical Study" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/ABayesianHybridDesignwithBorrowingfromHistoricalStudy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/ABayesianHybridDesignwithBorrowingfromHistoricalStudy.html">&lt;p&gt;In early phase drug development of combination therapy, the primary objective is to preliminarily assess whether there is additive activity when a novel agent combined with an established monotherapy. Due to potential feasibility issues with a large randomized study, uncontrolled single-arm trials have been the mainstream approach in cancer clinical trials. However, such trials often present significant challenges in deciding whether to proceed to the next phase of development. A hybrid design, leveraging data from a completed historical clinical study of the monotherapy, offers a valuable option to enhance study efficiency and improve informed decision-making. Compared to traditional single-arm designs, the hybrid design may significantly enhance power by borrowing external information, enabling a more robust assessment of activity. The primary challenge of hybrid design lies in handling information borrowing. We introduce a Bayesian dynamic power prior (DPP) framework with three components of controlling amount of dynamic borrowing. The framework offers flexible study design options with explicit interpretation of borrowing, allowing customization according to specific needs. Furthermore, the posterior distribution in the proposed framework has a closed form, offering significant advantages in computational efficiency. The proposed framework’s utility is demonstrated through simulations and a case study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.13177&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhaohua Lu, John Toso, Girma Ayele, Philip He</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In early phase drug development of combination therapy, the primary objective is to preliminarily assess whether there is additive activity when a novel agent combined with an established monotherapy. Due to potential feasibility issues with a large randomized study, uncontrolled single-arm trials have been the mainstream approach in cancer clinical trials. However, such trials often present significant challenges in deciding whether to proceed to the next phase of development. A hybrid design, leveraging data from a completed historical clinical study of the monotherapy, offers a valuable option to enhance study efficiency and improve informed decision-making. Compared to traditional single-arm designs, the hybrid design may significantly enhance power by borrowing external information, enabling a more robust assessment of activity. The primary challenge of hybrid design lies in handling information borrowing. We introduce a Bayesian dynamic power prior (DPP) framework with three components of controlling amount of dynamic borrowing. The framework offers flexible study design options with explicit interpretation of borrowing, allowing customization according to specific needs. Furthermore, the posterior distribution in the proposed framework has a closed form, offering significant advantages in computational efficiency. The proposed framework’s utility is demonstrated through simulations and a case study.</summary></entry><entry><title type="html">A Minimal Set of Parameters Based Depth-Dependent Distortion Model and Its Calibration Method for Stereo Vision Systems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/AMinimalSetofParametersBasedDepthDependentDistortionModelandItsCalibrationMethodforStereoVisionSystems.html" rel="alternate" type="text/html" title="A Minimal Set of Parameters Based Depth-Dependent Distortion Model and Its Calibration Method for Stereo Vision Systems" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/AMinimalSetofParametersBasedDepthDependentDistortionModelandItsCalibrationMethodforStereoVisionSystems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/AMinimalSetofParametersBasedDepthDependentDistortionModelandItsCalibrationMethodforStereoVisionSystems.html">&lt;p&gt;Depth position highly affects lens distortion, especially in close-range photography, which limits the measurement accuracy of existing stereo vision systems. Moreover, traditional depth-dependent distortion models and their calibration methods have remained complicated. In this work, we propose a minimal set of parameters based depth-dependent distortion model (MDM), which considers the radial and decentering distortions of the lens to improve the accuracy of stereo vision systems and simplify their calibration process. In addition, we present an easy and flexible calibration method for the MDM of stereo vision systems with a commonly used planar pattern, which requires cameras to observe the planar pattern in different orientations. The proposed technique is easy to use and flexible compared with classical calibration techniques for depth-dependent distortion models in which the lens must be perpendicular to the planar pattern. The experimental validation of the MDM and its calibration method showed that the MDM improved the calibration accuracy by 56.55% and 74.15% compared with the Li’s distortion model and traditional Brown’s distortion model. Besides, an iteration-based reconstruction method is proposed to iteratively estimate the depth information in the MDM during three-dimensional reconstruction. The results showed that the accuracy of the iteration-based reconstruction method was improved by 9.08% compared with that of the non-iteration reconstruction method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19242&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xin Ma, Puchen Zhu, Xiao Li, Xiaoyin Zheng, Jianshu Zhou, Xuchen Wang, Kwok Wai Samuel Au</name></author><category term="stat.ME" /><summary type="html">Depth position highly affects lens distortion, especially in close-range photography, which limits the measurement accuracy of existing stereo vision systems. Moreover, traditional depth-dependent distortion models and their calibration methods have remained complicated. In this work, we propose a minimal set of parameters based depth-dependent distortion model (MDM), which considers the radial and decentering distortions of the lens to improve the accuracy of stereo vision systems and simplify their calibration process. In addition, we present an easy and flexible calibration method for the MDM of stereo vision systems with a commonly used planar pattern, which requires cameras to observe the planar pattern in different orientations. The proposed technique is easy to use and flexible compared with classical calibration techniques for depth-dependent distortion models in which the lens must be perpendicular to the planar pattern. The experimental validation of the MDM and its calibration method showed that the MDM improved the calibration accuracy by 56.55% and 74.15% compared with the Li’s distortion model and traditional Brown’s distortion model. Besides, an iteration-based reconstruction method is proposed to iteratively estimate the depth information in the MDM during three-dimensional reconstruction. The results showed that the accuracy of the iteration-based reconstruction method was improved by 9.08% compared with that of the non-iteration reconstruction method.</summary></entry><entry><title type="html">A New Class of Realistic Spatio-Temporal Processes with Advection and Their Simulation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/ANewClassofRealisticSpatioTemporalProcesseswithAdvectionandTheirSimulation.html" rel="alternate" type="text/html" title="A New Class of Realistic Spatio-Temporal Processes with Advection and Their Simulation" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/ANewClassofRealisticSpatioTemporalProcesseswithAdvectionandTheirSimulation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/ANewClassofRealisticSpatioTemporalProcesseswithAdvectionandTheirSimulation.html">&lt;p&gt;Traveling phenomena, frequently observed in a variety of scientific disciplines including atmospheric science, seismography, and oceanography, have long suffered from limitations due to lack of realistic statistical modeling tools and simulation methods. Our work primarily addresses this, introducing more realistic and flexible models for spatio-temporal random fields. We break away from the traditional confines of the classic frozen field by either relaxing the assumption of a single deterministic velocity or rethinking the hypothesis regarding the spectrum shape, thus enhancing the realism of our models. While the proposed models stand out for their realism and flexibility, they are also paired with simulation algorithms that are equally or less computationally complex than the commonly used circulant embedding for Gaussian random fields in $\mathbb{R}^{2+1}$. This combination of realistic modeling with efficient simulation methods creates an effective solution for better understanding traveling phenomena.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.02756&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maria Laura Battagliola, Sofia Charlotta Olhede</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Traveling phenomena, frequently observed in a variety of scientific disciplines including atmospheric science, seismography, and oceanography, have long suffered from limitations due to lack of realistic statistical modeling tools and simulation methods. Our work primarily addresses this, introducing more realistic and flexible models for spatio-temporal random fields. We break away from the traditional confines of the classic frozen field by either relaxing the assumption of a single deterministic velocity or rethinking the hypothesis regarding the spectrum shape, thus enhancing the realism of our models. While the proposed models stand out for their realism and flexibility, they are also paired with simulation algorithms that are equally or less computationally complex than the commonly used circulant embedding for Gaussian random fields in $\mathbb{R}^{2+1}$. This combination of realistic modeling with efficient simulation methods creates an effective solution for better understanding traveling phenomena.</summary></entry><entry><title type="html">A model-free subdata selection method for classification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Amodelfreesubdataselectionmethodforclassification.html" rel="alternate" type="text/html" title="A model-free subdata selection method for classification" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Amodelfreesubdataselectionmethodforclassification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Amodelfreesubdataselectionmethodforclassification.html">&lt;p&gt;Subdata selection is a study of methods that select a small representative sample of the big data, the analysis of which is fast and statistically efficient. The existing subdata selection methods assume that the big data can be reasonably modeled using an underlying model, such as a (multinomial) logistic regression for classification problems. These methods work extremely well when the underlying modeling assumption is correct but often yield poor results otherwise. In this paper, we propose a model-free subdata selection method for classification problems, and the resulting subdata is called PED subdata. The PED subdata uses decision trees to find a partition of the data, followed by selecting an appropriate sample from each component of the partition. Random forests are used for analyzing the selected subdata. Our method can be employed for a general number of classes in the response and for both categorical and continuous predictors. We show analytically that the PED subdata results in a smaller Gini than a uniform subdata. Further, we demonstrate that the PED subdata has higher classification accuracy than other competing methods through extensive simulated and real datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19127&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rakhi Singh</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Subdata selection is a study of methods that select a small representative sample of the big data, the analysis of which is fast and statistically efficient. The existing subdata selection methods assume that the big data can be reasonably modeled using an underlying model, such as a (multinomial) logistic regression for classification problems. These methods work extremely well when the underlying modeling assumption is correct but often yield poor results otherwise. In this paper, we propose a model-free subdata selection method for classification problems, and the resulting subdata is called PED subdata. The PED subdata uses decision trees to find a partition of the data, followed by selecting an appropriate sample from each component of the partition. Random forests are used for analyzing the selected subdata. Our method can be employed for a general number of classes in the response and for both categorical and continuous predictors. We show analytically that the PED subdata results in a smaller Gini than a uniform subdata. Further, we demonstrate that the PED subdata has higher classification accuracy than other competing methods through extensive simulated and real datasets.</summary></entry><entry><title type="html">Analysis of Proximity Informed User Behavior in a Global Online Social Network</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/AnalysisofProximityInformedUserBehaviorinaGlobalOnlineSocialNetwork.html" rel="alternate" type="text/html" title="Analysis of Proximity Informed User Behavior in a Global Online Social Network" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/AnalysisofProximityInformedUserBehaviorinaGlobalOnlineSocialNetwork</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/AnalysisofProximityInformedUserBehaviorinaGlobalOnlineSocialNetwork.html">&lt;p&gt;Despite the earlier claim of “Death of Distance”, recent studies revealed that geographical proximity still greatly influences link formation in online social networks. However, it is unclear how physical distances are intertwined with users’ online behaviors in a virtual world. We study the role of spatial dependence on a global online social network with a dyadic Logit model. Results show country-specific patterns for distance effect on probabilities to build connections. Effects are stronger when the possibility for two people to meet in person exists. Relative to weak ties, dependence on proximity is looser for strong social ties.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18979&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nils Breitmar, Matthew C. Harding, Hanqiao Zhang</name></author><category term="stat.AP" /><summary type="html">Despite the earlier claim of “Death of Distance”, recent studies revealed that geographical proximity still greatly influences link formation in online social networks. However, it is unclear how physical distances are intertwined with users’ online behaviors in a virtual world. We study the role of spatial dependence on a global online social network with a dyadic Logit model. Results show country-specific patterns for distance effect on probabilities to build connections. Effects are stronger when the possibility for two people to meet in person exists. Relative to weak ties, dependence on proximity is looser for strong social ties.</summary></entry><entry><title type="html">Assessment of the quality of a prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Assessmentofthequalityofaprediction.html" rel="alternate" type="text/html" title="Assessment of the quality of a prediction" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Assessmentofthequalityofaprediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Assessmentofthequalityofaprediction.html">&lt;p&gt;Shannon defined the mutual information between two variables. We illustrate why the true mutual information between a variable and the predictions made by a prediction algorithm is not a suitable measure of prediction quality, but the apparent Shannon mutual information (ASI) is; indeed it is the unique prediction quality measure with either of two very different lists of desirable properties, as previously shown by de Finetti and other authors. However, estimating the uncertainty of the ASI is a difficult problem, because of long and non-symmetric heavy tails to the distribution of the individual values of $j(x,y)=\log\frac{Q_y(x)}{P(x)}$ We propose a Bayesian modelling method for the distribution of $j(x,y)$, from the posterior distribution of which the uncertainty in the ASI can be inferred. This method is based on Dirichlet-based mixtures of skew-Student distributions. We illustrate its use on data from a Bayesian model for prediction of the recurrence time of prostate cancer. We believe that this approach is generally appropriate for most problems, where it is infeasible to derive the explicit distribution of the samples of $j(x,y)$, though the precise modelling parameters may need adjustment to suit particular cases.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.15764&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roger Sewell, Elisabeth Crowe, Sharokh F. Shariat</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Shannon defined the mutual information between two variables. We illustrate why the true mutual information between a variable and the predictions made by a prediction algorithm is not a suitable measure of prediction quality, but the apparent Shannon mutual information (ASI) is; indeed it is the unique prediction quality measure with either of two very different lists of desirable properties, as previously shown by de Finetti and other authors. However, estimating the uncertainty of the ASI is a difficult problem, because of long and non-symmetric heavy tails to the distribution of the individual values of $j(x,y)=\log\frac{Q_y(x)}{P(x)}$ We propose a Bayesian modelling method for the distribution of $j(x,y)$, from the posterior distribution of which the uncertainty in the ASI can be inferred. This method is based on Dirichlet-based mixtures of skew-Student distributions. We illustrate its use on data from a Bayesian model for prediction of the recurrence time of prostate cancer. We believe that this approach is generally appropriate for most problems, where it is infeasible to derive the explicit distribution of the samples of $j(x,y)$, though the precise modelling parameters may need adjustment to suit particular cases.</summary></entry><entry><title type="html">Attacking Bayes: On the Adversarial Robustness of Bayesian Neural Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/AttackingBayesOntheAdversarialRobustnessofBayesianNeuralNetworks.html" rel="alternate" type="text/html" title="Attacking Bayes: On the Adversarial Robustness of Bayesian Neural Networks" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/AttackingBayesOntheAdversarialRobustnessofBayesianNeuralNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/AttackingBayesOntheAdversarialRobustnessofBayesianNeuralNetworks.html">&lt;p&gt;Adversarial examples have been shown to cause neural networks to fail on a wide range of vision and language tasks, but recent work has claimed that Bayesian neural networks (BNNs) are inherently robust to adversarial perturbations. In this work, we examine this claim. To study the adversarial robustness of BNNs, we investigate whether it is possible to successfully break state-of-the-art BNN inference methods and prediction pipelines using even relatively unsophisticated attacks for three tasks: (1) label prediction under the posterior predictive mean, (2) adversarial example detection with Bayesian predictive uncertainty, and (3) semantic shift detection. We find that BNNs trained with state-of-the-art approximate inference methods, and even BNNs trained with Hamiltonian Monte Carlo, are highly susceptible to adversarial attacks. We also identify various conceptual and experimental errors in previous works that claimed inherent adversarial robustness of BNNs and conclusively demonstrate that BNNs and uncertainty-aware Bayesian prediction pipelines are not inherently robust against adversarial attacks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19640&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yunzhen Feng, Tim G. J. Rudner, Nikolaos Tsilivis, Julia Kempe</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Adversarial examples have been shown to cause neural networks to fail on a wide range of vision and language tasks, but recent work has claimed that Bayesian neural networks (BNNs) are inherently robust to adversarial perturbations. In this work, we examine this claim. To study the adversarial robustness of BNNs, we investigate whether it is possible to successfully break state-of-the-art BNN inference methods and prediction pipelines using even relatively unsophisticated attacks for three tasks: (1) label prediction under the posterior predictive mean, (2) adversarial example detection with Bayesian predictive uncertainty, and (3) semantic shift detection. We find that BNNs trained with state-of-the-art approximate inference methods, and even BNNs trained with Hamiltonian Monte Carlo, are highly susceptible to adversarial attacks. We also identify various conceptual and experimental errors in previous works that claimed inherent adversarial robustness of BNNs and conclusively demonstrate that BNNs and uncertainty-aware Bayesian prediction pipelines are not inherently robust against adversarial attacks.</summary></entry><entry><title type="html">Causal Inference with Differentially Private (Clustered) Outcomes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/CausalInferencewithDifferentiallyPrivateClusteredOutcomes.html" rel="alternate" type="text/html" title="Causal Inference with Differentially Private (Clustered) Outcomes" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/CausalInferencewithDifferentiallyPrivateClusteredOutcomes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/CausalInferencewithDifferentiallyPrivateClusteredOutcomes.html">&lt;p&gt;Estimating causal effects from randomized experiments is only feasible if participants agree to reveal their potentially sensitive responses. Of the many ways of ensuring privacy, label differential privacy is a widely used measure of an algorithm’s privacy guarantee, which might encourage participants to share responses without running the risk of de-anonymization. Many differentially private mechanisms inject noise into the original data-set to achieve this privacy guarantee, which increases the variance of most statistical estimators and makes the precise measurement of causal effects difficult: there exists a fundamental privacy-variance trade-off to performing causal analyses from differentially private data. With the aim of achieving lower variance for stronger privacy guarantees, we suggest a new differential privacy mechanism, Cluster-DP, which leverages any given cluster structure of the data while still allowing for the estimation of causal effects. We show that, depending on an intuitive measure of cluster quality, we can improve the variance loss while maintaining our privacy guarantees. We compare its performance, theoretically and empirically, to that of its unclustered version and a more extreme uniform-prior version which does not use any of the original response distribution, both of which are special cases of the Cluster-DP algorithm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.00957&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Adel Javanmard, Vahab Mirrokni, Jean Pouget-Abadie</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Estimating causal effects from randomized experiments is only feasible if participants agree to reveal their potentially sensitive responses. Of the many ways of ensuring privacy, label differential privacy is a widely used measure of an algorithm’s privacy guarantee, which might encourage participants to share responses without running the risk of de-anonymization. Many differentially private mechanisms inject noise into the original data-set to achieve this privacy guarantee, which increases the variance of most statistical estimators and makes the precise measurement of causal effects difficult: there exists a fundamental privacy-variance trade-off to performing causal analyses from differentially private data. With the aim of achieving lower variance for stronger privacy guarantees, we suggest a new differential privacy mechanism, Cluster-DP, which leverages any given cluster structure of the data while still allowing for the estimation of causal effects. We show that, depending on an intuitive measure of cluster quality, we can improve the variance loss while maintaining our privacy guarantees. We compare its performance, theoretically and empirically, to that of its unclustered version and a more extreme uniform-prior version which does not use any of the original response distribution, both of which are special cases of the Cluster-DP algorithm.</summary></entry><entry><title type="html">Comparing Multivariate Distributions: A Novel Approach Using Optimal Transport-based Plots</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/ComparingMultivariateDistributionsANovelApproachUsingOptimalTransportbasedPlots.html" rel="alternate" type="text/html" title="Comparing Multivariate Distributions: A Novel Approach Using Optimal Transport-based Plots" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/ComparingMultivariateDistributionsANovelApproachUsingOptimalTransportbasedPlots</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/ComparingMultivariateDistributionsANovelApproachUsingOptimalTransportbasedPlots.html">&lt;p&gt;Quantile-Quantile (Q-Q) plots are widely used for assessing the distributional similarity between two datasets. Traditionally, Q-Q plots are constructed for univariate distributions, making them less effective in capturing complex dependencies present in multivariate data. In this paper, we propose a novel approach for constructing multivariate Q-Q plots, which extend the traditional Q-Q plot methodology to handle high-dimensional data. Our approach utilizes optimal transport (OT) and entropy-regularized optimal transport (EOT) to align the empirical quantiles of the two datasets. Additionally, we introduce another technique based on OT and EOT potentials which can effectively compare two multivariate datasets. Through extensive simulations and real data examples, we demonstrate the effectiveness of our proposed approach in capturing multivariate dependencies and identifying distributional differences such as tail behaviour. We also propose two test statistics based on the Q-Q and potential plots to compare two distributions rigorously.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19700&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sibsankar Singha, Marie Kratz, Sreekar Vadlamani</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Quantile-Quantile (Q-Q) plots are widely used for assessing the distributional similarity between two datasets. Traditionally, Q-Q plots are constructed for univariate distributions, making them less effective in capturing complex dependencies present in multivariate data. In this paper, we propose a novel approach for constructing multivariate Q-Q plots, which extend the traditional Q-Q plot methodology to handle high-dimensional data. Our approach utilizes optimal transport (OT) and entropy-regularized optimal transport (EOT) to align the empirical quantiles of the two datasets. Additionally, we introduce another technique based on OT and EOT potentials which can effectively compare two multivariate datasets. Through extensive simulations and real data examples, we demonstrate the effectiveness of our proposed approach in capturing multivariate dependencies and identifying distributional differences such as tail behaviour. We also propose two test statistics based on the Q-Q and potential plots to compare two distributions rigorously.</summary></entry><entry><title type="html">Correcting for confounding in longitudinal experiments: positioning non-linear mixed effects modeling as implementation of standardization using latent conditional exchangeability</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Correctingforconfoundinginlongitudinalexperimentspositioningnonlinearmixedeffectsmodelingasimplementationofstandardizationusinglatentconditionalexchangeability.html" rel="alternate" type="text/html" title="Correcting for confounding in longitudinal experiments: positioning non-linear mixed effects modeling as implementation of standardization using latent conditional exchangeability" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Correctingforconfoundinginlongitudinalexperimentspositioningnonlinearmixedeffectsmodelingasimplementationofstandardizationusinglatentconditionalexchangeability</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Correctingforconfoundinginlongitudinalexperimentspositioningnonlinearmixedeffectsmodelingasimplementationofstandardizationusinglatentconditionalexchangeability.html">&lt;p&gt;Non-linear mixed effects modeling and simulation (NLME M&amp;amp;S) is evaluated to be used for standardization with longitudinal data in presence of confounders. Standardization is a well-known method in causal inference to correct for confounding by analyzing and combining results from subgroups of patients. We show that non-linear mixed effects modeling is a particular implementation of standardization that conditions on individual parameters described by the random effects of the mixed effects model. Our motivation is that in pharmacometrics NLME M&amp;amp;S is routinely used to analyze clinical trials and to predict and compare potential outcomes of the same patient population under different treatment regimens. Such a comparison is a causal question sometimes referred to as causal prediction. Nonetheless, NLME M&amp;amp;S is rarely positioned as a method for causal prediction.
  As an example, a simulated clinical trial is used that assumes treatment confounder feedback in which early outcomes can cause deviations from the planned treatment schedule. Being interested in the outcome for the hypothetical situation that patients adhere to the planned treatment schedule, we put assumptions in a causal diagram. From the causal diagram, conditional independence assumptions are derived either using latent conditional exchangeability, conditioning on the individual parameters, or using sequential conditional exchangeability, conditioning on earlier outcomes. Both conditional independencies can be used to estimate the estimand of interest, e.g., with standardization, and they give unbiased estimates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19325&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Christian Bartels, Martina Scauda, Neva Coello, Thomas Dumortier, Bjoern Bornkamp, Giusi Moffa</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Non-linear mixed effects modeling and simulation (NLME M&amp;amp;S) is evaluated to be used for standardization with longitudinal data in presence of confounders. Standardization is a well-known method in causal inference to correct for confounding by analyzing and combining results from subgroups of patients. We show that non-linear mixed effects modeling is a particular implementation of standardization that conditions on individual parameters described by the random effects of the mixed effects model. Our motivation is that in pharmacometrics NLME M&amp;amp;S is routinely used to analyze clinical trials and to predict and compare potential outcomes of the same patient population under different treatment regimens. Such a comparison is a causal question sometimes referred to as causal prediction. Nonetheless, NLME M&amp;amp;S is rarely positioned as a method for causal prediction. As an example, a simulated clinical trial is used that assumes treatment confounder feedback in which early outcomes can cause deviations from the planned treatment schedule. Being interested in the outcome for the hypothetical situation that patients adhere to the planned treatment schedule, we put assumptions in a causal diagram. From the causal diagram, conditional independence assumptions are derived either using latent conditional exchangeability, conditioning on the individual parameters, or using sequential conditional exchangeability, conditioning on earlier outcomes. Both conditional independencies can be used to estimate the estimand of interest, e.g., with standardization, and they give unbiased estimates.</summary></entry><entry><title type="html">Data-adaptive structural change-point detection via isolation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Dataadaptivestructuralchangepointdetectionviaisolation.html" rel="alternate" type="text/html" title="Data-adaptive structural change-point detection via isolation" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Dataadaptivestructuralchangepointdetectionviaisolation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Dataadaptivestructuralchangepointdetectionviaisolation.html">&lt;p&gt;In this paper, a new data-adaptive method, called DAIS (Data Adaptive ISolation), is introduced for the estimation of the number and the location of change-points in a given data sequence. The proposed method can detect changes in various different signal structures; we focus on the examples of piecewise-constant and continuous, piecewise-linear signals. We highlight, however, that our algorithm can be extended to other frameworks, such as piecewise-quadratic signals. The data-adaptivity of our methodology lies in the fact that, at each step, and for the data under consideration, we search for the most prominent change-point in a targeted neighborhood of the data sequence that contains this change-point with high probability. Using a suitably chosen contrast function, the change-point will then get detected after being isolated in an interval. The isolation feature enhances estimation accuracy, while the data-adaptive nature of DAIS is advantageous regarding, mainly, computational complexity and accuracy. The simulation results presented indicate that DAIS is at least as accurate as state-of-the-art competitors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19344&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andreas Anastasiou, Sophia Loizidou</name></author><category term="stat.ME" /><summary type="html">In this paper, a new data-adaptive method, called DAIS (Data Adaptive ISolation), is introduced for the estimation of the number and the location of change-points in a given data sequence. The proposed method can detect changes in various different signal structures; we focus on the examples of piecewise-constant and continuous, piecewise-linear signals. We highlight, however, that our algorithm can be extended to other frameworks, such as piecewise-quadratic signals. The data-adaptivity of our methodology lies in the fact that, at each step, and for the data under consideration, we search for the most prominent change-point in a targeted neighborhood of the data sequence that contains this change-point with high probability. Using a suitably chosen contrast function, the change-point will then get detected after being isolated in an interval. The isolation feature enhances estimation accuracy, while the data-adaptive nature of DAIS is advantageous regarding, mainly, computational complexity and accuracy. The simulation results presented indicate that DAIS is at least as accurate as state-of-the-art competitors.</summary></entry><entry><title type="html">E-Valuating Classifier Two-Sample Tests</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/EValuatingClassifierTwoSampleTests.html" rel="alternate" type="text/html" title="E-Valuating Classifier Two-Sample Tests" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/EValuatingClassifierTwoSampleTests</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/EValuatingClassifierTwoSampleTests.html">&lt;p&gt;We introduce a powerful deep classifier two-sample test for high-dimensional data based on E-values, called E-value Classifier Two-Sample Test (E-C2ST). Our test combines ideas from existing work on split likelihood ratio tests and predictive independence tests. The resulting E-values are suitable for anytime-valid sequential two-sample tests. This feature allows for more effective use of data in constructing test statistics. Through simulations and real data applications, we empirically demonstrate that E-C2ST achieves enhanced statistical power by partitioning datasets into multiple batches beyond the conventional two-split (training and testing) approach of standard classifier two-sample tests. This strategy increases the power of the test while keeping the type I error well below the desired significance level.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.13027&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Teodora Pandeva, Tim Bakker, Christian A. Naesseth, Patrick Forré</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We introduce a powerful deep classifier two-sample test for high-dimensional data based on E-values, called E-value Classifier Two-Sample Test (E-C2ST). Our test combines ideas from existing work on split likelihood ratio tests and predictive independence tests. The resulting E-values are suitable for anytime-valid sequential two-sample tests. This feature allows for more effective use of data in constructing test statistics. Through simulations and real data applications, we empirically demonstrate that E-C2ST achieves enhanced statistical power by partitioning datasets into multiple batches beyond the conventional two-split (training and testing) approach of standard classifier two-sample tests. This strategy increases the power of the test while keeping the type I error well below the desired significance level.</summary></entry><entry><title type="html">Efficient Modeling of Spatial Extremes over Large Geographical Domains</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/EfficientModelingofSpatialExtremesoverLargeGeographicalDomains.html" rel="alternate" type="text/html" title="Efficient Modeling of Spatial Extremes over Large Geographical Domains" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/EfficientModelingofSpatialExtremesoverLargeGeographicalDomains</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/EfficientModelingofSpatialExtremesoverLargeGeographicalDomains.html">&lt;p&gt;Various natural phenomena exhibit spatial extremal dependence at short spatial distances. However, existing models proposed in the spatial extremes literature often assume that extremal dependence persists across the entire domain. This is a strong limitation when modeling extremes over large geographical domains, and yet it has been mostly overlooked in the literature. We here develop a more realistic Bayesian framework based on a novel Gaussian scale mixture model, with the Gaussian process component defined by a stochastic partial differential equation yielding a sparse precision matrix, and the random scale component modeled as a low-rank Pareto-tailed or Weibull-tailed spatial process determined by compactly-supported basis functions. We show that our proposed model is approximately tail-stationary and that it can capture a wide range of extremal dependence structures. Its inherently sparse structure allows fast Bayesian computations in high spatial dimensions based on a customized Markov chain Monte Carlo algorithm prioritizing calibration in the tail. We fit our model to analyze heavy monsoon rainfall data in Bangladesh. Our study shows that our model outperforms natural competitors and that it fits precipitation extremes well. We finally use the fitted model to draw inference on long-term return levels for marginal precipitation and spatial aggregates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.10248&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arnab Hazra, Raphaël Huser, David Bolin</name></author><category term="stat.ME" /><summary type="html">Various natural phenomena exhibit spatial extremal dependence at short spatial distances. However, existing models proposed in the spatial extremes literature often assume that extremal dependence persists across the entire domain. This is a strong limitation when modeling extremes over large geographical domains, and yet it has been mostly overlooked in the literature. We here develop a more realistic Bayesian framework based on a novel Gaussian scale mixture model, with the Gaussian process component defined by a stochastic partial differential equation yielding a sparse precision matrix, and the random scale component modeled as a low-rank Pareto-tailed or Weibull-tailed spatial process determined by compactly-supported basis functions. We show that our proposed model is approximately tail-stationary and that it can capture a wide range of extremal dependence structures. Its inherently sparse structure allows fast Bayesian computations in high spatial dimensions based on a customized Markov chain Monte Carlo algorithm prioritizing calibration in the tail. We fit our model to analyze heavy monsoon rainfall data in Bangladesh. Our study shows that our model outperforms natural competitors and that it fits precipitation extremes well. We finally use the fitted model to draw inference on long-term return levels for marginal precipitation and spatial aggregates.</summary></entry><entry><title type="html">Estimating Causal Effects with Double Machine Learning – A Method Evaluation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/EstimatingCausalEffectswithDoubleMachineLearningAMethodEvaluation.html" rel="alternate" type="text/html" title="Estimating Causal Effects with Double Machine Learning – A Method Evaluation" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/EstimatingCausalEffectswithDoubleMachineLearningAMethodEvaluation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/EstimatingCausalEffectswithDoubleMachineLearningAMethodEvaluation.html">&lt;p&gt;The estimation of causal effects with observational data continues to be a very active research area. In recent years, researchers have developed new frameworks which use machine learning to relax classical assumptions necessary for the estimation of causal effects. In this paper, we review one of the most prominent methods - “double/debiased machine learning” (DML) - and empirically evaluate it by comparing its performance on simulated data relative to more traditional statistical methods, before applying it to real-world data. Our findings indicate that the application of a suitably flexible machine learning algorithm within DML improves the adjustment for various nonlinear confounding relationships. This advantage enables a departure from traditional functional form assumptions typically necessary in causal effect estimation. However, we demonstrate that the method continues to critically depend on standard assumptions about causal structure and identification. When estimating the effects of air pollution on housing prices in our application, we find that DML estimates are consistently larger than estimates of less flexible methods. From our overall results, we provide actionable recommendations for specific choices researchers must make when applying DML in practice.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.14385&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jonathan Fuhr, Philipp Berens, Dominik Papies</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">The estimation of causal effects with observational data continues to be a very active research area. In recent years, researchers have developed new frameworks which use machine learning to relax classical assumptions necessary for the estimation of causal effects. In this paper, we review one of the most prominent methods - “double/debiased machine learning” (DML) - and empirically evaluate it by comparing its performance on simulated data relative to more traditional statistical methods, before applying it to real-world data. Our findings indicate that the application of a suitably flexible machine learning algorithm within DML improves the adjustment for various nonlinear confounding relationships. This advantage enables a departure from traditional functional form assumptions typically necessary in causal effect estimation. However, we demonstrate that the method continues to critically depend on standard assumptions about causal structure and identification. When estimating the effects of air pollution on housing prices in our application, we find that DML estimates are consistently larger than estimates of less flexible methods. From our overall results, we provide actionable recommendations for specific choices researchers must make when applying DML in practice.</summary></entry><entry><title type="html">Fast Adaptive Fourier Integration for Spectral Densities of Gaussian Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/FastAdaptiveFourierIntegrationforSpectralDensitiesofGaussianProcesses.html" rel="alternate" type="text/html" title="Fast Adaptive Fourier Integration for Spectral Densities of Gaussian Processes" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/FastAdaptiveFourierIntegrationforSpectralDensitiesofGaussianProcesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/FastAdaptiveFourierIntegrationforSpectralDensitiesofGaussianProcesses.html">&lt;p&gt;The specification of a covariance function is of paramount importance when employing Gaussian process models, but the requirement of positive definiteness severely limits those used in practice. Designing flexible stationary covariance functions is, however, straightforward in the spectral domain, where one needs only to supply a positive and symmetric spectral density. In this work, we introduce an adaptive integration framework for efficiently and accurately evaluating covariance functions and their derivatives at irregular locations directly from \textit{any} continuous, integrable spectral density. In order to make this approach computationally tractable, we employ high-order panel quadrature, the nonuniform fast Fourier transform, and a Nyquist-informed panel selection heuristic, and derive novel algebraic truncation error bounds which are used to monitor convergence. As a result, we demonstrate several orders of magnitude speedup compared to naive uniform quadrature approaches, allowing us to evaluate covariance functions from slowly decaying, singular spectral densities at millions of locations to a user-specified tolerance in seconds on a laptop. We then apply our methodology to perform gradient-based maximum likelihood estimation using a previously numerically infeasible long-memory spectral model for wind velocities below the atmospheric boundary layer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19053&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Paul G. Beckman, Christopher J. Geoga</name></author><category term="stat.CO" /><summary type="html">The specification of a covariance function is of paramount importance when employing Gaussian process models, but the requirement of positive definiteness severely limits those used in practice. Designing flexible stationary covariance functions is, however, straightforward in the spectral domain, where one needs only to supply a positive and symmetric spectral density. In this work, we introduce an adaptive integration framework for efficiently and accurately evaluating covariance functions and their derivatives at irregular locations directly from \textit{any} continuous, integrable spectral density. In order to make this approach computationally tractable, we employ high-order panel quadrature, the nonuniform fast Fourier transform, and a Nyquist-informed panel selection heuristic, and derive novel algebraic truncation error bounds which are used to monitor convergence. As a result, we demonstrate several orders of magnitude speedup compared to naive uniform quadrature approaches, allowing us to evaluate covariance functions from slowly decaying, singular spectral densities at millions of locations to a user-specified tolerance in seconds on a laptop. We then apply our methodology to perform gradient-based maximum likelihood estimation using a previously numerically infeasible long-memory spectral model for wind velocities below the atmospheric boundary layer.</summary></entry><entry><title type="html">Flexible Modeling of Nonstationary Extremal Dependence using Spatially-Fused LASSO and Ridge Penalties</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/FlexibleModelingofNonstationaryExtremalDependenceusingSpatiallyFusedLASSOandRidgePenalties.html" rel="alternate" type="text/html" title="Flexible Modeling of Nonstationary Extremal Dependence using Spatially-Fused LASSO and Ridge Penalties" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/FlexibleModelingofNonstationaryExtremalDependenceusingSpatiallyFusedLASSOandRidgePenalties</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/FlexibleModelingofNonstationaryExtremalDependenceusingSpatiallyFusedLASSOandRidgePenalties.html">&lt;p&gt;Statistical modeling of a nonstationary spatial extremal dependence structure is challenging. Max-stable processes are common choices for modeling spatially-indexed block maxima, where an assumption of stationarity is usual to make inference feasible. However, this assumption is often unrealistic for data observed over a large or complex domain. We propose a computationally-efficient method for estimating extremal dependence using a globally nonstationary, but locally-stationary, max-stable process by exploiting nonstationary kernel convolutions. We divide the spatial domain into a fine grid of subregions, assign each of them its own dependence parameters, and use LASSO ($L_1$) or ridge ($L_2$) penalties to obtain spatially-smooth parameter estimates. We then develop a novel data-driven algorithm to merge homogeneous neighboring subregions. The algorithm facilitates model parsimony and interpretability. To make our model suitable for high-dimensional data, we exploit a pairwise likelihood to draw inferences and discuss computational and statistical efficiency. An extensive simulation study demonstrates the superior performance of our proposed model and the subregion-merging algorithm over the approaches that either do not model nonstationarity or do not update the domain partition. We apply our proposed method to model monthly maximum temperatures at over 1400 sites in Nepal and the surrounding Himalayan and sub-Himalayan regions; we again observe significant improvements in model fit compared to a stationary process and a nonstationary process without subregion-merging. Furthermore, we demonstrate that the estimated merged partition is interpretable from a geographic perspective and leads to better model diagnostics by adequately reducing the number of subregion-specific parameters.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.05792&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xuanjie Shao, Arnab Hazra, Jordan Richards, Raphaël Huser</name></author><category term="stat.ME" /><summary type="html">Statistical modeling of a nonstationary spatial extremal dependence structure is challenging. Max-stable processes are common choices for modeling spatially-indexed block maxima, where an assumption of stationarity is usual to make inference feasible. However, this assumption is often unrealistic for data observed over a large or complex domain. We propose a computationally-efficient method for estimating extremal dependence using a globally nonstationary, but locally-stationary, max-stable process by exploiting nonstationary kernel convolutions. We divide the spatial domain into a fine grid of subregions, assign each of them its own dependence parameters, and use LASSO ($L_1$) or ridge ($L_2$) penalties to obtain spatially-smooth parameter estimates. We then develop a novel data-driven algorithm to merge homogeneous neighboring subregions. The algorithm facilitates model parsimony and interpretability. To make our model suitable for high-dimensional data, we exploit a pairwise likelihood to draw inferences and discuss computational and statistical efficiency. An extensive simulation study demonstrates the superior performance of our proposed model and the subregion-merging algorithm over the approaches that either do not model nonstationarity or do not update the domain partition. We apply our proposed method to model monthly maximum temperatures at over 1400 sites in Nepal and the surrounding Himalayan and sub-Himalayan regions; we again observe significant improvements in model fit compared to a stationary process and a nonstationary process without subregion-merging. Furthermore, we demonstrate that the estimated merged partition is interpretable from a geographic perspective and leads to better model diagnostics by adequately reducing the number of subregion-specific parameters.</summary></entry><entry><title type="html">Hierarchical Bayesian data selection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/HierarchicalBayesiandataselection.html" rel="alternate" type="text/html" title="Hierarchical Bayesian data selection" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/HierarchicalBayesiandataselection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/HierarchicalBayesiandataselection.html">&lt;p&gt;There are many issues that can cause problems when attempting to infer model parameters from data. Data and models are both imperfect, and as such there are multiple scenarios in which standard methods of inference will lead to misleading conclusions; corrupted data, models which are only representative of subsets of the data, or multiple regions in which the model is best fit using different parameters. Methods exist for the exclusion of some anomalous types of data, but in practice, data cleaning is often undertaken by hand before attempting to fit models to data. In this work, we will employ hierarchical Bayesian data selection; the simultaneous inference of both model parameters, and parameters which represent our belief that each observation within the data should be included in the inference. The aim, within a Bayesian setting, is to find the regions of observation space for which the model can well-represent the data, and to find the corresponding model parameters for those regions. A number of approaches will be explored, and applied to test problems in linear regression, and to the problem of fitting an ODE model, approximated by a finite difference method. The approaches are simple to implement, can aid mixing of Markov chains designed to sample from the arising densities, and are broadly applicable to many inferential problems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.03215&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Simon L. Cotter</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">There are many issues that can cause problems when attempting to infer model parameters from data. Data and models are both imperfect, and as such there are multiple scenarios in which standard methods of inference will lead to misleading conclusions; corrupted data, models which are only representative of subsets of the data, or multiple regions in which the model is best fit using different parameters. Methods exist for the exclusion of some anomalous types of data, but in practice, data cleaning is often undertaken by hand before attempting to fit models to data. In this work, we will employ hierarchical Bayesian data selection; the simultaneous inference of both model parameters, and parameters which represent our belief that each observation within the data should be included in the inference. The aim, within a Bayesian setting, is to find the regions of observation space for which the model can well-represent the data, and to find the corresponding model parameters for those regions. A number of approaches will be explored, and applied to test problems in linear regression, and to the problem of fitting an ODE model, approximated by a finite difference method. The approaches are simple to implement, can aid mixing of Markov chains designed to sample from the arising densities, and are broadly applicable to many inferential problems.</summary></entry><entry><title type="html">Identification and estimation of causal effects using non-concurrent controls in platform trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Identificationandestimationofcausaleffectsusingnonconcurrentcontrolsinplatformtrials.html" rel="alternate" type="text/html" title="Identification and estimation of causal effects using non-concurrent controls in platform trials" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Identificationandestimationofcausaleffectsusingnonconcurrentcontrolsinplatformtrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Identificationandestimationofcausaleffectsusingnonconcurrentcontrolsinplatformtrials.html">&lt;p&gt;Platform trials are multi-arm designs that simultaneously evaluate multiple treatments for a single disease within the same overall trial structure. Unlike traditional randomized controlled trials, they allow treatment arms to enter and exit the trial at distinct times while maintaining a control arm throughout. This control arm comprises both concurrent controls, where participants are randomized concurrently to either the treatment or control arm, and non-concurrent controls, who enter the trial when the treatment arm under study is unavailable. While flexible, platform trials introduce a unique challenge with the use of non-concurrent controls, raising questions about how to efficiently utilize their data to estimate treatment effects. Specifically, what estimands should be used to evaluate the causal effect of a treatment versus control? Under what assumptions can these estimands be identified and estimated? Do we achieve any efficiency gains? In this paper, we use structural causal models and counterfactuals to clarify estimands and formalize their identification in the presence of non-concurrent controls in platform trials. We also provide outcome regression, inverse probability weighting, and doubly robust estimators for their estimation. We discuss efficiency gains, demonstrate their performance in a simulation study, and apply them to the ACTT platform trial, resulting in a 20% improvement in precision.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19118&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michele Santacatterina, Federico Macchiavelli Giron, Xinyi Zhang, Ivan Diaz</name></author><category term="stat.ME" /><summary type="html">Platform trials are multi-arm designs that simultaneously evaluate multiple treatments for a single disease within the same overall trial structure. Unlike traditional randomized controlled trials, they allow treatment arms to enter and exit the trial at distinct times while maintaining a control arm throughout. This control arm comprises both concurrent controls, where participants are randomized concurrently to either the treatment or control arm, and non-concurrent controls, who enter the trial when the treatment arm under study is unavailable. While flexible, platform trials introduce a unique challenge with the use of non-concurrent controls, raising questions about how to efficiently utilize their data to estimate treatment effects. Specifically, what estimands should be used to evaluate the causal effect of a treatment versus control? Under what assumptions can these estimands be identified and estimated? Do we achieve any efficiency gains? In this paper, we use structural causal models and counterfactuals to clarify estimands and formalize their identification in the presence of non-concurrent controls in platform trials. We also provide outcome regression, inverse probability weighting, and doubly robust estimators for their estimation. We discuss efficiency gains, demonstrate their performance in a simulation study, and apply them to the ACTT platform trial, resulting in a 20% improvement in precision.</summary></entry><entry><title type="html">Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/IdentificationbynonGaussianityinstructuralthresholdandsmoothtransitionvectorautoregressivemodels.html" rel="alternate" type="text/html" title="Identification by non-Gaussianity in structural threshold and smooth transition vector autoregressive models" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/IdentificationbynonGaussianityinstructuralthresholdandsmoothtransitionvectorautoregressivemodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/IdentificationbynonGaussianityinstructuralthresholdandsmoothtransitionvectorautoregressivemodels.html">&lt;p&gt;Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian. We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes. Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy. In a structural logistic smooth transition vector autoregressive model consisting of two regimes, we find that a positive climate policy uncertainty shock decreases production in times of low economic policy uncertainty but slightly increases it in times of high economic policy uncertainty. The introduced methods are implemented to the accompanying R package sstvars.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19707&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Savi Virolainen</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Linear structural vector autoregressive models can be identified statistically without imposing restrictions on the model if the shocks are mutually independent and at most one of them is Gaussian. We show that this result extends to structural threshold and smooth transition vector autoregressive models incorporating a time-varying impact matrix defined as a weighted sum of the impact matrices of the regimes. Our empirical application studies the effects of the climate policy uncertainty shock on the U.S. macroeconomy. In a structural logistic smooth transition vector autoregressive model consisting of two regimes, we find that a positive climate policy uncertainty shock decreases production in times of low economic policy uncertainty but slightly increases it in times of high economic policy uncertainty. The introduced methods are implemented to the accompanying R package sstvars.</summary></entry><entry><title type="html">Lancaster correlation – a new dependence measure linked to maximum correlation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Lancastercorrelationanewdependencemeasurelinkedtomaximumcorrelation.html" rel="alternate" type="text/html" title="Lancaster correlation – a new dependence measure linked to maximum correlation" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Lancastercorrelationanewdependencemeasurelinkedtomaximumcorrelation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Lancastercorrelationanewdependencemeasurelinkedtomaximumcorrelation.html">&lt;p&gt;We suggest novel correlation coefficients which equal the maximum correlation for a class of bivariate Lancaster distributions while being only slightly smaller than maximum correlation for a variety of further bivariate distributions. In contrast to maximum correlation, however, our correlation coefficients allow for rank and moment-based estimators which are simple to compute and have tractable asymptotic distributions. Confidence intervals resulting from these asymptotic approximations and the covariance bootstrap show good finite-sample coverage. In a simulation, the power of asymptotic as well as permutation tests for independence based on our correlation measures compares favorably with competing methods based on distance correlation or rank coefficients for functional dependence, among others. Moreover, for the bivariate normal distribution, our correlation coefficients equal the absolute value of the Pearson correlation, an attractive feature for practitioners which is not shared by various competitors. We illustrate the practical usefulness of our methods in applications to two real data sets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.17872&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hajo Holzmann, Bernhard Klar</name></author><category term="stat.ME" /><summary type="html">We suggest novel correlation coefficients which equal the maximum correlation for a class of bivariate Lancaster distributions while being only slightly smaller than maximum correlation for a variety of further bivariate distributions. In contrast to maximum correlation, however, our correlation coefficients allow for rank and moment-based estimators which are simple to compute and have tractable asymptotic distributions. Confidence intervals resulting from these asymptotic approximations and the covariance bootstrap show good finite-sample coverage. In a simulation, the power of asymptotic as well as permutation tests for independence based on our correlation measures compares favorably with competing methods based on distance correlation or rank coefficients for functional dependence, among others. Moreover, for the bivariate normal distribution, our correlation coefficients equal the absolute value of the Pearson correlation, an attractive feature for practitioners which is not shared by various competitors. We illustrate the practical usefulness of our methods in applications to two real data sets.</summary></entry><entry><title type="html">Logistic regression with missing responses and predictors: a review of existing approaches and a case study</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Logisticregressionwithmissingresponsesandpredictorsareviewofexistingapproachesandacasestudy.html" rel="alternate" type="text/html" title="Logistic regression with missing responses and predictors: a review of existing approaches and a case study" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Logisticregressionwithmissingresponsesandpredictorsareviewofexistingapproachesandacasestudy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Logisticregressionwithmissingresponsesandpredictorsareviewofexistingapproachesandacasestudy.html">&lt;p&gt;In this work logistic regression when both the response and the predictor variables may be missing is considered. Several existing approaches are reviewed, including complete case analysis, inverse probability weighting, multiple imputation and maximum likelihood. The methods are compared in a simulation study, which serves to evaluate the bias, the variance and the mean squared error of the estimators for the regression coefficients. In the simulations, the maximum likelihood methodology is the one that presents the best results, followed by multiple imputation with five imputations, which is the second best. The methods are applied to a case study on the obesity for schoolchildren in the municipality of Viana do Castelo, North Portugal, where a logistic regression model is used to predict the International Obesity Task Force (IOTF) indicator from physical examinations and the past values of the obesity status. All the variables in the case study are potentially missing, with gender as the only exception. The results provided by the several methods are in well agreement, indicating the relevance of the past values of IOTF and physical scores for the prediction of obesity. Practical recommendations are given.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.03435&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Susana Rafaela Martins, Jacobo de Uña-Álvarez, María del Carmen Iglesias-Pérez</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In this work logistic regression when both the response and the predictor variables may be missing is considered. Several existing approaches are reviewed, including complete case analysis, inverse probability weighting, multiple imputation and maximum likelihood. The methods are compared in a simulation study, which serves to evaluate the bias, the variance and the mean squared error of the estimators for the regression coefficients. In the simulations, the maximum likelihood methodology is the one that presents the best results, followed by multiple imputation with five imputations, which is the second best. The methods are applied to a case study on the obesity for schoolchildren in the municipality of Viana do Castelo, North Portugal, where a logistic regression model is used to predict the International Obesity Task Force (IOTF) indicator from physical examinations and the past values of the obesity status. All the variables in the case study are potentially missing, with gender as the only exception. The results provided by the several methods are in well agreement, indicating the relevance of the past values of IOTF and physical scores for the prediction of obesity. Practical recommendations are given.</summary></entry><entry><title type="html">Multi-label Classification under Uncertainty: A Tree-based Conformal Prediction Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/MultilabelClassificationunderUncertaintyATreebasedConformalPredictionApproach.html" rel="alternate" type="text/html" title="Multi-label Classification under Uncertainty: A Tree-based Conformal Prediction Approach" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/MultilabelClassificationunderUncertaintyATreebasedConformalPredictionApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/MultilabelClassificationunderUncertaintyATreebasedConformalPredictionApproach.html">&lt;p&gt;Multi-label classification is a common challenge in various machine learning applications, where a single data instance can be associated with multiple classes simultaneously. The current paper proposes a novel tree-based method for multi-label classification using conformal prediction and multiple hypothesis testing. The proposed method employs hierarchical clustering with labelsets to develop a hierarchical tree, which is then formulated as a multiple-testing problem with a hierarchical structure. The split-conformal prediction method is used to obtain marginal conformal $p$-values for each tested hypothesis, and two \textit{hierarchical testing procedures} are developed based on marginal conformal $p$-values, including a hierarchical Bonferroni procedure and its modification for controlling the family-wise error rate. The prediction sets are thus formed based on the testing outcomes of these two procedures. We establish a theoretical guarantee of valid coverage for the prediction sets through proven family-wise error rate control of those two procedures. We demonstrate the effectiveness of our method in a simulation study and two real data analysis compared to other conformal methods for multi-label classification.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19472&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chhavi Tyagi, Wenge Guo</name></author><category term="stat.ME" /><summary type="html">Multi-label classification is a common challenge in various machine learning applications, where a single data instance can be associated with multiple classes simultaneously. The current paper proposes a novel tree-based method for multi-label classification using conformal prediction and multiple hypothesis testing. The proposed method employs hierarchical clustering with labelsets to develop a hierarchical tree, which is then formulated as a multiple-testing problem with a hierarchical structure. The split-conformal prediction method is used to obtain marginal conformal $p$-values for each tested hypothesis, and two \textit{hierarchical testing procedures} are developed based on marginal conformal $p$-values, including a hierarchical Bonferroni procedure and its modification for controlling the family-wise error rate. The prediction sets are thus formed based on the testing outcomes of these two procedures. We establish a theoretical guarantee of valid coverage for the prediction sets through proven family-wise error rate control of those two procedures. We demonstrate the effectiveness of our method in a simulation study and two real data analysis compared to other conformal methods for multi-label classification.</summary></entry><entry><title type="html">Multiscale scanning with nuisance parameters</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Multiscalescanningwithnuisanceparameters.html" rel="alternate" type="text/html" title="Multiscale scanning with nuisance parameters" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Multiscalescanningwithnuisanceparameters</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Multiscalescanningwithnuisanceparameters.html">&lt;p&gt;We develop a multiscale scanning method to find anomalies in a $d$-dimensional random field in the presence of nuisance parameters. This covers the common situation that either the baseline-level or additional parameters such as the variance are unknown and have to be estimated from the data. We argue that state of the art approaches to determine asymptotically correct critical values for multiscale scanning statistics will in general fail when such parameters are naively replaced by plug-in estimators. Opposed to this, we suggest to estimate the nuisance parameters on the largest scale and to use (only) smaller scales for multiscale scanning. We prove a uniform invariance principle for the resulting adjusted multiscale statistic (AMS), which is widely applicable and provides a computationally feasible way to simulate asymptotically correct critical values. We illustrate the implications of our theoretical results in a simulation study and in a real data example from super-resolution STED microscopy. This allows us to identify interesting regions inside a specimen in a pre-scan with controlled family-wise error rate.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.13301&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Claudia König, Axel Munk, Frank Werner</name></author><category term="stat.AP" /><summary type="html">We develop a multiscale scanning method to find anomalies in a $d$-dimensional random field in the presence of nuisance parameters. This covers the common situation that either the baseline-level or additional parameters such as the variance are unknown and have to be estimated from the data. We argue that state of the art approaches to determine asymptotically correct critical values for multiscale scanning statistics will in general fail when such parameters are naively replaced by plug-in estimators. Opposed to this, we suggest to estimate the nuisance parameters on the largest scale and to use (only) smaller scales for multiscale scanning. We prove a uniform invariance principle for the resulting adjusted multiscale statistic (AMS), which is widely applicable and provides a computationally feasible way to simulate asymptotically correct critical values. We illustrate the implications of our theoretical results in a simulation study and in a real data example from super-resolution STED microscopy. This allows us to identify interesting regions inside a specimen in a pre-scan with controlled family-wise error rate.</summary></entry><entry><title type="html">Optimal E-Values for Exponential Families: the Simple Case</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/OptimalEValuesforExponentialFamiliestheSimpleCase.html" rel="alternate" type="text/html" title="Optimal E-Values for Exponential Families: the Simple Case" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/OptimalEValuesforExponentialFamiliestheSimpleCase</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/OptimalEValuesforExponentialFamiliestheSimpleCase.html">&lt;p&gt;We provide a general condition under which e-variables in the form of a simple-vs.-simple likelihood ratio exist when the null hypothesis is a composite, multivariate exponential family. Such `simple’ e-variables are easy to compute and expected-log-optimal with respect to any stopping time. Simple e-variables were previously only known to exist in quite specific settings, but we offer a unifying theorem on their existence for testing exponential families. We start with a simple alternative $Q$ and a regular exponential family null. Together these induce a second exponential family ${\cal Q}$ containing $Q$, with the same sufficient statistic as the null. Our theorem shows that simple e-variables exist whenever the covariance matrices of ${\cal Q}$ and the null are in a certain relation. Examples in which this relation holds include some $k$-sample tests, Gaussian location- and scale tests, and tests for more general classes of natural exponential families.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19465&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Peter Grünwald, Tyron Lardy, Yunda Hao, Shaul K. Bar-Lev, Martijn de Jong</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We provide a general condition under which e-variables in the form of a simple-vs.-simple likelihood ratio exist when the null hypothesis is a composite, multivariate exponential family. Such `simple’ e-variables are easy to compute and expected-log-optimal with respect to any stopping time. Simple e-variables were previously only known to exist in quite specific settings, but we offer a unifying theorem on their existence for testing exponential families. We start with a simple alternative $Q$ and a regular exponential family null. Together these induce a second exponential family ${\cal Q}$ containing $Q$, with the same sufficient statistic as the null. Our theorem shows that simple e-variables exist whenever the covariance matrices of ${\cal Q}$ and the null are in a certain relation. Examples in which this relation holds include some $k$-sample tests, Gaussian location- and scale tests, and tests for more general classes of natural exponential families.</summary></entry><entry><title type="html">Orthogonal Bootstrap: Efficient Simulation of Input Uncertainty</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/OrthogonalBootstrapEfficientSimulationofInputUncertainty.html" rel="alternate" type="text/html" title="Orthogonal Bootstrap: Efficient Simulation of Input Uncertainty" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/OrthogonalBootstrapEfficientSimulationofInputUncertainty</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/OrthogonalBootstrapEfficientSimulationofInputUncertainty.html">&lt;p&gt;Bootstrap is a popular methodology for simulating input uncertainty. However, it can be computationally expensive when the number of samples is large. We propose a new approach called \textbf{Orthogonal Bootstrap} that reduces the number of required Monte Carlo replications. We decomposes the target being simulated into two parts: the \textit{non-orthogonal part} which has a closed-form result known as Infinitesimal Jackknife and the \textit{orthogonal part} which is easier to be simulated. We theoretically and numerically show that Orthogonal Bootstrap significantly reduces the computational cost of Bootstrap while improving empirical accuracy and maintaining the same width of the constructed interval.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19145&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kaizhao Liu, Jose Blanchet, Lexing Ying, Yiping Lu</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Bootstrap is a popular methodology for simulating input uncertainty. However, it can be computationally expensive when the number of samples is large. We propose a new approach called \textbf{Orthogonal Bootstrap} that reduces the number of required Monte Carlo replications. We decomposes the target being simulated into two parts: the \textit{non-orthogonal part} which has a closed-form result known as Infinitesimal Jackknife and the \textit{orthogonal part} which is easier to be simulated. We theoretically and numerically show that Orthogonal Bootstrap significantly reduces the computational cost of Bootstrap while improving empirical accuracy and maintaining the same width of the constructed interval.</summary></entry><entry><title type="html">PCA for Point Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/PCAforPointProcesses.html" rel="alternate" type="text/html" title="PCA for Point Processes" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/PCAforPointProcesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/PCAforPointProcesses.html">&lt;p&gt;We introduce a novel statistical framework for the analysis of replicated point processes that allows for the study of point pattern variability at a population level. By treating point process realizations as random measures, we adopt a functional analysis perspective and propose a form of functional Principal Component Analysis (fPCA) for point processes. The originality of our method is to base our analysis on the cumulative mass functions of the random measures which gives us a direct and interpretable analysis. Key theoretical contributions include establishing a Karhunen-Lo`{e}ve expansion for the random measures and a Mercer Theorem for covariance measures. We establish convergence in a strong sense, and introduce the concept of principal measures, which can be seen as latent processes governing the dynamics of the observed point patterns. We propose an easy-to-implement estimation strategy of eigenelements for which parametric rates are achieved. We fully characterize the solutions of our approach to Poisson and Hawkes processes and validate our methodology via simulations and diverse applications in seismology, single-cell biology and neurosiences, demonstrating its versatility and effectiveness. Our method is implemented in the pppca R-package.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19661&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Franck Picard, Vincent Rivoirard, Angelina Roche, Victor Panaretos</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We introduce a novel statistical framework for the analysis of replicated point processes that allows for the study of point pattern variability at a population level. By treating point process realizations as random measures, we adopt a functional analysis perspective and propose a form of functional Principal Component Analysis (fPCA) for point processes. The originality of our method is to base our analysis on the cumulative mass functions of the random measures which gives us a direct and interpretable analysis. Key theoretical contributions include establishing a Karhunen-Lo`{e}ve expansion for the random measures and a Mercer Theorem for covariance measures. We establish convergence in a strong sense, and introduce the concept of principal measures, which can be seen as latent processes governing the dynamics of the observed point patterns. We propose an easy-to-implement estimation strategy of eigenelements for which parametric rates are achieved. We fully characterize the solutions of our approach to Poisson and Hawkes processes and validate our methodology via simulations and diverse applications in seismology, single-cell biology and neurosiences, demonstrating its versatility and effectiveness. Our method is implemented in the pppca R-package.</summary></entry><entry><title type="html">Percentage Coefficient (bp) – Effect Size Analysis (Theory Paper 1)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/PercentageCoefficientbpEffectSizeAnalysisTheoryPaper1.html" rel="alternate" type="text/html" title="Percentage Coefficient (bp) – Effect Size Analysis (Theory Paper 1)" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/PercentageCoefficientbpEffectSizeAnalysisTheoryPaper1</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/PercentageCoefficientbpEffectSizeAnalysisTheoryPaper1.html">&lt;p&gt;Percentage coefficient (bp) has emerged in recent publications as an additional and alternative estimator of effect size for regression analysis. This paper retraces the theory behind the estimator. It’s posited that an estimator must first serve the fundamental function of enabling researchers and readers to comprehend an estimand, the target of estimation. It may then serve the instrumental function of enabling researchers and readers to compare two or more estimands. Defined as the regression coefficient when dependent variable (DV) and independent variable (IV) are both on conceptual 0-1 percentage scales, percentage coefficients (bp) feature 1) clearly comprehendible interpretation and 2) equitable scales for comparison. Thus, the coefficient (bp) serves both functions effectively and efficiently, thereby serving some needs not completely served by other indicators such as raw coefficient (bw) and standardized beta. Another fundamental premise of the functionalist theory is that “effect” is not a monolithic concept. Rather, it is a collection of compartments, each of which measures a component of the conglomerate that we call “effect.” A regression coefficient (b), for example, measures one aspect of effect, which is unit effect, aka efficiency, as it indicates the unit change in DV associated with a one-unit increase in IV. Percentage coefficient (bp) indicates the change in DV in percentage points associated with a whole scale increase in IV. It is meant to be an all-encompassing indicator of the all-encompassing concept of effect, but rather an interpretable and comparable indicator of efficiency, one of the key components of effect.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19495&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinshu Zhao , Dianshi Moses Li , Ze Zack Lai , Piper Liping Liu , Song Harris Ao , Fei You</name></author><category term="stat.AP," /><category term="stat.ME," /><category term="stat.OT" /><summary type="html">Percentage coefficient (bp) has emerged in recent publications as an additional and alternative estimator of effect size for regression analysis. This paper retraces the theory behind the estimator. It’s posited that an estimator must first serve the fundamental function of enabling researchers and readers to comprehend an estimand, the target of estimation. It may then serve the instrumental function of enabling researchers and readers to compare two or more estimands. Defined as the regression coefficient when dependent variable (DV) and independent variable (IV) are both on conceptual 0-1 percentage scales, percentage coefficients (bp) feature 1) clearly comprehendible interpretation and 2) equitable scales for comparison. Thus, the coefficient (bp) serves both functions effectively and efficiently, thereby serving some needs not completely served by other indicators such as raw coefficient (bw) and standardized beta. Another fundamental premise of the functionalist theory is that “effect” is not a monolithic concept. Rather, it is a collection of compartments, each of which measures a component of the conglomerate that we call “effect.” A regression coefficient (b), for example, measures one aspect of effect, which is unit effect, aka efficiency, as it indicates the unit change in DV associated with a one-unit increase in IV. Percentage coefficient (bp) indicates the change in DV in percentage points associated with a whole scale increase in IV. It is meant to be an all-encompassing indicator of the all-encompassing concept of effect, but rather an interpretable and comparable indicator of efficiency, one of the key components of effect.</summary></entry><entry><title type="html">Price Optimization Combining Conjoint Data and Purchase History: A Causal Modeling Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/PriceOptimizationCombiningConjointDataandPurchaseHistoryACausalModelingApproach.html" rel="alternate" type="text/html" title="Price Optimization Combining Conjoint Data and Purchase History: A Causal Modeling Approach" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/PriceOptimizationCombiningConjointDataandPurchaseHistoryACausalModelingApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/PriceOptimizationCombiningConjointDataandPurchaseHistoryACausalModelingApproach.html">&lt;p&gt;Pricing decisions of companies require an understanding of the causal effect of a price change on the demand. When real-life pricing experiments are infeasible, data-driven decision-making must be based on alternative data sources such as purchase history (sales data) and conjoint studies where a group of customers is asked to make imaginary purchases in an artificial setup. We present an approach for price optimization that combines population statistics, purchase history and conjoint data in a systematic way. We build on the recent advances in causal inference to identify and quantify the effect of price on the purchase probability at the customer level. The identification task is a transportability problem whose solution requires a parametric assumption on the differences between the conjoint study and real purchases. The causal effect is estimated using Bayesian methods that take into account the uncertainty of the data sources. The pricing decision is made by comparing the estimated posterior distributions of gross profit for different prices. The approach is demonstrated with simulated data resembling the features of real-world data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.16660&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lauri Valkonen, Santtu Tikka, Jouni Helske, Juha Karvanen</name></author><category term="stat.AP" /><summary type="html">Pricing decisions of companies require an understanding of the causal effect of a price change on the demand. When real-life pricing experiments are infeasible, data-driven decision-making must be based on alternative data sources such as purchase history (sales data) and conjoint studies where a group of customers is asked to make imaginary purchases in an artificial setup. We present an approach for price optimization that combines population statistics, purchase history and conjoint data in a systematic way. We build on the recent advances in causal inference to identify and quantify the effect of price on the purchase probability at the customer level. The identification task is a transportability problem whose solution requires a parametric assumption on the differences between the conjoint study and real purchases. The causal effect is estimated using Bayesian methods that take into account the uncertainty of the data sources. The pricing decision is made by comparing the estimated posterior distributions of gross profit for different prices. The approach is demonstrated with simulated data resembling the features of real-world data.</summary></entry><entry><title type="html">Regularized Estimation of Sparse Spectral Precision Matrices</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/RegularizedEstimationofSparseSpectralPrecisionMatrices.html" rel="alternate" type="text/html" title="Regularized Estimation of Sparse Spectral Precision Matrices" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/RegularizedEstimationofSparseSpectralPrecisionMatrices</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/RegularizedEstimationofSparseSpectralPrecisionMatrices.html">&lt;p&gt;Spectral precision matrix, the inverse of a spectral density matrix, is an object of central interest in frequency-domain analysis of multivariate time series. Estimation of spectral precision matrix is a key step in calculating partial coherency and graphical model selection of stationary time series. When the dimension of a multivariate time series is moderate to large, traditional estimators of spectral density matrices such as averaged periodograms tend to be severely ill-conditioned, and one needs to resort to suitable regularization strategies involving optimization over complex variables.
  In this work, we propose complex graphical Lasso (CGLASSO), an $\ell_1$-penalized estimator of spectral precision matrix based on local Whittle likelihood maximization. We develop fast $\textit{pathwise coordinate descent}$ algorithms for implementing CGLASSO on large dimensional time series data sets. At its core, our algorithmic development relies on a ring isomorphism between complex and real matrices that helps map a number of optimization problems over complex variables to similar optimization problems over real variables. This finding may be of independent interest and more broadly applicable for high-dimensional statistical analysis with complex-valued data. We also present a complete non-asymptotic theory of our proposed estimator which shows that consistent estimation is possible in high-dimensional regime as long as the underlying spectral precision matrix is suitably sparse. We compare the performance of CGLASSO with competing alternatives on simulated data sets, and use it to construct partial coherence network among brain regions from a real fMRI data set.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.11128&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Navonil Deb, Amy Kuceyeski, Sumanta Basu</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Spectral precision matrix, the inverse of a spectral density matrix, is an object of central interest in frequency-domain analysis of multivariate time series. Estimation of spectral precision matrix is a key step in calculating partial coherency and graphical model selection of stationary time series. When the dimension of a multivariate time series is moderate to large, traditional estimators of spectral density matrices such as averaged periodograms tend to be severely ill-conditioned, and one needs to resort to suitable regularization strategies involving optimization over complex variables. In this work, we propose complex graphical Lasso (CGLASSO), an $\ell_1$-penalized estimator of spectral precision matrix based on local Whittle likelihood maximization. We develop fast $\textit{pathwise coordinate descent}$ algorithms for implementing CGLASSO on large dimensional time series data sets. At its core, our algorithmic development relies on a ring isomorphism between complex and real matrices that helps map a number of optimization problems over complex variables to similar optimization problems over real variables. This finding may be of independent interest and more broadly applicable for high-dimensional statistical analysis with complex-valued data. We also present a complete non-asymptotic theory of our proposed estimator which shows that consistent estimation is possible in high-dimensional regime as long as the underlying spectral precision matrix is suitably sparse. We compare the performance of CGLASSO with competing alternatives on simulated data sets, and use it to construct partial coherence network among brain regions from a real fMRI data set.</summary></entry><entry><title type="html">Shifting the Paradigm: Estimating Heterogeneous Treatment Effects in the Development of Walkable Cities Design</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/ShiftingtheParadigmEstimatingHeterogeneousTreatmentEffectsintheDevelopmentofWalkableCitiesDesign.html" rel="alternate" type="text/html" title="Shifting the Paradigm: Estimating Heterogeneous Treatment Effects in the Development of Walkable Cities Design" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/ShiftingtheParadigmEstimatingHeterogeneousTreatmentEffectsintheDevelopmentofWalkableCitiesDesign</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/ShiftingtheParadigmEstimatingHeterogeneousTreatmentEffectsintheDevelopmentofWalkableCitiesDesign.html">&lt;p&gt;The transformation of urban environments to accommodate growing populations has profoundly impacted public health and well-being. This paper addresses the critical challenge of estimating the impact of urban design interventions on diverse populations. Traditional approaches, reliant on questionnaires and stated preference techniques, are limited by recall bias and capturing the complex dynamics between environmental attributes and individual characteristics. To address these challenges, we integrate Virtual Reality (VR) with observational causal inference methods to estimate heterogeneous treatment effects, specifically employing Targeted Maximum Likelihood Estimation (TMLE) for its robustness against model misspecification. Our innovative approach leverages VR-based experiment to collect data that reflects perceptual and experiential factors. The result shows the heterogeneous impacts of urban design elements on public health and underscore the necessity for personalized urban design interventions. This study not only extends the application of TMLE to built environment research but also informs public health policy by illuminating the nuanced effects of urban design on mental well-being and advocating for tailored strategies that foster equitable, health-promoting urban spaces.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.08208&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jie Zhu, Bojing Liao</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">The transformation of urban environments to accommodate growing populations has profoundly impacted public health and well-being. This paper addresses the critical challenge of estimating the impact of urban design interventions on diverse populations. Traditional approaches, reliant on questionnaires and stated preference techniques, are limited by recall bias and capturing the complex dynamics between environmental attributes and individual characteristics. To address these challenges, we integrate Virtual Reality (VR) with observational causal inference methods to estimate heterogeneous treatment effects, specifically employing Targeted Maximum Likelihood Estimation (TMLE) for its robustness against model misspecification. Our innovative approach leverages VR-based experiment to collect data that reflects perceptual and experiential factors. The result shows the heterogeneous impacts of urban design elements on public health and underscore the necessity for personalized urban design interventions. This study not only extends the application of TMLE to built environment research but also informs public health policy by illuminating the nuanced effects of urban design on mental well-being and advocating for tailored strategies that foster equitable, health-promoting urban spaces.</summary></entry><entry><title type="html">Sparse Interaction Neighborhood Selection for Markov Random Fields via Reversible Jump and Pseudoposteriors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/SparseInteractionNeighborhoodSelectionforMarkovRandomFieldsviaReversibleJumpandPseudoposteriors.html" rel="alternate" type="text/html" title="Sparse Interaction Neighborhood Selection for Markov Random Fields via Reversible Jump and Pseudoposteriors" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/SparseInteractionNeighborhoodSelectionforMarkovRandomFieldsviaReversibleJumpandPseudoposteriors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/SparseInteractionNeighborhoodSelectionforMarkovRandomFieldsviaReversibleJumpandPseudoposteriors.html">&lt;p&gt;We consider the problem of estimating the interacting neighborhood of a Markov Random Field model with finite support and homogeneous pairwise interactions based on relative positions of a two-dimensional lattice. Using a Bayesian framework, we propose a Reversible Jump Monte Carlo Markov Chain algorithm that jumps across subsets of a maximal range neighborhood, allowing us to perform model selection based on a marginal pseudoposterior distribution of models. To show the strength of our proposed methodology we perform a simulation study and apply it to a real dataset from a discrete texture image analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2204.05933&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Victor Freguglia, Nancy Lopes Garcia</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">We consider the problem of estimating the interacting neighborhood of a Markov Random Field model with finite support and homogeneous pairwise interactions based on relative positions of a two-dimensional lattice. Using a Bayesian framework, we propose a Reversible Jump Monte Carlo Markov Chain algorithm that jumps across subsets of a maximal range neighborhood, allowing us to perform model selection based on a marginal pseudoposterior distribution of models. To show the strength of our proposed methodology we perform a simulation study and apply it to a real dataset from a discrete texture image analysis.</summary></entry><entry><title type="html">Stochastic Graph Heat Modelling for Diffusion-based Connectivity Retrieval</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/StochasticGraphHeatModellingforDiffusionbasedConnectivityRetrieval.html" rel="alternate" type="text/html" title="Stochastic Graph Heat Modelling for Diffusion-based Connectivity Retrieval" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/StochasticGraphHeatModellingforDiffusionbasedConnectivityRetrieval</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/StochasticGraphHeatModellingforDiffusionbasedConnectivityRetrieval.html">&lt;p&gt;Heat diffusion describes the process by which heat flows from areas with higher temperatures to ones with lower temperatures. This concept was previously adapted to graph structures, whereby heat flows between nodes of a graph depending on the graph topology. Here, we combine the graph heat equation with the stochastic heat equation, which ultimately yields a model for multivariate time signals on a graph. We show theoretically how the model can be used to directly compute the diffusion-based connectivity structure from multivariate signals. Unlike other connectivity measures, our heat model-based approach is inherently multivariate and yields an absolute scaling factor, namely the graph thermal diffusivity, which captures the extent of heat-like graph propagation in the data. On two datasets, we show how the graph thermal diffusivity can be used to characterise Alzheimer’s disease (AD). We find that the graph thermal diffusivity is lower for AD patients than healthy controls and correlates with mini mental state examination (MMSE) scores, suggesting structural impairment in patients in line with previous findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.12785&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Stephan Goerttler, Fei He, Min Wu</name></author><category term="stat.ME" /><summary type="html">Heat diffusion describes the process by which heat flows from areas with higher temperatures to ones with lower temperatures. This concept was previously adapted to graph structures, whereby heat flows between nodes of a graph depending on the graph topology. Here, we combine the graph heat equation with the stochastic heat equation, which ultimately yields a model for multivariate time signals on a graph. We show theoretically how the model can be used to directly compute the diffusion-based connectivity structure from multivariate signals. Unlike other connectivity measures, our heat model-based approach is inherently multivariate and yields an absolute scaling factor, namely the graph thermal diffusivity, which captures the extent of heat-like graph propagation in the data. On two datasets, we show how the graph thermal diffusivity can be used to characterise Alzheimer’s disease (AD). We find that the graph thermal diffusivity is lower for AD patients than healthy controls and correlates with mini mental state examination (MMSE) scores, suggesting structural impairment in patients in line with previous findings.</summary></entry><entry><title type="html">The Impact of COVID-19 on Co-authorship and Economics Scholars’ Productivity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/TheImpactofCOVID19onCoauthorshipandEconomicsScholarsProductivity.html" rel="alternate" type="text/html" title="The Impact of COVID-19 on Co-authorship and Economics Scholars’ Productivity" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/TheImpactofCOVID19onCoauthorshipandEconomicsScholarsProductivity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/TheImpactofCOVID19onCoauthorshipandEconomicsScholarsProductivity.html">&lt;p&gt;The COVID-19 pandemic has disrupted traditional academic collaboration patterns, prompting a unique opportunity to analyze the influence of peer effects and coauthorship dynamics on research output. Using a novel dataset, this paper endeavors to make a first cut at investigating the role of peer effects on the productivity of economics scholars, measured by the number of publications, in both pre-pandemic and pandemic times. Results show that peer effect is significant for the pre-pandemic time but not for the pandemic time. The findings contribute to our understanding of how research collaboration influences knowledge production and may help guide policies aimed at fostering collaboration and enhancing research productivity in the academic community.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18980&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hanqiao Zhang, Joy D. Xiuyao Yang</name></author><category term="stat.AP" /><summary type="html">The COVID-19 pandemic has disrupted traditional academic collaboration patterns, prompting a unique opportunity to analyze the influence of peer effects and coauthorship dynamics on research output. Using a novel dataset, this paper endeavors to make a first cut at investigating the role of peer effects on the productivity of economics scholars, measured by the number of publications, in both pre-pandemic and pandemic times. Results show that peer effect is significant for the pre-pandemic time but not for the pandemic time. The findings contribute to our understanding of how research collaboration influences knowledge production and may help guide policies aimed at fostering collaboration and enhancing research productivity in the academic community.</summary></entry><entry><title type="html">The harms of class imbalance corrections for machine learning based prediction models: a simulation study</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Theharmsofclassimbalancecorrectionsformachinelearningbasedpredictionmodelsasimulationstudy.html" rel="alternate" type="text/html" title="The harms of class imbalance corrections for machine learning based prediction models: a simulation study" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Theharmsofclassimbalancecorrectionsformachinelearningbasedpredictionmodelsasimulationstudy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Theharmsofclassimbalancecorrectionsformachinelearningbasedpredictionmodelsasimulationstudy.html">&lt;p&gt;Risk prediction models are increasingly used in healthcare to aid in clinical decision making. In most clinical contexts, model calibration (i.e., assessing the reliability of risk estimates) is critical. Data available for model development are often not perfectly balanced with respect to the modeled outcome (i.e., individuals with vs. without the event of interest are not equally represented in the data). It is common for researchers to correct this class imbalance, yet, the effect of such imbalance corrections on the calibration of machine learning models is largely unknown. We studied the effect of imbalance corrections on model calibration for a variety of machine learning algorithms. Using extensive Monte Carlo simulations we compared the out-of-sample predictive performance of models developed with an imbalance correction to those developed without a correction for class imbalance across different data-generating scenarios (varying sample size, the number of predictors and event fraction). Our findings were illustrated in a case study using MIMIC-III data. In all simulation scenarios, prediction models developed without a correction for class imbalance consistently had equal or better calibration performance than prediction models developed with a correction for class imbalance. The miscalibration introduced by correcting for class imbalance was characterized by an over-estimation of risk and was not always able to be corrected with re-calibration. Correcting for class imbalance is not always necessary and may even be harmful for clinical prediction models which aim to produce reliable risk estimates on an individual basis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19494&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alex Carriero, Kim Luijken, Anne de Hond, Karel GM Moons, Ben van Calster, Maarten van Smeden</name></author><category term="stat.ME" /><summary type="html">Risk prediction models are increasingly used in healthcare to aid in clinical decision making. In most clinical contexts, model calibration (i.e., assessing the reliability of risk estimates) is critical. Data available for model development are often not perfectly balanced with respect to the modeled outcome (i.e., individuals with vs. without the event of interest are not equally represented in the data). It is common for researchers to correct this class imbalance, yet, the effect of such imbalance corrections on the calibration of machine learning models is largely unknown. We studied the effect of imbalance corrections on model calibration for a variety of machine learning algorithms. Using extensive Monte Carlo simulations we compared the out-of-sample predictive performance of models developed with an imbalance correction to those developed without a correction for class imbalance across different data-generating scenarios (varying sample size, the number of predictors and event fraction). Our findings were illustrated in a case study using MIMIC-III data. In all simulation scenarios, prediction models developed without a correction for class imbalance consistently had equal or better calibration performance than prediction models developed with a correction for class imbalance. The miscalibration introduced by correcting for class imbalance was characterized by an over-estimation of risk and was not always able to be corrected with re-calibration. Correcting for class imbalance is not always necessary and may even be harmful for clinical prediction models which aim to produce reliable risk estimates on an individual basis.</summary></entry><entry><title type="html">Transportability of model-based estimands in evidence synthesis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Transportabilityofmodelbasedestimandsinevidencesynthesis.html" rel="alternate" type="text/html" title="Transportability of model-based estimands in evidence synthesis" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Transportabilityofmodelbasedestimandsinevidencesynthesis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Transportabilityofmodelbasedestimandsinevidencesynthesis.html">&lt;p&gt;In evidence synthesis, effect modifiers are typically described as variables that induce treatment effect heterogeneity at the individual level, through treatment-covariate interactions in an outcome model parametrized at such level. As such, effect modification is defined with respect to a conditional measure, but marginal effect estimates are required for population-level decisions in health technology assessment. For non-collapsible measures, purely prognostic variables that are not determinants of treatment response at the individual level may modify marginal effects, even where there is individual-level treatment effect homogeneity. With heterogeneity, marginal effects for measures that are not directly collapsible cannot be expressed in terms of marginal covariate moments, and generally depend on the joint distribution of conditional effect measure modifiers and purely prognostic variables. There are implications for recommended practices in evidence synthesis. Unadjusted anchored indirect comparisons can be biased in the absence of individual-level treatment effect heterogeneity, or when marginal covariate moments are balanced across studies. Covariate adjustment may be necessary to account for cross-study imbalances in joint covariate distributions involving purely prognostic variables. In the absence of individual patient data for the target, covariate adjustment approaches are inherently limited in their ability to remove bias for measures that are not directly collapsible. Directly collapsible measures would facilitate the transportability of marginal effects between studies by: (1) reducing dependence on model-based covariate adjustment where there is individual-level treatment effect homogeneity or marginal covariate moments are balanced; and (2) facilitating the selection of baseline covariates for adjustment where there is individual-level treatment effect heterogeneity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.01757&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Antonio Remiro-Azócar</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In evidence synthesis, effect modifiers are typically described as variables that induce treatment effect heterogeneity at the individual level, through treatment-covariate interactions in an outcome model parametrized at such level. As such, effect modification is defined with respect to a conditional measure, but marginal effect estimates are required for population-level decisions in health technology assessment. For non-collapsible measures, purely prognostic variables that are not determinants of treatment response at the individual level may modify marginal effects, even where there is individual-level treatment effect homogeneity. With heterogeneity, marginal effects for measures that are not directly collapsible cannot be expressed in terms of marginal covariate moments, and generally depend on the joint distribution of conditional effect measure modifiers and purely prognostic variables. There are implications for recommended practices in evidence synthesis. Unadjusted anchored indirect comparisons can be biased in the absence of individual-level treatment effect heterogeneity, or when marginal covariate moments are balanced across studies. Covariate adjustment may be necessary to account for cross-study imbalances in joint covariate distributions involving purely prognostic variables. In the absence of individual patient data for the target, covariate adjustment approaches are inherently limited in their ability to remove bias for measures that are not directly collapsible. Directly collapsible measures would facilitate the transportability of marginal effects between studies by: (1) reducing dependence on model-based covariate adjustment where there is individual-level treatment effect homogeneity or marginal covariate moments are balanced; and (2) facilitating the selection of baseline covariates for adjustment where there is individual-level treatment effect heterogeneity.</summary></entry><entry><title type="html">Variational approximations of possibilistic inferential models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Variationalapproximationsofpossibilisticinferentialmodels.html" rel="alternate" type="text/html" title="Variational approximations of possibilistic inferential models" /><published>2024-05-01T00:00:00+00:00</published><updated>2024-05-01T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Variationalapproximationsofpossibilisticinferentialmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/01/Variationalapproximationsofpossibilisticinferentialmodels.html">&lt;p&gt;Inferential models (IMs) offer reliable, data-driven, possibilistic statistical inference. But despite IMs’ theoretical/foundational advantages, efficient computation in applications is a major challenge. This paper presents a simple and apparently powerful Monte Carlo-driven strategy for approximating the IM’s possibility contour, or at least its $\alpha$-level set for a specified $\alpha$. Our proposal utilizes a parametric family that, in a certain sense, approximately covers the credal set associated with the IM’s possibility measure, which is reminiscent of variational approximations now widely used in Bayesian statistics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19224&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Leonardo Cella, Ryan Martin</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Inferential models (IMs) offer reliable, data-driven, possibilistic statistical inference. But despite IMs’ theoretical/foundational advantages, efficient computation in applications is a major challenge. This paper presents a simple and apparently powerful Monte Carlo-driven strategy for approximating the IM’s possibility contour, or at least its $\alpha$-level set for a specified $\alpha$. Our proposal utilizes a parametric family that, in a certain sense, approximately covers the credal set associated with the IM’s possibility measure, which is reminiscent of variational approximations now widely used in Bayesian statistics.</summary></entry></feed>