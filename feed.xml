<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-02T07:14:15+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Bayesian joint longitudinal-survival model with a latent stochastic process for intensive longitudinal data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ABayesianjointlongitudinalsurvivalmodelwithalatentstochasticprocessforintensivelongitudinaldata.html" rel="alternate" type="text/html" title="A Bayesian joint longitudinal-survival model with a latent stochastic process for intensive longitudinal data" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ABayesianjointlongitudinalsurvivalmodelwithalatentstochasticprocessforintensivelongitudinaldata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ABayesianjointlongitudinalsurvivalmodelwithalatentstochasticprocessforintensivelongitudinaldata.html">&lt;p&gt;The availability of mobile health (mHealth) technology has enabled increased collection of intensive longitudinal data (ILD). ILD have potential to capture rapid fluctuations in outcomes that may be associated with changes in the risk of an event. However, existing methods for jointly modeling longitudinal and event-time outcomes are not well-equipped to handle ILD due to the high computational cost. We propose a joint longitudinal and time-to-event model suitable for analyzing ILD. In this model, we summarize a multivariate longitudinal outcome as a smaller number of time-varying latent factors. These latent factors, which are modeled using an Ornstein-Uhlenbeck stochastic process, capture the risk of a time-to-event outcome in a parametric hazard model. We take a Bayesian approach to fit our joint model and conduct simulations to assess its performance. We use it to analyze data from an mHealth study of smoking cessation. We summarize the longitudinal self-reported intensity of nine emotions as the psychological states of positive and negative affect. These time-varying latent states capture the risk of the first smoking lapse after attempted quit. Understanding factors associated with smoking lapse is of keen interest to smoking cessation researchers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00179&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Madeline R. Abbott, Walter H. Dempsey, Inbal Nahum-Shani, Lindsey N. Potter, David W. Wetter, Cho Y. Lam, Jeremy M. G. Taylor</name></author><category term="stat.ME" /><summary type="html">The availability of mobile health (mHealth) technology has enabled increased collection of intensive longitudinal data (ILD). ILD have potential to capture rapid fluctuations in outcomes that may be associated with changes in the risk of an event. However, existing methods for jointly modeling longitudinal and event-time outcomes are not well-equipped to handle ILD due to the high computational cost. We propose a joint longitudinal and time-to-event model suitable for analyzing ILD. In this model, we summarize a multivariate longitudinal outcome as a smaller number of time-varying latent factors. These latent factors, which are modeled using an Ornstein-Uhlenbeck stochastic process, capture the risk of a time-to-event outcome in a parametric hazard model. We take a Bayesian approach to fit our joint model and conduct simulations to assess its performance. We use it to analyze data from an mHealth study of smoking cessation. We summarize the longitudinal self-reported intensity of nine emotions as the psychological states of positive and negative affect. These time-varying latent states capture the risk of the first smoking lapse after attempted quit. Understanding factors associated with smoking lapse is of keen interest to smoking cessation researchers.</summary></entry><entry><title type="html">A Minimal Set of Parameters Based Depth-Dependent Distortion Model and Its Calibration Method for Stereo Vision Systems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/AMinimalSetofParametersBasedDepthDependentDistortionModelandItsCalibrationMethodforStereoVisionSystems.html" rel="alternate" type="text/html" title="A Minimal Set of Parameters Based Depth-Dependent Distortion Model and Its Calibration Method for Stereo Vision Systems" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/AMinimalSetofParametersBasedDepthDependentDistortionModelandItsCalibrationMethodforStereoVisionSystems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/AMinimalSetofParametersBasedDepthDependentDistortionModelandItsCalibrationMethodforStereoVisionSystems.html">&lt;p&gt;Depth position highly affects lens distortion, especially in close-range photography, which limits the measurement accuracy of existing stereo vision systems. Moreover, traditional depth-dependent distortion models and their calibration methods have remained complicated. In this work, we propose a minimal set of parameters based depth-dependent distortion model (MDM), which considers the radial and decentering distortions of the lens to improve the accuracy of stereo vision systems and simplify their calibration process. In addition, we present an easy and flexible calibration method for the MDM of stereo vision systems with a commonly used planar pattern, which requires cameras to observe the planar pattern in different orientations. The proposed technique is easy to use and flexible compared with classical calibration techniques for depth-dependent distortion models in which the lens must be perpendicular to the planar pattern. The experimental validation of the MDM and its calibration method showed that the MDM improved the calibration accuracy by 56.55% and 74.15% compared with the Li’s distortion model and traditional Brown’s distortion model. Besides, an iteration-based reconstruction method is proposed to iteratively estimate the depth information in the MDM during three-dimensional reconstruction. The results showed that the accuracy of the iteration-based reconstruction method was improved by 9.08% compared with that of the non-iteration reconstruction method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19242&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xin Ma, Puchen Zhu, Xiao Li, Xiaoyin Zheng, Jianshu Zhou, Xuchen Wang, Kwok Wai Samuel Au</name></author><category term="stat.ME" /><summary type="html">Depth position highly affects lens distortion, especially in close-range photography, which limits the measurement accuracy of existing stereo vision systems. Moreover, traditional depth-dependent distortion models and their calibration methods have remained complicated. In this work, we propose a minimal set of parameters based depth-dependent distortion model (MDM), which considers the radial and decentering distortions of the lens to improve the accuracy of stereo vision systems and simplify their calibration process. In addition, we present an easy and flexible calibration method for the MDM of stereo vision systems with a commonly used planar pattern, which requires cameras to observe the planar pattern in different orientations. The proposed technique is easy to use and flexible compared with classical calibration techniques for depth-dependent distortion models in which the lens must be perpendicular to the planar pattern. The experimental validation of the MDM and its calibration method showed that the MDM improved the calibration accuracy by 56.55% and 74.15% compared with the Li’s distortion model and traditional Brown’s distortion model. Besides, an iteration-based reconstruction method is proposed to iteratively estimate the depth information in the MDM during three-dimensional reconstruction. The results showed that the accuracy of the iteration-based reconstruction method was improved by 9.08% compared with that of the non-iteration reconstruction method.</summary></entry><entry><title type="html">A Revisit of the Optimal Excess-of-Loss Contract</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ARevisitoftheOptimalExcessofLossContract.html" rel="alternate" type="text/html" title="A Revisit of the Optimal Excess-of-Loss Contract" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ARevisitoftheOptimalExcessofLossContract</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ARevisitoftheOptimalExcessofLossContract.html">&lt;p&gt;It is well-known that Excess-of-Loss reinsurance has more marketability than Stop-Loss reinsurance, though Stop-Loss reinsurance is the most prominent setting discussed in the optimal (re)insurance design literature. We point out that optimal reinsurance policy under Stop-Loss leads to a zero insolvency probability, which motivates our paper. We provide a remedy to this peculiar property of the optimal Stop-Loss reinsurance contract by investigating the optimal Excess-of-Loss reinsurance contract instead. We also provide estimators for the optimal Excess-of-Loss and Stop-Loss contracts and investigate their statistical properties under many premium principle assumptions and various risk preferences, which according to our knowledge, have never been investigated in the literature. Simulated data and real-life data are used to illustrate our main theoretical findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00188&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ernest Aboagye, Vali Asimit, Tsz Chai Fung, Liang Peng, Qiuqi Wang</name></author><category term="stat.AP" /><summary type="html">It is well-known that Excess-of-Loss reinsurance has more marketability than Stop-Loss reinsurance, though Stop-Loss reinsurance is the most prominent setting discussed in the optimal (re)insurance design literature. We point out that optimal reinsurance policy under Stop-Loss leads to a zero insolvency probability, which motivates our paper. We provide a remedy to this peculiar property of the optimal Stop-Loss reinsurance contract by investigating the optimal Excess-of-Loss reinsurance contract instead. We also provide estimators for the optimal Excess-of-Loss and Stop-Loss contracts and investigate their statistical properties under many premium principle assumptions and various risk preferences, which according to our knowledge, have never been investigated in the literature. Simulated data and real-life data are used to illustrate our main theoretical findings.</summary></entry><entry><title type="html">A Statistical-Modelling Approach to Feedforward Neural Network Model Selection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/AStatisticalModellingApproachtoFeedforwardNeuralNetworkModelSelection.html" rel="alternate" type="text/html" title="A Statistical-Modelling Approach to Feedforward Neural Network Model Selection" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/AStatisticalModellingApproachtoFeedforwardNeuralNetworkModelSelection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/AStatisticalModellingApproachtoFeedforwardNeuralNetworkModelSelection.html">&lt;p&gt;Feedforward neural networks (FNNs) can be viewed as non-linear regression models, where covariates enter the model through a combination of weighted summations and non-linear functions. Although these models have some similarities to the approaches used within statistical modelling, the majority of neural network research has been conducted outside of the field of statistics. This has resulted in a lack of statistically-based methodology, and, in particular, there has been little emphasis on model parsimony. Determining the input layer structure is analogous to variable selection, while the structure for the hidden layer relates to model complexity. In practice, neural network model selection is often carried out by comparing models using out-of-sample performance. However, in contrast, the construction of an associated likelihood function opens the door to information-criteria-based variable and architecture selection. A novel model selection method, which performs both input- and hidden-node selection, is proposed using the Bayesian information criterion (BIC) for FNNs. The choice of BIC over out-of-sample performance as the model selection objective function leads to an increased probability of recovering the true model, while parsimoniously achieving favourable out-of-sample performance. Simulation studies are used to evaluate and justify the proposed method, and applications on real data are investigated.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2207.04248&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrew McInerney, Kevin Burke</name></author><category term="stat.ME" /><summary type="html">Feedforward neural networks (FNNs) can be viewed as non-linear regression models, where covariates enter the model through a combination of weighted summations and non-linear functions. Although these models have some similarities to the approaches used within statistical modelling, the majority of neural network research has been conducted outside of the field of statistics. This has resulted in a lack of statistically-based methodology, and, in particular, there has been little emphasis on model parsimony. Determining the input layer structure is analogous to variable selection, while the structure for the hidden layer relates to model complexity. In practice, neural network model selection is often carried out by comparing models using out-of-sample performance. However, in contrast, the construction of an associated likelihood function opens the door to information-criteria-based variable and architecture selection. A novel model selection method, which performs both input- and hidden-node selection, is proposed using the Bayesian information criterion (BIC) for FNNs. The choice of BIC over out-of-sample performance as the model selection objective function leads to an increased probability of recovering the true model, while parsimoniously achieving favourable out-of-sample performance. Simulation studies are used to evaluate and justify the proposed method, and applications on real data are investigated.</summary></entry><entry><title type="html">BayesBlend: Easy Model Blending using Pseudo-Bayesian Model Averaging, Stacking and Hierarchical Stacking in Python</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/BayesBlendEasyModelBlendingusingPseudoBayesianModelAveragingStackingandHierarchicalStackinginPython.html" rel="alternate" type="text/html" title="BayesBlend: Easy Model Blending using Pseudo-Bayesian Model Averaging, Stacking and Hierarchical Stacking in Python" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/BayesBlendEasyModelBlendingusingPseudoBayesianModelAveragingStackingandHierarchicalStackinginPython</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/BayesBlendEasyModelBlendingusingPseudoBayesianModelAveragingStackingandHierarchicalStackinginPython.html">&lt;p&gt;Averaging predictions from multiple competing inferential models frequently outperforms predictions from any single model, providing that models are optimally weighted to maximize predictive performance. This is particularly the case in so-called $\mathcal{M}$-open settings where the true model is not in the set of candidate models, and may be neither mathematically reifiable nor known precisely. This practice of model averaging has a rich history in statistics and machine learning, and there are currently a number of methods to estimate the weights for constructing model-averaged predictive distributions. Nonetheless, there are few existing software packages that can estimate model weights from the full variety of methods available, and none that blend model predictions into a coherent predictive distribution according to the estimated weights. In this paper, we introduce the BayesBlend Python package, which provides a user-friendly programming interface to estimate weights and blend multiple (Bayesian) models’ predictive distributions. BayesBlend implements pseudo-Bayesian model averaging, stacking and, uniquely, hierarchical Bayesian stacking to estimate model weights. We demonstrate the usage of BayesBlend with examples of insurance loss modeling.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00158&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nathaniel Haines, Conor Goold</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Averaging predictions from multiple competing inferential models frequently outperforms predictions from any single model, providing that models are optimally weighted to maximize predictive performance. This is particularly the case in so-called $\mathcal{M}$-open settings where the true model is not in the set of candidate models, and may be neither mathematically reifiable nor known precisely. This practice of model averaging has a rich history in statistics and machine learning, and there are currently a number of methods to estimate the weights for constructing model-averaged predictive distributions. Nonetheless, there are few existing software packages that can estimate model weights from the full variety of methods available, and none that blend model predictions into a coherent predictive distribution according to the estimated weights. In this paper, we introduce the BayesBlend Python package, which provides a user-friendly programming interface to estimate weights and blend multiple (Bayesian) models’ predictive distributions. BayesBlend implements pseudo-Bayesian model averaging, stacking and, uniquely, hierarchical Bayesian stacking to estimate model weights. We demonstrate the usage of BayesBlend with examples of insurance loss modeling.</summary></entry><entry><title type="html">Bayesian Posterior Interval Calibration to Improve the Interpretability of Observational Studies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/BayesianPosteriorIntervalCalibrationtoImprovetheInterpretabilityofObservationalStudies.html" rel="alternate" type="text/html" title="Bayesian Posterior Interval Calibration to Improve the Interpretability of Observational Studies" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/BayesianPosteriorIntervalCalibrationtoImprovetheInterpretabilityofObservationalStudies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/BayesianPosteriorIntervalCalibrationtoImprovetheInterpretabilityofObservationalStudies.html">&lt;p&gt;Observational healthcare data offer the potential to estimate causal effects of medical products on a large scale. However, the confidence intervals and p-values produced by observational studies only account for random error and fail to account for systematic error. As a consequence, operating characteristics such as confidence interval coverage and Type I error rates often deviate sharply from their nominal values and render interpretation impossible. While there is longstanding awareness of systematic error in observational studies, analytic approaches to empirically account for systematic error are relatively new. Several authors have proposed approaches using negative controls (also known as “falsification hypotheses”) and positive controls. The basic idea is to adjust confidence intervals and p-values in light of the bias (if any) detected in the analyses of the negative and positive control. In this work, we propose a Bayesian statistical procedure for posterior interval calibration that uses negative and positive controls. We show that the posterior interval calibration procedure restores nominal characteristics, such as 95% coverage of the true effect size by the 95% posterior interval.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.06002&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jami J. Mulgrave, David Madigan, George Hripcsak</name></author><category term="stat.AP" /><summary type="html">Observational healthcare data offer the potential to estimate causal effects of medical products on a large scale. However, the confidence intervals and p-values produced by observational studies only account for random error and fail to account for systematic error. As a consequence, operating characteristics such as confidence interval coverage and Type I error rates often deviate sharply from their nominal values and render interpretation impossible. While there is longstanding awareness of systematic error in observational studies, analytic approaches to empirically account for systematic error are relatively new. Several authors have proposed approaches using negative controls (also known as “falsification hypotheses”) and positive controls. The basic idea is to adjust confidence intervals and p-values in light of the bias (if any) detected in the analyses of the negative and positive control. In this work, we propose a Bayesian statistical procedure for posterior interval calibration that uses negative and positive controls. We show that the posterior interval calibration procedure restores nominal characteristics, such as 95% coverage of the true effect size by the 95% posterior interval.</summary></entry><entry><title type="html">Bayesian Varying-Effects Vector Autoregressive Models for Inference of Brain Connectivity Networks and Covariate Effects in Pediatric Traumatic Brain Injury</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/BayesianVaryingEffectsVectorAutoregressiveModelsforInferenceofBrainConnectivityNetworksandCovariateEffectsinPediatricTraumaticBrainInjury.html" rel="alternate" type="text/html" title="Bayesian Varying-Effects Vector Autoregressive Models for Inference of Brain Connectivity Networks and Covariate Effects in Pediatric Traumatic Brain Injury" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/BayesianVaryingEffectsVectorAutoregressiveModelsforInferenceofBrainConnectivityNetworksandCovariateEffectsinPediatricTraumaticBrainInjury</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/BayesianVaryingEffectsVectorAutoregressiveModelsforInferenceofBrainConnectivityNetworksandCovariateEffectsinPediatricTraumaticBrainInjury.html">&lt;p&gt;In this paper, we develop an analytical approach for estimating brain connectivity networks that accounts for subject heterogeneity. More specifically, we consider a novel extension of a multi-subject Bayesian vector autoregressive model that estimates group-specific directed brain connectivity networks and accounts for the effects of covariates on the network edges. We adopt a flexible approach, allowing for (possibly) non-linear effects of the covariates on edge strength via a novel Bayesian nonparametric prior that employs a weighted mixture of Gaussian processes. For posterior inference, we achieve computational scalability by implementing a variational Bayes scheme. Our approach enables simultaneous estimation of group-specific networks and selection of relevant covariate effects. We show improved performance over competing two-stage approaches on simulated data. We apply our method on resting-state fMRI data from children with a history of traumatic brain injury and healthy controls to estimate the effects of age and sex on the group-level connectivities. Our results highlight differences in the distribution of parent nodes. They also suggest alteration in the relation of age, with peak edge strength in children with traumatic brain injury (TBI), and differences in effective connectivity strength between males and females.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00535&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yangfan Ren, Nathan Osborne, Christine B. Peterson, Dana M. DeMaster, Linda Ewing-Cobbs, Marina Vannucci</name></author><category term="stat.ME" /><summary type="html">In this paper, we develop an analytical approach for estimating brain connectivity networks that accounts for subject heterogeneity. More specifically, we consider a novel extension of a multi-subject Bayesian vector autoregressive model that estimates group-specific directed brain connectivity networks and accounts for the effects of covariates on the network edges. We adopt a flexible approach, allowing for (possibly) non-linear effects of the covariates on edge strength via a novel Bayesian nonparametric prior that employs a weighted mixture of Gaussian processes. For posterior inference, we achieve computational scalability by implementing a variational Bayes scheme. Our approach enables simultaneous estimation of group-specific networks and selection of relevant covariate effects. We show improved performance over competing two-stage approaches on simulated data. We apply our method on resting-state fMRI data from children with a history of traumatic brain injury and healthy controls to estimate the effects of age and sex on the group-level connectivities. Our results highlight differences in the distribution of parent nodes. They also suggest alteration in the relation of age, with peak edge strength in children with traumatic brain injury (TBI), and differences in effective connectivity strength between males and females.</summary></entry><entry><title type="html">COVID anomaly in the correlation analysis of S&amp;amp;P 500 market states</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/COVIDanomalyinthecorrelationanalysisofSP500marketstates.html" rel="alternate" type="text/html" title="COVID anomaly in the correlation analysis of S&amp;amp;P 500 market states" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/COVIDanomalyinthecorrelationanalysisofSP500marketstates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/COVIDanomalyinthecorrelationanalysisofSP500marketstates.html">&lt;p&gt;Analyzing market states of the S&amp;amp;P 500 components on a time horizon January 3, 2006 to August 10, 2023, we found the appearance of a new market state not previously seen and we shall discuss its possible implications as an isolated state or as a beginning of a new general market condition. We study this in terms of the Pearson correlation matrix and relative correlation with respect to the S&amp;amp;P 500 index. In both cases the anomaly shows strongly.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.14830&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>M. Mijaíl Martínez-Ramos, Manan Vyas, Parisa Majai, Thomas H. Seligman</name></author><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Analyzing market states of the S&amp;amp;P 500 components on a time horizon January 3, 2006 to August 10, 2023, we found the appearance of a new market state not previously seen and we shall discuss its possible implications as an isolated state or as a beginning of a new general market condition. We study this in terms of the Pearson correlation matrix and relative correlation with respect to the S&amp;amp;P 500 index. In both cases the anomaly shows strongly.</summary></entry><entry><title type="html">Calibration of the rating transition model for high and low default portfolios</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Calibrationoftheratingtransitionmodelforhighandlowdefaultportfolios.html" rel="alternate" type="text/html" title="Calibration of the rating transition model for high and low default portfolios" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Calibrationoftheratingtransitionmodelforhighandlowdefaultportfolios</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Calibrationoftheratingtransitionmodelforhighandlowdefaultportfolios.html">&lt;p&gt;In this paper we develop Maximum likelihood (ML) based algorithms to calibrate the model parameters in credit rating transition models. Since the credit rating transition models are not Gaussian linear models, the celebrated Kalman filter is not suitable to compute the likelihood of observed migrations. Therefore, we develop a Laplace approximation of the likelihood function and as a result the Kalman filter can be used in the end to compute the likelihood function. This approach is applied to so-called high-default portfolios, in which the number of migrations (defaults) is large enough to obtain high accuracy of the Laplace approximation. By contrast, low-default portfolios have a limited number of observed migrations (defaults). Therefore, in order to calibrate low-default portfolios, we develop a ML algorithm using a particle filter (PF) and Gaussian process regression. Experiments show that both algorithms are efficient and produce accurate approximations of the likelihood function and the ML estimates of the model parameters.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00576&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jian He, Asma Khedher, Peter Spreij</name></author><category term="stat.ME" /><summary type="html">In this paper we develop Maximum likelihood (ML) based algorithms to calibrate the model parameters in credit rating transition models. Since the credit rating transition models are not Gaussian linear models, the celebrated Kalman filter is not suitable to compute the likelihood of observed migrations. Therefore, we develop a Laplace approximation of the likelihood function and as a result the Kalman filter can be used in the end to compute the likelihood function. This approach is applied to so-called high-default portfolios, in which the number of migrations (defaults) is large enough to obtain high accuracy of the Laplace approximation. By contrast, low-default portfolios have a limited number of observed migrations (defaults). Therefore, in order to calibrate low-default portfolios, we develop a ML algorithm using a particle filter (PF) and Gaussian process regression. Experiments show that both algorithms are efficient and produce accurate approximations of the likelihood function and the ML estimates of the model parameters.</summary></entry><entry><title type="html">Causal Inference with High-dimensional Discrete Covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/CausalInferencewithHighdimensionalDiscreteCovariates.html" rel="alternate" type="text/html" title="Causal Inference with High-dimensional Discrete Covariates" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/CausalInferencewithHighdimensionalDiscreteCovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/CausalInferencewithHighdimensionalDiscreteCovariates.html">&lt;p&gt;When estimating causal effects from observational studies, researchers often need to adjust for many covariates to deconfound the non-causal relationship between exposure and outcome, among which many covariates are discrete. The behavior of commonly used estimators in the presence of many discrete covariates is not well understood since their properties are often analyzed under structural assumptions including sparsity and smoothness, which do not apply in discrete settings. In this work, we study the estimation of causal effects in a model where the covariates required for confounding adjustment are discrete but high-dimensional, meaning the number of categories $d$ is comparable with or even larger than sample size $n$. Specifically, we show the mean squared error of commonly used regression, weighting and doubly robust estimators is bounded by $\frac{d^2}{n^2}+\frac{1}{n}$. We then prove the minimax lower bound for the average treatment effect is of order $\frac{d^2}{n^2 \log^2 n}+\frac{1}{n}$, which characterizes the fundamental difficulty of causal effect estimation in the high-dimensional discrete setting, and shows the estimators mentioned above are rate-optimal up to log-factors. We further consider additional structures that can be exploited, namely effect homogeneity and prior knowledge of the covariate distribution, and propose new estimators that enjoy faster convergence rates of order $\frac{d}{n^2} + \frac{1}{n}$, which achieve consistency in a broader regime. The results are illustrated empirically via simulation studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00118&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhenghao Zeng, Sivaraman Balakrishnan, Yanjun Han, Edward H. Kennedy</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">When estimating causal effects from observational studies, researchers often need to adjust for many covariates to deconfound the non-causal relationship between exposure and outcome, among which many covariates are discrete. The behavior of commonly used estimators in the presence of many discrete covariates is not well understood since their properties are often analyzed under structural assumptions including sparsity and smoothness, which do not apply in discrete settings. In this work, we study the estimation of causal effects in a model where the covariates required for confounding adjustment are discrete but high-dimensional, meaning the number of categories $d$ is comparable with or even larger than sample size $n$. Specifically, we show the mean squared error of commonly used regression, weighting and doubly robust estimators is bounded by $\frac{d^2}{n^2}+\frac{1}{n}$. We then prove the minimax lower bound for the average treatment effect is of order $\frac{d^2}{n^2 \log^2 n}+\frac{1}{n}$, which characterizes the fundamental difficulty of causal effect estimation in the high-dimensional discrete setting, and shows the estimators mentioned above are rate-optimal up to log-factors. We further consider additional structures that can be exploited, namely effect homogeneity and prior knowledge of the covariate distribution, and propose new estimators that enjoy faster convergence rates of order $\frac{d}{n^2} + \frac{1}{n}$, which achieve consistency in a broader regime. The results are illustrated empirically via simulation studies.</summary></entry><entry><title type="html">Conformal Risk Control for Ordinal Classification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ConformalRiskControlforOrdinalClassification.html" rel="alternate" type="text/html" title="Conformal Risk Control for Ordinal Classification" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ConformalRiskControlforOrdinalClassification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ConformalRiskControlforOrdinalClassification.html">&lt;p&gt;As a natural extension to the standard conformal prediction method, several conformal risk control methods have been recently developed and applied to various learning problems. In this work, we seek to control the conformal risk in expectation for ordinal classification tasks, which have broad applications to many real problems. For this purpose, we firstly formulated the ordinal classification task in the conformal risk control framework, and provided theoretic risk bounds of the risk control method. Then we proposed two types of loss functions specially designed for ordinal classification tasks, and developed corresponding algorithms to determine the prediction set for each case to control their risks at a desired level. We demonstrated the effectiveness of our proposed methods, and analyzed the difference between the two types of risks on three different datasets, including a simulated dataset, the UTKFace dataset and the diabetic retinopathy detection dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00417&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yunpeng Xu, Wenge Guo, Zhi Wei</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">As a natural extension to the standard conformal prediction method, several conformal risk control methods have been recently developed and applied to various learning problems. In this work, we seek to control the conformal risk in expectation for ordinal classification tasks, which have broad applications to many real problems. For this purpose, we firstly formulated the ordinal classification task in the conformal risk control framework, and provided theoretic risk bounds of the risk control method. Then we proposed two types of loss functions specially designed for ordinal classification tasks, and developed corresponding algorithms to determine the prediction set for each case to control their risks at a desired level. We demonstrated the effectiveness of our proposed methods, and analyzed the difference between the two types of risks on three different datasets, including a simulated dataset, the UTKFace dataset and the diabetic retinopathy detection dataset.</summary></entry><entry><title type="html">Conformal inference for random objects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Conformalinferenceforrandomobjects.html" rel="alternate" type="text/html" title="Conformal inference for random objects" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Conformalinferenceforrandomobjects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Conformalinferenceforrandomobjects.html">&lt;p&gt;We develop an inferential toolkit for analyzing object-valued responses, which correspond to data situated in general metric spaces, paired with Euclidean predictors within the conformal framework. To this end we introduce conditional profile average transport costs, where we compare distance profiles that correspond to one-dimensional distributions of probability mass falling into balls of increasing radius through the optimal transport cost when moving from one distance profile to another. The average transport cost to transport a given distance profile to all others is crucial for statistical inference in metric spaces and underpins the proposed conditional profile scores. A key feature of the proposed approach is to utilize the distribution of conditional profile average transport costs as conformity score for general metric space-valued responses, which facilitates the construction of prediction sets by the split conformal algorithm. We derive the uniform convergence rate of the proposed conformity score estimators and establish asymptotic conditional validity for the prediction sets. The finite sample performance for synthetic data in various metric spaces demonstrates that the proposed conditional profile score outperforms existing methods in terms of both coverage level and size of the resulting prediction sets, even in the special case of scalar and thus Euclidean responses. We also demonstrate the practical utility of conditional profile scores for network data from New York taxi trips and for compositional data reflecting energy sourcing of U.S. states.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00294&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hang Zhou, Hans-Georg Müller</name></author><category term="stat.ME" /><summary type="html">We develop an inferential toolkit for analyzing object-valued responses, which correspond to data situated in general metric spaces, paired with Euclidean predictors within the conformal framework. To this end we introduce conditional profile average transport costs, where we compare distance profiles that correspond to one-dimensional distributions of probability mass falling into balls of increasing radius through the optimal transport cost when moving from one distance profile to another. The average transport cost to transport a given distance profile to all others is crucial for statistical inference in metric spaces and underpins the proposed conditional profile scores. A key feature of the proposed approach is to utilize the distribution of conditional profile average transport costs as conformity score for general metric space-valued responses, which facilitates the construction of prediction sets by the split conformal algorithm. We derive the uniform convergence rate of the proposed conformity score estimators and establish asymptotic conditional validity for the prediction sets. The finite sample performance for synthetic data in various metric spaces demonstrates that the proposed conditional profile score outperforms existing methods in terms of both coverage level and size of the resulting prediction sets, even in the special case of scalar and thus Euclidean responses. We also demonstrate the practical utility of conditional profile scores for network data from New York taxi trips and for compositional data reflecting energy sourcing of U.S. states.</summary></entry><entry><title type="html">Conformalized Tensor Completion with Riemannian Optimization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ConformalizedTensorCompletionwithRiemannianOptimization.html" rel="alternate" type="text/html" title="Conformalized Tensor Completion with Riemannian Optimization" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ConformalizedTensorCompletionwithRiemannianOptimization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ConformalizedTensorCompletionwithRiemannianOptimization.html">&lt;p&gt;Tensor data, or multi-dimensional array, is a data format popular in multiple fields such as social network analysis, recommender systems, and brain imaging. It is not uncommon to observe tensor data containing missing values and tensor completion aims at estimating the missing values given the partially observed tensor. Sufficient efforts have been spared on devising scalable tensor completion algorithms but few on quantifying the uncertainty of the estimator. In this paper, we nest the uncertainty quantification (UQ) of tensor completion under a split conformal prediction framework and establish the connection of the UQ problem to a problem of estimating the missing propensity of each tensor entry. We model the data missingness of the tensor with a tensor Ising model parameterized by a low-rank tensor parameter. We propose to estimate the tensor parameter by maximum pseudo-likelihood estimation (MPLE) with a Riemannian gradient descent algorithm. Extensive simulation studies have been conducted to justify the validity of the resulting conformal interval. We apply our method to the regional total electron content (TEC) reconstruction problem.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00581&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hu Sun, Yang Chen</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Tensor data, or multi-dimensional array, is a data format popular in multiple fields such as social network analysis, recommender systems, and brain imaging. It is not uncommon to observe tensor data containing missing values and tensor completion aims at estimating the missing values given the partially observed tensor. Sufficient efforts have been spared on devising scalable tensor completion algorithms but few on quantifying the uncertainty of the estimator. In this paper, we nest the uncertainty quantification (UQ) of tensor completion under a split conformal prediction framework and establish the connection of the UQ problem to a problem of estimating the missing propensity of each tensor entry. We model the data missingness of the tensor with a tensor Ising model parameterized by a low-rank tensor parameter. We propose to estimate the tensor parameter by maximum pseudo-likelihood estimation (MPLE) with a Riemannian gradient descent algorithm. Extensive simulation studies have been conducted to justify the validity of the resulting conformal interval. We apply our method to the regional total electron content (TEC) reconstruction problem.</summary></entry><entry><title type="html">Distribution of lowest eigenvalue in $k$-body bosonic random matrix ensembles</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Distributionoflowesteigenvalueinkbodybosonicrandommatrixensembles.html" rel="alternate" type="text/html" title="Distribution of lowest eigenvalue in $k$-body bosonic random matrix ensembles" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Distributionoflowesteigenvalueinkbodybosonicrandommatrixensembles</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Distributionoflowesteigenvalueinkbodybosonicrandommatrixensembles.html">&lt;p&gt;We numerically study the distribution of the lowest eigenvalue of finite many-boson systems with $k$-body interactions modeled by Bosonic Embedded Gaussian Orthogonal [BEGOE($k$)] and Unitary [BEGUE($k$)] random matrix Ensembles. Following the recently established result that the $q$-normal describes the smooth form of the eigenvalue density of the $k$-body embedded ensembles, the first four moments of the distribution of lowest eigenvalues have been analyzed as a function of the $q$ parameter, with $q \sim 1$ for $k = 1$ and $q = 0$ for $k = m$; $m$ being the number of bosons. Our results show the distribution exhibits a smooth transition from Gaussian like for $q$ close to 1 to a modified Gumbel like for intermediate values of $q$ to the well-known Tracy-Widom distribution for $q=0$.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00190&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>N. D. Chavda, Priyanka Rao, V. K. B. Kota, Manan Vyas</name></author><category term="stat.AP" /><summary type="html">We numerically study the distribution of the lowest eigenvalue of finite many-boson systems with $k$-body interactions modeled by Bosonic Embedded Gaussian Orthogonal [BEGOE($k$)] and Unitary [BEGUE($k$)] random matrix Ensembles. Following the recently established result that the $q$-normal describes the smooth form of the eigenvalue density of the $k$-body embedded ensembles, the first four moments of the distribution of lowest eigenvalues have been analyzed as a function of the $q$ parameter, with $q \sim 1$ for $k = 1$ and $q = 0$ for $k = m$; $m$ being the number of bosons. Our results show the distribution exhibits a smooth transition from Gaussian like for $q$ close to 1 to a modified Gumbel like for intermediate values of $q$ to the well-known Tracy-Widom distribution for $q=0$.</summary></entry><entry><title type="html">Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data: Insights from Item Response Theory</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/EstimatingHeterogeneousTreatmentEffectswithItemLevelOutcomeDataInsightsfromItemResponseTheory.html" rel="alternate" type="text/html" title="Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data: Insights from Item Response Theory" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/EstimatingHeterogeneousTreatmentEffectswithItemLevelOutcomeDataInsightsfromItemResponseTheory</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/EstimatingHeterogeneousTreatmentEffectswithItemLevelOutcomeDataInsightsfromItemResponseTheory.html">&lt;p&gt;Analyses of heterogeneous treatment effects (HTE) are common in applied causal inference research. However, when outcomes are latent variables assessed via psychometric instruments such as educational tests, standard methods ignore the potential HTE that may exist among the individual items of the outcome measure. Failing to account for “item-level” HTE (IL-HTE) can lead to both estimated standard errors that are too small and identification challenges in the estimation of treatment-by-covariate interaction effects. We demonstrate how Item Response Theory (IRT) models that estimate a treatment effect for each assessment item can both address these challenges and provide new insights into HTE generally. This study articulates the theoretical rationale for the IL-HTE model and demonstrates its practical value using data from 20 randomized controlled trials in economics, education, and health. Our results show that the IL-HTE model reveals item-level variation masked by average treatment effects, provides more accurate statistical inference, allows for estimates of the generalizability of causal effects, resolves identification problems in the estimation of interaction effects, and provides estimates of standardized treatment effect sizes corrected for attenuation due to measurement error.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00161&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joshua B. Gilbert, Zachary Himmelsbach, James Soland, Mridul Joshi, Benjamin W. Domingue</name></author><category term="stat.ME" /><summary type="html">Analyses of heterogeneous treatment effects (HTE) are common in applied causal inference research. However, when outcomes are latent variables assessed via psychometric instruments such as educational tests, standard methods ignore the potential HTE that may exist among the individual items of the outcome measure. Failing to account for “item-level” HTE (IL-HTE) can lead to both estimated standard errors that are too small and identification challenges in the estimation of treatment-by-covariate interaction effects. We demonstrate how Item Response Theory (IRT) models that estimate a treatment effect for each assessment item can both address these challenges and provide new insights into HTE generally. This study articulates the theoretical rationale for the IL-HTE model and demonstrates its practical value using data from 20 randomized controlled trials in economics, education, and health. Our results show that the IL-HTE model reveals item-level variation masked by average treatment effects, provides more accurate statistical inference, allows for estimates of the generalizability of causal effects, resolves identification problems in the estimation of interaction effects, and provides estimates of standardized treatment effect sizes corrected for attenuation due to measurement error.</summary></entry><entry><title type="html">Euclid preparation. LensMC, weak lensing cosmic shear measurement with forward modelling and Markov Chain Monte Carlo sampling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/EuclidpreparationLensMCweaklensingcosmicshearmeasurementwithforwardmodellingandMarkovChainMonteCarlosampling.html" rel="alternate" type="text/html" title="Euclid preparation. LensMC, weak lensing cosmic shear measurement with forward modelling and Markov Chain Monte Carlo sampling" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/EuclidpreparationLensMCweaklensingcosmicshearmeasurementwithforwardmodellingandMarkovChainMonteCarlosampling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/EuclidpreparationLensMCweaklensingcosmicshearmeasurementwithforwardmodellingandMarkovChainMonteCarlosampling.html">&lt;p&gt;LensMC is a weak lensing shear measurement method developed for Euclid and Stage-IV surveys. It is based on forward modelling to deal with convolution by a point spread function with comparable size to many galaxies; sampling the posterior distribution of galaxy parameters via Markov Chain Monte Carlo; and marginalisation over nuisance parameters for each of the 1.5 billion galaxies observed by Euclid. The scientific performance is quantified through high-fidelity images based on the Euclid Flagship simulations and emulation of the Euclid VIS images; realistic clustering with a mean surface number density of 250 arcmin$^{-2}$ ($I_{\rm E}&amp;lt;29.5$) for galaxies, and 6 arcmin$^{-2}$ ($I_{\rm E}&amp;lt;26$) for stars; and a diffraction-limited chromatic point spread function with a full width at half maximum of $0.^{!\prime\prime}2$ and spatial variation across the field of view. Objects are measured with a density of 90 arcmin$^{-2}$ ($I_{\rm E}&amp;lt;26.5$) in 4500 deg$^2$. The total shear bias is broken down into measurement (our main focus here) and selection effects (which will be addressed elsewhere). We find: measurement multiplicative and additive biases of $m_1=(-3.6\pm0.2)\times10^{-3}$, $m_2=(-4.3\pm0.2)\times10^{-3}$, $c_1=(-1.78\pm0.03)\times10^{-4}$, $c_2=(0.09\pm0.03)\times10^{-4}$; a large detection bias with a multiplicative component of $1.2\times10^{-2}$ and an additive component of $-3\times10^{-4}$; and a measurement PSF leakage of $\alpha_1=(-9\pm3)\times10^{-4}$ and $\alpha_2=(2\pm3)\times10^{-4}$. When model bias is suppressed, the obtained measurement biases are close to Euclid requirement and largely dominated by undetected faint galaxies ($-5\times10^{-3}$). Although significant, model bias will be straightforward to calibrate given the weak sensitivity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00669&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Euclid Collaboration, G. Congedo , L. Miller , A. N. Taylor , N. Cross , C. A. J. Duncan , T. Kitching , N. Martinet , S. Matthew , T. Schrabback , M. Tewes , N. Welikala , N. Aghanim , A. Amara , S. Andreon , N. Auricchio , M. Baldi , S. Bardelli , R. Bender , C. Bodendorf , D. Bonino , E. Branchini , M. Brescia , J. Brinchmann , S. Camera , V. Capobianco , C. Carbone , V. F. Cardone , J. Carretero , S. Casas , F. J. Castander , M. Castellano , S. Cavuoti , A. Cimatti , C. J. Conselice , L. Conversi , Y. Copin , F. Courbin , H. M. Courtois , M. Cropper , A. Da Silva , H. Degaudenzi , A. M. Di Giorgio , J. Dinis , F. Dubath , X. Dupac , M. Farina , S. Farrens , S. Ferriol , P. Fosalba , M. Frailis , E. Franceschi , S. Galeotta , B. Garilli , B. Gillis , C. Giocoli , A. Grazian , F. Grupp , S. V. H. Haugan , M. S. Holliman , W. Holmes , F. Hormuth , A. Hornstrup , P. Hudelot , K. Jahnke , E. Keihänen , S. Kermiche , A. Kiessling , M. Kilbinger , B. Kubik , K. Kuijken , M. Kümmel , M. Kunz , H. Kurki-Suonio , S. Ligori , P. B. Lilje , V. Lindholm , I. Lloro , D. Maino , E. Maiorano , O. Mansutti , O. Marggraf , K. Markovic , F. Marulli , R. Massey , S. Maurogordato , H. J. McCracken , E. Medinaceli , S. Mei , M. Melchior , M. Meneghetti , E. Merlin , G. Meylan , M. Moresco , B. Morin , L. Moscardini , E. Munari , S. -M. Niemi , J. W. Nightingale , C. Padilla , S. Paltani , F. Pasian , K. Pedersen , W. J. Percival , V. Pettorino , S. Pires , G. Polenta , M. Poncet , L. A. Popa , L. Pozzetti , F. Raison , R. Rebolo , A. Renzi , J. Rhodes , G. Riccio , E. Romelli , M. Roncarelli , E. Rossetti , R. Saglia , D. Sapone , B. Sartoris , P. Schneider , A. Secroun , G. Seidel , S. Serrano , C. Sirignano , G. Sirri , L. Stanco , P. Tallada-Crespí , D. Tavagnacco , I. Tereno , R. Toledo-Moreo , F. Torradeflot , I. Tutusaus , E. A. Valentijn , L. Valenziano , T. Vassallo , A. Veropalumbo , Y. Wang , J. Weller , G. Zamorani , J. Zoubian , E. Zucca , A. Biviano , M. Bolzonella , A. Boucaud , E. Bozzo , C. Burigana , C. Colodro-Conde , D. Di Ferdinando , J. Graciá-Carpio , N. Mauri , C. Neissner , A. A. Nucita , Z. Sakr , V. Scottez , M. Tenti , M. Viel , M. Wiesmann , Y. Akrami , V. Allevato , S. Anselmi , C. Baccigalupi , M. Ballardini , S. Borgani , A. S. Borlaff , S. Bruton , R. Cabanac , A. Cappi , C. S. Carvalho , G. Castignani , T. Castro , G. Cañas-Herrera , K. C. Chambers , A. R. Cooray , J. Coupon , S. Davini , G. De Lucia , G. Desprez , S. Di Domizio , H. Dole , A. Díaz-Sánchez , J. A. Escartin Vigo , S. Escoffier , I. Ferrero , F. Finelli , L. Gabarra , J. García-Bellido , E. Gaztanaga , F. Giacomini , G. Gozaliasl , D. Guinet , A. Hall , H. Hildebrandt , S. Ilić , A. Jimenez Muñoz , S. Joudaki , J. J. E. Kajava , V. Kansal , D. Karagiannis , C. C. Kirkpatrick , L. Legrand , J. Macias-Perez , G. Maggio , M. Magliocchetti , R. Maoli , M. Martinelli , C. J. A. P. Martins , M. Maturi , L. Maurin , R. B. Metcalf , M. Migliaccio , P. Monaco , G. Morgante , S. Nadathur , L. Patrizii , A. Peel , A. Pezzotta , V. Popa , C. Porciani , D. Potter , M. Pöntinen , P. Reimberg , P. -F. Rocci , A. G. Sánchez , J. A. Schewtschenko , A. Schneider , E. Sefusatti , M. Sereno , P. Simon , A. Spurio Mancini , J. Stadel , J. Steinwagner , G. Testera , R. Teyssier , S. Toft , S. Tosi , A. Troja , M. Tucci , C. Valieri , J. Valiviita , D. Vergani</name></author><category term="stat.CO" /><summary type="html">LensMC is a weak lensing shear measurement method developed for Euclid and Stage-IV surveys. It is based on forward modelling to deal with convolution by a point spread function with comparable size to many galaxies; sampling the posterior distribution of galaxy parameters via Markov Chain Monte Carlo; and marginalisation over nuisance parameters for each of the 1.5 billion galaxies observed by Euclid. The scientific performance is quantified through high-fidelity images based on the Euclid Flagship simulations and emulation of the Euclid VIS images; realistic clustering with a mean surface number density of 250 arcmin$^{-2}$ ($I_{\rm E}&amp;lt;29.5$) for galaxies, and 6 arcmin$^{-2}$ ($I_{\rm E}&amp;lt;26$) for stars; and a diffraction-limited chromatic point spread function with a full width at half maximum of $0.^{!\prime\prime}2$ and spatial variation across the field of view. Objects are measured with a density of 90 arcmin$^{-2}$ ($I_{\rm E}&amp;lt;26.5$) in 4500 deg$^2$. The total shear bias is broken down into measurement (our main focus here) and selection effects (which will be addressed elsewhere). We find: measurement multiplicative and additive biases of $m_1=(-3.6\pm0.2)\times10^{-3}$, $m_2=(-4.3\pm0.2)\times10^{-3}$, $c_1=(-1.78\pm0.03)\times10^{-4}$, $c_2=(0.09\pm0.03)\times10^{-4}$; a large detection bias with a multiplicative component of $1.2\times10^{-2}$ and an additive component of $-3\times10^{-4}$; and a measurement PSF leakage of $\alpha_1=(-9\pm3)\times10^{-4}$ and $\alpha_2=(2\pm3)\times10^{-4}$. When model bias is suppressed, the obtained measurement biases are close to Euclid requirement and largely dominated by undetected faint galaxies ($-5\times10^{-3}$). Although significant, model bias will be straightforward to calibrate given the weak sensitivity.</summary></entry><entry><title type="html">Explainable Automatic Grading with Neural Additive Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ExplainableAutomaticGradingwithNeuralAdditiveModels.html" rel="alternate" type="text/html" title="Explainable Automatic Grading with Neural Additive Models" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ExplainableAutomaticGradingwithNeuralAdditiveModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ExplainableAutomaticGradingwithNeuralAdditiveModels.html">&lt;p&gt;The use of automatic short answer grading (ASAG) models may help alleviate the time burden of grading while encouraging educators to frequently incorporate open-ended items in their curriculum. However, current state-of-the-art ASAG models are large neural networks (NN) often described as “black box”, providing no explanation for which characteristics of an input are important for the produced output. This inexplicable nature can be frustrating to teachers and students when trying to interpret, or learn from an automatically-generated grade. To create a powerful yet intelligible ASAG model, we experiment with a type of model called a Neural Additive Model that combines the performance of a NN with the explainability of an additive model. We use a Knowledge Integration (KI) framework from the learning sciences to guide feature engineering to create inputs that reflect whether a student includes certain ideas in their response. We hypothesize that indicating the inclusion (or exclusion) of predefined ideas as features will be sufficient for the NAM to have good predictive power and interpretability, as this may guide a human scorer using a KI rubric. We compare the performance of the NAM with another explainable model, logistic regression, using the same features, and to a non-explainable neural model, DeBERTa, that does not require feature engineering.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00489&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aubrey Condor, Zachary Pardos</name></author><category term="stat.AP" /><summary type="html">The use of automatic short answer grading (ASAG) models may help alleviate the time burden of grading while encouraging educators to frequently incorporate open-ended items in their curriculum. However, current state-of-the-art ASAG models are large neural networks (NN) often described as “black box”, providing no explanation for which characteristics of an input are important for the produced output. This inexplicable nature can be frustrating to teachers and students when trying to interpret, or learn from an automatically-generated grade. To create a powerful yet intelligible ASAG model, we experiment with a type of model called a Neural Additive Model that combines the performance of a NN with the explainability of an additive model. We use a Knowledge Integration (KI) framework from the learning sciences to guide feature engineering to create inputs that reflect whether a student includes certain ideas in their response. We hypothesize that indicating the inclusion (or exclusion) of predefined ideas as features will be sufficient for the NAM to have good predictive power and interpretability, as this may guide a human scorer using a KI rubric. We compare the performance of the NAM with another explainable model, logistic regression, using the same features, and to a non-explainable neural model, DeBERTa, that does not require feature engineering.</summary></entry><entry><title type="html">FPGA Digital Dice using Pseudo Random Number Generator</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/FPGADigitalDiceusingPseudoRandomNumberGenerator.html" rel="alternate" type="text/html" title="FPGA Digital Dice using Pseudo Random Number Generator" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/FPGADigitalDiceusingPseudoRandomNumberGenerator</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/FPGADigitalDiceusingPseudoRandomNumberGenerator.html">&lt;p&gt;The goal of this project is to design a digital dice that displays dice numbers in real-time. The number is generated by a pseudo-random number generator (PRNG) using XORshift algorithm that is implemented in Verilog HDL on an FPGA. The digital dice is equipped with tilt sensor, display, power management circuit, and rechargeable battery hosted in a 3D printed dice casing. By shaking the digital dice, the tilt sensor signal produces a seed for the PRNG. This digital dice demonstrates a set of possible random numbers of 2, 4, 6, 8, 10, 12, 20, 100 that simulate the number of dice sides. The kit is named SUTDicey.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00308&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael Lim Kee Hian, Ten Wei Lin, Zachary Wu Xuan, Stephanie-Ann Loy, Maoyang Xiang, T. Hui Teo</name></author><category term="stat.AP" /><summary type="html">The goal of this project is to design a digital dice that displays dice numbers in real-time. The number is generated by a pseudo-random number generator (PRNG) using XORshift algorithm that is implemented in Verilog HDL on an FPGA. The digital dice is equipped with tilt sensor, display, power management circuit, and rechargeable battery hosted in a 3D printed dice casing. By shaking the digital dice, the tilt sensor signal produces a seed for the PRNG. This digital dice demonstrates a set of possible random numbers of 2, 4, 6, 8, 10, 12, 20, 100 that simulate the number of dice sides. The kit is named SUTDicey.</summary></entry><entry><title type="html">Finite-sample adjustments for comparing clustered adaptive interventions using data from a clustered SMART</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/FinitesampleadjustmentsforcomparingclusteredadaptiveinterventionsusingdatafromaclusteredSMART.html" rel="alternate" type="text/html" title="Finite-sample adjustments for comparing clustered adaptive interventions using data from a clustered SMART" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/FinitesampleadjustmentsforcomparingclusteredadaptiveinterventionsusingdatafromaclusteredSMART</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/FinitesampleadjustmentsforcomparingclusteredadaptiveinterventionsusingdatafromaclusteredSMART.html">&lt;p&gt;Adaptive interventions, aka dynamic treatment regimens, are sequences of pre-specified decision rules that guide the provision of treatment for an individual given information about their baseline and evolving needs, including in response to prior intervention. Clustered adaptive interventions (cAIs) extend this idea by guiding the provision of intervention at the level of clusters (e.g., clinics), but with the goal of improving outcomes at the level of individuals within the cluster (e.g., clinicians or patients within clinics). A clustered, sequential multiple-assignment randomized trials (cSMARTs) is a multistage, multilevel randomized trial design used to construct high-quality cAIs. In a cSMART, clusters are randomized at multiple intervention decision points; at each decision point, the randomization probability can depend on response to prior data. A challenge in cluster-randomized trials, including cSMARTs, is the deleterious effect of small samples of clusters on statistical inference, particularly via estimation of standard errors. \par This manuscript develops finite-sample adjustment (FSA) methods for making improved statistical inference about the causal effects of cAIs in a cSMART. The paper develops FSA methods that (i) scale variance estimators using a degree-of-freedom adjustment, (ii) reference a t distribution (instead of a normal), and (iii) employ a ``bias corrected” variance estimator. Method (iii) requires extensions that are unique to the analysis of cSMARTs. Extensive simulation experiments are used to test the performance of the methods. The methods are illustrated using the Adaptive School-based Implementation of CBT (ASIC) study, a cSMART designed to construct a cAI for improving the delivery of cognitive behavioral therapy (CBT) by school mental health professionals within high schools in Michigan.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00185&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wenchu Pan, Daniel Almirall, Amy M. Kilbourne, Andrew Quanbeck, Lu Wang</name></author><category term="stat.ME" /><summary type="html">Adaptive interventions, aka dynamic treatment regimens, are sequences of pre-specified decision rules that guide the provision of treatment for an individual given information about their baseline and evolving needs, including in response to prior intervention. Clustered adaptive interventions (cAIs) extend this idea by guiding the provision of intervention at the level of clusters (e.g., clinics), but with the goal of improving outcomes at the level of individuals within the cluster (e.g., clinicians or patients within clinics). A clustered, sequential multiple-assignment randomized trials (cSMARTs) is a multistage, multilevel randomized trial design used to construct high-quality cAIs. In a cSMART, clusters are randomized at multiple intervention decision points; at each decision point, the randomization probability can depend on response to prior data. A challenge in cluster-randomized trials, including cSMARTs, is the deleterious effect of small samples of clusters on statistical inference, particularly via estimation of standard errors. \par This manuscript develops finite-sample adjustment (FSA) methods for making improved statistical inference about the causal effects of cAIs in a cSMART. The paper develops FSA methods that (i) scale variance estimators using a degree-of-freedom adjustment, (ii) reference a t distribution (instead of a normal), and (iii) employ a ``bias corrected” variance estimator. Method (iii) requires extensions that are unique to the analysis of cSMARTs. Extensive simulation experiments are used to test the performance of the methods. The methods are illustrated using the Adaptive School-based Implementation of CBT (ASIC) study, a cSMART designed to construct a cAI for improving the delivery of cognitive behavioral therapy (CBT) by school mental health professionals within high schools in Michigan.</summary></entry><entry><title type="html">Group integrative dynamic factor models with application to multiple subject brain connectivity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Groupintegrativedynamicfactormodelswithapplicationtomultiplesubjectbrainconnectivity.html" rel="alternate" type="text/html" title="Group integrative dynamic factor models with application to multiple subject brain connectivity" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Groupintegrativedynamicfactormodelswithapplicationtomultiplesubjectbrainconnectivity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Groupintegrativedynamicfactormodelswithapplicationtomultiplesubjectbrainconnectivity.html">&lt;p&gt;This work introduces a novel framework for dynamic factor model-based group-level analysis of multiple subjects time series data, called GRoup Integrative DYnamic factor (GRIDY) models. The framework identifies and characterizes inter-subject similarities and differences between two pre-determined groups by considering a combination of group spatial information and individual temporal dynamics. Furthermore, it enables the identification of intra-subject similarities and differences over time by employing different model configurations for each subject. Methodologically, the framework combines a novel principal angle-based rank selection algorithm and a non-iterative integrative analysis framework. Inspired by simultaneous component analysis, this approach also reconstructs identifiable latent factor series with flexible covariance structures. The performance of the GRIDY models is evaluated through simulations conducted under various scenarios. An application is also presented to compare resting-state functional MRI data collected from multiple subjects in autism spectrum disorder and control groups.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.15330&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Younghoon Kim, Zachary F. Fisher, Vladas Pipiras</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">This work introduces a novel framework for dynamic factor model-based group-level analysis of multiple subjects time series data, called GRoup Integrative DYnamic factor (GRIDY) models. The framework identifies and characterizes inter-subject similarities and differences between two pre-determined groups by considering a combination of group spatial information and individual temporal dynamics. Furthermore, it enables the identification of intra-subject similarities and differences over time by employing different model configurations for each subject. Methodologically, the framework combines a novel principal angle-based rank selection algorithm and a non-iterative integrative analysis framework. Inspired by simultaneous component analysis, this approach also reconstructs identifiable latent factor series with flexible covariance structures. The performance of the GRIDY models is evaluated through simulations conducted under various scenarios. An application is also presented to compare resting-state functional MRI data collected from multiple subjects in autism spectrum disorder and control groups.</summary></entry><entry><title type="html">Implementing Bayesian inference on a stochastic CO2-based grey-box model for assessing indoor air quality in Canadian primary schools</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ImplementingBayesianinferenceonastochasticCO2basedgreyboxmodelforassessingindoorairqualityinCanadianprimaryschools.html" rel="alternate" type="text/html" title="Implementing Bayesian inference on a stochastic CO2-based grey-box model for assessing indoor air quality in Canadian primary schools" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ImplementingBayesianinferenceonastochasticCO2basedgreyboxmodelforassessingindoorairqualityinCanadianprimaryschools</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ImplementingBayesianinferenceonastochasticCO2basedgreyboxmodelforassessingindoorairqualityinCanadianprimaryschools.html">&lt;p&gt;The COVID-19 pandemic brought global attention to indoor air quality (IAQ), which is intrinsically linked to clean air change rates. Estimating the air change rate in indoor environments, however, remains challenging. It is primarily due to the uncertainties associated with the air change rate estimation, such as pollutant generation rates, dynamics including weather and occupancies, and the limitations of deterministic approaches to accommodate these factors. In this study, Bayesian inference was implemented on a stochastic CO2-based grey-box model to infer modeled parameters and quantify uncertainties. The accuracy and robustness of the ventilation rate and CO2 emission rate estimated by the model were confirmed with CO2 tracer gas experiments conducted in an airtight chamber. Both prior and posterior predictive checks (PPC) were performed to demonstrate the advantage of this approach. In addition, uncertainties in real-life contexts were quantified with an incremental variance {\sigma} for the Wiener process. This approach was later applied to evaluate the ventilation conditions within two primary school classrooms in Montreal. The Equivalent Clean Airflow Rate (ECAi) was calculated following ASHRAE 241, and an insufficient clean air supply within both classrooms was identified. A supplement of 800 cfm clear air delivery rate (CADR) from air-cleaning devices is recommended for a sufficient ECAi. Finally, steady-state CO2 thresholds (Climit, Ctarget, and Cideal) were carried out to indicate when ECAi requirements could be achieved under various mitigation strategies, such as portable air cleaners and in-room ultraviolet light, with CADR values ranging from 200 to 1000 cfm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00582&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shujie Yan , Jiwei Zou , Chang Shu , Justin Berquist , Vincent Brochu , Marc Veillette , Danlin Hou , Caroline Duchaine ,  Liang ,  Zhou ,  Zhiqiang ,  Zhai ,  Liangzhu ,  Wang</name></author><category term="stat.AP" /><summary type="html">The COVID-19 pandemic brought global attention to indoor air quality (IAQ), which is intrinsically linked to clean air change rates. Estimating the air change rate in indoor environments, however, remains challenging. It is primarily due to the uncertainties associated with the air change rate estimation, such as pollutant generation rates, dynamics including weather and occupancies, and the limitations of deterministic approaches to accommodate these factors. In this study, Bayesian inference was implemented on a stochastic CO2-based grey-box model to infer modeled parameters and quantify uncertainties. The accuracy and robustness of the ventilation rate and CO2 emission rate estimated by the model were confirmed with CO2 tracer gas experiments conducted in an airtight chamber. Both prior and posterior predictive checks (PPC) were performed to demonstrate the advantage of this approach. In addition, uncertainties in real-life contexts were quantified with an incremental variance {\sigma} for the Wiener process. This approach was later applied to evaluate the ventilation conditions within two primary school classrooms in Montreal. The Equivalent Clean Airflow Rate (ECAi) was calculated following ASHRAE 241, and an insufficient clean air supply within both classrooms was identified. A supplement of 800 cfm clear air delivery rate (CADR) from air-cleaning devices is recommended for a sufficient ECAi. Finally, steady-state CO2 thresholds (Climit, Ctarget, and Cideal) were carried out to indicate when ECAi requirements could be achieved under various mitigation strategies, such as portable air cleaners and in-room ultraviolet light, with CADR values ranging from 200 to 1000 cfm.</summary></entry><entry><title type="html">L0-regularized compressed sensing with Mean-field Coherent Ising Machines</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/L0regularizedcompressedsensingwithMeanfieldCoherentIsingMachines.html" rel="alternate" type="text/html" title="L0-regularized compressed sensing with Mean-field Coherent Ising Machines" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/L0regularizedcompressedsensingwithMeanfieldCoherentIsingMachines</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/L0regularizedcompressedsensingwithMeanfieldCoherentIsingMachines.html">&lt;p&gt;Coherent Ising Machine (CIM) is a network of optical parametric oscillators that solves combinatorial optimization problems by finding the ground state of an Ising Hamiltonian. As a practical application of CIM, Aonishi et al. proposed a quantum-classical hybrid system to solve optimization problems of L0-regularization-based compressed sensing (L0RBCS). Gunathilaka et al. has further enhanced the accuracy of the system. However, the computationally expensive CIM’s stochastic differential equations (SDEs) limit the use of digital hardware implementations. As an alternative to Gunathilaka et al.’s CIM SDEs used previously, we propose using the mean-field CIM (MF-CIM) model, which is a physics-inspired heuristic solver without quantum noise. MF-CIM surmounts the high computational cost due to the simple nature of the differential equations (DEs). Furthermore, our results indicate that the proposed model has similar performance to physically accurate SDEs in both artificial and magnetic resonance imaging data, paving the way for implementing CIM-based L0RBCS on digital hardware such as Field Programmable Gate Arrays (FPGAs).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00366&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mastiyage Don Sudeera Hasaranga Gunathilaka, Yoshitaka Inui, Satoshi Kako, Kazushi Mimura, Masato Okada, Yoshihisa Yamamoto, Toru Aonishi</name></author><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Coherent Ising Machine (CIM) is a network of optical parametric oscillators that solves combinatorial optimization problems by finding the ground state of an Ising Hamiltonian. As a practical application of CIM, Aonishi et al. proposed a quantum-classical hybrid system to solve optimization problems of L0-regularization-based compressed sensing (L0RBCS). Gunathilaka et al. has further enhanced the accuracy of the system. However, the computationally expensive CIM’s stochastic differential equations (SDEs) limit the use of digital hardware implementations. As an alternative to Gunathilaka et al.’s CIM SDEs used previously, we propose using the mean-field CIM (MF-CIM) model, which is a physics-inspired heuristic solver without quantum noise. MF-CIM surmounts the high computational cost due to the simple nature of the differential equations (DEs). Furthermore, our results indicate that the proposed model has similar performance to physically accurate SDEs in both artificial and magnetic resonance imaging data, paving the way for implementing CIM-based L0RBCS on digital hardware such as Field Programmable Gate Arrays (FPGAs).</summary></entry><entry><title type="html">Modern extreme value statistics for Utopian extremes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ModernextremevaluestatisticsforUtopianextremes.html" rel="alternate" type="text/html" title="Modern extreme value statistics for Utopian extremes" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ModernextremevaluestatisticsforUtopianextremes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/ModernextremevaluestatisticsforUtopianextremes.html">&lt;p&gt;Capturing the extremal behaviour of data often requires bespoke marginal and dependence models which are grounded in rigorous asymptotic theory, and hence provide reliable extrapolation into the upper tails of the data-generating distribution. We present a toolbox of four methodological frameworks, motivated by modern extreme value theory, that can be used to accurately estimate extreme exceedance probabilities or the corresponding level in either a univariate or multivariate setting. Our frameworks were used to facilitate the winning contribution of Team Yalla to the EVA (2023) Conference Data Challenge, which was organised for the 13$^\text{th}$ International Conference on Extreme Value Analysis. This competition comprised seven teams competing across four separate sub-challenges, with each requiring the modelling of data simulated from known, yet highly complex, statistical distributions, and extrapolation far beyond the range of the available samples in order to predict probabilities of extreme events. Data were constructed to be representative of real environmental data, sampled from the fantasy country of “Utopia”&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.11054&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jordan Richards, Noura Alotaibi, Daniela Cisneros, Yan Gong, Matheus B. Guerrero, Paolo Redondo, Xuanjie Shao</name></author><category term="stat.ME" /><summary type="html">Capturing the extremal behaviour of data often requires bespoke marginal and dependence models which are grounded in rigorous asymptotic theory, and hence provide reliable extrapolation into the upper tails of the data-generating distribution. We present a toolbox of four methodological frameworks, motivated by modern extreme value theory, that can be used to accurately estimate extreme exceedance probabilities or the corresponding level in either a univariate or multivariate setting. Our frameworks were used to facilitate the winning contribution of Team Yalla to the EVA (2023) Conference Data Challenge, which was organised for the 13$^\text{th}$ International Conference on Extreme Value Analysis. This competition comprised seven teams competing across four separate sub-challenges, with each requiring the modelling of data simulated from known, yet highly complex, statistical distributions, and extrapolation far beyond the range of the available samples in order to predict probabilities of extreme events. Data were constructed to be representative of real environmental data, sampled from the fantasy country of “Utopia”</summary></entry><entry><title type="html">Noisy Measurements Are Important, the Design of Census Products Is Much More Important</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/NoisyMeasurementsAreImportanttheDesignofCensusProductsIsMuchMoreImportant.html" rel="alternate" type="text/html" title="Noisy Measurements Are Important, the Design of Census Products Is Much More Important" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/NoisyMeasurementsAreImportanttheDesignofCensusProductsIsMuchMoreImportant</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/NoisyMeasurementsAreImportanttheDesignofCensusProductsIsMuchMoreImportant.html">&lt;p&gt;McCartan et al. (2023) call for “making differential privacy work for census data users.” This commentary explains why the 2020 Census Noisy Measurement Files (NMFs) are not the best focus for that plea. The August 2021 letter from 62 prominent researchers asking for production of the direct output of the differential privacy system deployed for the 2020 Census signaled the engagement of the scholarly community in the design of decennial census data products. NMFs, the raw statistics produced by the 2020 Census Disclosure Avoidance System before any post-processing, are one component of that design-the query strategy output. The more important component is the query workload output-the statistics released to the public. Optimizing the query workload-the Redistricting Data (P.L. 94-171) Summary File, specifically-could allow the privacy-loss budget to be more effectively managed. There could be fewer noisy measurements, no post-processing bias, and direct estimates of the uncertainty from disclosure avoidance for each published statistic.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.14191&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>John M. Abowd</name></author><category term="stat.AP" /><summary type="html">McCartan et al. (2023) call for “making differential privacy work for census data users.” This commentary explains why the 2020 Census Noisy Measurement Files (NMFs) are not the best focus for that plea. The August 2021 letter from 62 prominent researchers asking for production of the direct output of the differential privacy system deployed for the 2020 Census signaled the engagement of the scholarly community in the design of decennial census data products. NMFs, the raw statistics produced by the 2020 Census Disclosure Avoidance System before any post-processing, are one component of that design-the query strategy output. The more important component is the query workload output-the statistics released to the public. Optimizing the query workload-the Redistricting Data (P.L. 94-171) Summary File, specifically-could allow the privacy-loss budget to be more effectively managed. There could be fewer noisy measurements, no post-processing bias, and direct estimates of the uncertainty from disclosure avoidance for each published statistic.</summary></entry><entry><title type="html">On Binscatter</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OnBinscatter.html" rel="alternate" type="text/html" title="On Binscatter" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OnBinscatter</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OnBinscatter.html">&lt;p&gt;Binscatter is a popular method for visualizing bivariate relationships and conducting informal specification testing. We study the properties of this method formally and develop enhanced visualization and econometric binscatter tools. These include estimating conditional means with optimal binning and quantifying uncertainty. We also highlight a methodological problem related to covariate adjustment that can yield incorrect conclusions. We revisit two applications using our methodology and find substantially different results relative to those obtained using prior informal binscatter methods. General purpose software in Python, R, and Stata is provided. Our technical work is of independent interest for the nonparametric partition-based estimation literature.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1902.09608&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matias D. Cattaneo, Richard K. Crump, Max H. Farrell, Yingjie Feng</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Binscatter is a popular method for visualizing bivariate relationships and conducting informal specification testing. We study the properties of this method formally and develop enhanced visualization and econometric binscatter tools. These include estimating conditional means with optimal binning and quantifying uncertainty. We also highlight a methodological problem related to covariate adjustment that can yield incorrect conclusions. We revisit two applications using our methodology and find substantially different results relative to those obtained using prior informal binscatter methods. General purpose software in Python, R, and Stata is provided. Our technical work is of independent interest for the nonparametric partition-based estimation literature.</summary></entry><entry><title type="html">One-Bit Total Variation Denoising over Networks with Applications to Partially Observed Epidemics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OneBitTotalVariationDenoisingoverNetworkswithApplicationstoPartiallyObservedEpidemics.html" rel="alternate" type="text/html" title="One-Bit Total Variation Denoising over Networks with Applications to Partially Observed Epidemics" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OneBitTotalVariationDenoisingoverNetworkswithApplicationstoPartiallyObservedEpidemics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OneBitTotalVariationDenoisingoverNetworkswithApplicationstoPartiallyObservedEpidemics.html">&lt;p&gt;This paper introduces a novel approach for epidemic nowcasting and forecasting over networks using total variation (TV) denoising, a method inspired by classical signal processing techniques. Considering a network that models a population as a set of $n$ nodes characterized by their infection statuses $Y_i$ and that represents contacts as edges, we prove the consistency of graph-TV denoising for estimating the underlying infection probabilities ${p_i}_{ i \in {1,\cdots, n}}$ in the presence of Bernoulli noise. Our results provide an important extension of existing bounds derived in the Gaussian case to the study of binary variables – an approach hereafter referred to as one-bit total variation denoising. The methodology is further extended to handle incomplete observations, thereby expanding its relevance to various real-world situations where observations over the full graph may not be accessible. Focusing on the context of epidemics, we establish that one-bit total variation denoising enhances both nowcasting and forecasting accuracy in networks, as further evidenced by comprehensive numerical experiments and two real-world examples. The contributions of this paper lie in its theoretical developments, particularly in addressing the incomplete data case, thereby paving the way for more precise epidemic modelling and enhanced surveillance strategies in practical settings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00619&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Claire Donnat, Olga Klopp, Nicolas Verzelen</name></author><category term="stat.ME" /><summary type="html">This paper introduces a novel approach for epidemic nowcasting and forecasting over networks using total variation (TV) denoising, a method inspired by classical signal processing techniques. Considering a network that models a population as a set of $n$ nodes characterized by their infection statuses $Y_i$ and that represents contacts as edges, we prove the consistency of graph-TV denoising for estimating the underlying infection probabilities ${p_i}_{ i \in {1,\cdots, n}}$ in the presence of Bernoulli noise. Our results provide an important extension of existing bounds derived in the Gaussian case to the study of binary variables – an approach hereafter referred to as one-bit total variation denoising. The methodology is further extended to handle incomplete observations, thereby expanding its relevance to various real-world situations where observations over the full graph may not be accessible. Focusing on the context of epidemics, we establish that one-bit total variation denoising enhances both nowcasting and forecasting accuracy in networks, as further evidenced by comprehensive numerical experiments and two real-world examples. The contributions of this paper lie in its theoretical developments, particularly in addressing the incomplete data case, thereby paving the way for more precise epidemic modelling and enhanced surveillance strategies in practical settings.</summary></entry><entry><title type="html">Optimal Bias-Correction and Valid Inference in High-Dimensional Ridge Regression: A Closed-Form Solution</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OptimalBiasCorrectionandValidInferenceinHighDimensionalRidgeRegressionAClosedFormSolution.html" rel="alternate" type="text/html" title="Optimal Bias-Correction and Valid Inference in High-Dimensional Ridge Regression: A Closed-Form Solution" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OptimalBiasCorrectionandValidInferenceinHighDimensionalRidgeRegressionAClosedFormSolution</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OptimalBiasCorrectionandValidInferenceinHighDimensionalRidgeRegressionAClosedFormSolution.html">&lt;p&gt;Ridge regression is an indispensable tool in big data econometrics but suffers from bias issues affecting both statistical efficiency and scalability. We introduce an iterative strategy to correct the bias effectively when the dimension $p$ is less than the sample size $n$. For $p&amp;gt;n$, our method optimally reduces the bias to a level unachievable through linear transformations of the response. We employ a Ridge-Screening (RS) method to handle the remaining bias when $p&amp;gt;n$, creating a reduced model suitable for bias-correction. Under certain conditions, the selected model nests the true one, making RS a novel variable selection approach. We establish the asymptotic properties and valid inferences of our de-biased ridge estimators for both $p&amp;lt; n$ and $p&amp;gt;n$, where $p$ and $n$ may grow towards infinity, along with the number of iterations. Our method is validated using simulated and real-world data examples, providing a closed-form solution to bias challenges in ridge regression inferences.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00424&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhaoxing Gao</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Ridge regression is an indispensable tool in big data econometrics but suffers from bias issues affecting both statistical efficiency and scalability. We introduce an iterative strategy to correct the bias effectively when the dimension $p$ is less than the sample size $n$. For $p&amp;gt;n$, our method optimally reduces the bias to a level unachievable through linear transformations of the response. We employ a Ridge-Screening (RS) method to handle the remaining bias when $p&amp;gt;n$, creating a reduced model suitable for bias-correction. Under certain conditions, the selected model nests the true one, making RS a novel variable selection approach. We establish the asymptotic properties and valid inferences of our de-biased ridge estimators for both $p&amp;lt; n$ and $p&amp;gt;n$, where $p$ and $n$ may grow towards infinity, along with the number of iterations. Our method is validated using simulated and real-world data examples, providing a closed-form solution to bias challenges in ridge regression inferences.</summary></entry><entry><title type="html">Orthogonal Bootstrap: Efficient Simulation of Input Uncertainty</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OrthogonalBootstrapEfficientSimulationofInputUncertainty.html" rel="alternate" type="text/html" title="Orthogonal Bootstrap: Efficient Simulation of Input Uncertainty" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OrthogonalBootstrapEfficientSimulationofInputUncertainty</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OrthogonalBootstrapEfficientSimulationofInputUncertainty.html">&lt;p&gt;Bootstrap is a popular methodology for simulating input uncertainty. However, it can be computationally expensive when the number of samples is large. We propose a new approach called \textbf{Orthogonal Bootstrap} that reduces the number of required Monte Carlo replications. We decomposes the target being simulated into two parts: the \textit{non-orthogonal part} which has a closed-form result known as Infinitesimal Jackknife and the \textit{orthogonal part} which is easier to be simulated. We theoretically and numerically show that Orthogonal Bootstrap significantly reduces the computational cost of Bootstrap while improving empirical accuracy and maintaining the same width of the constructed interval.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.19145&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kaizhao Liu, Jose Blanchet, Lexing Ying, Yiping Lu</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Bootstrap is a popular methodology for simulating input uncertainty. However, it can be computationally expensive when the number of samples is large. We propose a new approach called \textbf{Orthogonal Bootstrap} that reduces the number of required Monte Carlo replications. We decomposes the target being simulated into two parts: the \textit{non-orthogonal part} which has a closed-form result known as Infinitesimal Jackknife and the \textit{orthogonal part} which is easier to be simulated. We theoretically and numerically show that Orthogonal Bootstrap significantly reduces the computational cost of Bootstrap while improving empirical accuracy and maintaining the same width of the constructed interval.</summary></entry><entry><title type="html">Posterior exploration for computationally intensive forward models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Posteriorexplorationforcomputationallyintensiveforwardmodels.html" rel="alternate" type="text/html" title="Posterior exploration for computationally intensive forward models" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Posteriorexplorationforcomputationallyintensiveforwardmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Posteriorexplorationforcomputationallyintensiveforwardmodels.html">&lt;p&gt;In this chapter, we address the challenge of exploring the posterior distributions of Bayesian inverse problems with computationally intensive forward models. We consider various multivariate proposal distributions, and compare them with single-site Metropolis updates. We show how fast, approximate models can be leveraged to improve the MCMC sampling efficiency.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00397&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mikkel B. Lykkegaard, Colin Fox, Dave Higdon, C. Shane Reese, J. David Moulton</name></author><category term="stat.CO" /><summary type="html">In this chapter, we address the challenge of exploring the posterior distributions of Bayesian inverse problems with computationally intensive forward models. We consider various multivariate proposal distributions, and compare them with single-site Metropolis updates. We show how fast, approximate models can be leveraged to improve the MCMC sampling efficiency.</summary></entry><entry><title type="html">Propensity weighting plus adjustment in proportional hazards model is not doubly robust</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Propensityweightingplusadjustmentinproportionalhazardsmodelisnotdoublyrobust.html" rel="alternate" type="text/html" title="Propensity weighting plus adjustment in proportional hazards model is not doubly robust" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Propensityweightingplusadjustmentinproportionalhazardsmodelisnotdoublyrobust</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Propensityweightingplusadjustmentinproportionalhazardsmodelisnotdoublyrobust.html">&lt;p&gt;Recently, it has become common for applied works to combine commonly used survival analysis modeling methods, such as the multivariable Cox model and propensity score weighting, with the intention of forming a doubly robust estimator of an exposure effect hazard ratio that is unbiased in large samples when either the Cox model or the propensity score model is correctly specified. This combination does not, in general, produce a doubly robust estimator, even after regression standardization, when there is truly a causal effect. We demonstrate via simulation this lack of double robustness for the semiparametric Cox model, the Weibull proportional hazards model, and a simple proportional hazards flexible parametric model, with both the latter models fit via maximum likelihood. We provide a novel proof that the combination of propensity score weighting and a proportional hazards survival model, fit either via full or partial likelihood, is consistent under the null of no causal effect of the exposure on the outcome under particular censoring mechanisms if either the propensity score or the outcome model is correctly specified and contains all confounders. Given our results suggesting that double robustness only exists under the null, we outline two simple alternative estimators that are doubly robust for the survival difference at a given time point (in the above sense), provided the censoring mechanism can be correctly modeled, and one doubly robust method of estimation for the full survival curve. We provide R code to use these estimators for estimation and inference in the supporting information.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.16207&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Erin E Gabriel, Michael C Sachs, Ingeborg Waernbaum, Els Goetghebeur, Paul F Blanche, Stijn Vansteelandt, Arvid Sjölander, Thomas Scheike</name></author><category term="stat.ME" /><summary type="html">Recently, it has become common for applied works to combine commonly used survival analysis modeling methods, such as the multivariable Cox model and propensity score weighting, with the intention of forming a doubly robust estimator of an exposure effect hazard ratio that is unbiased in large samples when either the Cox model or the propensity score model is correctly specified. This combination does not, in general, produce a doubly robust estimator, even after regression standardization, when there is truly a causal effect. We demonstrate via simulation this lack of double robustness for the semiparametric Cox model, the Weibull proportional hazards model, and a simple proportional hazards flexible parametric model, with both the latter models fit via maximum likelihood. We provide a novel proof that the combination of propensity score weighting and a proportional hazards survival model, fit either via full or partial likelihood, is consistent under the null of no causal effect of the exposure on the outcome under particular censoring mechanisms if either the propensity score or the outcome model is correctly specified and contains all confounders. Given our results suggesting that double robustness only exists under the null, we outline two simple alternative estimators that are doubly robust for the survival difference at a given time point (in the above sense), provided the censoring mechanism can be correctly modeled, and one doubly robust method of estimation for the full survival curve. We provide R code to use these estimators for estimation and inference in the supporting information.</summary></entry><entry><title type="html">Reevaluating coexistence and stability in ecosystem networks to address ecological transients: methods and implications</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Reevaluatingcoexistenceandstabilityinecosystemnetworkstoaddressecologicaltransientsmethodsandimplications.html" rel="alternate" type="text/html" title="Reevaluating coexistence and stability in ecosystem networks to address ecological transients: methods and implications" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Reevaluatingcoexistenceandstabilityinecosystemnetworkstoaddressecologicaltransientsmethodsandimplications</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/Reevaluatingcoexistenceandstabilityinecosystemnetworkstoaddressecologicaltransientsmethodsandimplications.html">&lt;p&gt;Representing ecosystems at equilibrium has been foundational for building ecological theories, forecasting species populations and planning conservation actions. The equilibrium “balance of nature” ideal suggests that populations will eventually stabilise to a coexisting balance of species. However, a growing body of literature argues that the equilibrium ideal is inappropriate for ecosystems. Here, we develop and demonstrate a new framework for representing ecosystems without considering equilibrium dynamics. Instead, far more pragmatic ecosystem models are constructed by considering population trajectories, regardless of whether they exhibit equilibrium or transient (i.e. non-equilibrium) behaviour. This novel framework maximally utilises readily available, but often overlooked, knowledge from field observations and expert elicitation, rather than relying on theoretical ecosystem properties. We developed innovative Bayesian algorithms to generate ecosystem models in this new statistical framework, without excessive computational burden. Our results reveal that our pragmatic framework could have a dramatic impact on conservation decision-making and enhance the realism of ecosystem models and forecasts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00333&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sarah A. Vollert, Christopher Drovandi, Matthew P. Adams</name></author><category term="stat.AP" /><summary type="html">Representing ecosystems at equilibrium has been foundational for building ecological theories, forecasting species populations and planning conservation actions. The equilibrium “balance of nature” ideal suggests that populations will eventually stabilise to a coexisting balance of species. However, a growing body of literature argues that the equilibrium ideal is inappropriate for ecosystems. Here, we develop and demonstrate a new framework for representing ecosystems without considering equilibrium dynamics. Instead, far more pragmatic ecosystem models are constructed by considering population trajectories, regardless of whether they exhibit equilibrium or transient (i.e. non-equilibrium) behaviour. This novel framework maximally utilises readily available, but often overlooked, knowledge from field observations and expert elicitation, rather than relying on theoretical ecosystem properties. We developed innovative Bayesian algorithms to generate ecosystem models in this new statistical framework, without excessive computational burden. Our results reveal that our pragmatic framework could have a dramatic impact on conservation decision-making and enhance the realism of ecosystem models and forecasts.</summary></entry><entry><title type="html">Regularizing threshold priors with sparse response patterns in Bayesian factor analysis with categorical indicators</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/RegularizingthresholdpriorswithsparseresponsepatternsinBayesianfactoranalysiswithcategoricalindicators.html" rel="alternate" type="text/html" title="Regularizing threshold priors with sparse response patterns in Bayesian factor analysis with categorical indicators" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/RegularizingthresholdpriorswithsparseresponsepatternsinBayesianfactoranalysiswithcategoricalindicators</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/RegularizingthresholdpriorswithsparseresponsepatternsinBayesianfactoranalysiswithcategoricalindicators.html">&lt;p&gt;Using instruments comprising ordered responses to items are ubiquitous for studying many constructs of interest. However, using such an item response format may lead to items with response categories infrequently endorsed or unendorsed completely. In maximum likelihood estimation, this results in non-existing estimates for thresholds. This work focuses on a Bayesian estimation approach to counter this issue. The issue changes from the existence of an estimate to how to effectively construct threshold priors. The proposed prior specification reconceptualizes the threshold prior as prior on the probability of each response category. A metric that is easier to manipulate while maintaining the necessary ordering constraints on the thresholds. The resulting induced-prior is more communicable, and we demonstrate comparable statistical efficiency that existing threshold priors. Evidence is provided using a simulated data set, a Monte Carlo simulation study, and an example multi-group item-factor model analysis. All analyses demonstrate how at least a relatively informative threshold prior is necessary to avoid inefficient posterior sampling and increase confidence in the coverage rates of posterior credible intervals.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.10503&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>R. Noah Padgett, Grant B. Morgan, Tim Lomas</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Using instruments comprising ordered responses to items are ubiquitous for studying many constructs of interest. However, using such an item response format may lead to items with response categories infrequently endorsed or unendorsed completely. In maximum likelihood estimation, this results in non-existing estimates for thresholds. This work focuses on a Bayesian estimation approach to counter this issue. The issue changes from the existence of an estimate to how to effectively construct threshold priors. The proposed prior specification reconceptualizes the threshold prior as prior on the probability of each response category. A metric that is easier to manipulate while maintaining the necessary ordering constraints on the thresholds. The resulting induced-prior is more communicable, and we demonstrate comparable statistical efficiency that existing threshold priors. Evidence is provided using a simulated data set, a Monte Carlo simulation study, and an example multi-group item-factor model analysis. All analyses demonstrate how at least a relatively informative threshold prior is necessary to avoid inefficient posterior sampling and increase confidence in the coverage rates of posterior credible intervals.</summary></entry><entry><title type="html">SARMA: Scalable Low-Rank High-Dimensional Autoregressive Moving Averages via Tensor Decomposition</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/SARMAScalableLowRankHighDimensionalAutoregressiveMovingAveragesviaTensorDecomposition.html" rel="alternate" type="text/html" title="SARMA: Scalable Low-Rank High-Dimensional Autoregressive Moving Averages via Tensor Decomposition" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/SARMAScalableLowRankHighDimensionalAutoregressiveMovingAveragesviaTensorDecomposition</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/SARMAScalableLowRankHighDimensionalAutoregressiveMovingAveragesviaTensorDecomposition.html">&lt;p&gt;Existing models for high-dimensional time series are overwhelmingly developed within the finite-order vector autoregressive (VAR) framework, whereas the more flexible vector autoregressive moving averages (VARMA) have been much less considered. This paper introduces a high-dimensional model for capturing VARMA dynamics, namely the Scalable ARMA (SARMA) model, by combining novel reparameterization and tensor decomposition techniques. To ensure identifiability and computational tractability, we first consider a reparameterization of the VARMA model and discover that this interestingly amounts to a Tucker-low-rank structure for the AR coefficient tensor along the temporal dimension. Motivated by this finding, we further consider Tucker decomposition across the response and predictor dimensions of the AR coefficient tensor, enabling factor extraction across variables and time lags. Additionally, we consider sparsity assumptions on the factor loadings to accomplish automatic variable selection and greater estimation efficiency. For the proposed model, we develop both rank-constrained and sparsity-inducing estimators. Algorithms and model selection methods are also provided. Simulation studies and empirical examples confirm the validity of our theory and advantages of our approaches over existing competitors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00626&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Feiqing Huang, Kexin Li, Yao Zheng</name></author><category term="stat.ME" /><summary type="html">Existing models for high-dimensional time series are overwhelmingly developed within the finite-order vector autoregressive (VAR) framework, whereas the more flexible vector autoregressive moving averages (VARMA) have been much less considered. This paper introduces a high-dimensional model for capturing VARMA dynamics, namely the Scalable ARMA (SARMA) model, by combining novel reparameterization and tensor decomposition techniques. To ensure identifiability and computational tractability, we first consider a reparameterization of the VARMA model and discover that this interestingly amounts to a Tucker-low-rank structure for the AR coefficient tensor along the temporal dimension. Motivated by this finding, we further consider Tucker decomposition across the response and predictor dimensions of the AR coefficient tensor, enabling factor extraction across variables and time lags. Additionally, we consider sparsity assumptions on the factor loadings to accomplish automatic variable selection and greater estimation efficiency. For the proposed model, we develop both rank-constrained and sparsity-inducing estimators. Algorithms and model selection methods are also provided. Simulation studies and empirical examples confirm the validity of our theory and advantages of our approaches over existing competitors.</summary></entry><entry><title type="html">Spatial-Temporal Extreme Modeling for Point-to-Area Random Effects (PARE)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/SpatialTemporalExtremeModelingforPointtoAreaRandomEffectsPARE.html" rel="alternate" type="text/html" title="Spatial-Temporal Extreme Modeling for Point-to-Area Random Effects (PARE)" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/SpatialTemporalExtremeModelingforPointtoAreaRandomEffectsPARE</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/SpatialTemporalExtremeModelingforPointtoAreaRandomEffectsPARE.html">&lt;p&gt;One measurement modality for rainfall is a fixed location rain gauge. However, extreme rainfall, flooding, and other climate extremes often occur at larger spatial scales and affect more than one location in a community. For example, in 2017 Hurricane Harvey impacted all of Houston and the surrounding region causing widespread flooding. Flood risk modeling requires understanding of rainfall for hydrologic regions, which may contain one or more rain gauges. Further, policy changes to address the risks and damages of natural hazards such as severe flooding are usually made at the community/neighborhood level or higher geo-spatial scale. Therefore, spatial-temporal methods which convert results from one spatial scale to another are especially useful in applications for evolving environmental extremes. We develop a point-to-area random effects (PARE) modeling strategy for understanding spatial-temporal extreme values at the areal level, when the core information are time series at point locations distributed over the region.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.17271&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carlynn Fagnant, Julia C. Schedler, Katherine B. Ensor</name></author><category term="stat.ME" /><summary type="html">One measurement modality for rainfall is a fixed location rain gauge. However, extreme rainfall, flooding, and other climate extremes often occur at larger spatial scales and affect more than one location in a community. For example, in 2017 Hurricane Harvey impacted all of Houston and the surrounding region causing widespread flooding. Flood risk modeling requires understanding of rainfall for hydrologic regions, which may contain one or more rain gauges. Further, policy changes to address the risks and damages of natural hazards such as severe flooding are usually made at the community/neighborhood level or higher geo-spatial scale. Therefore, spatial-temporal methods which convert results from one spatial scale to another are especially useful in applications for evolving environmental extremes. We develop a point-to-area random effects (PARE) modeling strategy for understanding spatial-temporal extreme values at the areal level, when the core information are time series at point locations distributed over the region.</summary></entry><entry><title type="html">Visualizing periodic stability in studies: the moving average meta-analysis (MA2)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/VisualizingperiodicstabilityinstudiesthemovingaveragemetaanalysisMA2.html" rel="alternate" type="text/html" title="Visualizing periodic stability in studies: the moving average meta-analysis (MA2)" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/VisualizingperiodicstabilityinstudiesthemovingaveragemetaanalysisMA2</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/VisualizingperiodicstabilityinstudiesthemovingaveragemetaanalysisMA2.html">&lt;p&gt;Relative clinical benefits are often visually explored and formally analysed through a (cumulative) meta-analysis. In this manuscript, we introduce and further explore the moving average meta-analysis to aid towards the exploration and visualization of periodic stability in a meta-analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.13640&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Konstantinos Pateras, Suhail A. R. Doi, Kit CB Roes, Polychronis Kostoulas</name></author><category term="stat.ME" /><summary type="html">Relative clinical benefits are often visually explored and formally analysed through a (cumulative) meta-analysis. In this manuscript, we introduce and further explore the moving average meta-analysis to aid towards the exploration and visualization of periodic stability in a meta-analysis.</summary></entry><entry><title type="html">Why You Should Not Trust Interpretations in Machine Learning: Adversarial Attacks on Partial Dependence Plots</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/WhyYouShouldNotTrustInterpretationsinMachineLearningAdversarialAttacksonPartialDependencePlots.html" rel="alternate" type="text/html" title="Why You Should Not Trust Interpretations in Machine Learning: Adversarial Attacks on Partial Dependence Plots" /><published>2024-05-02T00:00:00+00:00</published><updated>2024-05-02T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/WhyYouShouldNotTrustInterpretationsinMachineLearningAdversarialAttacksonPartialDependencePlots</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/WhyYouShouldNotTrustInterpretationsinMachineLearningAdversarialAttacksonPartialDependencePlots.html">&lt;p&gt;The adoption of artificial intelligence (AI) across industries has led to the widespread use of complex black-box models and interpretation tools for decision making. This paper proposes an adversarial framework to uncover the vulnerability of permutation-based interpretation methods for machine learning tasks, with a particular focus on partial dependence (PD) plots. This adversarial framework modifies the original black box model to manipulate its predictions for instances in the extrapolation domain. As a result, it produces deceptive PD plots that can conceal discriminatory behaviors while preserving most of the original model’s predictions. This framework can produce multiple fooled PD plots via a single model. By using real-world datasets including an auto insurance claims dataset and COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) dataset, our results show that it is possible to intentionally hide the discriminatory behavior of a predictor and make the black-box model appear neutral through interpretation tools like PD plots while retaining almost all the predictions of the original black-box model. Managerial insights for regulators and practitioners are provided based on the findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18702&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xi Xin, Giles Hooker, Fei Huang</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">The adoption of artificial intelligence (AI) across industries has led to the widespread use of complex black-box models and interpretation tools for decision making. This paper proposes an adversarial framework to uncover the vulnerability of permutation-based interpretation methods for machine learning tasks, with a particular focus on partial dependence (PD) plots. This adversarial framework modifies the original black box model to manipulate its predictions for instances in the extrapolation domain. As a result, it produces deceptive PD plots that can conceal discriminatory behaviors while preserving most of the original model’s predictions. This framework can produce multiple fooled PD plots via a single model. By using real-world datasets including an auto insurance claims dataset and COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) dataset, our results show that it is possible to intentionally hide the discriminatory behavior of a predictor and make the black-box model appear neutral through interpretation tools like PD plots while retaining almost all the predictions of the original black-box model. Managerial insights for regulators and practitioners are provided based on the findings.</summary></entry></feed>