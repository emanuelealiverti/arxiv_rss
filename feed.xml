<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-19T07:14:05+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Review of EMA Public Assessment Reports where Non-Proportional Hazards were Identified</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/AReviewofEMAPublicAssessmentReportswhereNonProportionalHazardswereIdentified.html" rel="alternate" type="text/html" title="A Review of EMA Public Assessment Reports where Non-Proportional Hazards were Identified" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/AReviewofEMAPublicAssessmentReportswhereNonProportionalHazardswereIdentified</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/AReviewofEMAPublicAssessmentReportswhereNonProportionalHazardswereIdentified.html">&lt;p&gt;While well-established methods for time-to-event data are available when the proportional hazards assumption holds, there is no consensus on the best approach under non-proportional hazards. A wide range of parametric and non-parametric methods for testing and estimation in this scenario have been proposed. In this review we identified EMA marketing authorization procedures where non-proportional hazards were raised as a potential issue in the risk-benefit assessment and extract relevant information on trial design and results reported in the corresponding European Assessment Reports (EPARs) available in the database at paediatricdata.eu.
  We identified 16 Marketing authorization procedures, reporting results on a total of 18 trials. Most procedures covered the authorization of treatments from the oncology domain. For the majority of trials NPH issues were related to a suspected delayed treatment effect, or different treatment effects in known subgroups. Issues related to censoring, or treatment switching were also identified. For most of the trials the primary analysis was performed using conventional methods assuming proportional hazards, even if NPH was anticipated. Differential treatment effects were addressed using stratification and delayed treatment effect considered for sample size planning. Even though, not considered in the primary analysis, some procedures reported extensive sensitivity analyses and model diagnostics evaluating the proportional hazards assumption. For a few procedures methods addressing NPH (e.g.~weighted log-rank tests) were used in the primary analysis. We extracted estimates of the median survival, hazard ratios, and time of survival curve separation. In addition, we digitized the KM curves to reconstruct close to individual patient level data. Extracted outcomes served as the basis for a simulation study of methods for time to event analysis under NPH.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12492&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Florian Klinglmueller, Norbert Benda, Tim Friede, Tobias Fellinger, Harald Heinzl, Andrew Hooker, Franz Koenig, Tim Mathes, Martin Posch, Florian Stampfer, Susanne Urach</name></author><category term="stat.AP" /><summary type="html">While well-established methods for time-to-event data are available when the proportional hazards assumption holds, there is no consensus on the best approach under non-proportional hazards. A wide range of parametric and non-parametric methods for testing and estimation in this scenario have been proposed. In this review we identified EMA marketing authorization procedures where non-proportional hazards were raised as a potential issue in the risk-benefit assessment and extract relevant information on trial design and results reported in the corresponding European Assessment Reports (EPARs) available in the database at paediatricdata.eu. We identified 16 Marketing authorization procedures, reporting results on a total of 18 trials. Most procedures covered the authorization of treatments from the oncology domain. For the majority of trials NPH issues were related to a suspected delayed treatment effect, or different treatment effects in known subgroups. Issues related to censoring, or treatment switching were also identified. For most of the trials the primary analysis was performed using conventional methods assuming proportional hazards, even if NPH was anticipated. Differential treatment effects were addressed using stratification and delayed treatment effect considered for sample size planning. Even though, not considered in the primary analysis, some procedures reported extensive sensitivity analyses and model diagnostics evaluating the proportional hazards assumption. For a few procedures methods addressing NPH (e.g.~weighted log-rank tests) were used in the primary analysis. We extracted estimates of the median survival, hazard ratios, and time of survival curve separation. In addition, we digitized the KM curves to reconstruct close to individual patient level data. Extracted outcomes served as the basis for a simulation study of methods for time to event analysis under NPH.</summary></entry><entry><title type="html">An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/AnEmbeddedDiachronicSenseChangeModelwithaCaseStudyfromAncientGreek.html" rel="alternate" type="text/html" title="An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/AnEmbeddedDiachronicSenseChangeModelwithaCaseStudyfromAncientGreek</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/AnEmbeddedDiachronicSenseChangeModelwithaCaseStudyfromAncientGreek.html">&lt;p&gt;Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small and sparse, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC (Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as ``kosmos’’ (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal changes in these representations. This paper introduces EDiSC, an Embedded DiSC model, which combines word embeddings with DiSC to provide superior model performance. It is shown empirically that EDiSC offers improved predictive accuracy, ground-truth recovery and uncertainty quantification, as well as better sampling efficiency and scalability properties with MCMC methods. The challenges of fitting these models are also discussed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.00541&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Schyan Zafar, Geoff K. Nicholls</name></author><category term="stat.ME" /><summary type="html">Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small and sparse, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC (Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as ``kosmos’’ (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal changes in these representations. This paper introduces EDiSC, an Embedded DiSC model, which combines word embeddings with DiSC to provide superior model performance. It is shown empirically that EDiSC offers improved predictive accuracy, ground-truth recovery and uncertainty quantification, as well as better sampling efficiency and scalability properties with MCMC methods. The challenges of fitting these models are also discussed.</summary></entry><entry><title type="html">Anatomy of Elite and Mass Polarization in Social Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/AnatomyofEliteandMassPolarizationinSocialNetworks.html" rel="alternate" type="text/html" title="Anatomy of Elite and Mass Polarization in Social Networks" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/AnatomyofEliteandMassPolarizationinSocialNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/AnatomyofEliteandMassPolarizationinSocialNetworks.html">&lt;p&gt;Existing methods for quantifying polarization in social networks typically report a single value describing the amount of polarization in a social system. While this approach can be used to confirm the observation that many societies have witnessed an increase in political polarization in recent years, it misses the complexities that could be used to understand the reasons behind this phenomenon. Notably, opposing groups can have unequal impact on polarization, and the elites are often understood to be more divided than the masses, making it critical to differentiate their roles in polarized systems. We propose a method to characterize these distinct hierarchies in polarized networks, enabling separate polarization measurements for these groups within a single social system. Applied to polarized topics in the Finnish Twittersphere surrounding the 2019 and 2023 parliamentary elections, our analysis reveals valuable insights: 1) The impact of opposing groups on observed polarization is rarely balanced, and 2) while the elite strongly contributes to structural polarization and consistently display greater alignment across various topics, the masses have also recently experienced a surge in issue alignment, a special form of polarization. Our findings suggest that the masses may not be as immune to an increasingly polarized environment as previously thought.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12525&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ali Salloum, Ted Hsuan Yun Chen, Mikko Kivelä</name></author><category term="stat.AP" /><summary type="html">Existing methods for quantifying polarization in social networks typically report a single value describing the amount of polarization in a social system. While this approach can be used to confirm the observation that many societies have witnessed an increase in political polarization in recent years, it misses the complexities that could be used to understand the reasons behind this phenomenon. Notably, opposing groups can have unequal impact on polarization, and the elites are often understood to be more divided than the masses, making it critical to differentiate their roles in polarized systems. We propose a method to characterize these distinct hierarchies in polarized networks, enabling separate polarization measurements for these groups within a single social system. Applied to polarized topics in the Finnish Twittersphere surrounding the 2019 and 2023 parliamentary elections, our analysis reveals valuable insights: 1) The impact of opposing groups on observed polarization is rarely balanced, and 2) while the elite strongly contributes to structural polarization and consistently display greater alignment across various topics, the masses have also recently experienced a surge in issue alignment, a special form of polarization. Our findings suggest that the masses may not be as immune to an increasingly polarized environment as previously thought.</summary></entry><entry><title type="html">Bayesian Consistency for Long Memory Processes: A Semiparametric Perspective</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/BayesianConsistencyforLongMemoryProcessesASemiparametricPerspective.html" rel="alternate" type="text/html" title="Bayesian Consistency for Long Memory Processes: A Semiparametric Perspective" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/BayesianConsistencyforLongMemoryProcessesASemiparametricPerspective</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/BayesianConsistencyforLongMemoryProcessesASemiparametricPerspective.html">&lt;p&gt;In this work, we will investigate a Bayesian approach to estimating the parameters of long memory models. Long memory, characterized by the phenomenon of hyperbolic autocorrelation decay in time series, has garnered significant attention. This is because, in many situations, the assumption of short memory, such as the Markovianity assumption, can be deemed too restrictive. Applications for long memory models can be readily found in fields such as astronomy, finance, and environmental sciences. However, current parametric and semiparametric approaches to modeling long memory present challenges, particularly in the estimation process.
  In this study, we will introduce various methods applied to this problem from a Bayesian perspective, along with a novel semiparametric approach for deriving the posterior distribution of the long memory parameter. Additionally, we will establish the asymptotic properties of the model. An advantage of this approach is that it allows to implement state-of-the-art efficient algorithms for nonparametric Bayesian models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12780&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Clara Grazian</name></author><category term="stat.ME" /><summary type="html">In this work, we will investigate a Bayesian approach to estimating the parameters of long memory models. Long memory, characterized by the phenomenon of hyperbolic autocorrelation decay in time series, has garnered significant attention. This is because, in many situations, the assumption of short memory, such as the Markovianity assumption, can be deemed too restrictive. Applications for long memory models can be readily found in fields such as astronomy, finance, and environmental sciences. However, current parametric and semiparametric approaches to modeling long memory present challenges, particularly in the estimation process. In this study, we will introduce various methods applied to this problem from a Bayesian perspective, along with a novel semiparametric approach for deriving the posterior distribution of the long memory parameter. Additionally, we will establish the asymptotic properties of the model. An advantage of this approach is that it allows to implement state-of-the-art efficient algorithms for nonparametric Bayesian models.</summary></entry><entry><title type="html">Bayesian Networks and Machine Learning for COVID-19 Severity Explanation and Demographic Symptom Classification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/BayesianNetworksandMachineLearningforCOVID19SeverityExplanationandDemographicSymptomClassification.html" rel="alternate" type="text/html" title="Bayesian Networks and Machine Learning for COVID-19 Severity Explanation and Demographic Symptom Classification" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/BayesianNetworksandMachineLearningforCOVID19SeverityExplanationandDemographicSymptomClassification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/BayesianNetworksandMachineLearningforCOVID19SeverityExplanationandDemographicSymptomClassification.html">&lt;p&gt;With the prevailing efforts to combat the coronavirus disease 2019 (COVID-19) pandemic, there are still uncertainties that are yet to be discovered about its spread, future impact, and resurgence. In this paper, we present a three-stage data-driven approach to distill the hidden information about COVID-19. The first stage employs a Bayesian network structure learning method to identify the causal relationships among COVID-19 symptoms and their intrinsic demographic variables. As a second stage, the output from the Bayesian network structure learning, serves as a useful guide to train an unsupervised machine learning (ML) algorithm that uncovers the similarities in patients’ symptoms through clustering. The final stage then leverages the labels obtained from clustering to train a demographic symptom identification (DSID) model which predicts a patient’s symptom class and the corresponding demographic probability distribution. We applied our method on the COVID-19 dataset obtained from the Centers for Disease Control and Prevention (CDC) in the United States. Results from the experiments show a testing accuracy of 99.99%, as against the 41.15% accuracy of a heuristic ML method. This strongly reveals the viability of our Bayesian network and ML approach in understanding the relationship between the virus symptoms, and providing insights on patients’ stratification towards reducing the severity of the virus.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.10807&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Oluwaseun T. Ajayi, Yu Cheng</name></author><category term="stat.ML," /><category term="stat.AP" /><summary type="html">With the prevailing efforts to combat the coronavirus disease 2019 (COVID-19) pandemic, there are still uncertainties that are yet to be discovered about its spread, future impact, and resurgence. In this paper, we present a three-stage data-driven approach to distill the hidden information about COVID-19. The first stage employs a Bayesian network structure learning method to identify the causal relationships among COVID-19 symptoms and their intrinsic demographic variables. As a second stage, the output from the Bayesian network structure learning, serves as a useful guide to train an unsupervised machine learning (ML) algorithm that uncovers the similarities in patients’ symptoms through clustering. The final stage then leverages the labels obtained from clustering to train a demographic symptom identification (DSID) model which predicts a patient’s symptom class and the corresponding demographic probability distribution. We applied our method on the COVID-19 dataset obtained from the Centers for Disease Control and Prevention (CDC) in the United States. Results from the experiments show a testing accuracy of 99.99%, as against the 41.15% accuracy of a heuristic ML method. This strongly reveals the viability of our Bayesian network and ML approach in understanding the relationship between the virus symptoms, and providing insights on patients’ stratification towards reducing the severity of the virus.</summary></entry><entry><title type="html">Bridging Binarization: Causal Inference with Dichotomized Continuous Treatments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/BridgingBinarizationCausalInferencewithDichotomizedContinuousTreatments.html" rel="alternate" type="text/html" title="Bridging Binarization: Causal Inference with Dichotomized Continuous Treatments" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/BridgingBinarizationCausalInferencewithDichotomizedContinuousTreatments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/BridgingBinarizationCausalInferencewithDichotomizedContinuousTreatments.html">&lt;p&gt;The average treatment effect (ATE) is a common parameter estimated in causal inference literature, but it is only defined for binary treatments. Thus, despite concerns raised by some researchers, many studies seeking to estimate the causal effect of a continuous treatment create a new binary treatment variable by dichotomizing the continuous values into two categories. In this paper, we affirm binarization as a statistically valid method for answering causal questions about continuous treatments by showing the equivalence between the binarized ATE and the difference in the average outcomes of two specific modified treatment policies. These policies impose cut-offs corresponding to the binarized treatment variable and assume preservation of relative self-selection. Relative self-selection is the ratio of the probability density of an individual having an exposure equal to one value of the continuous treatment variable versus another. The policies assume that, for any two values of the treatment variable with non-zero probability density after the cut-off, this ratio will remain unchanged. Through this equivalence, we clarify the assumptions underlying binarization and discuss how to properly interpret the resulting estimator. Additionally, we introduce a new target parameter that can be computed after binarization that considers the status-quo world. We argue that this parameter addresses more relevant causal questions than the traditional binarized ATE parameter. Finally, we present a simulation study to illustrate the implications of these assumptions when analyzing data and to demonstrate how to correctly implement estimators of the parameters discussed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07109&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kaitlyn J. Lee, Alan Hubbard, Alejandro Schuler</name></author><category term="stat.ME" /><summary type="html">The average treatment effect (ATE) is a common parameter estimated in causal inference literature, but it is only defined for binary treatments. Thus, despite concerns raised by some researchers, many studies seeking to estimate the causal effect of a continuous treatment create a new binary treatment variable by dichotomizing the continuous values into two categories. In this paper, we affirm binarization as a statistically valid method for answering causal questions about continuous treatments by showing the equivalence between the binarized ATE and the difference in the average outcomes of two specific modified treatment policies. These policies impose cut-offs corresponding to the binarized treatment variable and assume preservation of relative self-selection. Relative self-selection is the ratio of the probability density of an individual having an exposure equal to one value of the continuous treatment variable versus another. The policies assume that, for any two values of the treatment variable with non-zero probability density after the cut-off, this ratio will remain unchanged. Through this equivalence, we clarify the assumptions underlying binarization and discuss how to properly interpret the resulting estimator. Additionally, we introduce a new target parameter that can be computed after binarization that considers the status-quo world. We argue that this parameter addresses more relevant causal questions than the traditional binarized ATE parameter. Finally, we present a simulation study to illustrate the implications of these assumptions when analyzing data and to demonstrate how to correctly implement estimators of the parameters discussed.</summary></entry><entry><title type="html">Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/CausalGraphDiscoverywithRetrievalAugmentedGenerationbasedLargeLanguageModels.html" rel="alternate" type="text/html" title="Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/CausalGraphDiscoverywithRetrievalAugmentedGenerationbasedLargeLanguageModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/CausalGraphDiscoverywithRetrievalAugmentedGenerationbasedLargeLanguageModels.html">&lt;p&gt;Causal graph recovery is traditionally done using statistical estimation-based methods or based on individual’s knowledge about variables of interests. They often suffer from data collection biases and limitations of individuals’ knowledge. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that leverages LLMs to deduce causal relationships in general causal graph recovery tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs extracted from scientific publication database as well as experiment data about factors of interest to achieve this goal. Our method gives a prompting strategy to extract associational relationships among those factors and a mechanism to perform causality verification for these associations. Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets. More importantly, as causality among some factors may change as new research results emerge, our method show sensitivity to new evidence in the literature and can provide useful information for updating causal graphs accordingly.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.15301&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuzhe Zhang, Yipeng Zhang, Yidong Gan, Lina Yao, Chen Wang</name></author><category term="stat.ME" /><summary type="html">Causal graph recovery is traditionally done using statistical estimation-based methods or based on individual’s knowledge about variables of interests. They often suffer from data collection biases and limitations of individuals’ knowledge. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that leverages LLMs to deduce causal relationships in general causal graph recovery tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs extracted from scientific publication database as well as experiment data about factors of interest to achieve this goal. Our method gives a prompting strategy to extract associational relationships among those factors and a mechanism to perform causality verification for these associations. Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets. More importantly, as causality among some factors may change as new research results emerge, our method show sensitivity to new evidence in the literature and can provide useful information for updating causal graphs accordingly.</summary></entry><entry><title type="html">Clustering functional data with measurement errors: a simulation-based approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Clusteringfunctionaldatawithmeasurementerrorsasimulationbasedapproach.html" rel="alternate" type="text/html" title="Clustering functional data with measurement errors: a simulation-based approach" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Clusteringfunctionaldatawithmeasurementerrorsasimulationbasedapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Clusteringfunctionaldatawithmeasurementerrorsasimulationbasedapproach.html">&lt;p&gt;Clustering analysis of functional data, which comprises observations that evolve continuously over time or space, has gained increasing attention across various scientific disciplines. Practical applications often involve functional data that are contaminated with measurement errors arising from imprecise instruments, sampling errors, or other sources. These errors can significantly distort the inherent data structure, resulting in erroneous clustering outcomes. In this paper, we propose a simulation-based approach designed to mitigate the impact of measurement errors. Our proposed method estimates the distribution of functional measurement errors through repeated measurements. Subsequently, the clustering algorithm is applied to simulated data generated from the conditional distribution of the unobserved true functional data given the observed contaminated functional data, accounting for the adjustments made to rectify measurement errors. We illustrate through simulations show that the proposed method has improved numerical performance than the naive methods that neglect such errors. Our proposed method was applied to a childhood obesity study, giving more reliable clustering results&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11942&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tingyu Zhu, Lan Xue, Carmen Tekwe, Keith Diaz, Mark Benden, Roger Zoh</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Clustering analysis of functional data, which comprises observations that evolve continuously over time or space, has gained increasing attention across various scientific disciplines. Practical applications often involve functional data that are contaminated with measurement errors arising from imprecise instruments, sampling errors, or other sources. These errors can significantly distort the inherent data structure, resulting in erroneous clustering outcomes. In this paper, we propose a simulation-based approach designed to mitigate the impact of measurement errors. Our proposed method estimates the distribution of functional measurement errors through repeated measurements. Subsequently, the clustering algorithm is applied to simulated data generated from the conditional distribution of the unobserved true functional data given the observed contaminated functional data, accounting for the adjustments made to rectify measurement errors. We illustrate through simulations show that the proposed method has improved numerical performance than the naive methods that neglect such errors. Our proposed method was applied to a childhood obesity study, giving more reliable clustering results</summary></entry><entry><title type="html">DU-Shapley: A Shapley Value Proxy for Efficient Dataset Valuation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/DUShapleyAShapleyValueProxyforEfficientDatasetValuation.html" rel="alternate" type="text/html" title="DU-Shapley: A Shapley Value Proxy for Efficient Dataset Valuation" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/DUShapleyAShapleyValueProxyforEfficientDatasetValuation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/DUShapleyAShapleyValueProxyforEfficientDatasetValuation.html">&lt;p&gt;We consider the dataset valuation problem, that is, the problem of quantifying the incremental gain, to some relevant pre-defined utility of a machine learning task, of aggregating an individual dataset to others. The Shapley value is a natural tool to perform dataset valuation due to its formal axiomatic justification, which can be combined with Monte Carlo integration to overcome the computational tractability challenges. Such generic approximation methods, however, remain expensive in some cases. In this paper, we exploit the knowledge about the structure of the dataset valuation problem to devise more efficient Shapley value estimators. We propose a novel approximation, referred to as discrete uniform Shapley, which is expressed as an expectation under a discrete uniform distribution with support of reasonable size. We justify the relevancy of the proposed framework via asymptotic and non-asymptotic theoretical guarantees and illustrate its benefits via an extensive set of numerical experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.02071&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Felipe Garrido-Lucero, Benjamin Heymann, Maxime Vono, Patrick Loiseau, Vianney Perchet</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">We consider the dataset valuation problem, that is, the problem of quantifying the incremental gain, to some relevant pre-defined utility of a machine learning task, of aggregating an individual dataset to others. The Shapley value is a natural tool to perform dataset valuation due to its formal axiomatic justification, which can be combined with Monte Carlo integration to overcome the computational tractability challenges. Such generic approximation methods, however, remain expensive in some cases. In this paper, we exploit the knowledge about the structure of the dataset valuation problem to devise more efficient Shapley value estimators. We propose a novel approximation, referred to as discrete uniform Shapley, which is expressed as an expectation under a discrete uniform distribution with support of reasonable size. We justify the relevancy of the proposed framework via asymptotic and non-asymptotic theoretical guarantees and illustrate its benefits via an extensive set of numerical experiments.</summary></entry><entry><title type="html">Detecting Outbreaks Using a Latent Field: Part I – Spatial Modeling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/DetectingOutbreaksUsingaLatentFieldPartISpatialModeling.html" rel="alternate" type="text/html" title="Detecting Outbreaks Using a Latent Field: Part I – Spatial Modeling" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/DetectingOutbreaksUsingaLatentFieldPartISpatialModeling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/DetectingOutbreaksUsingaLatentFieldPartISpatialModeling.html">&lt;p&gt;In this paper, we develop a method to estimate the infection-rate of a disease, over a region, as a field that varies in space and time. To do so, we use time-series of case-counts of symptomatic patients as observed in the areal units that comprise the region. We also extend an epidemiological model, initially developed to represent the temporal dynamics in a single areal unit, to encompass multiple areal units. This is done using a (parameterized) Gaussian random field, whose structure is modeled using the dynamics in the case-counts, and which serves as a spatial prior, in the estimation process. The estimation is performed using an adaptive Markov chain Monte Carlo method, using COVID-19 case-count data collected from three adjacent counties in New Mexico, USA. We find that we can estimate both the temporal and spatial variation of the infection with sufficient accuracy to be useful in forecasting. Further, the ability to “borrow” information from neighboring areal units allows us to regularize the estimation in areal units with high variance (“poor quality”) data. The ability to forecast allows us to check whether the estimated infection-rate can be used to detect a change in the epidemiological dynamics e.g., the arrival of a new wave of infection, such as the fall wave of 2020 which arrived in New Mexico in mid-September 2020. We fashion a simple anomaly detector, conditioned on the estimated infection-rate and find that it performs better than a conventional surveillance algorithm that uses case-counts (and not the infection-rate) to detect the arrival of the same wave.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12810&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Cosmin Safta, Wyatt Bridgman, Jaideep Ray</name></author><category term="stat.AP" /><summary type="html">In this paper, we develop a method to estimate the infection-rate of a disease, over a region, as a field that varies in space and time. To do so, we use time-series of case-counts of symptomatic patients as observed in the areal units that comprise the region. We also extend an epidemiological model, initially developed to represent the temporal dynamics in a single areal unit, to encompass multiple areal units. This is done using a (parameterized) Gaussian random field, whose structure is modeled using the dynamics in the case-counts, and which serves as a spatial prior, in the estimation process. The estimation is performed using an adaptive Markov chain Monte Carlo method, using COVID-19 case-count data collected from three adjacent counties in New Mexico, USA. We find that we can estimate both the temporal and spatial variation of the infection with sufficient accuracy to be useful in forecasting. Further, the ability to “borrow” information from neighboring areal units allows us to regularize the estimation in areal units with high variance (“poor quality”) data. The ability to forecast allows us to check whether the estimated infection-rate can be used to detect a change in the epidemiological dynamics e.g., the arrival of a new wave of infection, such as the fall wave of 2020 which arrived in New Mexico in mid-September 2020. We fashion a simple anomaly detector, conditioned on the estimated infection-rate and find that it performs better than a conventional surveillance algorithm that uses case-counts (and not the infection-rate) to detect the arrival of the same wave.</summary></entry><entry><title type="html">Estimating the linear relation between variables that are never jointly observed</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Estimatingthelinearrelationbetweenvariablesthatareneverjointlyobserved.html" rel="alternate" type="text/html" title="Estimating the linear relation between variables that are never jointly observed" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Estimatingthelinearrelationbetweenvariablesthatareneverjointlyobserved</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Estimatingthelinearrelationbetweenvariablesthatareneverjointlyobserved.html">&lt;p&gt;In modern experimental science there is a commonly encountered problem of estimating the coefficients of a linear regression in the context where the variables of interest can never be observed simultaneously. Assuming that the global experiment can be decomposed into sub-experiments with distinct first moments, we propose two estimators of the linear regression that take this additional information into account. We consider an estimator based on moments, and an estimator based on optimal transport theory. These estimators are proven to be consistent as well as asymptotically Gaussian under weak hypotheses. The asymptotic variance has no explicit expression, except in some particular cases, for which reason a stratified bootstrap approach is developed to build confidence intervals for the estimated parameters, whose consistency is also shown. A simulation study, assessing and comparing the finite sample performances of these estimators, demonstrated the advantages of the bootstrap approach in multiple realistic scenarios. An application to in vivo experiments, conducted in the context of studying radio-induced adverse effects on mice, revealed important relationships between the biomarkers of interest that could not be identified with the considered naive approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.00140&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Polina Arsenteva, Mohamed Amine Benadjaoud, Hervé Cardot</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In modern experimental science there is a commonly encountered problem of estimating the coefficients of a linear regression in the context where the variables of interest can never be observed simultaneously. Assuming that the global experiment can be decomposed into sub-experiments with distinct first moments, we propose two estimators of the linear regression that take this additional information into account. We consider an estimator based on moments, and an estimator based on optimal transport theory. These estimators are proven to be consistent as well as asymptotically Gaussian under weak hypotheses. The asymptotic variance has no explicit expression, except in some particular cases, for which reason a stratified bootstrap approach is developed to build confidence intervals for the estimated parameters, whose consistency is also shown. A simulation study, assessing and comparing the finite sample performances of these estimators, demonstrated the advantages of the bootstrap approach in multiple realistic scenarios. An application to in vivo experiments, conducted in the context of studying radio-induced adverse effects on mice, revealed important relationships between the biomarkers of interest that could not be identified with the considered naive approach.</summary></entry><entry><title type="html">Exploring Intra and Inter-language Consistency in Embeddings with ICA</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/ExploringIntraandInterlanguageConsistencyinEmbeddingswithICA.html" rel="alternate" type="text/html" title="Exploring Intra and Inter-language Consistency in Embeddings with ICA" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/ExploringIntraandInterlanguageConsistencyinEmbeddingswithICA</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/ExploringIntraandInterlanguageConsistencyinEmbeddingswithICA.html">&lt;p&gt;Word embeddings represent words as multidimensional real vectors, facilitating data analysis and processing, but are often challenging to interpret. Independent Component Analysis (ICA) creates clearer semantic axes by identifying independent key features. Previous research has shown ICA’s potential to reveal universal semantic axes across languages. However, it lacked verification of the consistency of independent components within and across languages. We investigated the consistency of semantic axes in two ways: both within a single language and across multiple languages. We first probed into intra-language consistency, focusing on the reproducibility of axes by performing ICA multiple times and clustering the outcomes. Then, we statistically examined inter-language consistency by verifying those axes’ correspondences using statistical tests. We newly applied statistical methods to establish a robust framework that ensures the reliability and universality of semantic axes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12474&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rongzhi Li, Takeru Matsuda, Hitomi Yanaka</name></author><category term="stat.ME" /><summary type="html">Word embeddings represent words as multidimensional real vectors, facilitating data analysis and processing, but are often challenging to interpret. Independent Component Analysis (ICA) creates clearer semantic axes by identifying independent key features. Previous research has shown ICA’s potential to reveal universal semantic axes across languages. However, it lacked verification of the consistency of independent components within and across languages. We investigated the consistency of semantic axes in two ways: both within a single language and across multiple languages. We first probed into intra-language consistency, focusing on the reproducibility of axes by performing ICA multiple times and clustering the outcomes. Then, we statistically examined inter-language consistency by verifying those axes’ correspondences using statistical tests. We newly applied statistical methods to establish a robust framework that ensures the reliability and universality of semantic axes.</summary></entry><entry><title type="html">Fast post-process Bayesian inference with Variational Sparse Bayesian Quadrature</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/FastpostprocessBayesianinferencewithVariationalSparseBayesianQuadrature.html" rel="alternate" type="text/html" title="Fast post-process Bayesian inference with Variational Sparse Bayesian Quadrature" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/FastpostprocessBayesianinferencewithVariationalSparseBayesianQuadrature</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/FastpostprocessBayesianinferencewithVariationalSparseBayesianQuadrature.html">&lt;p&gt;In applied Bayesian inference scenarios, users may have access to a large number of pre-existing model evaluations, for example from maximum-a-posteriori (MAP) optimization runs. However, traditional approximate inference techniques make little to no use of this available information. We propose the framework of post-process Bayesian inference as a means to obtain a quick posterior approximation from existing target density evaluations, with no further model calls. Within this framework, we introduce Variational Sparse Bayesian Quadrature (VSBQ), a method for post-process approximate inference for models with black-box and potentially noisy likelihoods. VSBQ reuses existing target density evaluations to build a sparse Gaussian process (GP) surrogate model of the log posterior density function. Subsequently, we leverage sparse-GP Bayesian quadrature combined with variational inference to achieve fast approximate posterior inference over the surrogate. We validate our method on challenging synthetic scenarios and real-world applications from computational neuroscience. The experiments show that VSBQ builds high-quality posterior approximations by post-processing existing optimization traces, with no further model evaluations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.05263&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chengkun Li, Grégoire Clarté, Martin J{\o}rgensen, Luigi Acerbi</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">In applied Bayesian inference scenarios, users may have access to a large number of pre-existing model evaluations, for example from maximum-a-posteriori (MAP) optimization runs. However, traditional approximate inference techniques make little to no use of this available information. We propose the framework of post-process Bayesian inference as a means to obtain a quick posterior approximation from existing target density evaluations, with no further model calls. Within this framework, we introduce Variational Sparse Bayesian Quadrature (VSBQ), a method for post-process approximate inference for models with black-box and potentially noisy likelihoods. VSBQ reuses existing target density evaluations to build a sparse Gaussian process (GP) surrogate model of the log posterior density function. Subsequently, we leverage sparse-GP Bayesian quadrature combined with variational inference to achieve fast approximate posterior inference over the surrogate. We validate our method on challenging synthetic scenarios and real-world applications from computational neuroscience. The experiments show that VSBQ builds high-quality posterior approximations by post-processing existing optimization traces, with no further model evaluations.</summary></entry><entry><title type="html">High-dimensional Outlier Detection via Stability</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/HighdimensionalOutlierDetectionviaStability.html" rel="alternate" type="text/html" title="High-dimensional Outlier Detection via Stability" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/HighdimensionalOutlierDetectionviaStability</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/HighdimensionalOutlierDetectionviaStability.html">&lt;p&gt;The Minimum Covariance Determinant (MCD) method is a widely adopted tool for robust estimation and outlier detection. In this paper, we introduce a new framework for model selection in MCD with spectral embedding based on the notion of stability. Our best subset algorithm leverages principal component analysis for dimension reduction, statistical depths for effective initialization, and concentration steps for subset refinement. Subsequently, we construct a bootstrap procedure to estimate the instability of the best subset algorithm. The parameter combination exhibiting minimal instability proves ideal for the purposes of high-dimensional outlier detection, while the instability path offers insights into the inlier/outlier structure. We rigorously benchmark the proposed framework against existing MCD variants and illustrate its practical utility on two spectra data sets and a cancer genomics data set.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.14359&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Qiang Heng, Hui Shen, Kenneth Lange</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">The Minimum Covariance Determinant (MCD) method is a widely adopted tool for robust estimation and outlier detection. In this paper, we introduce a new framework for model selection in MCD with spectral embedding based on the notion of stability. Our best subset algorithm leverages principal component analysis for dimension reduction, statistical depths for effective initialization, and concentration steps for subset refinement. Subsequently, we construct a bootstrap procedure to estimate the instability of the best subset algorithm. The parameter combination exhibiting minimal instability proves ideal for the purposes of high-dimensional outlier detection, while the instability path offers insights into the inlier/outlier structure. We rigorously benchmark the proposed framework against existing MCD variants and illustrate its practical utility on two spectra data sets and a cancer genomics data set.</summary></entry><entry><title type="html">Identifying Genetic Variants for Obesity Incorporating Prior Insights: Quantile Regression with Insight Fusion for Ultra-high Dimensional Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/IdentifyingGeneticVariantsforObesityIncorporatingPriorInsightsQuantileRegressionwithInsightFusionforUltrahighDimensionalData.html" rel="alternate" type="text/html" title="Identifying Genetic Variants for Obesity Incorporating Prior Insights: Quantile Regression with Insight Fusion for Ultra-high Dimensional Data" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/IdentifyingGeneticVariantsforObesityIncorporatingPriorInsightsQuantileRegressionwithInsightFusionforUltrahighDimensionalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/IdentifyingGeneticVariantsforObesityIncorporatingPriorInsightsQuantileRegressionwithInsightFusionforUltrahighDimensionalData.html">&lt;p&gt;Obesity is widely recognized as a critical and pervasive health concern. We strive to identify important genetic risk factors from hundreds of thousands of single nucleotide polymorphisms (SNPs) for obesity. We propose and apply a novel Quantile Regression with Insight Fusion (QRIF) approach that can integrate insights from established studies or domain knowledge to simultaneously select variables and modeling for ultra-high dimensional genetic data, focusing on high conditional quantiles of body mass index (BMI) that are of most interest. We discover interesting new SNPs and shed new light on a comprehensive view of the underlying genetic risk factors for different levels of BMI. This may potentially pave the way for more precise and targeted treatment strategies. The QRIF approach intends to balance the trade-off between the prior insights and the observed data while being robust to potential false information. We further establish the desirable asymptotic properties under the challenging non-differentiable check loss functions via Huber loss approximation and nonconvex SCAD penalty via local linear approximation. Finally, we develop an efficient algorithm for the QRIF approach. Our simulation studies further demonstrate its effectiveness.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12212&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiantong Wang, Heng Lian, Yan Yu, Heping Zhang</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Obesity is widely recognized as a critical and pervasive health concern. We strive to identify important genetic risk factors from hundreds of thousands of single nucleotide polymorphisms (SNPs) for obesity. We propose and apply a novel Quantile Regression with Insight Fusion (QRIF) approach that can integrate insights from established studies or domain knowledge to simultaneously select variables and modeling for ultra-high dimensional genetic data, focusing on high conditional quantiles of body mass index (BMI) that are of most interest. We discover interesting new SNPs and shed new light on a comprehensive view of the underlying genetic risk factors for different levels of BMI. This may potentially pave the way for more precise and targeted treatment strategies. The QRIF approach intends to balance the trade-off between the prior insights and the observed data while being robust to potential false information. We further establish the desirable asymptotic properties under the challenging non-differentiable check loss functions via Huber loss approximation and nonconvex SCAD penalty via local linear approximation. Finally, we develop an efficient algorithm for the QRIF approach. Our simulation studies further demonstrate its effectiveness.</summary></entry><entry><title type="html">Identifying Sample Size and Accuracy and Precision of the Estimators in Case-Crossover Designs with Distributed Lags of Heteroskedastic Time-Varying Continuous Exposures Measured with Simple or Complex Error</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/IdentifyingSampleSizeandAccuracyandPrecisionoftheEstimatorsinCaseCrossoverDesignswithDistributedLagsofHeteroskedasticTimeVaryingContinuousExposuresMeasuredwithSimpleorComplexError.html" rel="alternate" type="text/html" title="Identifying Sample Size and Accuracy and Precision of the Estimators in Case-Crossover Designs with Distributed Lags of Heteroskedastic Time-Varying Continuous Exposures Measured with Simple or Complex Error" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/IdentifyingSampleSizeandAccuracyandPrecisionoftheEstimatorsinCaseCrossoverDesignswithDistributedLagsofHeteroskedasticTimeVaryingContinuousExposuresMeasuredwithSimpleorComplexError</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/IdentifyingSampleSizeandAccuracyandPrecisionoftheEstimatorsinCaseCrossoverDesignswithDistributedLagsofHeteroskedasticTimeVaryingContinuousExposuresMeasuredwithSimpleorComplexError.html">&lt;p&gt;Understanding of sample size, statistical power, and the accuracy and precision of the estimator in epidemiological research can facilitate power and bias analyses. However, such understanding can become complicated for several reasons. First, exposures varying spatiotemporally may be heteroskedastic. Second, distributed lags of exposures may be used to identify critical exposure time-windows. Third, exposure measurement error may exist, impacting the accuracy and/or precision of the estimator that consequently affects sample size and statistical power. Fourth, research may rely on different study designs, so understanding may differ. For example, case-crossover designs as matched case-control designs, are used to estimate health effects of short-term exposures. To address these gaps, I developed approximation equations for sample size, estimates of the estimators and standard errors, including polynomials for non-linear effect estimation. With air pollution exposure estimates, I examined approximations using statistical simulations. Overall, sample size, the accuracy and precision of the estimators can be approximated based on external information about validation, without validation data in hand. For distributed lags, approximations may perform well if residual confounding due to covariate measurement errors is not severe. This condition may be difficult to identify without validation data, so validation research is recommended in identifying critical exposure time-windows.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02369&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Honghyok Kim</name></author><category term="stat.ME" /><summary type="html">Understanding of sample size, statistical power, and the accuracy and precision of the estimator in epidemiological research can facilitate power and bias analyses. However, such understanding can become complicated for several reasons. First, exposures varying spatiotemporally may be heteroskedastic. Second, distributed lags of exposures may be used to identify critical exposure time-windows. Third, exposure measurement error may exist, impacting the accuracy and/or precision of the estimator that consequently affects sample size and statistical power. Fourth, research may rely on different study designs, so understanding may differ. For example, case-crossover designs as matched case-control designs, are used to estimate health effects of short-term exposures. To address these gaps, I developed approximation equations for sample size, estimates of the estimators and standard errors, including polynomials for non-linear effect estimation. With air pollution exposure estimates, I examined approximations using statistical simulations. Overall, sample size, the accuracy and precision of the estimators can be approximated based on external information about validation, without validation data in hand. For distributed lags, approximations may perform well if residual confounding due to covariate measurement errors is not severe. This condition may be difficult to identify without validation data, so validation research is recommended in identifying critical exposure time-windows.</summary></entry><entry><title type="html">Intrinsic Modeling of Shape-Constrained Functional Data, With Applications to Growth Curves and Activity Profiles</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/IntrinsicModelingofShapeConstrainedFunctionalDataWithApplicationstoGrowthCurvesandActivityProfiles.html" rel="alternate" type="text/html" title="Intrinsic Modeling of Shape-Constrained Functional Data, With Applications to Growth Curves and Activity Profiles" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/IntrinsicModelingofShapeConstrainedFunctionalDataWithApplicationstoGrowthCurvesandActivityProfiles</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/IntrinsicModelingofShapeConstrainedFunctionalDataWithApplicationstoGrowthCurvesandActivityProfiles.html">&lt;p&gt;Shape-constrained functional data encompass a wide array of application fields especially in the life sciences, such as activity profiling, growth curves, healthcare and mortality. Most existing methods for general functional data analysis often ignore that such data are subject to inherent shape constraints, while some specialized techniques rely on strict distributional assumptions. We propose an approach for modeling such data that harnesses the intrinsic geometry of functional trajectories by decomposing them into size and shape components. We focus on the two most prevalent shape constraints, positivity and monotonicity, and develop individual-level estimators for the size and shape components. Furthermore, we demonstrate the applicability of our approach by conducting subsequent analyses involving Fr&apos;{e}chet mean and Fr&apos;{e}chet regression and establish rates of convergence for the empirical estimators. Illustrative examples include simulations and data applications for activity profiles for Mediterranean fruit flies during their entire lifespan and for data from the Z&quot;{u}rich longitudinal growth study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12817&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Poorbita Kundu, Hans-Georg Müller</name></author><category term="stat.ME" /><summary type="html">Shape-constrained functional data encompass a wide array of application fields especially in the life sciences, such as activity profiling, growth curves, healthcare and mortality. Most existing methods for general functional data analysis often ignore that such data are subject to inherent shape constraints, while some specialized techniques rely on strict distributional assumptions. We propose an approach for modeling such data that harnesses the intrinsic geometry of functional trajectories by decomposing them into size and shape components. We focus on the two most prevalent shape constraints, positivity and monotonicity, and develop individual-level estimators for the size and shape components. Furthermore, we demonstrate the applicability of our approach by conducting subsequent analyses involving Fr&apos;{e}chet mean and Fr&apos;{e}chet regression and establish rates of convergence for the empirical estimators. Illustrative examples include simulations and data applications for activity profiles for Mediterranean fruit flies during their entire lifespan and for data from the Z&quot;{u}rich longitudinal growth study.</summary></entry><entry><title type="html">Lasso regularization for mixture experiments with noise variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Lassoregularizationformixtureexperimentswithnoisevariables.html" rel="alternate" type="text/html" title="Lasso regularization for mixture experiments with noise variables" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Lassoregularizationformixtureexperimentswithnoisevariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Lassoregularizationformixtureexperimentswithnoisevariables.html">&lt;p&gt;We apply classical and Bayesian lasso regularizations to a family of models with the presence of mixture and process variables. We analyse the performance of these estimates with respect to ordinary least squares estimators by a simulation study and a real data application. Our results demonstrate the superior performance of Bayesian lasso, particularly via coordinate ascent variational inference, in terms of variable selection accuracy and response optimization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12237&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Manuel González-Navarrete, Fabián Manríquez-Méndez, Manuel Pereira-Barahona</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">We apply classical and Bayesian lasso regularizations to a family of models with the presence of mixture and process variables. We analyse the performance of these estimates with respect to ordinary least squares estimators by a simulation study and a real data application. Our results demonstrate the superior performance of Bayesian lasso, particularly via coordinate ascent variational inference, in terms of variable selection accuracy and response optimization.</summary></entry><entry><title type="html">Mixed-resolution hybrid modeling in an element-based framework</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Mixedresolutionhybridmodelinginanelementbasedframework.html" rel="alternate" type="text/html" title="Mixed-resolution hybrid modeling in an element-based framework" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Mixedresolutionhybridmodelinginanelementbasedframework</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Mixedresolutionhybridmodelinginanelementbasedframework.html">&lt;p&gt;Computational modeling of a complex system is limited by the parts of the system with the least information. While detailed models and high-resolution data may be available for parts of a system, abstract relationships are often necessary to connect the parts and model the full system. For example, modeling food security necessitates the interaction of climate and socioeconomic factors, with models of system components existing at different levels of information in terms of granularity and resolution. Connecting these models is an ongoing challenge. In this work, we demonstrate methodology to quantize and integrate information from data and detailed component models alongside abstract relationships in a hybrid element-based modeling and simulation framework. In a case study of modeling food security, we apply quantization methods to generate (1) time-series model input from climate data and (2) a discrete representation of a component model (a statistical emulator of crop yield), which we then incorporate as an update rule in the hybrid element-based model, bridging differences in model granularity and resolution. Simulation of the hybrid element-based model recapitulated the trends of the original emulator, supporting the use of this methodology to integrate data and information from component models to simulate complex systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12028&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kara Bocan, Natasa Miskov-Zivanov</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Computational modeling of a complex system is limited by the parts of the system with the least information. While detailed models and high-resolution data may be available for parts of a system, abstract relationships are often necessary to connect the parts and model the full system. For example, modeling food security necessitates the interaction of climate and socioeconomic factors, with models of system components existing at different levels of information in terms of granularity and resolution. Connecting these models is an ongoing challenge. In this work, we demonstrate methodology to quantize and integrate information from data and detailed component models alongside abstract relationships in a hybrid element-based modeling and simulation framework. In a case study of modeling food security, we apply quantization methods to generate (1) time-series model input from climate data and (2) a discrete representation of a component model (a statistical emulator of crop yield), which we then incorporate as an update rule in the hybrid element-based model, bridging differences in model granularity and resolution. Simulation of the hybrid element-based model recapitulated the trends of the original emulator, supporting the use of this methodology to integrate data and information from component models to simulate complex systems.</summary></entry><entry><title type="html">Model-Based Inference and Experimental Design for Interference Using Partial Network Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/ModelBasedInferenceandExperimentalDesignforInterferenceUsingPartialNetworkData.html" rel="alternate" type="text/html" title="Model-Based Inference and Experimental Design for Interference Using Partial Network Data" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/ModelBasedInferenceandExperimentalDesignforInterferenceUsingPartialNetworkData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/ModelBasedInferenceandExperimentalDesignforInterferenceUsingPartialNetworkData.html">&lt;p&gt;The stable unit treatment value assumption states that the outcome of an individual is not affected by the treatment statuses of others, however in many real world applications, treatments can have an effect on many others beyond the immediately treated. Interference can generically be thought of as mediated through some network structure. In many empirically relevant situations however, complete network data (required to adjust for these spillover effects) are too costly or logistically infeasible to collect. Partially or indirectly observed network data (e.g., subsamples, aggregated relational data (ARD), egocentric sampling, or respondent-driven sampling) reduce the logistical and financial burden of collecting network data, but the statistical properties of treatment effect adjustments from these design strategies are only beginning to be explored. In this paper, we present a framework for the estimation and inference of treatment effect adjustments using partial network data through the lens of structural causal models. We also illustrate procedures to assign treatments using only partial network data, with the goal of either minimizing estimator variance or optimally seeding. We derive single network asymptotic results applicable to a variety of choices for an underlying graph model. We validate our approach using simulated experiments on observed graphs with applications to information diffusion in India and Malawi.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11940&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Steven Wilkins Reeves, Shane Lubold, Arun G. Chandrasekhar, Tyler H. McCormick</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.OT" /><summary type="html">The stable unit treatment value assumption states that the outcome of an individual is not affected by the treatment statuses of others, however in many real world applications, treatments can have an effect on many others beyond the immediately treated. Interference can generically be thought of as mediated through some network structure. In many empirically relevant situations however, complete network data (required to adjust for these spillover effects) are too costly or logistically infeasible to collect. Partially or indirectly observed network data (e.g., subsamples, aggregated relational data (ARD), egocentric sampling, or respondent-driven sampling) reduce the logistical and financial burden of collecting network data, but the statistical properties of treatment effect adjustments from these design strategies are only beginning to be explored. In this paper, we present a framework for the estimation and inference of treatment effect adjustments using partial network data through the lens of structural causal models. We also illustrate procedures to assign treatments using only partial network data, with the goal of either minimizing estimator variance or optimally seeding. We derive single network asymptotic results applicable to a variety of choices for an underlying graph model. We validate our approach using simulated experiments on observed graphs with applications to information diffusion in India and Malawi.</summary></entry><entry><title type="html">Model Selection for Causal Modeling in Missing Exposure Problems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/ModelSelectionforCausalModelinginMissingExposureProblems.html" rel="alternate" type="text/html" title="Model Selection for Causal Modeling in Missing Exposure Problems" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/ModelSelectionforCausalModelinginMissingExposureProblems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/ModelSelectionforCausalModelinginMissingExposureProblems.html">&lt;p&gt;In causal inference, properly selecting the propensity score (PS) model is a popular topic and has been widely investigated in observational studies. In addition, there is a large literature concerning the missing data problem. However, there are very few studies investigating the model selection issue for causal inference when the exposure is missing at random (MAR). In this paper, we discuss how to select both imputation and PS models, which can result in the smallest RMSE of the estimated causal effect. Then, we provide a new criterion, called the ``rank score” for evaluating the overall performance of both models. The simulation studies show that the full imputation plus the outcome-related PS models lead to the smallest RMSE and the rank score can also pick the best models. An application study is conducted to study the causal effect of CVD on the mortality of COVID-19 patients.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12171&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuliang Shi, Yeying Zhu, Joel A. Dubin</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In causal inference, properly selecting the propensity score (PS) model is a popular topic and has been widely investigated in observational studies. In addition, there is a large literature concerning the missing data problem. However, there are very few studies investigating the model selection issue for causal inference when the exposure is missing at random (MAR). In this paper, we discuss how to select both imputation and PS models, which can result in the smallest RMSE of the estimated causal effect. Then, we provide a new criterion, called the ``rank score” for evaluating the overall performance of both models. The simulation studies show that the full imputation plus the outcome-related PS models lead to the smallest RMSE and the rank score can also pick the best models. An application study is conducted to study the causal effect of CVD on the mortality of COVID-19 patients.</summary></entry><entry><title type="html">Neural Bayes estimators for censored inference with peaks-over-threshold models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/NeuralBayesestimatorsforcensoredinferencewithpeaksoverthresholdmodels.html" rel="alternate" type="text/html" title="Neural Bayes estimators for censored inference with peaks-over-threshold models" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/NeuralBayesestimatorsforcensoredinferencewithpeaksoverthresholdmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/NeuralBayesestimatorsforcensoredinferencewithpeaksoverthresholdmodels.html">&lt;p&gt;Making inference with spatial extremal dependence models can be computationally burdensome since they involve intractable and/or censored likelihoods. Building on recent advances in likelihood-free inference with neural Bayes estimators, that is, neural networks that approximate Bayes estimators, we develop highly efficient estimators for censored peaks-over-threshold models that {use data augmentation techniques} to encode censoring information in the neural network {input}. Our new method provides a paradigm shift that challenges traditional censored likelihood-based inference methods for spatial extremal dependence models. Our simulation studies highlight significant gains in both computational and statistical efficiency, relative to competing likelihood-based approaches, when applying our novel estimators to make inference with popular extremal dependence models, such as max-stable, $r$-Pareto, and random scale mixture process models. We also illustrate that it is possible to train a single neural Bayes estimator for a general censoring level, precluding the need to retrain the network when the censoring level is changed. We illustrate the efficacy of our estimators by making fast inference on hundreds-of-thousands of high-dimensional spatial extremal dependence models to assess extreme particulate matter 2.5 microns or less in diameter (${\rm PM}_{2.5}$) concentration over the whole of Saudi Arabia.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.15642&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jordan Richards, Matthew Sainsbury-Dale, Andrew Zammit-Mangion, Raphaël Huser</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Making inference with spatial extremal dependence models can be computationally burdensome since they involve intractable and/or censored likelihoods. Building on recent advances in likelihood-free inference with neural Bayes estimators, that is, neural networks that approximate Bayes estimators, we develop highly efficient estimators for censored peaks-over-threshold models that {use data augmentation techniques} to encode censoring information in the neural network {input}. Our new method provides a paradigm shift that challenges traditional censored likelihood-based inference methods for spatial extremal dependence models. Our simulation studies highlight significant gains in both computational and statistical efficiency, relative to competing likelihood-based approaches, when applying our novel estimators to make inference with popular extremal dependence models, such as max-stable, $r$-Pareto, and random scale mixture process models. We also illustrate that it is possible to train a single neural Bayes estimator for a general censoring level, precluding the need to retrain the network when the censoring level is changed. We illustrate the efficacy of our estimators by making fast inference on hundreds-of-thousands of high-dimensional spatial extremal dependence models to assess extreme particulate matter 2.5 microns or less in diameter (${\rm PM}_{2.5}$) concentration over the whole of Saudi Arabia.</summary></entry><entry><title type="html">Numerically robust square root implementations of statistical linear regression filters and smoothers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Numericallyrobustsquarerootimplementationsofstatisticallinearregressionfiltersandsmoothers.html" rel="alternate" type="text/html" title="Numerically robust square root implementations of statistical linear regression filters and smoothers" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Numericallyrobustsquarerootimplementationsofstatisticallinearregressionfiltersandsmoothers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Numericallyrobustsquarerootimplementationsofstatisticallinearregressionfiltersandsmoothers.html">&lt;p&gt;In this article, square-root formulations of the statistical linear regression filter and smoother are developed. Crucially, the method uses QR decompositions rather than Cholesky downdates. This makes the method inherently more numerically robust than the downdate based methods, which may fail in the face of rounding errors. This increased robustness is demonstrated in an ill-conditioned problem, where it is compared against a reference implementation in both double and single precision arithmetic. The new implementation is found to be more robust, when implemented in lower precision arithmetic as compared to the alternative.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05188&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Filip Tronarp</name></author><category term="stat.ME" /><summary type="html">In this article, square-root formulations of the statistical linear regression filter and smoother are developed. Crucially, the method uses QR decompositions rather than Cholesky downdates. This makes the method inherently more numerically robust than the downdate based methods, which may fail in the face of rounding errors. This increased robustness is demonstrated in an ill-conditioned problem, where it is compared against a reference implementation in both double and single precision arithmetic. The new implementation is found to be more robust, when implemented in lower precision arithmetic as compared to the alternative.</summary></entry><entry><title type="html">Orthogonal and Linear Regressions and Pencils of Confocal Quadrics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/OrthogonalandLinearRegressionsandPencilsofConfocalQuadrics.html" rel="alternate" type="text/html" title="Orthogonal and Linear Regressions and Pencils of Confocal Quadrics" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/OrthogonalandLinearRegressionsandPencilsofConfocalQuadrics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/OrthogonalandLinearRegressionsandPencilsofConfocalQuadrics.html">&lt;p&gt;This paper enhances and develops bridges between statistics, mechanics, and geometry. For a given system of points in $\mathbb R^k$ representing a sample of full rank, we construct an explicit pencil of confocal quadrics with the following properties: (i) All the hyperplanes for which the hyperplanar moments of inertia for the given system of points are equal, are tangent to the same quadrics from the pencil of quadrics. As an application, we develop regularization procedures for the orthogonal least square method, analogues of lasso and ridge methods from linear regression. (ii) For any given point $P$ among all the hyperplanes that contain it, the best fit is the tangent hyperplane to the quadric from the confocal pencil corresponding to the maximal Jacobi coordinate of the point $P$; the worst fit among the hyperplanes containing $P$ is the tangent hyperplane to the ellipsoid from the confocal pencil that contains $P$. The confocal pencil of quadrics provides a universal tool to solve the restricted principal component analysis restricted at any given point. Both results (i) and (ii) can be seen as generalizations of the classical result of Pearson on orthogonal regression. They have natural and important applications in the statistics of the errors-in-variables models (EIV). For the classical linear regressions we provide a geometric characterization of hyperplanes of least squares in a given direction among all hyperplanes which contain a given point. The obtained results have applications in restricted regressions, both ordinary and orthogonal ones. For the latter, a new formula for test statistic is derived. The developed methods and results are illustrated in natural statistics examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2209.01679&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vladimir Dragović, Borislav Gajić</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This paper enhances and develops bridges between statistics, mechanics, and geometry. For a given system of points in $\mathbb R^k$ representing a sample of full rank, we construct an explicit pencil of confocal quadrics with the following properties: (i) All the hyperplanes for which the hyperplanar moments of inertia for the given system of points are equal, are tangent to the same quadrics from the pencil of quadrics. As an application, we develop regularization procedures for the orthogonal least square method, analogues of lasso and ridge methods from linear regression. (ii) For any given point $P$ among all the hyperplanes that contain it, the best fit is the tangent hyperplane to the quadric from the confocal pencil corresponding to the maximal Jacobi coordinate of the point $P$; the worst fit among the hyperplanes containing $P$ is the tangent hyperplane to the ellipsoid from the confocal pencil that contains $P$. The confocal pencil of quadrics provides a universal tool to solve the restricted principal component analysis restricted at any given point. Both results (i) and (ii) can be seen as generalizations of the classical result of Pearson on orthogonal regression. They have natural and important applications in the statistics of the errors-in-variables models (EIV). For the classical linear regressions we provide a geometric characterization of hyperplanes of least squares in a given direction among all hyperplanes which contain a given point. The obtained results have applications in restricted regressions, both ordinary and orthogonal ones. For the latter, a new formula for test statistic is derived. The developed methods and results are illustrated in natural statistics examples.</summary></entry><entry><title type="html">Perturbation-based Effect Measures for Compositional Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/PerturbationbasedEffectMeasuresforCompositionalData.html" rel="alternate" type="text/html" title="Perturbation-based Effect Measures for Compositional Data" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/PerturbationbasedEffectMeasuresforCompositionalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/PerturbationbasedEffectMeasuresforCompositionalData.html">&lt;p&gt;Existing effect measures for compositional features are inadequate for many modern applications for two reasons. First, modern datasets with compositional covariates, for example in microbiome research, display traits such as high-dimensionality and sparsity that can be poorly modelled with traditional parametric approaches. Second, assessing – in an unbiased way – how summary statistics of a composition (e.g., racial diversity) affect a response variable is not straightforward. In this work, we propose a framework based on hypothetical data perturbations that addresses both issues. Unlike many existing effect measures for compositional features, we do not define our effects based on a parametric model or a transformation of the data. Instead, we use perturbations to define interpretable statistical functionals on the compositions themselves, which we call average perturbation effects. These effects naturally account for confounding that biases frequently used marginal dependence analyses. We show how average perturbation effects can be estimated efficiently by deriving a perturbation-dependent reparametrization and applying semiparametric estimation techniques. We analyze the proposed estimators empirically on simulated and semi-synthetic data and demonstrate advantages over existing techniques on data from New York schools and microbiome data. For all proposed estimators, we provide confidence intervals with uniform asymptotic coverage guarantees.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.18501&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anton Rask Lundborg, Niklas Pfister</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Existing effect measures for compositional features are inadequate for many modern applications for two reasons. First, modern datasets with compositional covariates, for example in microbiome research, display traits such as high-dimensionality and sparsity that can be poorly modelled with traditional parametric approaches. Second, assessing – in an unbiased way – how summary statistics of a composition (e.g., racial diversity) affect a response variable is not straightforward. In this work, we propose a framework based on hypothetical data perturbations that addresses both issues. Unlike many existing effect measures for compositional features, we do not define our effects based on a parametric model or a transformation of the data. Instead, we use perturbations to define interpretable statistical functionals on the compositions themselves, which we call average perturbation effects. These effects naturally account for confounding that biases frequently used marginal dependence analyses. We show how average perturbation effects can be estimated efficiently by deriving a perturbation-dependent reparametrization and applying semiparametric estimation techniques. We analyze the proposed estimators empirically on simulated and semi-synthetic data and demonstrate advantages over existing techniques on data from New York schools and microbiome data. For all proposed estimators, we provide confidence intervals with uniform asymptotic coverage guarantees.</summary></entry><entry><title type="html">Polynomial Chaos Surrogate Construction for Random Fields with Parametric Uncertainty</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/PolynomialChaosSurrogateConstructionforRandomFieldswithParametricUncertainty.html" rel="alternate" type="text/html" title="Polynomial Chaos Surrogate Construction for Random Fields with Parametric Uncertainty" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/PolynomialChaosSurrogateConstructionforRandomFieldswithParametricUncertainty</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/PolynomialChaosSurrogateConstructionforRandomFieldswithParametricUncertainty.html">&lt;p&gt;Engineering and applied science rely on computational experiments to rigorously study physical systems. The mathematical models used to probe these systems are highly complex, and sampling-intensive studies often require prohibitively many simulations for acceptable accuracy. Surrogate models provide a means of circumventing the high computational expense of sampling such complex models. In particular, polynomial chaos expansions (PCEs) have been successfully used for uncertainty quantification studies of deterministic models where the dominant source of uncertainty is parametric. We discuss an extension to conventional PCE surrogate modeling to enable surrogate construction for stochastic computational models that have intrinsic noise in addition to parametric uncertainty. We develop a PCE surrogate on a joint space of intrinsic and parametric uncertainty, enabled by Rosenblatt transformations, and then extend the construction to random field data via the Karhunen-Loeve expansion. We then take advantage of closed-form solutions for computing PCE Sobol indices to perform a global sensitivity analysis of the model which quantifies the intrinsic noise contribution to the overall model output variance. Additionally, the resulting joint PCE is generative in the sense that it allows generating random realizations at any input parameter setting that are statistically approximately equivalent to realizations from the underlying stochastic model. The method is demonstrated on a chemical catalysis example model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.00553&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joy N. Mueller, Khachik Sargsyan, Craig J. Daniels, Habib N. Najm</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">Engineering and applied science rely on computational experiments to rigorously study physical systems. The mathematical models used to probe these systems are highly complex, and sampling-intensive studies often require prohibitively many simulations for acceptable accuracy. Surrogate models provide a means of circumventing the high computational expense of sampling such complex models. In particular, polynomial chaos expansions (PCEs) have been successfully used for uncertainty quantification studies of deterministic models where the dominant source of uncertainty is parametric. We discuss an extension to conventional PCE surrogate modeling to enable surrogate construction for stochastic computational models that have intrinsic noise in addition to parametric uncertainty. We develop a PCE surrogate on a joint space of intrinsic and parametric uncertainty, enabled by Rosenblatt transformations, and then extend the construction to random field data via the Karhunen-Loeve expansion. We then take advantage of closed-form solutions for computing PCE Sobol indices to perform a global sensitivity analysis of the model which quantifies the intrinsic noise contribution to the overall model output variance. Additionally, the resulting joint PCE is generative in the sense that it allows generating random realizations at any input parameter setting that are statistically approximately equivalent to realizations from the underlying stochastic model. The method is demonstrated on a chemical catalysis example model.</summary></entry><entry><title type="html">SUrvival Control Chart EStimation Software in R: the success package</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/SUrvivalControlChartEStimationSoftwareinRthesuccesspackage.html" rel="alternate" type="text/html" title="SUrvival Control Chart EStimation Software in R: the success package" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/SUrvivalControlChartEStimationSoftwareinRthesuccesspackage</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/SUrvivalControlChartEStimationSoftwareinRthesuccesspackage.html">&lt;p&gt;Monitoring the quality of statistical processes has been of great importance, mostly in industrial applications. Control charts are widely used for this purpose, but often lack the possibility to monitor survival outcomes. Recently, inspecting survival outcomes has become of interest, especially in medical settings where outcomes often depend on risk factors of patients. For this reason many new survival control charts have been devised and existing ones have been extended to incorporate survival outcomes. The R package success allows users to construct risk-adjusted control charts for survival data. Functions to determine control chart parameters are included, which can be used even without expert knowledge on the subject of control charts. The package allows to create static as well as interactive charts, which are built using ggplot2 (Wickham 2016) and plotly (Sievert 2020).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.07658&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Daniel Gomon, Marta Fiocco, Hein Putter, Mirko Signorelli</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Monitoring the quality of statistical processes has been of great importance, mostly in industrial applications. Control charts are widely used for this purpose, but often lack the possibility to monitor survival outcomes. Recently, inspecting survival outcomes has become of interest, especially in medical settings where outcomes often depend on risk factors of patients. For this reason many new survival control charts have been devised and existing ones have been extended to incorporate survival outcomes. The R package success allows users to construct risk-adjusted control charts for survival data. Functions to determine control chart parameters are included, which can be used even without expert knowledge on the subject of control charts. The package allows to create static as well as interactive charts, which are built using ggplot2 (Wickham 2016) and plotly (Sievert 2020).</summary></entry><entry><title type="html">Simultaneous comparisons of the variances of k treatments with that of a control: a Levene-Dunnett type procedure</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/SimultaneouscomparisonsofthevariancesofktreatmentswiththatofacontrolaLeveneDunnetttypeprocedure.html" rel="alternate" type="text/html" title="Simultaneous comparisons of the variances of k treatments with that of a control: a Levene-Dunnett type procedure" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/SimultaneouscomparisonsofthevariancesofktreatmentswiththatofacontrolaLeveneDunnetttypeprocedure</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/SimultaneouscomparisonsofthevariancesofktreatmentswiththatofacontrolaLeveneDunnetttypeprocedure.html">&lt;p&gt;There are some global tests for heterogeneity of variance in k-sample one-way layouts, but few consider pairwise comparisons between treatment levels. For experimental designs with a control, comparisons of the variances between the treatment levels and the control are of interest - in analogy to the location parameter with the Dunnett (1955) procedure. Such a many-to-one approach for variances is proposed using the Levene transformation, a kind of residuals. Its properties are characterized with simulation studies and corresponding data examples are evaluated with R code.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11892&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ludwig A. Hothorn</name></author><category term="stat.ME" /><summary type="html">There are some global tests for heterogeneity of variance in k-sample one-way layouts, but few consider pairwise comparisons between treatment levels. For experimental designs with a control, comparisons of the variances between the treatment levels and the control are of interest - in analogy to the location parameter with the Dunnett (1955) procedure. Such a many-to-one approach for variances is proposed using the Levene transformation, a kind of residuals. Its properties are characterized with simulation studies and corresponding data examples are evaluated with R code.</summary></entry><entry><title type="html">Sparsity-Constraint Optimization via Splicing Iteration</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/SparsityConstraintOptimizationviaSplicingIteration.html" rel="alternate" type="text/html" title="Sparsity-Constraint Optimization via Splicing Iteration" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/SparsityConstraintOptimizationviaSplicingIteration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/SparsityConstraintOptimizationviaSplicingIteration.html">&lt;p&gt;Sparsity-constraint optimization has wide applicability in signal processing, statistics, and machine learning. Existing fast algorithms must burdensomely tune parameters, such as the step size or the implementation of precise stop criteria, which may be challenging to determine in practice. To address this issue, we develop an algorithm named Sparsity-Constraint Optimization via sPlicing itEration (SCOPE) to optimize nonlinear differential objective functions with strong convexity and smoothness in low dimensional subspaces. Algorithmically, the SCOPE algorithm converges effectively without tuning parameters. Theoretically, SCOPE has a linear convergence rate and converges to a solution that recovers the true support set when it correctly specifies the sparsity. We also develop parallel theoretical results without restricted-isometry-property-type conditions. We apply SCOPE’s versatility and power to solve sparse quadratic optimization, learn sparse classifiers, and recover sparse Markov networks for binary variables. The numerical results on these specific tasks reveal that SCOPE perfectly identifies the true support set with a 10–1000 speedup over the standard exact solver, confirming SCOPE’s algorithmic and theoretical merits. Our open-source Python package skscope based on C++ implementation is publicly available on GitHub, reaching a ten-fold speedup on the competing convex relaxation methods implemented by the cvxpy library.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12017&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zezhi Wang, Jin Zhu, Junxian Zhu, Borui Tang, Hongmei Lin, Xueqin Wang</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Sparsity-constraint optimization has wide applicability in signal processing, statistics, and machine learning. Existing fast algorithms must burdensomely tune parameters, such as the step size or the implementation of precise stop criteria, which may be challenging to determine in practice. To address this issue, we develop an algorithm named Sparsity-Constraint Optimization via sPlicing itEration (SCOPE) to optimize nonlinear differential objective functions with strong convexity and smoothness in low dimensional subspaces. Algorithmically, the SCOPE algorithm converges effectively without tuning parameters. Theoretically, SCOPE has a linear convergence rate and converges to a solution that recovers the true support set when it correctly specifies the sparsity. We also develop parallel theoretical results without restricted-isometry-property-type conditions. We apply SCOPE’s versatility and power to solve sparse quadratic optimization, learn sparse classifiers, and recover sparse Markov networks for binary variables. The numerical results on these specific tasks reveal that SCOPE perfectly identifies the true support set with a 10–1000 speedup over the standard exact solver, confirming SCOPE’s algorithmic and theoretical merits. Our open-source Python package skscope based on C++ implementation is publicly available on GitHub, reaching a ten-fold speedup on the competing convex relaxation methods implemented by the cvxpy library.</summary></entry><entry><title type="html">Spatial von-Mises Fisher Regression for Directional Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/SpatialvonMisesFisherRegressionforDirectionalData.html" rel="alternate" type="text/html" title="Spatial von-Mises Fisher Regression for Directional Data" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/SpatialvonMisesFisherRegressionforDirectionalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/SpatialvonMisesFisherRegressionforDirectionalData.html">&lt;p&gt;Spatially varying directional data are routinely observed in several modern applications such as meteorology, biology, geophysics, and engineering, etc. However, only a few approaches are available for covariate-dependent statistical analysis for such data. To address this gap, we propose a novel generalized linear model to analyze such that using a von Mises Fisher (vMF) distributed error structure. Using a novel link function that relies on the transformation between Cartesian and spherical coordinates, we regress the vMF-distributed directional data on the external covariates. This regression model enables us to quantify the impact of external factors on the observed directional data. Furthermore, we impose the spatial dependence using an autoregressive model, appropriately accounting for the directional dependence in the outcome. This novel specification renders computational efficiency and flexibility. In addition, a comprehensive Bayesian inferential toolbox is thoroughly developed and applied to our analysis. Subsequently, employing our regression model to the Alzheimer’s Disease Neuroimaging Initiative (ADNI) data, we gain new insights into the relationship between cognitive impairment and the orientations of brain fibers along with examining empirical efficacy through simulation experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2207.08321&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhou Lan, Arkaprava Roy</name></author><category term="stat.AP" /><summary type="html">Spatially varying directional data are routinely observed in several modern applications such as meteorology, biology, geophysics, and engineering, etc. However, only a few approaches are available for covariate-dependent statistical analysis for such data. To address this gap, we propose a novel generalized linear model to analyze such that using a von Mises Fisher (vMF) distributed error structure. Using a novel link function that relies on the transformation between Cartesian and spherical coordinates, we regress the vMF-distributed directional data on the external covariates. This regression model enables us to quantify the impact of external factors on the observed directional data. Furthermore, we impose the spatial dependence using an autoregressive model, appropriately accounting for the directional dependence in the outcome. This novel specification renders computational efficiency and flexibility. In addition, a comprehensive Bayesian inferential toolbox is thoroughly developed and applied to our analysis. Subsequently, employing our regression model to the Alzheimer’s Disease Neuroimaging Initiative (ADNI) data, we gain new insights into the relationship between cognitive impairment and the orientations of brain fibers along with examining empirical efficacy through simulation experiments.</summary></entry><entry><title type="html">Statistical Principles for Platform Trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/StatisticalPrinciplesforPlatformTrials.html" rel="alternate" type="text/html" title="Statistical Principles for Platform Trials" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/StatisticalPrinciplesforPlatformTrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/StatisticalPrinciplesforPlatformTrials.html">&lt;p&gt;While within a clinical study there may be multiple doses and endpoints, across different studies each study will result in either an approval or a lack of approval of the drug compound studied. The term False Approval Rate (FAR) is the term this paper utilizes to represent the proportion of drug compounds that lack efficacy incorrectly approved by regulators. (In the U.S., compounds that have efficacy and are approved are not involved in the FAR consideration, according to our reading of the relevant U.S. Congressional statute).
  While Tukey’s (1953) Error Rate Familywise (ERFw) is meant to be applied within a clinical study, Tukey’s (1953) Error Rate per Family (ERpF), defined along-side ERFw, is meant to be applied across studies. We show that controlling Error Rate Familywise (ERFw) within a clinical study at 5% in turn controls Error Rate per Family (ERpF) across studies at 5-per-100, regardless of whether the studies are correlated or not. Further, we show that ongoing regulatory practice, the additive multiplicity adjustment method of controlling ERpF, is controlling False Approval Rate (FAR) exactly (not conservatively) at 5-per-100 (even for Platform trials).
  In contrast, if a regulatory agency chooses to control the False Discovery Rate (FDR) across studies at 5% instead, then this change in policy from ERpF control to FDR control will result in incorrectly approving drug compounds that lack efficacy at a rate higher than 5-per-100, because in essence it gives the industry additional rewards for successfully developing compounds that have efficacy and are approved. Seems to us the discussion of such a change in policy would be at a level higher than merely statistical, needing harmonizsation/harmonization (In the U.S., policy is set by the Congress).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.12728&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinping Cui, Emily Ouyang, Yi Liu, Jingjing Schneider, Hong Tian, Bushi Wang, Jason C. Hsu</name></author><category term="stat.ME" /><summary type="html">While within a clinical study there may be multiple doses and endpoints, across different studies each study will result in either an approval or a lack of approval of the drug compound studied. The term False Approval Rate (FAR) is the term this paper utilizes to represent the proportion of drug compounds that lack efficacy incorrectly approved by regulators. (In the U.S., compounds that have efficacy and are approved are not involved in the FAR consideration, according to our reading of the relevant U.S. Congressional statute). While Tukey’s (1953) Error Rate Familywise (ERFw) is meant to be applied within a clinical study, Tukey’s (1953) Error Rate per Family (ERpF), defined along-side ERFw, is meant to be applied across studies. We show that controlling Error Rate Familywise (ERFw) within a clinical study at 5% in turn controls Error Rate per Family (ERpF) across studies at 5-per-100, regardless of whether the studies are correlated or not. Further, we show that ongoing regulatory practice, the additive multiplicity adjustment method of controlling ERpF, is controlling False Approval Rate (FAR) exactly (not conservatively) at 5-per-100 (even for Platform trials). In contrast, if a regulatory agency chooses to control the False Discovery Rate (FDR) across studies at 5% instead, then this change in policy from ERpF control to FDR control will result in incorrectly approving drug compounds that lack efficacy at a rate higher than 5-per-100, because in essence it gives the industry additional rewards for successfully developing compounds that have efficacy and are approved. Seems to us the discussion of such a change in policy would be at a level higher than merely statistical, needing harmonizsation/harmonization (In the U.S., policy is set by the Congress).</summary></entry><entry><title type="html">Statistical significance revisited</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Statisticalsignificancerevisited.html" rel="alternate" type="text/html" title="Statistical significance revisited" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Statisticalsignificancerevisited</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/Statisticalsignificancerevisited.html">&lt;p&gt;Statistical significance measures the reliability of a result obtained from a random experiment. We investigate the number of repetitions needed for a statistical result to have a certain significance. In the first step, we consider binomially distributed variables in the example of medication testing with fixed placebo efficacy, asking how many experiments are needed in order to achieve a significance of 95 %. In the next step, we take the probability distribution of the placebo efficacy into account, which to the best of our knowledge has not been done so far. Depending on the specifics, we show that in order to obtain identical significance, it may be necessary to perform twice as many experiments than in a setting where the placebo distribution is neglected. We proceed by considering more general probability distributions and close with comments on some erroneous assumptions on probability distributions which lead, for instance, to a trivial explanation of the fat tail.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.00262&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maike Tormählen, Galiya Klinkova, Michael Grabinski</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Statistical significance measures the reliability of a result obtained from a random experiment. We investigate the number of repetitions needed for a statistical result to have a certain significance. In the first step, we consider binomially distributed variables in the example of medication testing with fixed placebo efficacy, asking how many experiments are needed in order to achieve a significance of 95 %. In the next step, we take the probability distribution of the placebo efficacy into account, which to the best of our knowledge has not been done so far. Depending on the specifics, we show that in order to obtain identical significance, it may be necessary to perform twice as many experiments than in a setting where the placebo distribution is neglected. We proceed by considering more general probability distributions and close with comments on some erroneous assumptions on probability distributions which lead, for instance, to a trivial explanation of the fat tail.</summary></entry><entry><title type="html">Teleporter Theory: A General and Simple Approach for Modeling Cross-World Counterfactual Causality</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/TeleporterTheoryAGeneralandSimpleApproachforModelingCrossWorldCounterfactualCausality.html" rel="alternate" type="text/html" title="Teleporter Theory: A General and Simple Approach for Modeling Cross-World Counterfactual Causality" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/TeleporterTheoryAGeneralandSimpleApproachforModelingCrossWorldCounterfactualCausality</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/TeleporterTheoryAGeneralandSimpleApproachforModelingCrossWorldCounterfactualCausality.html">&lt;p&gt;Leveraging the development of structural causal model (SCM), researchers can establish graphical models for exploring the causal mechanisms behind machine learning techniques. As the complexity of machine learning applications rises, single-world interventionism causal analysis encounters theoretical adaptation limitations. Accordingly, cross-world counterfactual approach extends our understanding of causality beyond observed data, enabling hypothetical reasoning about alternative scenarios. However, the joint involvement of cross-world variables, encompassing counterfactual variables and real-world variables, challenges the construction of the graphical model. Twin network is a subtle attempt, establishing a symbiotic relationship, to bridge the gap between graphical modeling and the introduction of counterfactuals albeit with room for improvement in generalization. In this regard, we demonstrate the theoretical breakdowns of twin networks in certain cross-world counterfactual scenarios. To this end, we propose a novel teleporter theory to establish a general and simple graphical representation of counterfactuals, which provides criteria for determining teleporter variables to connect multiple worlds. In theoretical application, we determine that introducing the proposed teleporter theory can directly obtain the conditional independence between counterfactual variables and real-world variables from the cross-world SCM without requiring complex algebraic derivations. Accordingly, we can further identify counterfactual causal effects through cross-world symbolic derivation. We demonstrate the generality of the teleporter theory to the practical application. Adhering to the proposed theory, we build a plug-and-play module, and the effectiveness of which are substantiated by experiments on benchmarks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.11501&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiangmeng Li, Bin Qin, Qirui Ji, Yi Li, Wenwen Qiang, Jianwen Cao, Fanjiang Xu</name></author><category term="stat.ME" /><summary type="html">Leveraging the development of structural causal model (SCM), researchers can establish graphical models for exploring the causal mechanisms behind machine learning techniques. As the complexity of machine learning applications rises, single-world interventionism causal analysis encounters theoretical adaptation limitations. Accordingly, cross-world counterfactual approach extends our understanding of causality beyond observed data, enabling hypothetical reasoning about alternative scenarios. However, the joint involvement of cross-world variables, encompassing counterfactual variables and real-world variables, challenges the construction of the graphical model. Twin network is a subtle attempt, establishing a symbiotic relationship, to bridge the gap between graphical modeling and the introduction of counterfactuals albeit with room for improvement in generalization. In this regard, we demonstrate the theoretical breakdowns of twin networks in certain cross-world counterfactual scenarios. To this end, we propose a novel teleporter theory to establish a general and simple graphical representation of counterfactuals, which provides criteria for determining teleporter variables to connect multiple worlds. In theoretical application, we determine that introducing the proposed teleporter theory can directly obtain the conditional independence between counterfactual variables and real-world variables from the cross-world SCM without requiring complex algebraic derivations. Accordingly, we can further identify counterfactual causal effects through cross-world symbolic derivation. We demonstrate the generality of the teleporter theory to the practical application. Adhering to the proposed theory, we build a plug-and-play module, and the effectiveness of which are substantiated by experiments on benchmarks.</summary></entry><entry><title type="html">The Modified Combo i3+3 Design for Novel-Novel Combination Dose-Finding Trials in Oncology</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/TheModifiedComboi33DesignforNovelNovelCombinationDoseFindingTrialsinOncology.html" rel="alternate" type="text/html" title="The Modified Combo i3+3 Design for Novel-Novel Combination Dose-Finding Trials in Oncology" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/TheModifiedComboi33DesignforNovelNovelCombinationDoseFindingTrialsinOncology</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/TheModifiedComboi33DesignforNovelNovelCombinationDoseFindingTrialsinOncology.html">&lt;p&gt;We consider a modified Ci3+3 (MCi3+3) design for dual-agent dose-finding trials in which both agents are tested on multiple doses. This usually happens when the agents are novel therapies. The MCi3+3 design offers a two-stage or three-stage version, depending on the practical need. The first stage begins with single-agent dose escalation, the second stage launches a model-free combination dose finding for both agents, and optionally, the third stage follows with a model-based design. MCi3+3 aims to maintain a relatively simple framework to facilitate practical application, while also address challenges that are unique to novel-novel combination dose finding. Through simulations, we demonstrate that the MCi3+3 design adeptly manages various toxicity scenarios. It exhibits operational characteristics on par with other combination designs, while offering an enhanced safety profile. The design is motivated and tested for a real-life clinical trial.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.12666&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiaxin Liu, Shijie Yuan, Qiqi Deng, Yuan Ji</name></author><category term="stat.AP" /><summary type="html">We consider a modified Ci3+3 (MCi3+3) design for dual-agent dose-finding trials in which both agents are tested on multiple doses. This usually happens when the agents are novel therapies. The MCi3+3 design offers a two-stage or three-stage version, depending on the practical need. The first stage begins with single-agent dose escalation, the second stage launches a model-free combination dose finding for both agents, and optionally, the third stage follows with a model-based design. MCi3+3 aims to maintain a relatively simple framework to facilitate practical application, while also address challenges that are unique to novel-novel combination dose finding. Through simulations, we demonstrate that the MCi3+3 design adeptly manages various toxicity scenarios. It exhibits operational characteristics on par with other combination designs, while offering an enhanced safety profile. The design is motivated and tested for a real-life clinical trial.</summary></entry><entry><title type="html">Using Autodiff to Estimate Posterior Moments, Marginals and Samples</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/UsingAutodifftoEstimatePosteriorMomentsMarginalsandSamples.html" rel="alternate" type="text/html" title="Using Autodiff to Estimate Posterior Moments, Marginals and Samples" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/UsingAutodifftoEstimatePosteriorMomentsMarginalsandSamples</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/UsingAutodifftoEstimatePosteriorMomentsMarginalsandSamples.html">&lt;p&gt;Importance sampling is a popular technique in Bayesian inference: by reweighting samples drawn from a proposal distribution we are able to obtain samples and moment estimates from a Bayesian posterior over latent variables. Recent work, however, indicates that importance sampling scales poorly – in order to accurately approximate the true posterior, the required number of importance samples grows is exponential in the number of latent variables [Chatterjee and Diaconis, 2018]. Massively parallel importance sampling works around this issue by drawing $K$ samples for each of the $n$ latent variables and reasoning about all $K^n$ combinations of latent samples. In principle, we can reason efficiently over $K^n$ combinations of samples by exploiting conditional independencies in the generative model. However, in practice this requires complex algorithms that traverse backwards through the graphical model, and we need separate backward traversals for each computation (posterior expectations, marginals and samples). Our contribution is to exploit the source term trick from physics to entirely avoid the need to hand-write backward traversals. Instead, we demonstrate how to simply and easily compute all the required quantities – posterior expectations, marginals and samples – by differentiating through a slightly modified marginal likelihood estimator.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.17374&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sam Bowyer, Thomas Heap, Laurence Aitchison</name></author><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Importance sampling is a popular technique in Bayesian inference: by reweighting samples drawn from a proposal distribution we are able to obtain samples and moment estimates from a Bayesian posterior over latent variables. Recent work, however, indicates that importance sampling scales poorly – in order to accurately approximate the true posterior, the required number of importance samples grows is exponential in the number of latent variables [Chatterjee and Diaconis, 2018]. Massively parallel importance sampling works around this issue by drawing $K$ samples for each of the $n$ latent variables and reasoning about all $K^n$ combinations of latent samples. In principle, we can reason efficiently over $K^n$ combinations of samples by exploiting conditional independencies in the generative model. However, in practice this requires complex algorithms that traverse backwards through the graphical model, and we need separate backward traversals for each computation (posterior expectations, marginals and samples). Our contribution is to exploit the source term trick from physics to entirely avoid the need to hand-write backward traversals. Instead, we demonstrate how to simply and easily compute all the required quantities – posterior expectations, marginals and samples – by differentiating through a slightly modified marginal likelihood estimator.</summary></entry><entry><title type="html">Valid Cross-Covariance Models via Multivariate Mixtures with an Application to the Confluent Hypergeometric Class</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/ValidCrossCovarianceModelsviaMultivariateMixtureswithanApplicationtotheConfluentHypergeometricClass.html" rel="alternate" type="text/html" title="Valid Cross-Covariance Models via Multivariate Mixtures with an Application to the Confluent Hypergeometric Class" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/ValidCrossCovarianceModelsviaMultivariateMixtureswithanApplicationtotheConfluentHypergeometricClass</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/ValidCrossCovarianceModelsviaMultivariateMixtureswithanApplicationtotheConfluentHypergeometricClass.html">&lt;p&gt;Modeling of multivariate random fields through Gaussian processes calls for the construction of valid cross-covariance functions describing the dependence between any two component processes at different spatial locations. The required validity conditions often present challenges that lead to complicated restrictions on the parameter space. The purpose of this work is to present techniques using multivariate mixtures for establishing validity that are simultaneously simplified and comprehensive. This is accomplished using results on conditionally negative semidefinite matrices and the Schur product theorem. For illustration, we use the recently-introduced Confluent Hypergeometric (CH) class of covariance functions. In addition, we establish the spectral density of the Confluent Hypergeometric covariance and use this to construct valid multivariate models as well as propose new cross-covariances. Our approach leads to valid multivariate cross-covariance models that inherit the desired marginal properties of the Confluent Hypergeometric model and outperform the multivariate Mat&apos;ern model in out-of-sample prediction under slowly-decaying correlation of the underlying multivariate random field. We also establish properties of the new models, including results on equivalence of Gaussian measures. We demonstrate the new model’s use for multivariate oceanography dataset consisting of temperature, salinity and oxygen, as measured by autonomous floats in the Southern Ocean.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.05682&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Drew Yarger, Anindya Bhadra</name></author><category term="stat.ME" /><summary type="html">Modeling of multivariate random fields through Gaussian processes calls for the construction of valid cross-covariance functions describing the dependence between any two component processes at different spatial locations. The required validity conditions often present challenges that lead to complicated restrictions on the parameter space. The purpose of this work is to present techniques using multivariate mixtures for establishing validity that are simultaneously simplified and comprehensive. This is accomplished using results on conditionally negative semidefinite matrices and the Schur product theorem. For illustration, we use the recently-introduced Confluent Hypergeometric (CH) class of covariance functions. In addition, we establish the spectral density of the Confluent Hypergeometric covariance and use this to construct valid multivariate models as well as propose new cross-covariances. Our approach leads to valid multivariate cross-covariance models that inherit the desired marginal properties of the Confluent Hypergeometric model and outperform the multivariate Mat&apos;ern model in out-of-sample prediction under slowly-decaying correlation of the underlying multivariate random field. We also establish properties of the new models, including results on equivalence of Gaussian measures. We demonstrate the new model’s use for multivariate oceanography dataset consisting of temperature, salinity and oxygen, as measured by autonomous floats in the Southern Ocean.</summary></entry><entry><title type="html">When Graph Neural Network Meets Causality: Opportunities, Methodologies and An Outlook</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/WhenGraphNeuralNetworkMeetsCausalityOpportunitiesMethodologiesandAnOutlook.html" rel="alternate" type="text/html" title="When Graph Neural Network Meets Causality: Opportunities, Methodologies and An Outlook" /><published>2024-06-19T00:00:00+00:00</published><updated>2024-06-19T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/WhenGraphNeuralNetworkMeetsCausalityOpportunitiesMethodologiesandAnOutlook</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/WhenGraphNeuralNetworkMeetsCausalityOpportunitiesMethodologiesandAnOutlook.html">&lt;p&gt;Graph Neural Networks (GNNs) have emerged as powerful representation learning tools for capturing complex dependencies within diverse graph-structured data. Despite their success in a wide range of graph mining tasks, GNNs have raised serious concerns regarding their trustworthiness, including susceptibility to distribution shift, biases towards certain populations, and lack of explainability. Recently, integrating causal learning techniques into GNNs has sparked numerous ground-breaking studies since many GNN trustworthiness issues can be alleviated by capturing the underlying data causality rather than superficial correlations. In this survey, we comprehensively review recent research efforts on Causality-Inspired GNNs (CIGNNs). Specifically, we first employ causal tools to analyze the primary trustworthiness risks of existing GNNs, underscoring the necessity for GNNs to comprehend the causal mechanisms within graph data. Moreover, we introduce a taxonomy of CIGNNs based on the type of causal learning capability they are equipped with, i.e., causal reasoning and causal representation learning. Besides, we systematically introduce typical methods within each category and discuss how they mitigate trustworthiness risks. Finally, we summarize useful resources and discuss several future directions, hoping to shed light on new research opportunities in this emerging field. The representative papers, along with open-source data and codes, are available in https://github.com/usail-hkust/Causality-Inspired-GNNs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.12477&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wenzhao Jiang, Hao Liu, Hui Xiong</name></author><category term="stat.ME" /><summary type="html">Graph Neural Networks (GNNs) have emerged as powerful representation learning tools for capturing complex dependencies within diverse graph-structured data. Despite their success in a wide range of graph mining tasks, GNNs have raised serious concerns regarding their trustworthiness, including susceptibility to distribution shift, biases towards certain populations, and lack of explainability. Recently, integrating causal learning techniques into GNNs has sparked numerous ground-breaking studies since many GNN trustworthiness issues can be alleviated by capturing the underlying data causality rather than superficial correlations. In this survey, we comprehensively review recent research efforts on Causality-Inspired GNNs (CIGNNs). Specifically, we first employ causal tools to analyze the primary trustworthiness risks of existing GNNs, underscoring the necessity for GNNs to comprehend the causal mechanisms within graph data. Moreover, we introduce a taxonomy of CIGNNs based on the type of causal learning capability they are equipped with, i.e., causal reasoning and causal representation learning. Besides, we systematically introduce typical methods within each category and discuss how they mitigate trustworthiness risks. Finally, we summarize useful resources and discuss several future directions, hoping to shed light on new research opportunities in this emerging field. The representative papers, along with open-source data and codes, are available in https://github.com/usail-hkust/Causality-Inspired-GNNs.</summary></entry></feed>