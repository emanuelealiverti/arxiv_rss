<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-17T07:13:39+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html"></title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/2024-05-17-Thecaseforspecifyingtheidealtargettrial.html" rel="alternate" type="text/html" title="" /><published>2024-05-17T07:13:39+00:00</published><updated>2024-05-17T07:13:39+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/2024-05-17-Thecaseforspecifyingtheidealtargettrial</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/2024-05-17-Thecaseforspecifyingtheidealtargettrial.html">&lt;p&gt;The target trial is an increasingly popular conceptual device for guiding the design and analysis of observational studies that seek to perform causal inference. As tends to occur with concepts like this, there is variability in how certain aspects of the approach are understood, which may lead to potentially consequential differences in how the approach is taught, implemented, and interpreted in practice. In this commentary, we provide a perspective on two of these aspects: how the target trial should be specified, and relatedly, how the target trial fits within a formal causal inference framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10026&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emanuele Aliverti</name></author></entry><entry><title type="html">A Framework for Improving the Reliability of Black-box Variational Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/AFrameworkforImprovingtheReliabilityofBlackboxVariationalInference.html" rel="alternate" type="text/html" title="A Framework for Improving the Reliability of Black-box Variational Inference" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/AFrameworkforImprovingtheReliabilityofBlackboxVariationalInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/AFrameworkforImprovingtheReliabilityofBlackboxVariationalInference.html">&lt;p&gt;Black-box variational inference (BBVI) now sees widespread use in machine learning and statistics as a fast yet flexible alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, stochastic optimization methods for BBVI remain unreliable and require substantial expertise and hand-tuning to apply effectively. In this paper, we propose Robust and Automated Black-box VI (RABVI), a framework for improving the reliability of BBVI optimization. RABVI is based on rigorously justified automation techniques, includes just a small number of intuitive tuning parameters, and detects inaccurate estimates of the optimal variational approximation. RABVI adaptively decreases the learning rate by detecting convergence of the fixed–learning-rate iterates, then estimates the symmetrized Kullback–Leibler (KL) divergence between the current variational approximation and the optimal one. It also employs a novel optimization termination criterion that enables the user to balance desired accuracy against computational cost by comparing (i) the predicted relative decrease in the symmetrized KL divergence if a smaller learning were used and (ii) the predicted computation required to converge with the smaller learning rate. We validate the robustness and accuracy of RABVI through carefully designed simulation studies and on a diverse set of real-world model and data examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.15945&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Manushi Welandawe, Michael Riis Andersen, Aki Vehtari, Jonathan H. Huggins</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Black-box variational inference (BBVI) now sees widespread use in machine learning and statistics as a fast yet flexible alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, stochastic optimization methods for BBVI remain unreliable and require substantial expertise and hand-tuning to apply effectively. In this paper, we propose Robust and Automated Black-box VI (RABVI), a framework for improving the reliability of BBVI optimization. RABVI is based on rigorously justified automation techniques, includes just a small number of intuitive tuning parameters, and detects inaccurate estimates of the optimal variational approximation. RABVI adaptively decreases the learning rate by detecting convergence of the fixed–learning-rate iterates, then estimates the symmetrized Kullback–Leibler (KL) divergence between the current variational approximation and the optimal one. It also employs a novel optimization termination criterion that enables the user to balance desired accuracy against computational cost by comparing (i) the predicted relative decrease in the symmetrized KL divergence if a smaller learning were used and (ii) the predicted computation required to converge with the smaller learning rate. We validate the robustness and accuracy of RABVI through carefully designed simulation studies and on a diverse set of real-world model and data examples.</summary></entry><entry><title type="html">A Gaussian Process Model for Ordinal Data with Applications to Chemoinformatics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/AGaussianProcessModelforOrdinalDatawithApplicationstoChemoinformatics.html" rel="alternate" type="text/html" title="A Gaussian Process Model for Ordinal Data with Applications to Chemoinformatics" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/AGaussianProcessModelforOrdinalDatawithApplicationstoChemoinformatics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/AGaussianProcessModelforOrdinalDatawithApplicationstoChemoinformatics.html">&lt;p&gt;With the proliferation of screening tools for chemical testing, it is now possible to create vast databases of chemicals easily. However, rigorous statistical methodologies employed to analyse these databases are in their infancy, and further development to facilitate chemical discovery is imperative. In this paper, we present conditional Gaussian process models to predict ordinal outcomes from chemical experiments, where the inputs are chemical compounds. We implement the Tanimoto distance, a metric on the chemical space, within the covariance of the Gaussian processes to capture correlated effects in the chemical space. A novel aspect of our model is that the kernel contains a scaling parameter, a feature not previously examined in the literature, that controls the strength of the correlation between elements of the chemical space. Using molecular fingerprints, a numerical representation of a compound’s location within the chemical space, we show that accounting for correlation amongst chemical compounds improves predictive performance over the uncorrelated model, where effects are assumed to be independent. Moreover, we present a genetic algorithm for the facilitation of chemical discovery and identification of important features to the compound’s efficacy. A simulation study is conducted to demonstrate the suitability of the proposed methods. Our proposed methods are demonstrated on a hazard classification problem of organic solvents.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09989&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arron Gosnell, Evangelos Evangelou</name></author><category term="stat.AP," /><category term="stat.ME," /><category term="stat.ML" /><summary type="html">With the proliferation of screening tools for chemical testing, it is now possible to create vast databases of chemicals easily. However, rigorous statistical methodologies employed to analyse these databases are in their infancy, and further development to facilitate chemical discovery is imperative. In this paper, we present conditional Gaussian process models to predict ordinal outcomes from chemical experiments, where the inputs are chemical compounds. We implement the Tanimoto distance, a metric on the chemical space, within the covariance of the Gaussian processes to capture correlated effects in the chemical space. A novel aspect of our model is that the kernel contains a scaling parameter, a feature not previously examined in the literature, that controls the strength of the correlation between elements of the chemical space. Using molecular fingerprints, a numerical representation of a compound’s location within the chemical space, we show that accounting for correlation amongst chemical compounds improves predictive performance over the uncorrelated model, where effects are assumed to be independent. Moreover, we present a genetic algorithm for the facilitation of chemical discovery and identification of important features to the compound’s efficacy. A simulation study is conducted to demonstrate the suitability of the proposed methods. Our proposed methods are demonstrated on a hazard classification problem of organic solvents.</summary></entry><entry><title type="html">A Stable and Efficient Covariate-Balancing Estimator for Causal Survival Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/AStableandEfficientCovariateBalancingEstimatorforCausalSurvivalEffects.html" rel="alternate" type="text/html" title="A Stable and Efficient Covariate-Balancing Estimator for Causal Survival Effects" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/AStableandEfficientCovariateBalancingEstimatorforCausalSurvivalEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/AStableandEfficientCovariateBalancingEstimatorforCausalSurvivalEffects.html">&lt;p&gt;We propose an empirically stable and asymptotically efficient covariate-balancing approach to the problem of estimating survival causal effects in data with conditionally-independent censoring. This addresses a challenge often encountered in state-of-the-art nonparametric methods: the use of inverses of small estimated probabilities and the resulting amplification of estimation error. We validate our theoretical results in experiments on synthetic and semi-synthetic data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.02278&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Khiem Pham, David A. Hirshberg, Phuong-Mai Huynh-Pham, Michele Santacatterina, Ser-Nam Lim, Ramin Zabih</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We propose an empirically stable and asymptotically efficient covariate-balancing approach to the problem of estimating survival causal effects in data with conditionally-independent censoring. This addresses a challenge often encountered in state-of-the-art nonparametric methods: the use of inverses of small estimated probabilities and the resulting amplification of estimation error. We validate our theoretical results in experiments on synthetic and semi-synthetic data.</summary></entry><entry><title type="html">A Unification of Exchangeability and Continuous Exposure and Confounder Measurement Errors: Probabilistic Exchangeability</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/AUnificationofExchangeabilityandContinuousExposureandConfounderMeasurementErrorsProbabilisticExchangeability.html" rel="alternate" type="text/html" title="A Unification of Exchangeability and Continuous Exposure and Confounder Measurement Errors: Probabilistic Exchangeability" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/AUnificationofExchangeabilityandContinuousExposureandConfounderMeasurementErrorsProbabilisticExchangeability</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/AUnificationofExchangeabilityandContinuousExposureandConfounderMeasurementErrorsProbabilisticExchangeability.html">&lt;p&gt;Exchangeability concerning a continuous exposure, X, implies no confounding bias when identifying average exposure effects of X, AEE(X). When X is measured with error (Xep), two challenges arise in identifying AEE(X). Firstly, exchangeability regarding Xep does not equal exchangeability regarding X. Secondly, the non-differential error assumption (NDEA) could be overly stringent in practice. To address them, this article proposes unifying exchangeability and exposure and confounder measurement errors with three novel concepts. The first, Probabilistic Exchangeability (PE), states that the outcomes of those with Xep=e are probabilistically exchangeable with the outcomes of those truly exposed to X=eT. The relationship between AEE(Xep) and AEE(X) in risk difference and ratio scales is mathematically expressed as a probabilistic certainty, termed exchangeability probability (Pe). Squared Pe (Pe2) quantifies the extent to which AEE(Xep) differs from AEE(X) due to exposure measurement error through mechanisms not akin to confounding mechanisms. The coefficient of determination (R2) in the regression of Xep against X may sometimes be sufficient to measure Pe2. The second concept, Emergent Pseudo Confounding (EPC), describes the bias introduced by exposure measurement error through mechanisms akin to confounding mechanisms. PE requires controlling for EPC, which is weaker than NDEA. The third, Emergent Confounding, describes when bias due to confounder measurement error arises. Adjustment for E(P)C can be performed like confounding adjustment. This paper provides maximum insight into when AEE(Xep) is an appropriate surrogate of AEE(X) and how to measure the difference between these two. Differential errors could be addressed and may not compromise causal inference.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07910&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Honghyok Kim</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Exchangeability concerning a continuous exposure, X, implies no confounding bias when identifying average exposure effects of X, AEE(X). When X is measured with error (Xep), two challenges arise in identifying AEE(X). Firstly, exchangeability regarding Xep does not equal exchangeability regarding X. Secondly, the non-differential error assumption (NDEA) could be overly stringent in practice. To address them, this article proposes unifying exchangeability and exposure and confounder measurement errors with three novel concepts. The first, Probabilistic Exchangeability (PE), states that the outcomes of those with Xep=e are probabilistically exchangeable with the outcomes of those truly exposed to X=eT. The relationship between AEE(Xep) and AEE(X) in risk difference and ratio scales is mathematically expressed as a probabilistic certainty, termed exchangeability probability (Pe). Squared Pe (Pe2) quantifies the extent to which AEE(Xep) differs from AEE(X) due to exposure measurement error through mechanisms not akin to confounding mechanisms. The coefficient of determination (R2) in the regression of Xep against X may sometimes be sufficient to measure Pe2. The second concept, Emergent Pseudo Confounding (EPC), describes the bias introduced by exposure measurement error through mechanisms akin to confounding mechanisms. PE requires controlling for EPC, which is weaker than NDEA. The third, Emergent Confounding, describes when bias due to confounder measurement error arises. Adjustment for E(P)C can be performed like confounding adjustment. This paper provides maximum insight into when AEE(Xep) is an appropriate surrogate of AEE(X) and how to measure the difference between these two. Differential errors could be addressed and may not compromise causal inference.</summary></entry><entry><title type="html">Alternative ranking measures to predict international football results</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/Alternativerankingmeasurestopredictinternationalfootballresults.html" rel="alternate" type="text/html" title="Alternative ranking measures to predict international football results" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/Alternativerankingmeasurestopredictinternationalfootballresults</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/Alternativerankingmeasurestopredictinternationalfootballresults.html">&lt;p&gt;Over the last few years, there has been a growing interest in the prediction and modelling of competitive sports outcomes, with particular emphasis placed on this area by the Bayesian statistics and machine learning communities. In this paper, we have carried out a comparative evaluation of statistical and machine learning models to assess their predictive performance for the 2022 World Cup and for the 2024 Africa Cup of Nations by evaluating alternative summaries of past performances related to the involved teams. More specifically, we consider the Bayesian Bradley-Terry-Davidson model, which is a widely used statistical framework for ranking items based on paired comparisons that have been applied successfully in various domains, including football. The analysis was performed including in some canonical goal-based models both the Bradley-Terry-Davidson derived ranking and the widely recognized Coca-Cola FIFA ranking commonly adopted by football fans and amateurs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10247&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roberto Macrì Demartino, Leonardo Egidi, Nicola Torelli</name></author><category term="stat.AP" /><summary type="html">Over the last few years, there has been a growing interest in the prediction and modelling of competitive sports outcomes, with particular emphasis placed on this area by the Bayesian statistics and machine learning communities. In this paper, we have carried out a comparative evaluation of statistical and machine learning models to assess their predictive performance for the 2022 World Cup and for the 2024 Africa Cup of Nations by evaluating alternative summaries of past performances related to the involved teams. More specifically, we consider the Bayesian Bradley-Terry-Davidson model, which is a widely used statistical framework for ranking items based on paired comparisons that have been applied successfully in various domains, including football. The analysis was performed including in some canonical goal-based models both the Bradley-Terry-Davidson derived ranking and the widely recognized Coca-Cola FIFA ranking commonly adopted by football fans and amateurs.</summary></entry><entry><title type="html">Assessing course difficulty and the effect of weather in amateur cross country running races</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/Assessingcoursedifficultyandtheeffectofweatherinamateurcrosscountryrunningraces.html" rel="alternate" type="text/html" title="Assessing course difficulty and the effect of weather in amateur cross country running races" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/Assessingcoursedifficultyandtheeffectofweatherinamateurcrosscountryrunningraces</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/Assessingcoursedifficultyandtheeffectofweatherinamateurcrosscountryrunningraces.html">&lt;p&gt;Cross country running races are different to track and road races in that the courses are not typically accurately measured and the condition of the course can have a strong effect on the finish times of the participants. In this paper we investigate these effects by modelling the finish times of all participants in 28 cross country running races over 5 seasons in the North East of England. We model the natural logarithm of the finish times using linear mixed effects models for both the senior men’s and senior women’s races. We investigate the effects of weather and underfoot conditions using windspeed and rainfall as covariates, fit distance as a covariate, and investigate the effect of time via the season of the race, in particular investigating any evidence of a pre- to post-Covid effect. We use random athlete effects to model the participant to participant variability and identify the most difficult courses using random course effects. The statistical inference is Bayesian. We assess model adequacy by comparing samples from the posterior predictive distribution of finish times to the observed distribution of finish times in each race. We find strong differences between the difficulty of the courses, effects of rainfall in the month of the race and the previous month to increase finish times and an effect of increasing distance increasing finish times. We find no evidence that windspeed affects finish times.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09865&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kevin J Wilson, Nina Wilson</name></author><category term="stat.AP" /><summary type="html">Cross country running races are different to track and road races in that the courses are not typically accurately measured and the condition of the course can have a strong effect on the finish times of the participants. In this paper we investigate these effects by modelling the finish times of all participants in 28 cross country running races over 5 seasons in the North East of England. We model the natural logarithm of the finish times using linear mixed effects models for both the senior men’s and senior women’s races. We investigate the effects of weather and underfoot conditions using windspeed and rainfall as covariates, fit distance as a covariate, and investigate the effect of time via the season of the race, in particular investigating any evidence of a pre- to post-Covid effect. We use random athlete effects to model the participant to participant variability and identify the most difficult courses using random course effects. The statistical inference is Bayesian. We assess model adequacy by comparing samples from the posterior predictive distribution of finish times to the observed distribution of finish times in each race. We find strong differences between the difficulty of the courses, effects of rainfall in the month of the race and the previous month to increase finish times and an effect of increasing distance increasing finish times. We find no evidence that windspeed affects finish times.</summary></entry><entry><title type="html">Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation.html" rel="alternate" type="text/html" title="Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation.html">&lt;p&gt;There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using inverse probability weighting (IPW). However, due to their numerically unstable IPW weights, these methods suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator by weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Our experimental results show that by effectively correcting the weight values, our proposed method outperforms the existing ones, including traditional weighting schemes. Our code is available at https://github.com/ychika/DPSW.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17483&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yoichi Chikahara, Kansei Ushiyama</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using inverse probability weighting (IPW). However, due to their numerically unstable IPW weights, these methods suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator by weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Our experimental results show that by effectively correcting the weight values, our proposed method outperforms the existing ones, including traditional weighting schemes. Our code is available at https://github.com/ychika/DPSW.</summary></entry><entry><title type="html">Enhancing Maritime Trajectory Forecasting via H3 Index and Causal Language Modelling (CLM)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/EnhancingMaritimeTrajectoryForecastingviaH3IndexandCausalLanguageModellingCLM.html" rel="alternate" type="text/html" title="Enhancing Maritime Trajectory Forecasting via H3 Index and Causal Language Modelling (CLM)" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/EnhancingMaritimeTrajectoryForecastingviaH3IndexandCausalLanguageModellingCLM</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/EnhancingMaritimeTrajectoryForecastingviaH3IndexandCausalLanguageModellingCLM.html">&lt;p&gt;The prediction of ship trajectories is a growing field of study in artificial intelligence. Traditional methods rely on the use of LSTM, GRU networks, and even Transformer architectures for the prediction of spatio-temporal series. This study proposes a viable alternative for predicting these trajectories using only GNSS positions. It considers this spatio-temporal problem as a natural language processing problem. The latitude/longitude coordinates of AIS messages are transformed into cell identifiers using the H3 index. Thanks to the pseudo-octal representation, it becomes easier for language models to learn the spatial hierarchy of the H3 index. The method is compared with a classical Kalman filter, widely used in the maritime domain, and introduces the Fr&apos;echet distance as the main evaluation metric. We show that it is possible to predict ship trajectories quite precisely up to 8 hours with 30 minutes of context. We demonstrate that this alternative works well enough to predict trajectories worldwide.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09596&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nicolas Drapier, Aladine Chetouani, Aurélien Chateigner</name></author><category term="stat.ME" /><summary type="html">The prediction of ship trajectories is a growing field of study in artificial intelligence. Traditional methods rely on the use of LSTM, GRU networks, and even Transformer architectures for the prediction of spatio-temporal series. This study proposes a viable alternative for predicting these trajectories using only GNSS positions. It considers this spatio-temporal problem as a natural language processing problem. The latitude/longitude coordinates of AIS messages are transformed into cell identifiers using the H3 index. Thanks to the pseudo-octal representation, it becomes easier for language models to learn the spatial hierarchy of the H3 index. The method is compared with a classical Kalman filter, widely used in the maritime domain, and introduces the Fr&apos;echet distance as the main evaluation metric. We show that it is possible to predict ship trajectories quite precisely up to 8 hours with 30 minutes of context. We demonstrate that this alternative works well enough to predict trajectories worldwide.</summary></entry><entry><title type="html">Exploring uniformity and maximum entropy distribution on torus through intrinsic geometry: Application to protein-chemistry</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/ExploringuniformityandmaximumentropydistributionontorusthroughintrinsicgeometryApplicationtoproteinchemistry.html" rel="alternate" type="text/html" title="Exploring uniformity and maximum entropy distribution on torus through intrinsic geometry: Application to protein-chemistry" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/ExploringuniformityandmaximumentropydistributionontorusthroughintrinsicgeometryApplicationtoproteinchemistry</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/ExploringuniformityandmaximumentropydistributionontorusthroughintrinsicgeometryApplicationtoproteinchemistry.html">&lt;p&gt;A generic family of distributions, defined on the surface of a curved torus is introduced using the area element of it. The area uniformity and the maximum entropy distribution are identified using the trigonometric moments of the proposed family. A marginal distribution is obtained as a three-parameter modification of the von Mises distribution that encompasses the von Mises, Cardioid, and Uniform distributions as special cases. The proposed family of the marginal distribution exhibits both symmetric and asymmetric, unimodal or bimodal shapes, contingent upon parameters. Furthermore, we scrutinize a two-parameter symmetric submodel, examining its moments, measure of variation, Kullback-Leibler divergence, and maximum likelihood estimation, among other properties. In addition, we introduce a modified acceptance-rejection sampling with a thin envelope obtained from the upper-Riemann-sum of a circular density, achieving a high rate of acceptance. This proposed sampling scheme will accelerate the empirical studies for a large-scale simulation reducing the processing time. Furthermore, we extend the Uniform, Wrapped Cauchy, and Kato-Jones distributions to the surface of the curved torus and implemented the proposed bivariate toroidal distribution for different groups of protein data, namely, $\alpha$-helix, $\beta$-sheet, and their mixture. A marginal of this proposed distribution is fitted to the wind direction data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09149&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Surojit Biswas, Buddhananda Banerjee</name></author><category term="stat.ME" /><summary type="html">A generic family of distributions, defined on the surface of a curved torus is introduced using the area element of it. The area uniformity and the maximum entropy distribution are identified using the trigonometric moments of the proposed family. A marginal distribution is obtained as a three-parameter modification of the von Mises distribution that encompasses the von Mises, Cardioid, and Uniform distributions as special cases. The proposed family of the marginal distribution exhibits both symmetric and asymmetric, unimodal or bimodal shapes, contingent upon parameters. Furthermore, we scrutinize a two-parameter symmetric submodel, examining its moments, measure of variation, Kullback-Leibler divergence, and maximum likelihood estimation, among other properties. In addition, we introduce a modified acceptance-rejection sampling with a thin envelope obtained from the upper-Riemann-sum of a circular density, achieving a high rate of acceptance. This proposed sampling scheme will accelerate the empirical studies for a large-scale simulation reducing the processing time. Furthermore, we extend the Uniform, Wrapped Cauchy, and Kato-Jones distributions to the surface of the curved torus and implemented the proposed bivariate toroidal distribution for different groups of protein data, namely, $\alpha$-helix, $\beta$-sheet, and their mixture. A marginal of this proposed distribution is fitted to the wind direction data.</summary></entry><entry><title type="html">Fully Latent Principal Stratification With Measurement Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/FullyLatentPrincipalStratificationWithMeasurementModels.html" rel="alternate" type="text/html" title="Fully Latent Principal Stratification With Measurement Models" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/FullyLatentPrincipalStratificationWithMeasurementModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/FullyLatentPrincipalStratificationWithMeasurementModels.html">&lt;p&gt;There is wide agreement on the importance of implementation data from randomized effectiveness studies in behavioral science; however, there are few methods available to incorporate these data into causal models, especially when they are multivariate or longitudinal, and interest is in low-dimensional summaries. We introduce a framework for studying how treatment effects vary between subjects who implement an intervention differently, combining principal stratification with latent variable measurement models; since principal strata are latent in both treatment arms, we call it “fully-latent principal stratification” or FLPS. We describe FLPS models including item-response-theory measurement, show that they are feasible in a simulation study, and illustrate them in an analysis of hint usage from a randomized study of computerized mathematics tutors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.04047&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sooyong Lee, Adam C Sales, Hyeon-Ah Kang, Tiffany A. Whittaker</name></author><category term="stat.ME" /><summary type="html">There is wide agreement on the importance of implementation data from randomized effectiveness studies in behavioral science; however, there are few methods available to incorporate these data into causal models, especially when they are multivariate or longitudinal, and interest is in low-dimensional summaries. We introduce a framework for studying how treatment effects vary between subjects who implement an intervention differently, combining principal stratification with latent variable measurement models; since principal strata are latent in both treatment arms, we call it “fully-latent principal stratification” or FLPS. We describe FLPS models including item-response-theory measurement, show that they are feasible in a simulation study, and illustrate them in an analysis of hint usage from a randomized study of computerized mathematics tutors.</summary></entry><entry><title type="html">Hidden Markov Models for Multivariate Panel Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/HiddenMarkovModelsforMultivariatePanelData.html" rel="alternate" type="text/html" title="Hidden Markov Models for Multivariate Panel Data" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/HiddenMarkovModelsforMultivariatePanelData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/HiddenMarkovModelsforMultivariatePanelData.html">&lt;p&gt;While advances continue to be made in model-based clustering, challenges persist in modeling various data types such as panel data. Multivariate panel data present difficulties for clustering algorithms due to the unique correlation structure, a consequence of taking observations on several subjects over multiple time points. Additionally, panel data are often plagued by missing data and dropouts, presenting issues for estimation algorithms. This research presents a family of hidden Markov models that compensate for the unique correlation structures that arise in panel data. A modified expectation-maximization algorithm capable of handling missing not at random data and dropout is presented and used to perform model estimation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.04122&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mackenzie R. Neal, Alexa A. Sochaniwsky, Paul D. McNicholas</name></author><category term="stat.ME" /><summary type="html">While advances continue to be made in model-based clustering, challenges persist in modeling various data types such as panel data. Multivariate panel data present difficulties for clustering algorithms due to the unique correlation structure, a consequence of taking observations on several subjects over multiple time points. Additionally, panel data are often plagued by missing data and dropouts, presenting issues for estimation algorithms. This research presents a family of hidden Markov models that compensate for the unique correlation structures that arise in panel data. A modified expectation-maximization algorithm capable of handling missing not at random data and dropout is presented and used to perform model estimation.</summary></entry><entry><title type="html">Identification of Single-Treatment Effects in Factorial Experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/IdentificationofSingleTreatmentEffectsinFactorialExperiments.html" rel="alternate" type="text/html" title="Identification of Single-Treatment Effects in Factorial Experiments" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/IdentificationofSingleTreatmentEffectsinFactorialExperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/IdentificationofSingleTreatmentEffectsinFactorialExperiments.html">&lt;p&gt;Despite their cost, randomized controlled trials (RCTs) are widely regarded as gold-standard evidence in disciplines ranging from social science to medicine. In recent decades, researchers have increasingly sought to reduce the resource burden of repeated RCTs with factorial designs that simultaneously test multiple hypotheses, e.g. experiments that evaluate the effects of many medications or products simultaneously. Here I show that when multiple interventions are randomized in experiments, the effect any single intervention would have outside the experimental setting is not identified absent heroic assumptions, even if otherwise perfectly realistic conditions are achieved. This happens because single-treatment effects involve a counterfactual world with a single focal intervention, allowing other variables to take their natural values (which may be confounded or modified by the focal intervention). In contrast, observational studies and factorial experiments provide information about potential-outcome distributions with zero and multiple interventions, respectively. In this paper, I formalize sufficient conditions for the identifiability of those isolated quantities. I show that researchers who rely on this type of design have to justify either linearity of functional forms or – in the nonparametric case – specify with Directed Acyclic Graphs how variables are related in the real world. Finally, I develop nonparametric sharp bounds – i.e., maximally informative best-/worst-case estimates consistent with limited RCT data – that show when extrapolations about effect signs are empirically justified. These new results are illustrated with simulated data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09797&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Guilherme Duarte</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.OT" /><summary type="html">Despite their cost, randomized controlled trials (RCTs) are widely regarded as gold-standard evidence in disciplines ranging from social science to medicine. In recent decades, researchers have increasingly sought to reduce the resource burden of repeated RCTs with factorial designs that simultaneously test multiple hypotheses, e.g. experiments that evaluate the effects of many medications or products simultaneously. Here I show that when multiple interventions are randomized in experiments, the effect any single intervention would have outside the experimental setting is not identified absent heroic assumptions, even if otherwise perfectly realistic conditions are achieved. This happens because single-treatment effects involve a counterfactual world with a single focal intervention, allowing other variables to take their natural values (which may be confounded or modified by the focal intervention). In contrast, observational studies and factorial experiments provide information about potential-outcome distributions with zero and multiple interventions, respectively. In this paper, I formalize sufficient conditions for the identifiability of those isolated quantities. I show that researchers who rely on this type of design have to justify either linearity of functional forms or – in the nonparametric case – specify with Directed Acyclic Graphs how variables are related in the real world. Finally, I develop nonparametric sharp bounds – i.e., maximally informative best-/worst-case estimates consistent with limited RCT data – that show when extrapolations about effect signs are empirically justified. These new results are illustrated with simulated data.</summary></entry><entry><title type="html">Large-scale Data Integration using Matrix Denoising and Geometric Factor Matching</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/LargescaleDataIntegrationusingMatrixDenoisingandGeometricFactorMatching.html" rel="alternate" type="text/html" title="Large-scale Data Integration using Matrix Denoising and Geometric Factor Matching" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/LargescaleDataIntegrationusingMatrixDenoisingandGeometricFactorMatching</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/LargescaleDataIntegrationusingMatrixDenoisingandGeometricFactorMatching.html">&lt;p&gt;Unsupervised integrative analysis of multiple data sources has become common place and scalable algorithms are necessary to accommodate ever increasing availability of data. Only few currently methods have estimation speed as their focus, and those that do are only applicable to restricted data layouts such as different data types measured on the same observation units. We introduce a novel point of view on low-rank matrix integration phrased as a graph estimation problem which allows development of a method, large-scale Collective Matrix Factorization (lsCMF), which is able to integrate data in flexible layouts in a speedy fashion. It utilizes a matrix denoising framework for rank estimation and geometric properties of singular vectors to efficiently integrate data. The quick estimation speed of lsCMF while retaining good estimation of data structure is then demonstrated in simulation studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10036&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Felix Held</name></author><category term="stat.ME" /><summary type="html">Unsupervised integrative analysis of multiple data sources has become common place and scalable algorithms are necessary to accommodate ever increasing availability of data. Only few currently methods have estimation speed as their focus, and those that do are only applicable to restricted data layouts such as different data types measured on the same observation units. We introduce a novel point of view on low-rank matrix integration phrased as a graph estimation problem which allows development of a method, large-scale Collective Matrix Factorization (lsCMF), which is able to integrate data in flexible layouts in a speedy fashion. It utilizes a matrix denoising framework for rank estimation and geometric properties of singular vectors to efficiently integrate data. The quick estimation speed of lsCMF while retaining good estimation of data structure is then demonstrated in simulation studies.</summary></entry><entry><title type="html">Multivariate strong invariance principle and uncertainty assessment for time in-homogeneous cyclic MCMC samplers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/MultivariatestronginvarianceprincipleanduncertaintyassessmentfortimeinhomogeneouscyclicMCMCsamplers.html" rel="alternate" type="text/html" title="Multivariate strong invariance principle and uncertainty assessment for time in-homogeneous cyclic MCMC samplers" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/MultivariatestronginvarianceprincipleanduncertaintyassessmentfortimeinhomogeneouscyclicMCMCsamplers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/MultivariatestronginvarianceprincipleanduncertaintyassessmentfortimeinhomogeneouscyclicMCMCsamplers.html">&lt;p&gt;Time in-homogeneous cyclic Markov chain Monte Carlo (MCMC) samplers, including deterministic scan Gibbs samplers and Metropolis within Gibbs samplers, are extensively used for sampling from multi-dimensional distributions. We establish a multivariate strong invariance principle (SIP) for Markov chains associated with these samplers. The rate of this SIP essentially aligns with the tightest rate available for time homogeneous Markov chains. The SIP implies the strong law of large numbers (SLLN) and the central limit theorem (CLT), and plays an essential role in uncertainty assessments. Using the SIP, we give conditions under which the multivariate batch means estimator for estimating the covariance matrix in the multivariate CLT is strongly consistent. Additionally, we provide conditions for a multivariate fixed volume sequential termination rule, which is associated with the concept of effective sample size (ESS), to be asymptotically valid. Our uncertainty assessment tools are demonstrated through various numerical experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10194&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Haoxiang Li, Qian Qin</name></author><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Time in-homogeneous cyclic Markov chain Monte Carlo (MCMC) samplers, including deterministic scan Gibbs samplers and Metropolis within Gibbs samplers, are extensively used for sampling from multi-dimensional distributions. We establish a multivariate strong invariance principle (SIP) for Markov chains associated with these samplers. The rate of this SIP essentially aligns with the tightest rate available for time homogeneous Markov chains. The SIP implies the strong law of large numbers (SLLN) and the central limit theorem (CLT), and plays an essential role in uncertainty assessments. Using the SIP, we give conditions under which the multivariate batch means estimator for estimating the covariance matrix in the multivariate CLT is strongly consistent. Additionally, we provide conditions for a multivariate fixed volume sequential termination rule, which is associated with the concept of effective sample size (ESS), to be asymptotically valid. Our uncertainty assessment tools are demonstrated through various numerical experiments.</summary></entry><entry><title type="html">No More Mumbles: Enhancing Robot Intelligibility through Speech Adaptation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/NoMoreMumblesEnhancingRobotIntelligibilitythroughSpeechAdaptation.html" rel="alternate" type="text/html" title="No More Mumbles: Enhancing Robot Intelligibility through Speech Adaptation" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/NoMoreMumblesEnhancingRobotIntelligibilitythroughSpeechAdaptation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/NoMoreMumblesEnhancingRobotIntelligibilitythroughSpeechAdaptation.html">&lt;p&gt;Spoken language interaction is at the heart of interpersonal communication, and people flexibly adapt their speech to different individuals and environments. It is surprising that robots, and by extension other digital devices, are not equipped to adapt their speech and instead rely on fixed speech parameters, which often hinder comprehension by the user. We conducted a speech comprehension study involving 39 participants who were exposed to different environmental and contextual conditions. During the experiment, the robot articulated words using different vocal parameters, and the participants were tasked with both recognising the spoken words and rating their subjective impression of the robot’s speech. The experiment’s primary outcome shows that spaces with good acoustic quality positively correlate with intelligibility and user experience. However, increasing the distance between the user and the robot exacerbated the user experience, while distracting background sounds significantly reduced speech recognition accuracy and user satisfaction. We next built an adaptive voice for the robot. For this, the robot needs to know how difficult it is for a user to understand spoken language in a particular setting. We present a prediction model that rates how annoying the ambient acoustic environment is and, consequentially, how hard it is to understand someone in this setting. Then, we develop a convolutional neural network model to adapt the robot’s speech parameters to different users and spaces, while taking into account the influence of ambient acoustics on intelligibility. Finally, we present an evaluation with 27 users, demonstrating superior intelligibility and user experience with adaptive voice parameters compared to fixed voice.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09708&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Qiaoqiao Ren, Yuanbo Hou, Dick Botteldooren, Tony Belpaeme</name></author><category term="stat.CO" /><summary type="html">Spoken language interaction is at the heart of interpersonal communication, and people flexibly adapt their speech to different individuals and environments. It is surprising that robots, and by extension other digital devices, are not equipped to adapt their speech and instead rely on fixed speech parameters, which often hinder comprehension by the user. We conducted a speech comprehension study involving 39 participants who were exposed to different environmental and contextual conditions. During the experiment, the robot articulated words using different vocal parameters, and the participants were tasked with both recognising the spoken words and rating their subjective impression of the robot’s speech. The experiment’s primary outcome shows that spaces with good acoustic quality positively correlate with intelligibility and user experience. However, increasing the distance between the user and the robot exacerbated the user experience, while distracting background sounds significantly reduced speech recognition accuracy and user satisfaction. We next built an adaptive voice for the robot. For this, the robot needs to know how difficult it is for a user to understand spoken language in a particular setting. We present a prediction model that rates how annoying the ambient acoustic environment is and, consequentially, how hard it is to understand someone in this setting. Then, we develop a convolutional neural network model to adapt the robot’s speech parameters to different users and spaces, while taking into account the influence of ambient acoustics on intelligibility. Finally, we present an evaluation with 27 users, demonstrating superior intelligibility and user experience with adaptive voice parameters compared to fixed voice.</summary></entry><entry><title type="html">On foundation of generative statistics with F-entropy: a gradient-based approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach.html" rel="alternate" type="text/html" title="On foundation of generative statistics with F-entropy: a gradient-based approach" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach.html">&lt;p&gt;This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can {\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g. missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisher’s divergence. (The F-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for generative model selection. Possible future projects include discrete data and Bayesian variational inference.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05389&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bing Cheng, Howell Tong</name></author><category term="stat.ME" /><summary type="html">This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can {\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g. missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisher’s divergence. (The F-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for generative model selection. Possible future projects include discrete data and Bayesian variational inference.</summary></entry><entry><title type="html">Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/OptimalAggregationofPredictionIntervalsunderUnsupervisedDomainShift.html" rel="alternate" type="text/html" title="Optimal Aggregation of Prediction Intervals under Unsupervised Domain Shift" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/OptimalAggregationofPredictionIntervalsunderUnsupervisedDomainShift</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/OptimalAggregationofPredictionIntervalsunderUnsupervisedDomainShift.html">&lt;p&gt;As machine learning models are increasingly deployed in dynamic environments, it becomes paramount to assess and quantify uncertainties associated with distribution shifts. A distribution shift occurs when the underlying data-generating process changes, leading to a deviation in the model’s performance. The prediction interval, which captures the range of likely outcomes for a given prediction, serves as a crucial tool for characterizing uncertainties induced by their underlying distribution. In this paper, we propose methodologies for aggregating prediction intervals to obtain one with minimal width and adequate coverage on the target domain under unsupervised domain shift, under which we have labeled samples from a related source domain and unlabeled covariates from the target domain. Our analysis encompasses scenarios where the source and the target domain are related via i) a bounded density ratio, and ii) a measure-preserving transformation. Our proposed methodologies are computationally efficient and easy to implement. Beyond illustrating the performance of our method through a real-world dataset, we also delve into the theoretical details. This includes establishing rigorous theoretical guarantees, coupled with finite sample bounds, regarding the coverage and width of our prediction intervals. Our approach excels in practical applications and is underpinned by a solid theoretical framework, ensuring its reliability and effectiveness across diverse contexts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10302&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiawei Ge, Debarghya Mukherjee, Jianqing Fan</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">As machine learning models are increasingly deployed in dynamic environments, it becomes paramount to assess and quantify uncertainties associated with distribution shifts. A distribution shift occurs when the underlying data-generating process changes, leading to a deviation in the model’s performance. The prediction interval, which captures the range of likely outcomes for a given prediction, serves as a crucial tool for characterizing uncertainties induced by their underlying distribution. In this paper, we propose methodologies for aggregating prediction intervals to obtain one with minimal width and adequate coverage on the target domain under unsupervised domain shift, under which we have labeled samples from a related source domain and unlabeled covariates from the target domain. Our analysis encompasses scenarios where the source and the target domain are related via i) a bounded density ratio, and ii) a measure-preserving transformation. Our proposed methodologies are computationally efficient and easy to implement. Beyond illustrating the performance of our method through a real-world dataset, we also delve into the theoretical details. This includes establishing rigorous theoretical guarantees, coupled with finite sample bounds, regarding the coverage and width of our prediction intervals. Our approach excels in practical applications and is underpinned by a solid theoretical framework, ensuring its reliability and effectiveness across diverse contexts.</summary></entry><entry><title type="html">Process-based Inference for Spatial Energetics Using Bayesian Predictive Stacking</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/ProcessbasedInferenceforSpatialEnergeticsUsingBayesianPredictiveStacking.html" rel="alternate" type="text/html" title="Process-based Inference for Spatial Energetics Using Bayesian Predictive Stacking" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/ProcessbasedInferenceforSpatialEnergeticsUsingBayesianPredictiveStacking</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/ProcessbasedInferenceforSpatialEnergeticsUsingBayesianPredictiveStacking.html">&lt;p&gt;Rapid developments in streaming data technologies have enabled real-time monitoring of human activity that can deliver high-resolution data on health variables over trajectories or paths carved out by subjects as they conduct their daily physical activities. Wearable devices, such as wrist-worn sensors that monitor gross motor activity, have become prevalent and have kindled the emerging field of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spatial energetics&apos;&apos; in environmental health sciences. We devise a Bayesian inferential framework for analyzing such data while accounting for information available on specific spatial coordinates comprising a trajectory or path using a Global Positioning System (GPS) device embedded within the wearable device. We offer full probabilistic inference with uncertainty quantification using spatial-temporal process models adapted for data generated from&lt;/code&gt;actigraph’’ units as the subject traverses a path or trajectory in their daily routine. Anticipating the need for fast inference for mobile health data, we pursue exact inference using conjugate Bayesian models and employ predictive stacking to assimilate inference across these individual models. This circumvents issues with iterative estimation algorithms such as Markov chain Monte Carlo. We devise Bayesian predictive stacking in this context for models that treat time as discrete epochs and that treat time as continuous. We illustrate our methods with simulation experiments and analysis of data from the Physical Activity through Sustainable Transport Approaches (PASTA-LA) study conducted by the Fielding School of Public Health at the University of California, Los Angeles.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09906&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tomoya Wakayama, Sudipto Banerjee</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Rapid developments in streaming data technologies have enabled real-time monitoring of human activity that can deliver high-resolution data on health variables over trajectories or paths carved out by subjects as they conduct their daily physical activities. Wearable devices, such as wrist-worn sensors that monitor gross motor activity, have become prevalent and have kindled the emerging field of spatial energetics&apos;&apos; in environmental health sciences. We devise a Bayesian inferential framework for analyzing such data while accounting for information available on specific spatial coordinates comprising a trajectory or path using a Global Positioning System (GPS) device embedded within the wearable device. We offer full probabilistic inference with uncertainty quantification using spatial-temporal process models adapted for data generated fromactigraph’’ units as the subject traverses a path or trajectory in their daily routine. Anticipating the need for fast inference for mobile health data, we pursue exact inference using conjugate Bayesian models and employ predictive stacking to assimilate inference across these individual models. This circumvents issues with iterative estimation algorithms such as Markov chain Monte Carlo. We devise Bayesian predictive stacking in this context for models that treat time as discrete epochs and that treat time as continuous. We illustrate our methods with simulation experiments and analysis of data from the Physical Activity through Sustainable Transport Approaches (PASTA-LA) study conducted by the Fielding School of Public Health at the University of California, Los Angeles.</summary></entry><entry><title type="html">Quantization-based LHS for dependent inputs : application to sensitivity analysis of environmental models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/QuantizationbasedLHSfordependentinputsapplicationtosensitivityanalysisofenvironmentalmodels.html" rel="alternate" type="text/html" title="Quantization-based LHS for dependent inputs : application to sensitivity analysis of environmental models" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/QuantizationbasedLHSfordependentinputsapplicationtosensitivityanalysisofenvironmentalmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/QuantizationbasedLHSfordependentinputsapplicationtosensitivityanalysisofenvironmentalmodels.html">&lt;p&gt;Numerical modeling is essential for comprehending intricate physical phenomena in different domains. To handle complexity, sensitivity analysis, particularly screening, is crucial for identifying influential input parameters. Kernel-based methods, such as the Hilbert Schmidt Independence Criterion (HSIC), are valuable for analyzing dependencies between inputs and outputs. Moreover, due to the computational expense of such models, metamodels (or surrogate models) are often unavoidable. Implementing metamodels and HSIC requires data from the original model, which leads to the need for space-filling designs. While existing methods like Latin Hypercube Sampling (LHS) are effective for independent variables, incorporating dependence is challenging. This paper introduces a novel LHS variant, Quantization-based LHS, which leverages Voronoi vector quantization to address correlated inputs. The method ensures comprehensive coverage of stratified variables, enhancing distribution across marginals. The paper outlines expectation estimators based on Quantization-based LHS in various dependency settings, demonstrating their unbiasedness. The method is applied on several models of growing complexities, first on simple examples to illustrate the theory, then on more complex environmental hydrological models, when the dependence is known or not, and with more and more interactive processes and factors. The last application is on the digital twin of a French vineyard catchment (Beaujolais region) to design a vegetative filter strip and reduce water, sediment and pesticide transfers from the fields to the river. Quantization-based LHS is used to compute HSIC measures and independence tests, demonstrating its usefulness, especially in the context of complex models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09887&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Guerlain Lambert, Céline Helbert, Claire Lauvernet</name></author><category term="stat.ME" /><summary type="html">Numerical modeling is essential for comprehending intricate physical phenomena in different domains. To handle complexity, sensitivity analysis, particularly screening, is crucial for identifying influential input parameters. Kernel-based methods, such as the Hilbert Schmidt Independence Criterion (HSIC), are valuable for analyzing dependencies between inputs and outputs. Moreover, due to the computational expense of such models, metamodels (or surrogate models) are often unavoidable. Implementing metamodels and HSIC requires data from the original model, which leads to the need for space-filling designs. While existing methods like Latin Hypercube Sampling (LHS) are effective for independent variables, incorporating dependence is challenging. This paper introduces a novel LHS variant, Quantization-based LHS, which leverages Voronoi vector quantization to address correlated inputs. The method ensures comprehensive coverage of stratified variables, enhancing distribution across marginals. The paper outlines expectation estimators based on Quantization-based LHS in various dependency settings, demonstrating their unbiasedness. The method is applied on several models of growing complexities, first on simple examples to illustrate the theory, then on more complex environmental hydrological models, when the dependence is known or not, and with more and more interactive processes and factors. The last application is on the digital twin of a French vineyard catchment (Beaujolais region) to design a vegetative filter strip and reduce water, sediment and pesticide transfers from the fields to the river. Quantization-based LHS is used to compute HSIC measures and independence tests, demonstrating its usefulness, especially in the context of complex models.</summary></entry><entry><title type="html">Randomly sampling bipartite networks with fixed degree sequences</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/Randomlysamplingbipartitenetworkswithfixeddegreesequences.html" rel="alternate" type="text/html" title="Randomly sampling bipartite networks with fixed degree sequences" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/Randomlysamplingbipartitenetworkswithfixeddegreesequences</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/Randomlysamplingbipartitenetworkswithfixeddegreesequences.html">&lt;p&gt;Statistical analysis of bipartite networks frequently requires randomly sampling from the set of all bipartite networks with the same degree sequence as an observed network. Trade algorithms offer an efficient way to generate samples of bipartite networks by incrementally `trading’ the positions of some of their edges. However, it is difficult to know how many such trades are required to ensure that the sample is random. I propose a stopping rule that focuses on the distance between sampled networks and the observed network, and stops performing trades when this distribution stabilizes. Analyses demonstrate that, for over 650 different degree sequences, using this stopping rule ensures a random sample with a high probability, and that it is practical for use in empirical applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.04937&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zachary P. Neal</name></author><category term="stat.ME" /><summary type="html">Statistical analysis of bipartite networks frequently requires randomly sampling from the set of all bipartite networks with the same degree sequence as an observed network. Trade algorithms offer an efficient way to generate samples of bipartite networks by incrementally `trading’ the positions of some of their edges. However, it is difficult to know how many such trades are required to ensure that the sample is random. I propose a stopping rule that focuses on the distance between sampled networks and the observed network, and stops performing trades when this distribution stabilizes. Analyses demonstrate that, for over 650 different degree sequences, using this stopping rule ensures a random sample with a high probability, and that it is practical for use in empirical applications.</summary></entry><entry><title type="html">Robust Point Matching with Distance Profiles</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/RobustPointMatchingwithDistanceProfiles.html" rel="alternate" type="text/html" title="Robust Point Matching with Distance Profiles" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/RobustPointMatchingwithDistanceProfiles</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/RobustPointMatchingwithDistanceProfiles.html">&lt;p&gt;While matching procedures based on pairwise distances are conceptually appealing and thus favored in practice, theoretical guarantees for such procedures are rarely found in the literature. We propose and analyze matching procedures based on distance profiles that are easily implementable in practice, showing these procedures are robust to outliers and noise. We demonstrate the performance of the proposed method using a real data example and provide simulation studies to complement the theoretical findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.12641&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>YoonHaeng Hur, Yuehaw Khoo</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">While matching procedures based on pairwise distances are conceptually appealing and thus favored in practice, theoretical guarantees for such procedures are rarely found in the literature. We propose and analyze matching procedures based on distance profiles that are easily implementable in practice, showing these procedures are robust to outliers and noise. We demonstrate the performance of the proposed method using a real data example and provide simulation studies to complement the theoretical findings.</summary></entry><entry><title type="html">Solving Fredholm Integral Equations of the First Kind via Wasserstein Gradient Flows</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/SolvingFredholmIntegralEquationsoftheFirstKindviaWassersteinGradientFlows.html" rel="alternate" type="text/html" title="Solving Fredholm Integral Equations of the First Kind via Wasserstein Gradient Flows" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/SolvingFredholmIntegralEquationsoftheFirstKindviaWassersteinGradientFlows</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/SolvingFredholmIntegralEquationsoftheFirstKindviaWassersteinGradientFlows.html">&lt;p&gt;Solving Fredholm equations of the first kind is crucial in many areas of the applied sciences. In this work we adopt a probabilistic and variational point of view by considering a minimization problem in the space of probability measures with an entropic regularization. Contrary to classical approaches which discretize the domain of the solutions, we introduce an algorithm to asymptotically sample from the unique solution of the regularized minimization problem. As a result our estimators do not depend on any underlying grid and have better scalability properties than most existing methods. Our algorithm is based on a particle approximation of the solution of a McKean–Vlasov stochastic differential equation associated with the Wasserstein gradient flow of our variational formulation. We prove the convergence towards a minimizer and provide practical guidelines for its numerical implementation. Finally, our method is compared with other approaches on several examples including density deconvolution and epidemiology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2209.09936&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Francesca R. Crucinio, Valentin De Bortoli, Arnaud Doucet, Adam M. Johansen</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Solving Fredholm equations of the first kind is crucial in many areas of the applied sciences. In this work we adopt a probabilistic and variational point of view by considering a minimization problem in the space of probability measures with an entropic regularization. Contrary to classical approaches which discretize the domain of the solutions, we introduce an algorithm to asymptotically sample from the unique solution of the regularized minimization problem. As a result our estimators do not depend on any underlying grid and have better scalability properties than most existing methods. Our algorithm is based on a particle approximation of the solution of a McKean–Vlasov stochastic differential equation associated with the Wasserstein gradient flow of our variational formulation. We prove the convergence towards a minimizer and provide practical guidelines for its numerical implementation. Finally, our method is compared with other approaches on several examples including density deconvolution and epidemiology.</summary></entry><entry><title type="html">Sparse and Orthogonal Low-rank Collective Matrix Factorization (solrCMF): Efficient data integration in flexible layouts</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/SparseandOrthogonalLowrankCollectiveMatrixFactorizationsolrCMFEfficientdataintegrationinflexiblelayouts.html" rel="alternate" type="text/html" title="Sparse and Orthogonal Low-rank Collective Matrix Factorization (solrCMF): Efficient data integration in flexible layouts" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/SparseandOrthogonalLowrankCollectiveMatrixFactorizationsolrCMFEfficientdataintegrationinflexiblelayouts</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/SparseandOrthogonalLowrankCollectiveMatrixFactorizationsolrCMFEfficientdataintegrationinflexiblelayouts.html">&lt;p&gt;Interest in unsupervised methods for joint analysis of heterogeneous data sources has risen in recent years. Low-rank latent factor models have proven to be an effective tool for data integration and have been extended to a large number of data source layouts. Of particular interest is the separation of variation present in data sources into shared and individual subspaces. In addition, interpretability of estimated latent factors is crucial to further understanding.
  We present sparse and orthogonal low-rank Collective Matrix Factorization (solrCMF) to estimate low-rank latent factor models for flexible data layouts. These encompass traditional multi-view (one group, multiple data types) and multi-grid (multiple groups, multiple data types) layouts, as well as augmented layouts, which allow the inclusion of side information between data types or groups. In addition, solrCMF allows tensor-like layouts (repeated layers), estimates interpretable factors, and determines variation structure among factors and data sources.
  Using a penalized optimization approach, we automatically separate variability into the globally and partially shared as well as individual components and estimate sparse representations of factors. To further increase interpretability of factors, we enforce orthogonality between them. Estimation is performed efficiently in a recent multi-block ADMM framework which we adapted to support embedded manifold constraints.
  The performance of solrCMF is demonstrated in simulation studies and compares favorably to existing methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.10067&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Felix Held, Jacob Lindbäck, Rebecka Jörnsten</name></author><category term="stat.ME" /><summary type="html">Interest in unsupervised methods for joint analysis of heterogeneous data sources has risen in recent years. Low-rank latent factor models have proven to be an effective tool for data integration and have been extended to a large number of data source layouts. Of particular interest is the separation of variation present in data sources into shared and individual subspaces. In addition, interpretability of estimated latent factors is crucial to further understanding. We present sparse and orthogonal low-rank Collective Matrix Factorization (solrCMF) to estimate low-rank latent factor models for flexible data layouts. These encompass traditional multi-view (one group, multiple data types) and multi-grid (multiple groups, multiple data types) layouts, as well as augmented layouts, which allow the inclusion of side information between data types or groups. In addition, solrCMF allows tensor-like layouts (repeated layers), estimates interpretable factors, and determines variation structure among factors and data sources. Using a penalized optimization approach, we automatically separate variability into the globally and partially shared as well as individual components and estimate sparse representations of factors. To further increase interpretability of factors, we enforce orthogonality between them. Estimation is performed efficiently in a recent multi-block ADMM framework which we adapted to support embedded manifold constraints. The performance of solrCMF is demonstrated in simulation studies and compares favorably to existing methods.</summary></entry><entry><title type="html">The $\kappa$-generalised Distribution for Stock Returns</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/ThekappageneralisedDistributionforStockReturns.html" rel="alternate" type="text/html" title="The $\kappa$-generalised Distribution for Stock Returns" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/ThekappageneralisedDistributionforStockReturns</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/ThekappageneralisedDistributionforStockReturns.html">&lt;p&gt;Empirical evidence shows stock returns are often heavy-tailed rather than normally distributed. The $\kappa$-generalised distribution, originated in the context of statistical physics by Kaniadakis, is characterised by the $\kappa$-exponential function that is asymptotically exponential for small values and asymptotically power law for large values. This proves to be a useful property and makes it a good candidate distribution for many types of quantities. In this paper we focus on fitting historic daily stock returns for the FTSE 100 and the top 100 Nasdaq stocks. Using a Monte-Carlo goodness of fit test there is evidence that the $\kappa$-generalised distribution is a good fit for a significant proportion of the 200 stock returns analysed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09929&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Samuel Forbes</name></author><category term="stat.AP" /><summary type="html">Empirical evidence shows stock returns are often heavy-tailed rather than normally distributed. The $\kappa$-generalised distribution, originated in the context of statistical physics by Kaniadakis, is characterised by the $\kappa$-exponential function that is asymptotically exponential for small values and asymptotically power law for large values. This proves to be a useful property and makes it a good candidate distribution for many types of quantities. In this paper we focus on fitting historic daily stock returns for the FTSE 100 and the top 100 Nasdaq stocks. Using a Monte-Carlo goodness of fit test there is evidence that the $\kappa$-generalised distribution is a good fit for a significant proportion of the 200 stock returns analysed.</summary></entry><entry><title type="html">The reliability of the gender Implicit Association Test (gIAT) for high-ability careers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/ThereliabilityofthegenderImplicitAssociationTestgIATforhighabilitycareers.html" rel="alternate" type="text/html" title="The reliability of the gender Implicit Association Test (gIAT) for high-ability careers" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/ThereliabilityofthegenderImplicitAssociationTestgIATforhighabilitycareers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/ThereliabilityofthegenderImplicitAssociationTestgIATforhighabilitycareers.html">&lt;p&gt;Males outnumber females in many high-ability careers in the fields of science, technology, engineering, and mathematics, STEM, and academic medicine, to name a few. These differences are often attributed to subconscious bias as measured by the gender Implicit Association Test, gIAT. We compute p-value plots for results from two meta-analyses, one examines the predictive power of gIAT, and the other examines the predictive power of vocational interests, i.e. personal interests, and behaviors, for explaining gender differences in high-ability careers. The results are clear, the gender Implicit Association Test provides little or no information on male versus female differences, whereas vocational interests are strongly predictive. Researchers of implicit bias should expand their modeling to include additional relevant covariates. In short, these meta-analyses provide no support for the gender Implicit Association Test influencing choice and gender differences of high-ability careers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.10300&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>S. Stanley Young, Warren B. Kindzierski</name></author><category term="stat.AP" /><summary type="html">Males outnumber females in many high-ability careers in the fields of science, technology, engineering, and mathematics, STEM, and academic medicine, to name a few. These differences are often attributed to subconscious bias as measured by the gender Implicit Association Test, gIAT. We compute p-value plots for results from two meta-analyses, one examines the predictive power of gIAT, and the other examines the predictive power of vocational interests, i.e. personal interests, and behaviors, for explaining gender differences in high-ability careers. The results are clear, the gender Implicit Association Test provides little or no information on male versus female differences, whereas vocational interests are strongly predictive. Researchers of implicit bias should expand their modeling to include additional relevant covariates. In short, these meta-analyses provide no support for the gender Implicit Association Test influencing choice and gender differences of high-ability careers.</summary></entry><entry><title type="html">Trajecctory-Based Individualized Treatment Rules</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/TrajecctoryBasedIndividualizedTreatmentRules.html" rel="alternate" type="text/html" title="Trajecctory-Based Individualized Treatment Rules" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/TrajecctoryBasedIndividualizedTreatmentRules</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/TrajecctoryBasedIndividualizedTreatmentRules.html">&lt;p&gt;A core component of precision medicine research involves optimizing individualized treatment rules (ITRs) based on patient characteristics. Many studies used to estimate ITRs are longitudinal in nature, collecting outcomes over time. Yet, to date, methods developed to estimate ITRs often ignore the longitudinal structure of the data. Information available from the longitudinal nature of the data can be especially useful in mental health studies. Although treatment means might appear similar, understanding the trajectory of outcomes over time can reveal important differences between treatments and placebo effects. This longitudinal perspective is especially beneficial in mental health research, where subtle shifts in outcome patterns can hold significant implications. Despite numerous studies involving the collection of outcome data across various time points, most precision medicine methods used to develop ITRs overlook the information available from the longitudinal structure. The prevalence of missing data in such studies exacerbates the issue, as neglecting the longitudinal nature of the data can significantly impair the effectiveness of treatment rules. This paper develops a powerful longitudinal trajectory-based ITR construction method that incorporates baseline variables, via a single-index or biosignature, into the modeling of longitudinal outcomes. This trajectory-based ITR approach substantially minimizes the negative impact of missing data compared to more traditional ITR approaches. The approach is illustrated through simulation studies and a clinical trial for depression, contrasting it with more traditional ITRs that ignore longitudinal information.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09810&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lanqiu Yao, Thaddeus Tarpey</name></author><category term="stat.ME" /><summary type="html">A core component of precision medicine research involves optimizing individualized treatment rules (ITRs) based on patient characteristics. Many studies used to estimate ITRs are longitudinal in nature, collecting outcomes over time. Yet, to date, methods developed to estimate ITRs often ignore the longitudinal structure of the data. Information available from the longitudinal nature of the data can be especially useful in mental health studies. Although treatment means might appear similar, understanding the trajectory of outcomes over time can reveal important differences between treatments and placebo effects. This longitudinal perspective is especially beneficial in mental health research, where subtle shifts in outcome patterns can hold significant implications. Despite numerous studies involving the collection of outcome data across various time points, most precision medicine methods used to develop ITRs overlook the information available from the longitudinal structure. The prevalence of missing data in such studies exacerbates the issue, as neglecting the longitudinal nature of the data can significantly impair the effectiveness of treatment rules. This paper develops a powerful longitudinal trajectory-based ITR construction method that incorporates baseline variables, via a single-index or biosignature, into the modeling of longitudinal outcomes. This trajectory-based ITR approach substantially minimizes the negative impact of missing data compared to more traditional ITR approaches. The approach is illustrated through simulation studies and a clinical trial for depression, contrasting it with more traditional ITRs that ignore longitudinal information.</summary></entry><entry><title type="html">Treatment bootstrapping: A new approach to quantify uncertainty of average treatment effect estimates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/TreatmentbootstrappingAnewapproachtoquantifyuncertaintyofaveragetreatmenteffectestimates.html" rel="alternate" type="text/html" title="Treatment bootstrapping: A new approach to quantify uncertainty of average treatment effect estimates" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/TreatmentbootstrappingAnewapproachtoquantifyuncertaintyofaveragetreatmenteffectestimates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/TreatmentbootstrappingAnewapproachtoquantifyuncertaintyofaveragetreatmenteffectestimates.html">&lt;p&gt;This paper proposes a new non-parametric bootstrap method to quantify the uncertainty of average treatment effect estimate for the treated from matching estimators. More specifically, it seeks to quantify the uncertainty associated with the average treatment effect estimate for the treated by bootstrapping the treatment group only and finding the counterpart control group by pair matching on estimated propensity score without replacement. We demonstrate the validity of this approach and compare it with existing bootstrap approaches through Monte Carlo simulation and analysis of a real world data set. The results indicate that the proposed approach constructs confidence intervals and standard errors that have 95 percent or above coverage rate and better precision compared with existing bootstrap approaches, while these measures also depend on percent treated in the sample data and the sample size.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.11683&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jing Li</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">This paper proposes a new non-parametric bootstrap method to quantify the uncertainty of average treatment effect estimate for the treated from matching estimators. More specifically, it seeks to quantify the uncertainty associated with the average treatment effect estimate for the treated by bootstrapping the treatment group only and finding the counterpart control group by pair matching on estimated propensity score without replacement. We demonstrate the validity of this approach and compare it with existing bootstrap approaches through Monte Carlo simulation and analysis of a real world data set. The results indicate that the proposed approach constructs confidence intervals and standard errors that have 95 percent or above coverage rate and better precision compared with existing bootstrap approaches, while these measures also depend on percent treated in the sample data and the sample size.</summary></entry><entry><title type="html">Uniform Pessimistic Risk and Optimal Portfolio</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/UniformPessimisticRiskandOptimalPortfolio.html" rel="alternate" type="text/html" title="Uniform Pessimistic Risk and Optimal Portfolio" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/UniformPessimisticRiskandOptimalPortfolio</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/UniformPessimisticRiskandOptimalPortfolio.html">&lt;p&gt;The optimal allocation of assets has been widely discussed with the theoretical analysis of risk measures, and pessimism is one of the most attractive approaches beyond the conventional optimal portfolio model. The $\alpha$-risk plays a crucial role in deriving a broad class of pessimistic optimal portfolios. However, estimating an optimal portfolio assessed by a pessimistic risk is still challenging due to the absence of a computationally tractable model. In this study, we propose an integral of $\alpha$-risk called the \textit{uniform pessimistic risk} and the computational algorithm to obtain an optimal portfolio based on the risk. Further, we investigate the theoretical properties of the proposed risk in view of three different approaches: multiple quantile regression, the proper scoring rule, and distributionally robust optimization. Real data analysis of three stock datasets (S\&amp;amp;P500, CSI500, KOSPI200) demonstrates the usefulness of the proposed risk and portfolio model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.07158&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sungchul Hong, Jong-June Jeon</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">The optimal allocation of assets has been widely discussed with the theoretical analysis of risk measures, and pessimism is one of the most attractive approaches beyond the conventional optimal portfolio model. The $\alpha$-risk plays a crucial role in deriving a broad class of pessimistic optimal portfolios. However, estimating an optimal portfolio assessed by a pessimistic risk is still challenging due to the absence of a computationally tractable model. In this study, we propose an integral of $\alpha$-risk called the \textit{uniform pessimistic risk} and the computational algorithm to obtain an optimal portfolio based on the risk. Further, we investigate the theoretical properties of the proposed risk in view of three different approaches: multiple quantile regression, the proper scoring rule, and distributionally robust optimization. Real data analysis of three stock datasets (S\&amp;amp;P500, CSI500, KOSPI200) demonstrates the usefulness of the proposed risk and portfolio model.</summary></entry><entry><title type="html">When is Plasmode simulation superior to parametric simulation when estimating the MSE of the least squares estimator in linear regression?</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/WhenisPlasmodesimulationsuperiortoparametricsimulationwhenestimatingtheMSEoftheleastsquaresestimatorinlinearregression.html" rel="alternate" type="text/html" title="When is Plasmode simulation superior to parametric simulation when estimating the MSE of the least squares estimator in linear regression?" /><published>2024-05-17T00:00:00+00:00</published><updated>2024-05-17T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/WhenisPlasmodesimulationsuperiortoparametricsimulationwhenestimatingtheMSEoftheleastsquaresestimatorinlinearregression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/17/WhenisPlasmodesimulationsuperiortoparametricsimulationwhenestimatingtheMSEoftheleastsquaresestimatorinlinearregression.html">&lt;p&gt;Simulation is a crucial tool for the evaluation and comparison of statistical methods. How to design fair and neutral simulation studies is therefore of great interest for researchers developing new methods and practitioners confronted with the choice of the most suitable method. The term simulation usually refers to parametric simulation, that is, computer experiments using artificial data made up of pseudo-random numbers. Plasmode simulation, that is, computer experiments using the combination of resampling feature data from a real-life dataset and generating the target variable with a known user-selected outcome-generating model (OGM), is an alternative that is often claimed to produce more realistic data. We compare parametric and Plasmode simulation for the example of estimating the mean squared error (MSE) of the least squares estimator (LSE) in linear regression. If the true underlying data-generating process (DGP) and the OGM were known, parametric simulation would obviously be the best choice in terms of estimating the MSE well. However, in reality, both are usually unknown, so researchers have to make assumptions: in Plasmode simulation for the OGM, in parametric simulation for both DGP and OGM. Most likely, these assumptions do not exactly reflect the truth. Here, we aim to find out how assumptions deviating from the true DGP and the true OGM affect the performance of parametric and Plasmode simulations in the context of MSE estimation for the LSE and in which situations which simulation type is preferable. Our results suggest that the preferable simulation method depends on many factors, including the number of features, and on how and to what extent the assumptions of a parametric simulation differ from the true DGP. Also, the resampling strategy used for Plasmode influences the results. In particular, subsampling with a small sampling proportion can be recommended.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.04077&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marieke Stolte, Nicholas Schreck, Alla Slynko, Maral Saadati, Axel Benner, Jörg Rahnenführer, Andrea Bommert</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Simulation is a crucial tool for the evaluation and comparison of statistical methods. How to design fair and neutral simulation studies is therefore of great interest for researchers developing new methods and practitioners confronted with the choice of the most suitable method. The term simulation usually refers to parametric simulation, that is, computer experiments using artificial data made up of pseudo-random numbers. Plasmode simulation, that is, computer experiments using the combination of resampling feature data from a real-life dataset and generating the target variable with a known user-selected outcome-generating model (OGM), is an alternative that is often claimed to produce more realistic data. We compare parametric and Plasmode simulation for the example of estimating the mean squared error (MSE) of the least squares estimator (LSE) in linear regression. If the true underlying data-generating process (DGP) and the OGM were known, parametric simulation would obviously be the best choice in terms of estimating the MSE well. However, in reality, both are usually unknown, so researchers have to make assumptions: in Plasmode simulation for the OGM, in parametric simulation for both DGP and OGM. Most likely, these assumptions do not exactly reflect the truth. Here, we aim to find out how assumptions deviating from the true DGP and the true OGM affect the performance of parametric and Plasmode simulations in the context of MSE estimation for the LSE and in which situations which simulation type is preferable. Our results suggest that the preferable simulation method depends on many factors, including the number of features, and on how and to what extent the assumptions of a parametric simulation differ from the true DGP. Also, the resampling strategy used for Plasmode influences the results. In particular, subsampling with a small sampling proportion can be recommended.</summary></entry></feed>