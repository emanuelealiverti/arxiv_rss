<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-12T07:14:04+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Framework for Efficient Model Evaluation through Stratification, Sampling, and Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/AFrameworkforEfficientModelEvaluationthroughStratificationSamplingandEstimation.html" rel="alternate" type="text/html" title="A Framework for Efficient Model Evaluation through Stratification, Sampling, and Estimation" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/AFrameworkforEfficientModelEvaluationthroughStratificationSamplingandEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/AFrameworkforEfficientModelEvaluationthroughStratificationSamplingandEstimation.html">&lt;p&gt;Model performance evaluation is a critical and expensive task in machine learning and computer vision. Without clear guidelines, practitioners often estimate model accuracy using a one-time random selection of the data. However, by employing tailored sampling and estimation strategies, one can obtain more precise estimates and reduce annotation costs. In this paper, we propose a statistical framework for model evaluation that includes stratification, sampling, and estimation components. We examine the statistical properties of each component and evaluate their efficiency (precision). One key result of our work is that stratification via k-means clustering based on accurate predictions of model performance yields efficient estimators. Our experiments on computer vision datasets show that this method consistently provides more precise accuracy estimates than the traditional simple random sampling, even with substantial efficiency gains of 10x. We also find that model-assisted estimators, which leverage predictions of model accuracy on the unlabeled portion of the dataset, are generally more efficient than the traditional estimates based solely on the labeled data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07320&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Riccardo Fogliato, Pratik Patil, Mathew Monfort, Pietro Perona</name></author><category term="stat.AP" /><summary type="html">Model performance evaluation is a critical and expensive task in machine learning and computer vision. Without clear guidelines, practitioners often estimate model accuracy using a one-time random selection of the data. However, by employing tailored sampling and estimation strategies, one can obtain more precise estimates and reduce annotation costs. In this paper, we propose a statistical framework for model evaluation that includes stratification, sampling, and estimation components. We examine the statistical properties of each component and evaluate their efficiency (precision). One key result of our work is that stratification via k-means clustering based on accurate predictions of model performance yields efficient estimators. Our experiments on computer vision datasets show that this method consistently provides more precise accuracy estimates than the traditional simple random sampling, even with substantial efficiency gains of 10x. We also find that model-assisted estimators, which leverage predictions of model accuracy on the unlabeled portion of the dataset, are generally more efficient than the traditional estimates based solely on the labeled data.</summary></entry><entry><title type="html">A Novel Nonlinear Nonparametric Correlation Measurement With A Case Study on Surface Roughness in Finish Turning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ANovelNonlinearNonparametricCorrelationMeasurementWithACaseStudyonSurfaceRoughnessinFinishTurning.html" rel="alternate" type="text/html" title="A Novel Nonlinear Nonparametric Correlation Measurement With A Case Study on Surface Roughness in Finish Turning" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ANovelNonlinearNonparametricCorrelationMeasurementWithACaseStudyonSurfaceRoughnessinFinishTurning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ANovelNonlinearNonparametricCorrelationMeasurementWithACaseStudyonSurfaceRoughnessinFinishTurning.html">&lt;p&gt;Estimating the correlation coefficient has been a daunting work with the increasing complexity of dataset’s pattern. One of the problems in manufacturing applications consists of the estimation of a critical process variable during a machining operation from directly measurable process variables. For example, the prediction of surface roughness of a workpiece during finish turning processes. In this paper, we did exhaustive study on the existing popular correlation coefficients: Pearson correlation coefficient, Spearman’s rank correlation coefficient, Kendall’s Tau correlation coefficient, Fechner correlation coefficient, and Nonlinear correlation coefficient. However, no one of them can capture all the nonlinear and linear correlations. So, we represent a universal non-linear non-parametric correlation measurement, g-correlation coefficient. Unlike other correlation measurements, g-correlation doesn’t require assumptions and pick the dominating patterns of the dataset after examining all the major patterns no matter it is linear or nonlinear. Results of testing on both linearly correlated and non-linearly correlated dataset and comparison with the introduced correlation coefficients in literature show that g-correlation is robust on all the linearly correlated dataset and outperforms for some non-linearly correlated dataset. Results of the application of different correlation concepts to surface roughness assessment show that g-correlation has a central role among all standard concepts of correlation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06924&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ming Luo, Srinivasan Radhakrishnan, Sagar Kamarthi</name></author><category term="stat.ME" /><summary type="html">Estimating the correlation coefficient has been a daunting work with the increasing complexity of dataset’s pattern. One of the problems in manufacturing applications consists of the estimation of a critical process variable during a machining operation from directly measurable process variables. For example, the prediction of surface roughness of a workpiece during finish turning processes. In this paper, we did exhaustive study on the existing popular correlation coefficients: Pearson correlation coefficient, Spearman’s rank correlation coefficient, Kendall’s Tau correlation coefficient, Fechner correlation coefficient, and Nonlinear correlation coefficient. However, no one of them can capture all the nonlinear and linear correlations. So, we represent a universal non-linear non-parametric correlation measurement, g-correlation coefficient. Unlike other correlation measurements, g-correlation doesn’t require assumptions and pick the dominating patterns of the dataset after examining all the major patterns no matter it is linear or nonlinear. Results of testing on both linearly correlated and non-linearly correlated dataset and comparison with the introduced correlation coefficients in literature show that g-correlation is robust on all the linearly correlated dataset and outperforms for some non-linearly correlated dataset. Results of the application of different correlation concepts to surface roughness assessment show that g-correlation has a central role among all standard concepts of correlation.</summary></entry><entry><title type="html">Application of Black-Litterman Bayesian in Statistical Arbitrage</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ApplicationofBlackLittermanBayesianinStatisticalArbitrage.html" rel="alternate" type="text/html" title="Application of Black-Litterman Bayesian in Statistical Arbitrage" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ApplicationofBlackLittermanBayesianinStatisticalArbitrage</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ApplicationofBlackLittermanBayesianinStatisticalArbitrage.html">&lt;p&gt;\begin{abstract} In this paper, we integrated the statistical arbitrage strategy, pairs trading, into the Black-Litterman model and constructed efficient mean-variance portfolios. Typically, pairs trading underperforms under volatile or distressed market condition because the selected asset pairs fail to revert to equilibrium within the investment horizon. By enhancing this strategy with the Black-Litterman portfolio optimization, we achieved superior performance compared to the S\&amp;amp;P 500 market index under both normal and extreme market conditions. Furthermore, this research presents an innovative idea of incorporating traditional pairs trading strategies into the portfolio optimization framework in a scalable and systematic manner.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06706&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Qiqin Zhou</name></author><category term="stat.AP" /><summary type="html">\begin{abstract} In this paper, we integrated the statistical arbitrage strategy, pairs trading, into the Black-Litterman model and constructed efficient mean-variance portfolios. Typically, pairs trading underperforms under volatile or distressed market condition because the selected asset pairs fail to revert to equilibrium within the investment horizon. By enhancing this strategy with the Black-Litterman portfolio optimization, we achieved superior performance compared to the S\&amp;amp;P 500 market index under both normal and extreme market conditions. Furthermore, this research presents an innovative idea of incorporating traditional pairs trading strategies into the portfolio optimization framework in a scalable and systematic manner.</summary></entry><entry><title type="html">Bayesian Parametric Methods for Deriving Distribution of Restricted Mean Survival Time</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/BayesianParametricMethodsforDerivingDistributionofRestrictedMeanSurvivalTime.html" rel="alternate" type="text/html" title="Bayesian Parametric Methods for Deriving Distribution of Restricted Mean Survival Time" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/BayesianParametricMethodsforDerivingDistributionofRestrictedMeanSurvivalTime</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/BayesianParametricMethodsforDerivingDistributionofRestrictedMeanSurvivalTime.html">&lt;p&gt;We propose a Bayesian method for deriving the distribution of restricted mean survival time (RMST) using posterior samples, which accounts for covariates and heterogeneity among clusters based on a parametric model for survival time. We derive an explicit RMST equation by devising an integral of the survival function, allowing for the calculation of not only the mean and credible interval but also the mode, median, and probability of exceeding a certain value. Additionally, We propose two methods: one using random effects to account for heterogeneity among clusters and another utilizing frailty. We developed custom Stan code for the exponential, Weibull, log-normal frailty, and log-logistic models, as they cannot be processed using the brm functions in R. We evaluate our proposed methods through computer simulations and analyze real data from the eight Empowered Action Group states in India to confirm consistent results across states after adjusting for cluster differences. In conclusion, we derived explicit RMST formulas for parametric models and their distributions, enabling the calculation of the mean, median, mode, and credible interval. Our simulations confirmed the robustness of the proposed methods, and using the shrinkage effect allowed for more accurate results for each cluster.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06071&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Keisuke Hanada, Masahiro Kojima</name></author><category term="stat.ME" /><summary type="html">We propose a Bayesian method for deriving the distribution of restricted mean survival time (RMST) using posterior samples, which accounts for covariates and heterogeneity among clusters based on a parametric model for survival time. We derive an explicit RMST equation by devising an integral of the survival function, allowing for the calculation of not only the mean and credible interval but also the mode, median, and probability of exceeding a certain value. Additionally, We propose two methods: one using random effects to account for heterogeneity among clusters and another utilizing frailty. We developed custom Stan code for the exponential, Weibull, log-normal frailty, and log-logistic models, as they cannot be processed using the brm functions in R. We evaluate our proposed methods through computer simulations and analyze real data from the eight Empowered Action Group states in India to confirm consistent results across states after adjusting for cluster differences. In conclusion, we derived explicit RMST formulas for parametric models and their distributions, enabling the calculation of the mean, median, mode, and credible interval. Our simulations confirmed the robustness of the proposed methods, and using the shrinkage effect allowed for more accurate results for each cluster.</summary></entry><entry><title type="html">Binary De Bruijn Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/BinaryDeBruijnProcesses.html" rel="alternate" type="text/html" title="Binary De Bruijn Processes" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/BinaryDeBruijnProcesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/BinaryDeBruijnProcesses.html">&lt;p&gt;Binary time series data are very common in many applications, and are typically modelled independently via a Bernoulli process with a single probability of success. However, the probability of a success can be dependent on the outcome successes of past events. Presented here is a novel approach for modelling binary time series data called a binary de Bruijn process which takes into account temporal correlation. The structure is derived from de Bruijn Graphs - a directed graph, where given a set of symbols, V, and a ‘word’ length, m, the nodes of the graph consist of all possible sequences of V of length m. De Bruijn Graphs are equivalent to mth order Markov chains, where the ‘word’ length controls the number of states that each individual state is dependent on. This increases correlation over a wider area. To quantify how clustered a sequence generated from a de Bruijn process is, the run lengths of letters are observed along with run length properties. Inference is also presented along with two application examples: precipitation data and the Oxford and Cambridge boat race.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.16921&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Louise Kimpton, Peter Challenor, Henry Wynn</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Binary time series data are very common in many applications, and are typically modelled independently via a Bernoulli process with a single probability of success. However, the probability of a success can be dependent on the outcome successes of past events. Presented here is a novel approach for modelling binary time series data called a binary de Bruijn process which takes into account temporal correlation. The structure is derived from de Bruijn Graphs - a directed graph, where given a set of symbols, V, and a ‘word’ length, m, the nodes of the graph consist of all possible sequences of V of length m. De Bruijn Graphs are equivalent to mth order Markov chains, where the ‘word’ length controls the number of states that each individual state is dependent on. This increases correlation over a wider area. To quantify how clustered a sequence generated from a de Bruijn process is, the run lengths of letters are observed along with run length properties. Inference is also presented along with two application examples: precipitation data and the Oxford and Cambridge boat race.</summary></entry><entry><title type="html">Boosted Conformal Prediction Intervals</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/BoostedConformalPredictionIntervals.html" rel="alternate" type="text/html" title="Boosted Conformal Prediction Intervals" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/BoostedConformalPredictionIntervals</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/BoostedConformalPredictionIntervals.html">&lt;p&gt;This paper introduces a boosted conformal procedure designed to tailor conformalized prediction intervals toward specific desired properties, such as enhanced conditional coverage or reduced interval length. We employ machine learning techniques, notably gradient boosting, to systematically improve upon a predefined conformity score function. This process is guided by carefully constructed loss functions that measure the deviation of prediction intervals from the targeted properties. The procedure operates post-training, relying solely on model predictions and without modifying the trained model (e.g., the deep network). Systematic experiments demonstrate that starting from conventional conformal methods, our boosted procedure achieves substantial improvements in reducing interval length and decreasing deviation from target conditional coverage.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07449&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ran Xie, Rina Foygel Barber, Emmanuel J. Candès</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">This paper introduces a boosted conformal procedure designed to tailor conformalized prediction intervals toward specific desired properties, such as enhanced conditional coverage or reduced interval length. We employ machine learning techniques, notably gradient boosting, to systematically improve upon a predefined conformity score function. This process is guided by carefully constructed loss functions that measure the deviation of prediction intervals from the targeted properties. The procedure operates post-training, relying solely on model predictions and without modifying the trained model (e.g., the deep network). Systematic experiments demonstrate that starting from conventional conformal methods, our boosted procedure achieves substantial improvements in reducing interval length and decreasing deviation from target conditional coverage.</summary></entry><entry><title type="html">Calibrating Car-Following Models via Bayesian Dynamic Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/CalibratingCarFollowingModelsviaBayesianDynamicRegression.html" rel="alternate" type="text/html" title="Calibrating Car-Following Models via Bayesian Dynamic Regression" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/CalibratingCarFollowingModelsviaBayesianDynamicRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/CalibratingCarFollowingModelsviaBayesianDynamicRegression.html">&lt;p&gt;Car-following behavior modeling is critical for understanding traffic flow dynamics and developing high-fidelity microscopic simulation models. Most existing impulse-response car-following models prioritize computational efficiency and interpretability by using a parsimonious nonlinear function based on immediate preceding state observations. However, this approach disregards historical information, limiting its ability to explain real-world driving data. Consequently, serially correlated residuals are commonly observed when calibrating these models with actual trajectory data, hindering their ability to capture complex and stochastic phenomena. To address this limitation, we propose a dynamic regression framework incorporating time series models, such as autoregressive processes, to capture error dynamics. This statistically rigorous calibration outperforms the simple assumption of independent errors and enables more accurate simulation and prediction by leveraging higher-order historical information. We validate the effectiveness of our framework using HighD and OpenACC data, demonstrating improved probabilistic simulations. In summary, our framework preserves the parsimonious nature of traditional car-following models while offering enhanced probabilistic simulations. The code of this work is available at https://github.com/Chengyuan-Zhang/IDM_Bayesian_Calibration.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.03340&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chengyuan Zhang, Wenshuo Wang, Lijun Sun</name></author><category term="stat.AP" /><summary type="html">Car-following behavior modeling is critical for understanding traffic flow dynamics and developing high-fidelity microscopic simulation models. Most existing impulse-response car-following models prioritize computational efficiency and interpretability by using a parsimonious nonlinear function based on immediate preceding state observations. However, this approach disregards historical information, limiting its ability to explain real-world driving data. Consequently, serially correlated residuals are commonly observed when calibrating these models with actual trajectory data, hindering their ability to capture complex and stochastic phenomena. To address this limitation, we propose a dynamic regression framework incorporating time series models, such as autoregressive processes, to capture error dynamics. This statistically rigorous calibration outperforms the simple assumption of independent errors and enables more accurate simulation and prediction by leveraging higher-order historical information. We validate the effectiveness of our framework using HighD and OpenACC data, demonstrating improved probabilistic simulations. In summary, our framework preserves the parsimonious nature of traditional car-following models while offering enhanced probabilistic simulations. The code of this work is available at https://github.com/Chengyuan-Zhang/IDM_Bayesian_Calibration.</summary></entry><entry><title type="html">Causality for Complex Continuous-time Functional Longitudinal Studies with Dynamic Treatment Regimes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/CausalityforComplexContinuoustimeFunctionalLongitudinalStudieswithDynamicTreatmentRegimes.html" rel="alternate" type="text/html" title="Causality for Complex Continuous-time Functional Longitudinal Studies with Dynamic Treatment Regimes" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/CausalityforComplexContinuoustimeFunctionalLongitudinalStudieswithDynamicTreatmentRegimes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/CausalityforComplexContinuoustimeFunctionalLongitudinalStudieswithDynamicTreatmentRegimes.html">&lt;p&gt;Causal inference in longitudinal studies is often hampered by treatment-confounder feedback. Existing methods typically assume discrete time steps or step-like data changes, which we term &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;regular and irregular functional studies,&apos;&apos; limiting their applicability to studies with continuous monitoring data, like intensive care units or continuous glucose monitoring. These studies, which we formally term&lt;/code&gt;functional longitudinal studies,’’ require new approaches. Moreover, existing methods tailored for ``functional longitudinal studies’’ can only investigate static treatment regimes, which are independent of historical covariates or treatments, leading to either stringent parametric assumptions or strong positivity assumptions. This restriction has limited the range of causal questions these methods can answer and their practicality. We address these limitations by developing a nonparametric framework for functional longitudinal data, accommodating dynamic treatment regimes that depend on historical covariates or treatments, and may or may not depend on the actual treatment administered. To build intuition and explain our approach, we provide a comprehensive review of existing methods for regular and irregular longitudinal studies. We then formally define the potential outcomes and causal effects of interest, develop identification assumptions, and derive g-computation and inverse probability weighting formulas through novel applications of stochastic process and measure theory. Additionally, we compute the efficient influence curve using semiparametric theory. Our framework generalizes existing literature, and achieves double robustness under specific conditions. Finally, to aid interpretation, we provide sufficient and intuitive conditions for our identification assumptions, enhancing the applicability of our methodology to real-world scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06868&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrew Ying</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Causal inference in longitudinal studies is often hampered by treatment-confounder feedback. Existing methods typically assume discrete time steps or step-like data changes, which we term regular and irregular functional studies,&apos;&apos; limiting their applicability to studies with continuous monitoring data, like intensive care units or continuous glucose monitoring. These studies, which we formally termfunctional longitudinal studies,’’ require new approaches. Moreover, existing methods tailored for ``functional longitudinal studies’’ can only investigate static treatment regimes, which are independent of historical covariates or treatments, leading to either stringent parametric assumptions or strong positivity assumptions. This restriction has limited the range of causal questions these methods can answer and their practicality. We address these limitations by developing a nonparametric framework for functional longitudinal data, accommodating dynamic treatment regimes that depend on historical covariates or treatments, and may or may not depend on the actual treatment administered. To build intuition and explain our approach, we provide a comprehensive review of existing methods for regular and irregular longitudinal studies. We then formally define the potential outcomes and causal effects of interest, develop identification assumptions, and derive g-computation and inverse probability weighting formulas through novel applications of stochastic process and measure theory. Additionally, we compute the efficient influence curve using semiparametric theory. Our framework generalizes existing literature, and achieves double robustness under specific conditions. Finally, to aid interpretation, we provide sufficient and intuitive conditions for our identification assumptions, enhancing the applicability of our methodology to real-world scenarios.</summary></entry><entry><title type="html">Classical Myelo-Proliferative Neoplasms emergence and development based on real life incidence and mathematical modeling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ClassicalMyeloProliferativeNeoplasmsemergenceanddevelopmentbasedonreallifeincidenceandmathematicalmodeling.html" rel="alternate" type="text/html" title="Classical Myelo-Proliferative Neoplasms emergence and development based on real life incidence and mathematical modeling" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ClassicalMyeloProliferativeNeoplasmsemergenceanddevelopmentbasedonreallifeincidenceandmathematicalmodeling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ClassicalMyeloProliferativeNeoplasmsemergenceanddevelopmentbasedonreallifeincidenceandmathematicalmodeling.html">&lt;p&gt;Mathematical modeling offers the opportunity to test hypothesis concerning Myeloproliferative emergence and development. We tested different mathematical models based on a training cohort (n=264 patients) (Registre de la c\^ote d’Or) to determine the emergence and evolution times before JAK2V617F classical Myeloproliferative disorders (respectively Polycythemia Vera and Essential Thrombocytemia) are diagnosed. We dissected the time before diagnosis as two main periods: the time from embryonic development for the JAK2V617F mutation to occur, not disappear and enter in proliferation, and a second time corresponding to the expansion of the clonal population until diagnosis. We demonstrate using progressively complexified models that the rate of active mutation occurrence is not constant and doesn’t just rely on individual variability, but rather increases with age and takes a median time of 63.1+/-13 years. A contrario, the expansion time can be considered as constant: 8.8 years once the mutation has emerged. Results were validated in an external cohort (national FIMBANK Cohort, n=1248 patients). Analyzing JAK2V617F Essential Thrombocytema versus Polycythemia Vera, we noticed that the first period of time (rate of active homozygous mutation occurrence) for PV takes approximatively 1.5 years more than for ET to develop when the expansion time was quasi-similar. In conclusion, our multi-step approach and the ultimate time-dependent model of MPN emergence and development demonstrates that the emergence of a JAK2V617F mutation should be linked to an aging mechanism, and indicates a 8-9 years period of time to develop a full MPN.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06765&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ana Fernández Baranda, Vincent Bansaye, Evelyne Lauret, Morgane Mounier, Valérie Ugo, Sylvie Méléard, Stéphane Giraudier</name></author><category term="stat.AP" /><summary type="html">Mathematical modeling offers the opportunity to test hypothesis concerning Myeloproliferative emergence and development. We tested different mathematical models based on a training cohort (n=264 patients) (Registre de la c\^ote d’Or) to determine the emergence and evolution times before JAK2V617F classical Myeloproliferative disorders (respectively Polycythemia Vera and Essential Thrombocytemia) are diagnosed. We dissected the time before diagnosis as two main periods: the time from embryonic development for the JAK2V617F mutation to occur, not disappear and enter in proliferation, and a second time corresponding to the expansion of the clonal population until diagnosis. We demonstrate using progressively complexified models that the rate of active mutation occurrence is not constant and doesn’t just rely on individual variability, but rather increases with age and takes a median time of 63.1+/-13 years. A contrario, the expansion time can be considered as constant: 8.8 years once the mutation has emerged. Results were validated in an external cohort (national FIMBANK Cohort, n=1248 patients). Analyzing JAK2V617F Essential Thrombocytema versus Polycythemia Vera, we noticed that the first period of time (rate of active homozygous mutation occurrence) for PV takes approximatively 1.5 years more than for ET to develop when the expansion time was quasi-similar. In conclusion, our multi-step approach and the ultimate time-dependent model of MPN emergence and development demonstrates that the emergence of a JAK2V617F mutation should be linked to an aging mechanism, and indicates a 8-9 years period of time to develop a full MPN.</summary></entry><entry><title type="html">Controlling Counterfactual Harm in Decision Support Systems Based on Prediction Sets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ControllingCounterfactualHarminDecisionSupportSystemsBasedonPredictionSets.html" rel="alternate" type="text/html" title="Controlling Counterfactual Harm in Decision Support Systems Based on Prediction Sets" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ControllingCounterfactualHarminDecisionSupportSystemsBasedonPredictionSets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ControllingCounterfactualHarminDecisionSupportSystemsBasedonPredictionSets.html">&lt;p&gt;Decision support systems based on prediction sets help humans solve multiclass classification tasks by narrowing down the set of potential label values to a subset of them, namely a prediction set, and asking them to always predict label values from the prediction sets. While this type of systems have been proven to be effective at improving the average accuracy of the predictions made by humans, by restricting human agency, they may cause harm$\unicode{x2014}$a human who has succeeded at predicting the ground-truth label of an instance on their own may have failed had they used these systems. In this paper, our goal is to control how frequently a decision support system based on prediction sets may cause harm, by design. To this end, we start by characterizing the above notion of harm using the theoretical framework of structural causal models. Then, we show that, under a natural, albeit unverifiable, monotonicity assumption, we can estimate how frequently a system may cause harm using only predictions made by humans on their own. Further, we also show that, under a weaker monotonicity assumption, which can be verified experimentally, we can bound how frequently a system may cause harm again using only predictions made by humans on their own. Building upon these assumptions, we introduce a computational framework to design decision support systems based on prediction sets that are guaranteed to cause harm less frequently than a user-specified value using conformal risk control. We validate our framework using real human predictions from two different human subject studies and show that, in decision support systems based on prediction sets, there is a trade-off between accuracy and counterfactual harm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06671&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Eleni Straitouri, Suhas Thejaswi, Manuel Gomez Rodriguez</name></author><category term="stat.ME" /><summary type="html">Decision support systems based on prediction sets help humans solve multiclass classification tasks by narrowing down the set of potential label values to a subset of them, namely a prediction set, and asking them to always predict label values from the prediction sets. While this type of systems have been proven to be effective at improving the average accuracy of the predictions made by humans, by restricting human agency, they may cause harm$\unicode{x2014}$a human who has succeeded at predicting the ground-truth label of an instance on their own may have failed had they used these systems. In this paper, our goal is to control how frequently a decision support system based on prediction sets may cause harm, by design. To this end, we start by characterizing the above notion of harm using the theoretical framework of structural causal models. Then, we show that, under a natural, albeit unverifiable, monotonicity assumption, we can estimate how frequently a system may cause harm using only predictions made by humans on their own. Further, we also show that, under a weaker monotonicity assumption, which can be verified experimentally, we can bound how frequently a system may cause harm again using only predictions made by humans on their own. Building upon these assumptions, we introduce a computational framework to design decision support systems based on prediction sets that are guaranteed to cause harm less frequently than a user-specified value using conformal risk control. We validate our framework using real human predictions from two different human subject studies and show that, in decision support systems based on prediction sets, there is a trade-off between accuracy and counterfactual harm.</summary></entry><entry><title type="html">Convergence rate of random scan Coordinate Ascent Variational Inference under log-concavity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ConvergencerateofrandomscanCoordinateAscentVariationalInferenceunderlogconcavity.html" rel="alternate" type="text/html" title="Convergence rate of random scan Coordinate Ascent Variational Inference under log-concavity" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ConvergencerateofrandomscanCoordinateAscentVariationalInferenceunderlogconcavity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ConvergencerateofrandomscanCoordinateAscentVariationalInferenceunderlogconcavity.html">&lt;p&gt;The Coordinate Ascent Variational Inference scheme is a popular algorithm used to compute the mean-field approximation of a probability distribution of interest. We analyze its random scan version, under log-concavity assumptions on the target density. Our approach builds on the recent work of M. Arnese and D. Lacker, \emph{Convergence of coordinate ascent variational inference for log-concave measures via optimal transport} [arXiv:2404.08792] which studies the deterministic scan version of the algorithm, phrasing it as a block-coordinate descent algorithm in the space of probability distributions endowed with the geometry of optimal transport. We obtain tight rates for the random scan version, which imply that the total number of factor updates required to converge scales linearly with the condition number and the number of blocks of the target distribution. By contrast, available bounds for the deterministic scan case scale quadratically in the same quantities, which is analogue to what happens for optimization of convex functions in Euclidean spaces.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07292&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hugo Lavenant, Giacomo Zanella</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">The Coordinate Ascent Variational Inference scheme is a popular algorithm used to compute the mean-field approximation of a probability distribution of interest. We analyze its random scan version, under log-concavity assumptions on the target density. Our approach builds on the recent work of M. Arnese and D. Lacker, \emph{Convergence of coordinate ascent variational inference for log-concave measures via optimal transport} [arXiv:2404.08792] which studies the deterministic scan version of the algorithm, phrasing it as a block-coordinate descent algorithm in the space of probability distributions endowed with the geometry of optimal transport. We obtain tight rates for the random scan version, which imply that the total number of factor updates required to converge scales linearly with the condition number and the number of blocks of the target distribution. By contrast, available bounds for the deterministic scan case scale quadratically in the same quantities, which is analogue to what happens for optimization of convex functions in Euclidean spaces.</summary></entry><entry><title type="html">Data-Driven Switchback Experiments: Theoretical Tradeoffs and Empirical Bayes Designs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/DataDrivenSwitchbackExperimentsTheoreticalTradeoffsandEmpiricalBayesDesigns.html" rel="alternate" type="text/html" title="Data-Driven Switchback Experiments: Theoretical Tradeoffs and Empirical Bayes Designs" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/DataDrivenSwitchbackExperimentsTheoreticalTradeoffsandEmpiricalBayesDesigns</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/DataDrivenSwitchbackExperimentsTheoreticalTradeoffsandEmpiricalBayesDesigns.html">&lt;p&gt;We study the design and analysis of switchback experiments conducted on a single aggregate unit. The design problem is to partition the continuous time space into intervals and switch treatments between intervals, in order to minimize the estimation error of the treatment effect. We show that the estimation error depends on four factors: carryover effects, periodicity, serially correlated outcomes, and impacts from simultaneous experiments. We derive a rigorous bias-variance decomposition and show the tradeoffs of the estimation error from these factors. The decomposition provides three new insights in choosing a design: First, balancing the periodicity between treated and control intervals reduces the variance; second, switching less frequently reduces the bias from carryover effects while increasing the variance from correlated outcomes, and vice versa; third, randomizing interval start and end points reduces both bias and variance from simultaneous experiments. Combining these insights, we propose a new empirical Bayes design approach. This approach uses prior data and experiments for designing future experiments. We illustrate this approach using real data from a ride-sharing platform, yielding a design that reduces MSE by 33% compared to the status quo design used on the platform.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06768&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ruoxuan Xiong, Alex Chin, Sean J. Taylor</name></author><category term="stat.ME" /><summary type="html">We study the design and analysis of switchback experiments conducted on a single aggregate unit. The design problem is to partition the continuous time space into intervals and switch treatments between intervals, in order to minimize the estimation error of the treatment effect. We show that the estimation error depends on four factors: carryover effects, periodicity, serially correlated outcomes, and impacts from simultaneous experiments. We derive a rigorous bias-variance decomposition and show the tradeoffs of the estimation error from these factors. The decomposition provides three new insights in choosing a design: First, balancing the periodicity between treated and control intervals reduces the variance; second, switching less frequently reduces the bias from carryover effects while increasing the variance from correlated outcomes, and vice versa; third, randomizing interval start and end points reduces both bias and variance from simultaneous experiments. Combining these insights, we propose a new empirical Bayes design approach. This approach uses prior data and experiments for designing future experiments. We illustrate this approach using real data from a ride-sharing platform, yielding a design that reduces MSE by 33% compared to the status quo design used on the platform.</summary></entry><entry><title type="html">Data-driven Power Flow Linearization: Simulation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/DatadrivenPowerFlowLinearizationSimulation.html" rel="alternate" type="text/html" title="Data-driven Power Flow Linearization: Simulation" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/DatadrivenPowerFlowLinearizationSimulation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/DatadrivenPowerFlowLinearizationSimulation.html">&lt;p&gt;Building on the theoretical insights of Part I, this paper, as the second part of the tutorial, dives deeper into data-driven power flow linearization (DPFL), focusing on comprehensive numerical testing. The necessity of these simulations stems from the theoretical analysis’s inherent limitations, particularly the challenge of identifying the differences in real-world performance among DPFL methods with overlapping theoretical capabilities and/or limitations. The absence of a comprehensive numerical comparison of DPFL approaches in the literature also motivates this paper, especially given the fact that over 95% of existing DPFL studies have not provided any open-source codes. To bridge the gap, this paper first reviews existing DPFL experiments, examining the adopted test scenarios, load fluctuation settings, data sources, considerations for data noise/outliers, and the comparison made so far. Subsequently, this paper evaluates a total of 44 methods, containing over 30 existing DPFL approaches, some innovative DPFL techniques, and several classic physics-driven power flow linearization methods for benchmarking. The evaluation spans various dimensions, including generalizability, applicability, accuracy, and computational efficiency, using various different test cases scaling from 9-bus to 1354-bus systems. The numerical analysis identifies and examines significant trends and consistent findings across all methods under various test cases. It also offers theoretical insights into phenomena like under-performance, failure, excessive computation times, etc. Overall, this paper identifies the differences in the performances of the wide range of DPFL methods, reveals gaps not evident from theoretical discussions, assists in method selection for real-world applications, and provides thorough discussions on open questions within DPFL research, indicating ten potential future directions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06833&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mengshuo Jia, Gabriela Hug, Ning Zhang, Zhaojian Wang, Yi Wang, Chongqing Kang</name></author><category term="stat.AP" /><summary type="html">Building on the theoretical insights of Part I, this paper, as the second part of the tutorial, dives deeper into data-driven power flow linearization (DPFL), focusing on comprehensive numerical testing. The necessity of these simulations stems from the theoretical analysis’s inherent limitations, particularly the challenge of identifying the differences in real-world performance among DPFL methods with overlapping theoretical capabilities and/or limitations. The absence of a comprehensive numerical comparison of DPFL approaches in the literature also motivates this paper, especially given the fact that over 95% of existing DPFL studies have not provided any open-source codes. To bridge the gap, this paper first reviews existing DPFL experiments, examining the adopted test scenarios, load fluctuation settings, data sources, considerations for data noise/outliers, and the comparison made so far. Subsequently, this paper evaluates a total of 44 methods, containing over 30 existing DPFL approaches, some innovative DPFL techniques, and several classic physics-driven power flow linearization methods for benchmarking. The evaluation spans various dimensions, including generalizability, applicability, accuracy, and computational efficiency, using various different test cases scaling from 9-bus to 1354-bus systems. The numerical analysis identifies and examines significant trends and consistent findings across all methods under various test cases. It also offers theoretical insights into phenomena like under-performance, failure, excessive computation times, etc. Overall, this paper identifies the differences in the performances of the wide range of DPFL methods, reveals gaps not evident from theoretical discussions, assists in method selection for real-world applications, and provides thorough discussions on open questions within DPFL research, indicating ten potential future directions.</summary></entry><entry><title type="html">Efficient combination of observational and experimental datasets under general restrictions on outcome mean functions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Efficientcombinationofobservationalandexperimentaldatasetsundergeneralrestrictionsonoutcomemeanfunctions.html" rel="alternate" type="text/html" title="Efficient combination of observational and experimental datasets under general restrictions on outcome mean functions" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Efficientcombinationofobservationalandexperimentaldatasetsundergeneralrestrictionsonoutcomemeanfunctions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Efficientcombinationofobservationalandexperimentaldatasetsundergeneralrestrictionsonoutcomemeanfunctions.html">&lt;p&gt;A researcher collecting data from a randomized controlled trial (RCT) often has access to an auxiliary observational dataset that may be confounded or otherwise biased for estimating causal effects. Common modeling assumptions impose restrictions on the outcome mean function - the conditional expectation of the outcome of interest given observed covariates - in the two datasets. Running examples from the literature include settings where the observational dataset is subject to outcome-mediated selection bias or to confounding bias taking an assumed parametric form. We propose a succinct framework to derive the efficient influence function for any identifiable pathwise differentiable estimand under a general class of restrictions on the outcome mean function. This uncovers surprising results that with homoskedastic outcomes and a constant propensity score in the RCT, even strong parametric assumptions cannot improve the semiparametric lower bound for estimating various average treatment effects. We then leverage double machine learning to construct a one-step estimator that achieves the semiparametric efficiency bound even in cases when the outcome mean function and other nuisance parameters are estimated nonparametrically. The goal is to empower a researcher with custom, previously unstudied modeling restrictions on the outcome mean function to systematically construct causal estimators that maximially leverage their assumptions for variance reduction. We demonstrate the finite sample precision gains of our estimator over existing approaches in extensions of various numerical studies and data examples from the literature.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06941&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Harrison H. Li</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">A researcher collecting data from a randomized controlled trial (RCT) often has access to an auxiliary observational dataset that may be confounded or otherwise biased for estimating causal effects. Common modeling assumptions impose restrictions on the outcome mean function - the conditional expectation of the outcome of interest given observed covariates - in the two datasets. Running examples from the literature include settings where the observational dataset is subject to outcome-mediated selection bias or to confounding bias taking an assumed parametric form. We propose a succinct framework to derive the efficient influence function for any identifiable pathwise differentiable estimand under a general class of restrictions on the outcome mean function. This uncovers surprising results that with homoskedastic outcomes and a constant propensity score in the RCT, even strong parametric assumptions cannot improve the semiparametric lower bound for estimating various average treatment effects. We then leverage double machine learning to construct a one-step estimator that achieves the semiparametric efficiency bound even in cases when the outcome mean function and other nuisance parameters are estimated nonparametrically. The goal is to empower a researcher with custom, previously unstudied modeling restrictions on the outcome mean function to systematically construct causal estimators that maximially leverage their assumptions for variance reduction. We demonstrate the finite sample precision gains of our estimator over existing approaches in extensions of various numerical studies and data examples from the literature.</summary></entry><entry><title type="html">Estimating Gaussian mixtures using sparse polynomial moment systems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/EstimatingGaussianmixturesusingsparsepolynomialmomentsystems.html" rel="alternate" type="text/html" title="Estimating Gaussian mixtures using sparse polynomial moment systems" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/EstimatingGaussianmixturesusingsparsepolynomialmomentsystems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/EstimatingGaussianmixturesusingsparsepolynomialmomentsystems.html">&lt;p&gt;The method of moments is a classical statistical technique for density estimation that solves a system of moment equations to estimate the parameters of an unknown distribution. A fundamental question critical to understanding identifiability asks how many moment equations are needed to get finitely many solutions and how many solutions there are. We answer this question for classes of Gaussian mixture models using the tools of polyhedral geometry. In addition, we show that a generic Gaussian $k$-mixture model is identifiable from its first $3k+2$ moments. Using these results, we present a homotopy algorithm that performs parameter recovery for high dimensional Gaussian mixture models where the number of paths tracked scales linearly in the dimension.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.15675&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Julia Lindberg, Carlos Améndola, Jose Israel Rodriguez</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">The method of moments is a classical statistical technique for density estimation that solves a system of moment equations to estimate the parameters of an unknown distribution. A fundamental question critical to understanding identifiability asks how many moment equations are needed to get finitely many solutions and how many solutions there are. We answer this question for classes of Gaussian mixture models using the tools of polyhedral geometry. In addition, we show that a generic Gaussian $k$-mixture model is identifiable from its first $3k+2$ moments. Using these results, we present a homotopy algorithm that performs parameter recovery for high dimensional Gaussian mixture models where the number of paths tracked scales linearly in the dimension.</summary></entry><entry><title type="html">Hybrid$^2$ Neural ODE Causal Modeling and an Application to Glycemic Response</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Hybrid2NeuralODECausalModelingandanApplicationtoGlycemicResponse.html" rel="alternate" type="text/html" title="Hybrid$^2$ Neural ODE Causal Modeling and an Application to Glycemic Response" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Hybrid2NeuralODECausalModelingandanApplicationtoGlycemicResponse</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Hybrid2NeuralODECausalModelingandanApplicationtoGlycemicResponse.html">&lt;p&gt;Hybrid models composing mechanistic ODE-based dynamics with flexible and expressive neural network components have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning). The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. Unfortunately, as the hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost. We address this problem by leveraging another common source of domain knowledge: \emph{ranking} of treatment effects for a set of interventions, even if the precise treatment effect is unknown. We encode this information in a \emph{causal loss} that we combine with the standard predictive loss to arrive at a \emph{hybrid loss} that biases our learning towards causally valid hybrid models. We demonstrate our ability to achieve a win-win, state-of-the-art predictive performance \emph{and} causal validity, in the challenging task of modeling glucose dynamics post-exercise in individuals with type 1 diabetes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.17233&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bob Junyi Zou, Matthew E. Levine, Dessi P. Zaharieva, Ramesh Johari, Emily B. Fox</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Hybrid models composing mechanistic ODE-based dynamics with flexible and expressive neural network components have grown rapidly in popularity, especially in scientific domains where such ODE-based modeling offers important interpretability and validated causal grounding (e.g., for counterfactual reasoning). The incorporation of mechanistic models also provides inductive bias in standard blackbox modeling approaches, critical when learning from small datasets or partially observed, complex systems. Unfortunately, as the hybrid models become more flexible, the causal grounding provided by the mechanistic model can quickly be lost. We address this problem by leveraging another common source of domain knowledge: \emph{ranking} of treatment effects for a set of interventions, even if the precise treatment effect is unknown. We encode this information in a \emph{causal loss} that we combine with the standard predictive loss to arrive at a \emph{hybrid loss} that biases our learning towards causally valid hybrid models. We demonstrate our ability to achieve a win-win, state-of-the-art predictive performance \emph{and} causal validity, in the challenging task of modeling glucose dynamics post-exercise in individuals with type 1 diabetes.</summary></entry><entry><title type="html">Integrating Marketing Channels into Quantile Transformation and Bayesian Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/IntegratingMarketingChannelsintoQuantileTransformationandBayesianOptimizationofEnsembleKernelsforSalesPredictionwithGaussianProcessModels.html" rel="alternate" type="text/html" title="Integrating Marketing Channels into Quantile Transformation and Bayesian Optimization of Ensemble Kernels for Sales Prediction with Gaussian Process Models" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/IntegratingMarketingChannelsintoQuantileTransformationandBayesianOptimizationofEnsembleKernelsforSalesPredictionwithGaussianProcessModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/IntegratingMarketingChannelsintoQuantileTransformationandBayesianOptimizationofEnsembleKernelsforSalesPredictionwithGaussianProcessModels.html">&lt;p&gt;This study introduces an innovative Gaussian Process (GP) model utilizing an ensemble kernel that integrates Radial Basis Function (RBF), Rational Quadratic, and Mat&apos;ern kernels for product sales forecasting. By applying Bayesian optimization, we efficiently find the optimal weights for each kernel, enhancing the model’s ability to handle complex sales data patterns. Our approach significantly outperforms traditional GP models, achieving a notable 98\% accuracy and superior performance across key metrics including Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination ($R^2$). This advancement underscores the effectiveness of ensemble kernels and Bayesian optimization in improving predictive accuracy, offering profound implications for machine learning applications in sales forecasting.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.09386&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shahin Mirshekari, Negin Hayeri Motedayen, Mohammad Ensaf</name></author><category term="stat.AP" /><summary type="html">This study introduces an innovative Gaussian Process (GP) model utilizing an ensemble kernel that integrates Radial Basis Function (RBF), Rational Quadratic, and Mat&apos;ern kernels for product sales forecasting. By applying Bayesian optimization, we efficiently find the optimal weights for each kernel, enhancing the model’s ability to handle complex sales data patterns. Our approach significantly outperforms traditional GP models, achieving a notable 98\% accuracy and superior performance across key metrics including Mean Squared Error (MSE), Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Coefficient of Determination ($R^2$). This advancement underscores the effectiveness of ensemble kernels and Bayesian optimization in improving predictive accuracy, offering profound implications for machine learning applications in sales forecasting.</summary></entry><entry><title type="html">Modeling of New Energy Vehicles’ Impact on Urban Ecology Focusing on Behavior</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ModelingofNewEnergyVehiclesImpactonUrbanEcologyFocusingonBehavior.html" rel="alternate" type="text/html" title="Modeling of New Energy Vehicles’ Impact on Urban Ecology Focusing on Behavior" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ModelingofNewEnergyVehiclesImpactonUrbanEcologyFocusingonBehavior</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ModelingofNewEnergyVehiclesImpactonUrbanEcologyFocusingonBehavior.html">&lt;p&gt;The surging demand for new energy vehicles is driven by the imperative to conserve energy, reduce emissions, and enhance the ecological ambiance. By conducting behavioral analysis and mining usage patterns of new energy vehicles, particular patterns can be identified. For instance, overloading the battery, operating with low battery power, and driving at excessive speeds can all detrimentally affect the battery’s performance. To assess the impact of such driving behavior on the urban ecology, an environmental computational modeling method has been proposed to simulate the interaction between new energy vehicles and the environment. To extend the time series data of the vehicle’s entire life cycle and the ecological environment within the model sequence data, the LSTM model with Bayesian optimizer is utilized for simulation. The analysis revealed the detrimental effects of poor driving behavior on the environment.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06602&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Run-Xuan Tang</name></author><category term="stat.AP" /><summary type="html">The surging demand for new energy vehicles is driven by the imperative to conserve energy, reduce emissions, and enhance the ecological ambiance. By conducting behavioral analysis and mining usage patterns of new energy vehicles, particular patterns can be identified. For instance, overloading the battery, operating with low battery power, and driving at excessive speeds can all detrimentally affect the battery’s performance. To assess the impact of such driving behavior on the urban ecology, an environmental computational modeling method has been proposed to simulate the interaction between new energy vehicles and the environment. To extend the time series data of the vehicle’s entire life cycle and the ecological environment within the model sequence data, the LSTM model with Bayesian optimizer is utilized for simulation. The analysis revealed the detrimental effects of poor driving behavior on the environment.</summary></entry><entry><title type="html">New density/likelihood representations for Gibbs models based on generating functionals of point processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/NewdensitylikelihoodrepresentationsforGibbsmodelsbasedongeneratingfunctionalsofpointprocesses.html" rel="alternate" type="text/html" title="New density/likelihood representations for Gibbs models based on generating functionals of point processes" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/NewdensitylikelihoodrepresentationsforGibbsmodelsbasedongeneratingfunctionalsofpointprocesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/NewdensitylikelihoodrepresentationsforGibbsmodelsbasedongeneratingfunctionalsofpointprocesses.html">&lt;p&gt;Deriving exact density functions for Gibbs point processes has been challenging due to their general intractability, stemming from the intractability of their normalising constants/partition functions. This paper offers a solution to this open problem by exploiting a recent alternative representation of point process densities. Here, for a finite point process, the density is expressed as the void probability multiplied by a higher-order Papangelou conditional intensity function. By leveraging recent results on dependent thinnings, exact expressions for generating functionals and void probabilities of locally stable point processes are derived. Consequently, exact expressions for density/likelihood functions, partition functions and posterior densities are also obtained. The paper finally extends the results to locally stable Gibbsian random fields on lattices by representing them as point processes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07075&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ottmar Cronie</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Deriving exact density functions for Gibbs point processes has been challenging due to their general intractability, stemming from the intractability of their normalising constants/partition functions. This paper offers a solution to this open problem by exploiting a recent alternative representation of point process densities. Here, for a finite point process, the density is expressed as the void probability multiplied by a higher-order Papangelou conditional intensity function. By leveraging recent results on dependent thinnings, exact expressions for generating functionals and void probabilities of locally stable point processes are derived. Consequently, exact expressions for density/likelihood functions, partition functions and posterior densities are also obtained. The paper finally extends the results to locally stable Gibbsian random fields on lattices by representing them as point processes.</summary></entry><entry><title type="html">Optimization-based Causal Estimation from Heterogenous Environments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/OptimizationbasedCausalEstimationfromHeterogenousEnvironments.html" rel="alternate" type="text/html" title="Optimization-based Causal Estimation from Heterogenous Environments" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/OptimizationbasedCausalEstimationfromHeterogenousEnvironments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/OptimizationbasedCausalEstimationfromHeterogenousEnvironments.html">&lt;p&gt;This paper presents a new optimization approach to causal estimation. Given data that contains covariates and an outcome, which covariates are causes of the outcome, and what is the strength of the causality? In classical machine learning (ML), the goal of optimization is to maximize predictive accuracy. However, some covariates might exhibit a non-causal association with the outcome. Such spurious associations provide predictive power for classical ML, but they prevent us from causally interpreting the result. This paper proposes CoCo, an optimization algorithm that bridges the gap between pure prediction and causal inference. CoCo leverages the recently-proposed idea of environments, datasets of covariates/response where the causal relationships remain invariant but where the distribution of the covariates changes from environment to environment. Given datasets from multiple environments-and ones that exhibit sufficient heterogeneity-CoCo maximizes an objective for which the only solution is the causal solution. We describe the theoretical foundations of this approach and demonstrate its effectiveness on simulated and real datasets. Compared to classical ML and existing methods, CoCo provides more accurate estimates of the causal model and more accurate predictions under interventions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2109.11990&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mingzhang Yin, Yixin Wang, David M. Blei</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">This paper presents a new optimization approach to causal estimation. Given data that contains covariates and an outcome, which covariates are causes of the outcome, and what is the strength of the causality? In classical machine learning (ML), the goal of optimization is to maximize predictive accuracy. However, some covariates might exhibit a non-causal association with the outcome. Such spurious associations provide predictive power for classical ML, but they prevent us from causally interpreting the result. This paper proposes CoCo, an optimization algorithm that bridges the gap between pure prediction and causal inference. CoCo leverages the recently-proposed idea of environments, datasets of covariates/response where the causal relationships remain invariant but where the distribution of the covariates changes from environment to environment. Given datasets from multiple environments-and ones that exhibit sufficient heterogeneity-CoCo maximizes an objective for which the only solution is the causal solution. We describe the theoretical foundations of this approach and demonstrate its effectiveness on simulated and real datasets. Compared to classical ML and existing methods, CoCo provides more accurate estimates of the causal model and more accurate predictions under interventions.</summary></entry><entry><title type="html">Power Analysis for Experiments with Clustered Data, Ratio Metrics, and Regression for Covariate Adjustment</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/PowerAnalysisforExperimentswithClusteredDataRatioMetricsandRegressionforCovariateAdjustment.html" rel="alternate" type="text/html" title="Power Analysis for Experiments with Clustered Data, Ratio Metrics, and Regression for Covariate Adjustment" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/PowerAnalysisforExperimentswithClusteredDataRatioMetricsandRegressionforCovariateAdjustment</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/PowerAnalysisforExperimentswithClusteredDataRatioMetricsandRegressionforCovariateAdjustment.html">&lt;p&gt;We describe how to calculate standard errors for A/B tests that include clustered data, ratio metrics, and/or covariate adjustment. We may do this for power analysis/sample size calculations prior to running an experiment using historical data, or after an experiment for hypothesis testing and confidence intervals. The different applications have a common framework, using the sample variance of certain residuals. The framework is compatible with modular software, can be plugged into standard tools, doesn’t require computing covariance matrices, and is numerically stable. Using this approach we estimate that covariate adjustment gives a median 66% variance reduction for a key metric, reducing experiment run time by 66%.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06834&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tim Hesterberg , Ben Knight</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We describe how to calculate standard errors for A/B tests that include clustered data, ratio metrics, and/or covariate adjustment. We may do this for power analysis/sample size calculations prior to running an experiment using historical data, or after an experiment for hypothesis testing and confidence intervals. The different applications have a common framework, using the sample variance of certain residuals. The framework is compatible with modular software, can be plugged into standard tools, doesn’t require computing covariance matrices, and is numerically stable. Using this approach we estimate that covariate adjustment gives a median 66% variance reduction for a key metric, reducing experiment run time by 66%.</summary></entry><entry><title type="html">Robust Inverse Graphics via Probabilistic Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/RobustInverseGraphicsviaProbabilisticInference.html" rel="alternate" type="text/html" title="Robust Inverse Graphics via Probabilistic Inference" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/RobustInverseGraphicsviaProbabilisticInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/RobustInverseGraphicsviaProbabilisticInference.html">&lt;p&gt;How do we infer a 3D scene from a single image in the presence of corruptions like rain, snow or fog? Straightforward domain randomization relies on knowing the family of corruptions ahead of time. Here, we propose a Bayesian approach-dubbed robust inverse graphics (RIG)-that relies on a strong scene prior and an uninformative uniform corruption prior, making it applicable to a wide range of corruptions. Given a single image, RIG performs posterior inference jointly over the scene and the corruption. We demonstrate this idea by training a neural radiance field (NeRF) scene prior and using a secondary NeRF to represent the corruptions over which we place an uninformative prior. RIG, trained only on clean data, outperforms depth estimators and alternative NeRF approaches that perform point estimation instead of full inference. The results hold for a number of scene prior architectures based on normalizing flows and diffusion models. For the latter, we develop reconstruction-guidance with auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is applicable in the presence of auxiliary latent variables such as the corruption. RIG demonstrates how scene priors can be used beyond generation tasks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.01915&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tuan Anh Le, Pavel Sountsov, Matthew D. Hoffman, Ben Lee, Brian Patton, Rif A. Saurous</name></author><category term="stat.CO" /><summary type="html">How do we infer a 3D scene from a single image in the presence of corruptions like rain, snow or fog? Straightforward domain randomization relies on knowing the family of corruptions ahead of time. Here, we propose a Bayesian approach-dubbed robust inverse graphics (RIG)-that relies on a strong scene prior and an uninformative uniform corruption prior, making it applicable to a wide range of corruptions. Given a single image, RIG performs posterior inference jointly over the scene and the corruption. We demonstrate this idea by training a neural radiance field (NeRF) scene prior and using a secondary NeRF to represent the corruptions over which we place an uninformative prior. RIG, trained only on clean data, outperforms depth estimators and alternative NeRF approaches that perform point estimation instead of full inference. The results hold for a number of scene prior architectures based on normalizing flows and diffusion models. For the latter, we develop reconstruction-guidance with auxiliary latents (ReGAL)-a diffusion conditioning algorithm that is applicable in the presence of auxiliary latent variables such as the corruption. RIG demonstrates how scene priors can be used beyond generation tasks.</summary></entry><entry><title type="html">Seemingly unrelated Bayesian additive regression trees for cost-effectiveness analyses in healthcare</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/SeeminglyunrelatedBayesianadditiveregressiontreesforcosteffectivenessanalysesinhealthcare.html" rel="alternate" type="text/html" title="Seemingly unrelated Bayesian additive regression trees for cost-effectiveness analyses in healthcare" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/SeeminglyunrelatedBayesianadditiveregressiontreesforcosteffectivenessanalysesinhealthcare</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/SeeminglyunrelatedBayesianadditiveregressiontreesforcosteffectivenessanalysesinhealthcare.html">&lt;p&gt;In recent years, theoretical results and simulation evidence have shown Bayesian additive regression trees to be a highly-effective method for nonparametric regression. Motivated by cost-effectiveness analyses in health economics, where interest lies in jointly modelling the costs of healthcare treatments and the associated health-related quality of life experienced by a patient, we propose a multivariate extension of BART applicable in regression and classification analyses with several correlated outcome variables. Our framework overcomes some key limitations of existing multivariate BART models by allowing each individual response to be associated with different ensembles of trees, while still handling dependencies between the outcomes. In the case of continuous outcomes, our model is essentially a nonparametric version of seemingly unrelated regression. Likewise, our proposal for binary outcomes is a nonparametric generalisation of the multivariate probit model. We give suggestions for easily interpretable prior distributions, which allow specification of both informative and uninformative priors. We provide detailed discussions of MCMC sampling methods to conduct posterior inference. Our methods are implemented in the R package `suBART’. We showcase their performance through extensive simulations and an application to an empirical case study from health economics. By also accommodating propensity scores in a manner befitting a causal analysis, we find substantial evidence for a novel trauma care intervention’s cost-effectiveness.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.02228&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jonas Esser, Mateus Maia, Andrew C. Parnell, Judith Bosmans, Hanneke van Dongen, Thomas Klausch, Keefe Murphy</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In recent years, theoretical results and simulation evidence have shown Bayesian additive regression trees to be a highly-effective method for nonparametric regression. Motivated by cost-effectiveness analyses in health economics, where interest lies in jointly modelling the costs of healthcare treatments and the associated health-related quality of life experienced by a patient, we propose a multivariate extension of BART applicable in regression and classification analyses with several correlated outcome variables. Our framework overcomes some key limitations of existing multivariate BART models by allowing each individual response to be associated with different ensembles of trees, while still handling dependencies between the outcomes. In the case of continuous outcomes, our model is essentially a nonparametric version of seemingly unrelated regression. Likewise, our proposal for binary outcomes is a nonparametric generalisation of the multivariate probit model. We give suggestions for easily interpretable prior distributions, which allow specification of both informative and uninformative priors. We provide detailed discussions of MCMC sampling methods to conduct posterior inference. Our methods are implemented in the R package `suBART’. We showcase their performance through extensive simulations and an application to an empirical case study from health economics. By also accommodating propensity scores in a manner befitting a causal analysis, we find substantial evidence for a novel trauma care intervention’s cost-effectiveness.</summary></entry><entry><title type="html">Sensitivity Analysis for the Test-Negative Design</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/SensitivityAnalysisfortheTestNegativeDesign.html" rel="alternate" type="text/html" title="Sensitivity Analysis for the Test-Negative Design" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/SensitivityAnalysisfortheTestNegativeDesign</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/SensitivityAnalysisfortheTestNegativeDesign.html">&lt;p&gt;The test-negative design has become popular for evaluating the effectiveness of post-licensure vaccines using observational data. In addition to its logistical convenience on data collection, the design is also believed to control for the differential health-care-seeking behavior between vaccinated and unvaccinated individuals, which is an important while often unmeasured confounder between the vaccination and infection. Hence, the design has been employed routinely to monitor seasonal flu vaccines and more recently to measure the COVID-19 vaccine effectiveness. Despite its popularity, the design has been questioned, in particular about its ability to fully control for the unmeasured confounding. In this paper, we explore deviations from a perfect test-negative design, and propose various sensitivity analysis methods for estimating the effect of vaccination measured by the causal odds ratio on the subpopulation of individuals with good health-care-seeking behavior. We start with point identification of the causal odds ratio under a test-negative design, considering two forms of assumptions on the unmeasured confounder. These assumptions then lead to two approaches for conducting sensitivity analysis, addressing the influence of the unmeasured confounding in different ways. Specifically, one approach investigates partial control for unmeasured confounder in the test-negative design, while the other examines the impact of unmeasured confounder on both vaccination and infection. Furthermore, these approaches can be combined to provide narrower bounds on the true causal odds ratio, and can be further extended to sharpen the bounds by restricting the treatment effect heterogeneity. Finally, we apply the proposed methods to evaluate the effectiveness of COVID-19 vaccines using observational data from test-negative designs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06980&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Soumyabrata Kundu, Peng Ding, Xinran Li, Jingshu Wang</name></author><category term="stat.ME" /><summary type="html">The test-negative design has become popular for evaluating the effectiveness of post-licensure vaccines using observational data. In addition to its logistical convenience on data collection, the design is also believed to control for the differential health-care-seeking behavior between vaccinated and unvaccinated individuals, which is an important while often unmeasured confounder between the vaccination and infection. Hence, the design has been employed routinely to monitor seasonal flu vaccines and more recently to measure the COVID-19 vaccine effectiveness. Despite its popularity, the design has been questioned, in particular about its ability to fully control for the unmeasured confounding. In this paper, we explore deviations from a perfect test-negative design, and propose various sensitivity analysis methods for estimating the effect of vaccination measured by the causal odds ratio on the subpopulation of individuals with good health-care-seeking behavior. We start with point identification of the causal odds ratio under a test-negative design, considering two forms of assumptions on the unmeasured confounder. These assumptions then lead to two approaches for conducting sensitivity analysis, addressing the influence of the unmeasured confounding in different ways. Specifically, one approach investigates partial control for unmeasured confounder in the test-negative design, while the other examines the impact of unmeasured confounder on both vaccination and infection. Furthermore, these approaches can be combined to provide narrower bounds on the true causal odds ratio, and can be further extended to sharpen the bounds by restricting the treatment effect heterogeneity. Finally, we apply the proposed methods to evaluate the effectiveness of COVID-19 vaccines using observational data from test-negative designs.</summary></entry><entry><title type="html">Spatial autoregressive model with measurement error in covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Spatialautoregressivemodelwithmeasurementerrorincovariates.html" rel="alternate" type="text/html" title="Spatial autoregressive model with measurement error in covariates" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Spatialautoregressivemodelwithmeasurementerrorincovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Spatialautoregressivemodelwithmeasurementerrorincovariates.html">&lt;p&gt;The Spatial AutoRegressive model (SAR) is commonly used in studies involving spatial and network data to estimate the spatial or network peer influence and the effects of covariates on the response, taking into account the spatial or network dependence. While the model can be efficiently estimated with a Quasi maximum likelihood approach (QMLE), the detrimental effect of covariate measurement error on the QMLE and how to remedy it is currently unknown. If covariates are measured with error, then the QMLE may not have the $\sqrt{n}$ convergence and may even be inconsistent even when a node is influenced by only a limited number of other nodes or spatial units. We develop a measurement error-corrected ML estimator (ME-QMLE) for the parameters of the SAR model when covariates are measured with error. The ME-QMLE possesses statistical consistency and asymptotic normality properties. We consider two types of applications. The first is when the true covariate cannot be measured directly, and a proxy is observed instead. The second one involves including latent homophily factors estimated with error from the network for estimating peer influence. Our numerical results verify the bias correction property of the estimator and the accuracy of the standard error estimates in finite samples. We illustrate the method on a real dataset related to county-level death rates from the COVID-19 pandemic.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.04593&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Subhadeep Paul, Shanjukta Nath</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">The Spatial AutoRegressive model (SAR) is commonly used in studies involving spatial and network data to estimate the spatial or network peer influence and the effects of covariates on the response, taking into account the spatial or network dependence. While the model can be efficiently estimated with a Quasi maximum likelihood approach (QMLE), the detrimental effect of covariate measurement error on the QMLE and how to remedy it is currently unknown. If covariates are measured with error, then the QMLE may not have the $\sqrt{n}$ convergence and may even be inconsistent even when a node is influenced by only a limited number of other nodes or spatial units. We develop a measurement error-corrected ML estimator (ME-QMLE) for the parameters of the SAR model when covariates are measured with error. The ME-QMLE possesses statistical consistency and asymptotic normality properties. We consider two types of applications. The first is when the true covariate cannot be measured directly, and a proxy is observed instead. The second one involves including latent homophily factors estimated with error from the network for estimating peer influence. Our numerical results verify the bias correction property of the estimator and the accuracy of the standard error estimates in finite samples. We illustrate the method on a real dataset related to county-level death rates from the COVID-19 pandemic.</summary></entry><entry><title type="html">Spline-Based Multi-State Models for Analyzing Disease Progression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/SplineBasedMultiStateModelsforAnalyzingDiseaseProgression.html" rel="alternate" type="text/html" title="Spline-Based Multi-State Models for Analyzing Disease Progression" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/SplineBasedMultiStateModelsforAnalyzingDiseaseProgression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/SplineBasedMultiStateModelsforAnalyzingDiseaseProgression.html">&lt;p&gt;Motivated by disease progression-related studies, we propose an estimation method for fitting general non-homogeneous multi-state Markov models. The proposal can handle many types of multi-state processes, with several states and various combinations of observation schemes (e.g., intermittent, exactly observed, censored), and allows for the transition intensities to be flexibly modelled through additive (spline-based) predictors. The algorithm is based on a computationally efficient and stable penalized maximum likelihood estimation approach which exploits the information provided by the analytical Hessian matrix of the model log-likelihood. The proposed modeling framework is employed in case studies that aim at modeling the onset of cardiac allograft vasculopathy, and cognitive decline due to aging, where novel patterns are uncovered. To support applicability and reproducibility, all developed tools are implemented in the R package flexmsm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.05345&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alessia Eletti, Giampiero Marra, Rosalba Radice</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Motivated by disease progression-related studies, we propose an estimation method for fitting general non-homogeneous multi-state Markov models. The proposal can handle many types of multi-state processes, with several states and various combinations of observation schemes (e.g., intermittent, exactly observed, censored), and allows for the transition intensities to be flexibly modelled through additive (spline-based) predictors. The algorithm is based on a computationally efficient and stable penalized maximum likelihood estimation approach which exploits the information provided by the analytical Hessian matrix of the model log-likelihood. The proposed modeling framework is employed in case studies that aim at modeling the onset of cardiac allograft vasculopathy, and cognitive decline due to aging, where novel patterns are uncovered. To support applicability and reproducibility, all developed tools are implemented in the R package flexmsm.</summary></entry><entry><title type="html">Statistics in Phonetics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/StatisticsinPhonetics.html" rel="alternate" type="text/html" title="Statistics in Phonetics" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/StatisticsinPhonetics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/StatisticsinPhonetics.html">&lt;p&gt;Phonetics is the scientific field concerned with the study of how speech is produced, heard and perceived. It abounds with data, such as acoustic speech recordings, neuroimaging data, or articulatory data. In this paper, we provide an introduction to different areas of phonetics (acoustic phonetics, sociophonetics, speech perception, articulatory phonetics, speech inversion, sound change, and speech technology), an overview of the statistical methods for analyzing their data, and an introduction to the signal processing methods commonly applied to speech recordings. A major transition in the statistical modeling of phonetic data has been the shift from fixed effects to random effects regression models, the modeling of curve data (for instance via GAMMs or FDA methods), and the use of Bayesian methods. This shift has been driven in part by the increased focus on large speech corpora in phonetics, which has been driven by machine learning methods such as forced alignment. We conclude by identifying opportunities for future research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.07567&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shahin Tavakoli, Beatrice Matteo, Davide Pigoli, Eleanor Chodroff, John Coleman, Michele Gubian, Margaret E. L. Renwick, Morgan Sonderegger</name></author><category term="stat.AP" /><summary type="html">Phonetics is the scientific field concerned with the study of how speech is produced, heard and perceived. It abounds with data, such as acoustic speech recordings, neuroimaging data, or articulatory data. In this paper, we provide an introduction to different areas of phonetics (acoustic phonetics, sociophonetics, speech perception, articulatory phonetics, speech inversion, sound change, and speech technology), an overview of the statistical methods for analyzing their data, and an introduction to the signal processing methods commonly applied to speech recordings. A major transition in the statistical modeling of phonetic data has been the shift from fixed effects to random effects regression models, the modeling of curve data (for instance via GAMMs or FDA methods), and the use of Bayesian methods. This shift has been driven in part by the increased focus on large speech corpora in phonetics, which has been driven by machine learning methods such as forced alignment. We conclude by identifying opportunities for future research.</summary></entry><entry><title type="html">The Treatment of Ties in Rank-Biased Overlap</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/TheTreatmentofTiesinRankBiasedOverlap.html" rel="alternate" type="text/html" title="The Treatment of Ties in Rank-Biased Overlap" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/TheTreatmentofTiesinRankBiasedOverlap</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/TheTreatmentofTiesinRankBiasedOverlap.html">&lt;p&gt;Rank-Biased Overlap (RBO) is a similarity measure for indefinite rankings: it is top-weighted, and can be computed when only a prefix of the rankings is known or when they have only some items in common. It is widely used for instance to analyze differences between search engines by comparing the rankings of documents they retrieve for the same queries. In these situations, though, it is very frequent to find tied documents that have the same score. Unfortunately, the treatment of ties in RBO remains superficial and incomplete, in the sense that it is not clear how to calculate it from the ranking prefixes only. In addition, the existing way of dealing with ties is very different from the one traditionally followed in the field of Statistics, most notably found in rank correlation coefficients such as Kendall’s and Spearman’s. In this paper we propose a generalized formulation for RBO to handle ties, thanks to which we complete the original definitions by showing how to perform prefix evaluation. We also use it to fully develop two variants that align with the ones found in the Statistics literature: one when there is a reference ranking to compare to, and one when there is not. Overall, these three variants provide researchers with flexibility when comparing rankings with RBO, by clearly determining what ties mean, and how they should be treated. Finally, using both synthetic and TREC data, we demonstrate the use of these new tie-aware RBO measures. We show that the scores may differ substantially from the original tie-unaware RBO measure, where ties had to be broken at random or by arbitrary criteria such as by document ID. Overall, these results evidence the need for a proper account of ties in rank similarity measures such as RBO.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07121&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matteo Corsi, Julián Urbano</name></author><category term="stat.ME" /><summary type="html">Rank-Biased Overlap (RBO) is a similarity measure for indefinite rankings: it is top-weighted, and can be computed when only a prefix of the rankings is known or when they have only some items in common. It is widely used for instance to analyze differences between search engines by comparing the rankings of documents they retrieve for the same queries. In these situations, though, it is very frequent to find tied documents that have the same score. Unfortunately, the treatment of ties in RBO remains superficial and incomplete, in the sense that it is not clear how to calculate it from the ranking prefixes only. In addition, the existing way of dealing with ties is very different from the one traditionally followed in the field of Statistics, most notably found in rank correlation coefficients such as Kendall’s and Spearman’s. In this paper we propose a generalized formulation for RBO to handle ties, thanks to which we complete the original definitions by showing how to perform prefix evaluation. We also use it to fully develop two variants that align with the ones found in the Statistics literature: one when there is a reference ranking to compare to, and one when there is not. Overall, these three variants provide researchers with flexibility when comparing rankings with RBO, by clearly determining what ties mean, and how they should be treated. Finally, using both synthetic and TREC data, we demonstrate the use of these new tie-aware RBO measures. We show that the scores may differ substantially from the original tie-unaware RBO measure, where ties had to be broken at random or by arbitrary criteria such as by document ID. Overall, these results evidence the need for a proper account of ties in rank similarity measures such as RBO.</summary></entry><entry><title type="html">The green hydrogen ambition and implementation gap</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Thegreenhydrogenambitionandimplementationgap.html" rel="alternate" type="text/html" title="The green hydrogen ambition and implementation gap" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Thegreenhydrogenambitionandimplementationgap</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Thegreenhydrogenambitionandimplementationgap.html">&lt;p&gt;Green hydrogen is critical for decarbonising hard-to-electrify sectors, but faces high costs and investment risks. Here we define and quantify the green hydrogen ambition and implementation gap, showing that meeting hydrogen expectations will remain challenging despite surging announcements of projects and subsidies. Tracking 137 projects over three years, we identify a wide 2022 implementation gap with only 2% of global capacity announcements finished on schedule. In contrast, the 2030 ambition gap towards 1.5{\deg}C scenarios is gradually closing as the announced project pipeline has nearly tripled to 441 GW within three years. However, we estimate that, without carbon pricing, realising all these projects would require global subsidies of $1.6 trillion ($1.2 - 2.6 trillion range), far exceeding announced subsidies. Given past and future implementation gaps, policymakers must prepare for prolonged green hydrogen scarcity. Policy support needs to secure hydrogen investments, but should focus on applications where hydrogen is indispensable.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07210&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Adrian Odenweller, Falko Ueckerdt</name></author><category term="stat.AP" /><summary type="html">Green hydrogen is critical for decarbonising hard-to-electrify sectors, but faces high costs and investment risks. Here we define and quantify the green hydrogen ambition and implementation gap, showing that meeting hydrogen expectations will remain challenging despite surging announcements of projects and subsidies. Tracking 137 projects over three years, we identify a wide 2022 implementation gap with only 2% of global capacity announcements finished on schedule. In contrast, the 2030 ambition gap towards 1.5{\deg}C scenarios is gradually closing as the announced project pipeline has nearly tripled to 441 GW within three years. However, we estimate that, without carbon pricing, realising all these projects would require global subsidies of $1.6 trillion ($1.2 - 2.6 trillion range), far exceeding announced subsidies. Given past and future implementation gaps, policymakers must prepare for prolonged green hydrogen scarcity. Policy support needs to secure hydrogen investments, but should focus on applications where hydrogen is indispensable.</summary></entry><entry><title type="html">Training and Validating a Treatment Recommender with Partial Verification Evidence</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/TrainingandValidatingaTreatmentRecommenderwithPartialVerificationEvidence.html" rel="alternate" type="text/html" title="Training and Validating a Treatment Recommender with Partial Verification Evidence" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/TrainingandValidatingaTreatmentRecommenderwithPartialVerificationEvidence</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/TrainingandValidatingaTreatmentRecommenderwithPartialVerificationEvidence.html">&lt;p&gt;Current clinical decision support systems (DSS) are trained and validated on observational data from the target clinic. This is problematic for treatments validated in a randomized clinical trial (RCT), but not yet introduced in any clinic. In this work, we report on a method for training and validating the DSS using the RCT data. The key challenges we address are of missingness – missing rationale for treatment assignment (the assignment is at random), and missing verification evidence, since the effectiveness of a treatment for a patient can only be verified (ground truth) for treatments what were actually assigned to a patient. We use data from a multi-armed RCT that investigated the effectiveness of single- and combination- treatments for 240+ tinnitus patients recruited and treated in 5 clinical centers.
  To deal with the ‘missing rationale’ challenge, we re-model the target variable (outcome) in order to suppress the effect of the randomly-assigned treatment, and control on the effect of treatment in general. Our methods are also robust to missing values in features and with a small number of patients per RCT arm. We deal with ‘missing verification evidence’ by using counterfactual treatment verification, which compares the effectiveness of the DSS recommendations to the effectiveness of the RCT assignments when they are aligned v/s not aligned.
  We demonstrate that our approach leverages the RCT data for learning and verification, by showing that the DSS suggests treatments that improve the outcome. The results are limited through the small number of patients per treatment; while our ensemble is designed to mitigate this effect, the predictive performance of the methods is affected by the smallness of the data. We provide a basis for the establishment of decision supporting routines on treatments that have been tested in RCTs but have not yet been deployed clinically.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06654&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vishnu Unnikrishnan, Clara Puga, Miro Schleicher, Uli Niemann, Berthod Langguth, Stefan Schoisswohl, Birgit Mazurek, Rilana Cima, Jose Antonio Lopez-Escamez, Dimitris Kikidis, Eleftheria Vellidou, Ruediger Pryss, Winfried Schlee, Myra Spiliopoulou</name></author><category term="stat.ME" /><summary type="html">Current clinical decision support systems (DSS) are trained and validated on observational data from the target clinic. This is problematic for treatments validated in a randomized clinical trial (RCT), but not yet introduced in any clinic. In this work, we report on a method for training and validating the DSS using the RCT data. The key challenges we address are of missingness – missing rationale for treatment assignment (the assignment is at random), and missing verification evidence, since the effectiveness of a treatment for a patient can only be verified (ground truth) for treatments what were actually assigned to a patient. We use data from a multi-armed RCT that investigated the effectiveness of single- and combination- treatments for 240+ tinnitus patients recruited and treated in 5 clinical centers. To deal with the ‘missing rationale’ challenge, we re-model the target variable (outcome) in order to suppress the effect of the randomly-assigned treatment, and control on the effect of treatment in general. Our methods are also robust to missing values in features and with a small number of patients per RCT arm. We deal with ‘missing verification evidence’ by using counterfactual treatment verification, which compares the effectiveness of the DSS recommendations to the effectiveness of the RCT assignments when they are aligned v/s not aligned. We demonstrate that our approach leverages the RCT data for learning and verification, by showing that the DSS suggests treatments that improve the outcome. The results are limited through the small number of patients per treatment; while our ensemble is designed to mitigate this effect, the predictive performance of the methods is affected by the smallness of the data. We provide a basis for the establishment of decision supporting routines on treatments that have been tested in RCTs but have not yet been deployed clinically.</summary></entry><entry><title type="html">Tree balance in phylogenetic models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Treebalanceinphylogeneticmodels.html" rel="alternate" type="text/html" title="Tree balance in phylogenetic models" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Treebalanceinphylogeneticmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/Treebalanceinphylogeneticmodels.html">&lt;p&gt;Tree shape statistics, particularly measures of tree (im)balance, play an important role in the analysis of the shape of phylogenetic trees. With applications ranging from testing evolutionary models to studying the impact of fertility inheritance and selection, or tumor development and language evolution, the assessment of tree balance is crucial. Currently, a multitude of at least 30 (im)balance indices can be found in the literature, alongside numerous other tree shape statistics.
  This diversity prompts essential questions: How can we minimize the selection of indices to mitigate the challenges of multiple testing? Is there a preeminent balance index tailored to specific tasks? Previous studies comparing the statistical power of indices in detecting trees deviating from the Yule model have been limited in scope, utilizing only a subset of indices and alternative tree models.
  This research expands upon the examination of index power, encompassing all established indices and a broader array of alternative models. Our investigation reveals distinct groups of balance indices better suited for different tree models, suggesting that decisions on balance index selection can be enhanced with prior knowledge. Furthermore, we present the \textsf{R} software package \textsf{poweRbal} which allows the inclusion of new indices and models, thus facilitating future research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.05185&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sophie J. Kersting, Kristina Wicke, Mareike Fischer</name></author><category term="stat.AP" /><summary type="html">Tree shape statistics, particularly measures of tree (im)balance, play an important role in the analysis of the shape of phylogenetic trees. With applications ranging from testing evolutionary models to studying the impact of fertility inheritance and selection, or tumor development and language evolution, the assessment of tree balance is crucial. Currently, a multitude of at least 30 (im)balance indices can be found in the literature, alongside numerous other tree shape statistics. This diversity prompts essential questions: How can we minimize the selection of indices to mitigate the challenges of multiple testing? Is there a preeminent balance index tailored to specific tasks? Previous studies comparing the statistical power of indices in detecting trees deviating from the Yule model have been limited in scope, utilizing only a subset of indices and alternative tree models. This research expands upon the examination of index power, encompassing all established indices and a broader array of alternative models. Our investigation reveals distinct groups of balance indices better suited for different tree models, suggesting that decisions on balance index selection can be enhanced with prior knowledge. Furthermore, we present the \textsf{R} software package \textsf{poweRbal} which allows the inclusion of new indices and models, thus facilitating future research.</summary></entry><entry><title type="html">ULV: A robust statistical method for clustered data, with applications to multisubject, single-cell omics data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ULVArobuststatisticalmethodforclustereddatawithapplicationstomultisubjectsinglecellomicsdata.html" rel="alternate" type="text/html" title="ULV: A robust statistical method for clustered data, with applications to multisubject, single-cell omics data" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ULVArobuststatisticalmethodforclustereddatawithapplicationstomultisubjectsinglecellomicsdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/ULVArobuststatisticalmethodforclustereddatawithapplicationstomultisubjectsinglecellomicsdata.html">&lt;p&gt;Molecular and genomic technological advancements have greatly enhanced our understanding of biological processes by allowing us to quantify key biological variables such as gene expression, protein levels, and microbiome compositions. These breakthroughs have enabled us to achieve increasingly higher levels of resolution in our measurements, exemplified by our ability to comprehensively profile biological information at the single-cell level. However, the analysis of such data faces several critical challenges: limited number of individuals, non-normality, potential dropouts, outliers, and repeated measurements from the same individual. In this article, we propose a novel method, which we call U-statistic based latent variable (ULV). Our proposed method takes advantage of the robustness of rank-based statistics and exploits the statistical efficiency of parametric methods for small sample sizes. It is a computationally feasible framework that addresses all the issues mentioned above simultaneously. An additional advantage of ULV is its flexibility in modeling various types of single-cell data, including both RNA and protein abundance. The usefulness of our method is demonstrated in two studies: a single-cell proteomics study of acute myelogenous leukemia (AML) and a single-cell RNA study of COVID-19 symptoms. In the AML study, ULV successfully identified differentially expressed proteins that would have been missed by the pseudobulk version of the Wilcoxon rank-sum test. In the COVID-19 study, ULV identified genes associated with covariates such as age and gender, and genes that would be missed without adjusting for covariates. The differentially expressed genes identified by our method are less biased toward genes with high expression levels. Furthermore, ULV identified additional gene pathways likely contributing to the mechanisms of COVID-19 severity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06767&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mingyu Du, Kevin Johnston, Veronica Berrocal, Wei Li, Xiangmin Xu, Zhaoxia Yu</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Molecular and genomic technological advancements have greatly enhanced our understanding of biological processes by allowing us to quantify key biological variables such as gene expression, protein levels, and microbiome compositions. These breakthroughs have enabled us to achieve increasingly higher levels of resolution in our measurements, exemplified by our ability to comprehensively profile biological information at the single-cell level. However, the analysis of such data faces several critical challenges: limited number of individuals, non-normality, potential dropouts, outliers, and repeated measurements from the same individual. In this article, we propose a novel method, which we call U-statistic based latent variable (ULV). Our proposed method takes advantage of the robustness of rank-based statistics and exploits the statistical efficiency of parametric methods for small sample sizes. It is a computationally feasible framework that addresses all the issues mentioned above simultaneously. An additional advantage of ULV is its flexibility in modeling various types of single-cell data, including both RNA and protein abundance. The usefulness of our method is demonstrated in two studies: a single-cell proteomics study of acute myelogenous leukemia (AML) and a single-cell RNA study of COVID-19 symptoms. In the AML study, ULV successfully identified differentially expressed proteins that would have been missed by the pseudobulk version of the Wilcoxon rank-sum test. In the COVID-19 study, ULV identified genes associated with covariates such as age and gender, and genes that would be missed without adjusting for covariates. The differentially expressed genes identified by our method are less biased toward genes with high expression levels. Furthermore, ULV identified additional gene pathways likely contributing to the mechanisms of COVID-19 severity.</summary></entry><entry><title type="html">Unbiased Markov Chain Monte Carlo: what, why, and how</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/UnbiasedMarkovChainMonteCarlowhatwhyandhow.html" rel="alternate" type="text/html" title="Unbiased Markov Chain Monte Carlo: what, why, and how" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/UnbiasedMarkovChainMonteCarlowhatwhyandhow</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/UnbiasedMarkovChainMonteCarlowhatwhyandhow.html">&lt;p&gt;This document presents methods to remove the initialization or burn-in bias from Markov chain Monte Carlo (MCMC) estimates, with consequences on parallel computing, convergence diagnostics and performance assessment. The document is written as an introduction to these methods for MCMC users. Some theoretical results are mentioned, but the focus is on the methodology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06851&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yves F. Atchadé, Pierre E. Jacob</name></author><category term="stat.ME" /><summary type="html">This document presents methods to remove the initialization or burn-in bias from Markov chain Monte Carlo (MCMC) estimates, with consequences on parallel computing, convergence diagnostics and performance assessment. The document is written as an introduction to these methods for MCMC users. Some theoretical results are mentioned, but the focus is on the methodology.</summary></entry><entry><title type="html">Where to place a mosquito trap for West Nile Virus surveillance?</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/WheretoplaceamosquitotrapforWestNileVirussurveillance.html" rel="alternate" type="text/html" title="Where to place a mosquito trap for West Nile Virus surveillance?" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/WheretoplaceamosquitotrapforWestNileVirussurveillance</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/WheretoplaceamosquitotrapforWestNileVirussurveillance.html">&lt;p&gt;The rapid spread of West Nile Virus (WNV) is a growing concern. With no vaccines or specific medications available, prevention through mosquito control is the only solution to curb the spread. Mosquito traps, used to detect viral presence in mosquito populations, are essential tools for WNV surveillance. But how do we decide where to place a mosquito trap? And what makes a good trap location, anyway?
  We present a robust statistical approach to determine a mosquito trap’s ability to predict human WNV cases in the Chicago metropolitan area and its suburbs. We then use this value to detect the landscape, demographic, and socioeconomic factors associated with a mosquito trap’s predictive ability. This approach enables resource-limited mosquito control programs to identify better trap locations while reducing trap numbers to increase trap-based surveillance efficiency. The approach can also be applied to a wide range of different environmental surveillance programs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.06920&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anwesha Chakravarti, Bo Li, Dan Bartlett, Patrick Irwin, Rebecca Smith</name></author><category term="stat.AP" /><summary type="html">The rapid spread of West Nile Virus (WNV) is a growing concern. With no vaccines or specific medications available, prevention through mosquito control is the only solution to curb the spread. Mosquito traps, used to detect viral presence in mosquito populations, are essential tools for WNV surveillance. But how do we decide where to place a mosquito trap? And what makes a good trap location, anyway? We present a robust statistical approach to determine a mosquito trap’s ability to predict human WNV cases in the Chicago metropolitan area and its suburbs. We then use this value to detect the landscape, demographic, and socioeconomic factors associated with a mosquito trap’s predictive ability. This approach enables resource-limited mosquito control programs to identify better trap locations while reducing trap numbers to increase trap-based surveillance efficiency. The approach can also be applied to a wide range of different environmental surveillance programs.</summary></entry><entry><title type="html">Will Southeast Asia be the next global manufacturing hub? A multiway cointegration, causality, and dynamic connectedness analyses on factors influencing offshore decisions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/WillSoutheastAsiabethenextglobalmanufacturinghubAmultiwaycointegrationcausalityanddynamicconnectednessanalysesonfactorsinfluencingoffshoredecisions.html" rel="alternate" type="text/html" title="Will Southeast Asia be the next global manufacturing hub? A multiway cointegration, causality, and dynamic connectedness analyses on factors influencing offshore decisions" /><published>2024-06-12T00:00:00+00:00</published><updated>2024-06-12T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/WillSoutheastAsiabethenextglobalmanufacturinghubAmultiwaycointegrationcausalityanddynamicconnectednessanalysesonfactorsinfluencingoffshoredecisions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/12/WillSoutheastAsiabethenextglobalmanufacturinghubAmultiwaycointegrationcausalityanddynamicconnectednessanalysesonfactorsinfluencingoffshoredecisions.html">&lt;p&gt;The COVID-19 pandemic has compelled multinational corporations to diversify their global supply chain risk and to relocate their factories to Southeast Asian countries beyond China. Such recent phenomena provide a good opportunity to understand the factors that influenced offshore decisions in the last two decades. We propose a new conceptual framework based on econometric approaches to examine the relationships between these factors. Firstly, the Vector Auto Regression (VAR) for multi-way cointegration analysis by a Johansen test as well as the embedding Granger causality analysis to examine offshore decisions–innovation, technology readiness, infrastructure, foreign direct investment (FDI), and intermediate imports. Secondly, a Quantile Vector Autoregressive (QVAR) model is used to assess the dynamic connectedness among Southeast Asian countries based on the offshore factors. This study explores a system-wide experiment to evaluate the spillover effects of offshore decisions. It reports a comprehensive analysis using time-series data collected from the World Bank. The results of the cointegration, causality, and dynamic connectedness analyses show that a subset of Southeast Asian countries have spillover effects on each other. These countries present a multi-way cointegration and dynamic connectedness relationship. The study contributes to policymaking by providing a data-driven innovative approach through a new conceptual framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.07525&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Haibo Wang, Lutfu S. Sua, Jun Huang, Jaime Ortiz, Bahram Alidaee</name></author><category term="stat.AP" /><summary type="html">The COVID-19 pandemic has compelled multinational corporations to diversify their global supply chain risk and to relocate their factories to Southeast Asian countries beyond China. Such recent phenomena provide a good opportunity to understand the factors that influenced offshore decisions in the last two decades. We propose a new conceptual framework based on econometric approaches to examine the relationships between these factors. Firstly, the Vector Auto Regression (VAR) for multi-way cointegration analysis by a Johansen test as well as the embedding Granger causality analysis to examine offshore decisions–innovation, technology readiness, infrastructure, foreign direct investment (FDI), and intermediate imports. Secondly, a Quantile Vector Autoregressive (QVAR) model is used to assess the dynamic connectedness among Southeast Asian countries based on the offshore factors. This study explores a system-wide experiment to evaluate the spillover effects of offshore decisions. It reports a comprehensive analysis using time-series data collected from the World Bank. The results of the cointegration, causality, and dynamic connectedness analyses show that a subset of Southeast Asian countries have spillover effects on each other. These countries present a multi-way cointegration and dynamic connectedness relationship. The study contributes to policymaking by providing a data-driven innovative approach through a new conceptual framework.</summary></entry></feed>