<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-04-30T07:14:20+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Biased Estimator for MinMax Sampling and Distributed Aggregation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ABiasedEstimatorforMinMaxSamplingandDistributedAggregation.html" rel="alternate" type="text/html" title="A Biased Estimator for MinMax Sampling and Distributed Aggregation" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ABiasedEstimatorforMinMaxSamplingandDistributedAggregation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ABiasedEstimatorforMinMaxSamplingandDistributedAggregation.html">&lt;p&gt;MinMax sampling is a technique for downsampling a real-valued vector which minimizes the maximum variance over all vector components. This approach is useful for reducing the amount of data that must be sent over a constrained network link (e.g. in the wide-area). MinMax can provide unbiased estimates of the vector elements, along with unbiased estimates of aggregates when vectors are combined from multiple locations. In this work, we propose a biased MinMax estimation scheme, B-MinMax, which trades an increase in estimator bias for a reduction in variance. We prove that when no aggregation is performed, B-MinMax obtains a strictly lower MSE compared to the unbiased MinMax estimator. When aggregation is required, B-MinMax is preferable when sample sizes are small or the number of aggregated vectors is limited. Our experiments show that this approach can substantially reduce the MSE for MinMax sampling in many practical settings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17690&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joel Wolfrath, Abhishek Chandra</name></author><category term="stat.AP" /><summary type="html">MinMax sampling is a technique for downsampling a real-valued vector which minimizes the maximum variance over all vector components. This approach is useful for reducing the amount of data that must be sent over a constrained network link (e.g. in the wide-area). MinMax can provide unbiased estimates of the vector elements, along with unbiased estimates of aggregates when vectors are combined from multiple locations. In this work, we propose a biased MinMax estimation scheme, B-MinMax, which trades an increase in estimator bias for a reduction in variance. We prove that when no aggregation is performed, B-MinMax obtains a strictly lower MSE compared to the unbiased MinMax estimator. When aggregation is required, B-MinMax is preferable when sample sizes are small or the number of aggregated vectors is limited. Our experiments show that this approach can substantially reduce the MSE for MinMax sampling in many practical settings.</summary></entry><entry><title type="html">A General Causal Inference Framework for Cross-Sectional Observational Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AGeneralCausalInferenceFrameworkforCrossSectionalObservationalData.html" rel="alternate" type="text/html" title="A General Causal Inference Framework for Cross-Sectional Observational Data" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AGeneralCausalInferenceFrameworkforCrossSectionalObservationalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AGeneralCausalInferenceFrameworkforCrossSectionalObservationalData.html">&lt;p&gt;Causal inference methods for observational data are highly regarded due to their wide applicability. While there are already numerous methods available for de-confounding bias, these methods generally assume that covariates consist solely of confounders or make naive assumptions about the covariates. Such assumptions face challenges in both theory and practice, particularly when dealing with high-dimensional covariates. Relaxing these naive assumptions and identifying the confounding covariates that truly require correction can effectively enhance the practical significance of these methods. Therefore, this paper proposes a General Causal Inference (GCI) framework specifically designed for cross-sectional observational data, which precisely identifies the key confounding covariates and provides corresponding identification algorithm. Specifically, based on progressive derivations of the Markov property on Directed Acyclic Graph, we conclude that the key confounding covariates are equivalent to the common root ancestors of the treatment and the outcome variable. Building upon this conclusion, the GCI framework is composed of a novel Ancestor Set Identification (ASI) algorithm and de-confounding inference methods. Firstly, the ASI algorithm is theoretically supported by the conditional independence properties and causal asymmetry between variables, enabling the identification of key confounding covariates. Subsequently, the identified confounding covariates are used in the de-confounding inference methods to obtain unbiased causal effect estimation, which can support informed decision-making. Extensive experiments on synthetic datasets demonstrate that the GCI framework can effectively identify the critical confounding covariates and significantly improve the precision, stability, and interpretability of causal inference in observational studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18197&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yonghe Zhao, Huiyan Sun</name></author><category term="stat.ME" /><summary type="html">Causal inference methods for observational data are highly regarded due to their wide applicability. While there are already numerous methods available for de-confounding bias, these methods generally assume that covariates consist solely of confounders or make naive assumptions about the covariates. Such assumptions face challenges in both theory and practice, particularly when dealing with high-dimensional covariates. Relaxing these naive assumptions and identifying the confounding covariates that truly require correction can effectively enhance the practical significance of these methods. Therefore, this paper proposes a General Causal Inference (GCI) framework specifically designed for cross-sectional observational data, which precisely identifies the key confounding covariates and provides corresponding identification algorithm. Specifically, based on progressive derivations of the Markov property on Directed Acyclic Graph, we conclude that the key confounding covariates are equivalent to the common root ancestors of the treatment and the outcome variable. Building upon this conclusion, the GCI framework is composed of a novel Ancestor Set Identification (ASI) algorithm and de-confounding inference methods. Firstly, the ASI algorithm is theoretically supported by the conditional independence properties and causal asymmetry between variables, enabling the identification of key confounding covariates. Subsequently, the identified confounding covariates are used in the de-confounding inference methods to obtain unbiased causal effect estimation, which can support informed decision-making. Extensive experiments on synthetic datasets demonstrate that the GCI framework can effectively identify the critical confounding covariates and significantly improve the precision, stability, and interpretability of causal inference in observational studies.</summary></entry><entry><title type="html">A General Framework for Random Effects Models for Binary, Ordinal, Count Type and Continuous Dependent Variables Including Variable Selection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AGeneralFrameworkforRandomEffectsModelsforBinaryOrdinalCountTypeandContinuousDependentVariablesIncludingVariableSelection.html" rel="alternate" type="text/html" title="A General Framework for Random Effects Models for Binary, Ordinal, Count Type and Continuous Dependent Variables Including Variable Selection" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AGeneralFrameworkforRandomEffectsModelsforBinaryOrdinalCountTypeandContinuousDependentVariablesIncludingVariableSelection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AGeneralFrameworkforRandomEffectsModelsforBinaryOrdinalCountTypeandContinuousDependentVariablesIncludingVariableSelection.html">&lt;p&gt;A general random effects model is proposed that allows for continuous as well as discrete distributions of the responses. Responses can be unrestricted continuous, bounded continuous, binary, ordered categorical or given in the form of counts. The distribution of the responses is not restricted to exponential families, which is a severe restriction in generalized mixed models. Generalized mixed models use fixed distributions for responses, for example the Poisson distribution in count data, which has the disadvantage of not accounting for overdispersion. By using a response function and a thresholds function the proposed mixed thresholds model can account for a variety of alternative distributions that often show better fits than fixed distributions used within the generalized linear model framework. A particular strength of the model is that it provides a tool for joint modeling, responses may be of different types, some can be discrete, others continuous. In addition to introducing the mixed thresholds model parameter sparsity is addressed. Random effects models can contain a large number of parameters, in particular if effects have to be assumed as measurement-specific. Methods to obtain sparser representations are proposed and illustrated. The methods are shown to work in the thresholds model but could also be adapted to other modeling approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17792&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Gerhard Tutz</name></author><category term="stat.ME" /><summary type="html">A general random effects model is proposed that allows for continuous as well as discrete distributions of the responses. Responses can be unrestricted continuous, bounded continuous, binary, ordered categorical or given in the form of counts. The distribution of the responses is not restricted to exponential families, which is a severe restriction in generalized mixed models. Generalized mixed models use fixed distributions for responses, for example the Poisson distribution in count data, which has the disadvantage of not accounting for overdispersion. By using a response function and a thresholds function the proposed mixed thresholds model can account for a variety of alternative distributions that often show better fits than fixed distributions used within the generalized linear model framework. A particular strength of the model is that it provides a tool for joint modeling, responses may be of different types, some can be discrete, others continuous. In addition to introducing the mixed thresholds model parameter sparsity is addressed. Random effects models can contain a large number of parameters, in particular if effects have to be assumed as measurement-specific. Methods to obtain sparser representations are proposed and illustrated. The methods are shown to work in the thresholds model but could also be adapted to other modeling approaches.</summary></entry><entry><title type="html">AI-driven non-intrusive uncertainty quantification of advanced nuclear fuels for digital twin-enabling technology</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AIdrivennonintrusiveuncertaintyquantificationofadvancednuclearfuelsfordigitaltwinenablingtechnology.html" rel="alternate" type="text/html" title="AI-driven non-intrusive uncertainty quantification of advanced nuclear fuels for digital twin-enabling technology" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AIdrivennonintrusiveuncertaintyquantificationofadvancednuclearfuelsfordigitaltwinenablingtechnology</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AIdrivennonintrusiveuncertaintyquantificationofadvancednuclearfuelsfordigitaltwinenablingtechnology.html">&lt;p&gt;In response to the urgent need to establish AI/ML-integrated Digital Twin (DT) technology within next-generation nuclear systems, advancements in modeling methods and simulation codes are necessary. The increased complexity of models demands significant computational resources to quantify their uncertainties. To address this challenge, a data-driven non-intrusive uncertainty quantification method via polynomial chaos expansion is introduced as an efficient strategy within the finite element analysis-based fuel performance code BISON. Models of and fuels, alongside SiC/SiC cladding material, were prepared to demonstrate the proposed method. The impact of four independent uncertain input variables on the system output was quantified, requiring fewer than 100 BISON simulations for each model. This approach not only accelerates the modeling and simulation task but also enhances the reliability in the development of DT-enabling technologies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.13687&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kazuma Kobayashi, Dinesh Kumar, Syed Bahauddin Alam</name></author><category term="stat.CO," /><category term="stat.AP" /><summary type="html">In response to the urgent need to establish AI/ML-integrated Digital Twin (DT) technology within next-generation nuclear systems, advancements in modeling methods and simulation codes are necessary. The increased complexity of models demands significant computational resources to quantify their uncertainties. To address this challenge, a data-driven non-intrusive uncertainty quantification method via polynomial chaos expansion is introduced as an efficient strategy within the finite element analysis-based fuel performance code BISON. Models of and fuels, alongside SiC/SiC cladding material, were prepared to demonstrate the proposed method. The impact of four independent uncertain input variables on the system output was quantified, requiring fewer than 100 BISON simulations for each model. This approach not only accelerates the modeling and simulation task but also enhances the reliability in the development of DT-enabling technologies.</summary></entry><entry><title type="html">A cautious approach to constraint-based causal model selection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Acautiousapproachtoconstraintbasedcausalmodelselection.html" rel="alternate" type="text/html" title="A cautious approach to constraint-based causal model selection" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Acautiousapproachtoconstraintbasedcausalmodelselection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Acautiousapproachtoconstraintbasedcausalmodelselection.html">&lt;p&gt;We study the data-driven selection of causal graphical models using constraint-based algorithms, which determine the existence or non-existence of edges (causal connections) in a graph based on testing a series of conditional independence hypotheses. In settings where the ultimate scientific goal is to use the selected graph to inform estimation of some causal effect of interest (e.g., by selecting a valid and sufficient set of adjustment variables), we argue that a “cautious” approach to graph selection should control the probability of falsely removing edges and prefer dense, rather than sparse, graphs. We propose a simple inversion of the usual conditional independence testing procedure: to remove an edge, test the null hypothesis of conditional association greater than some user-specified threshold, rather than the null of independence. This equivalence testing formulation to testing independence constraints leads to a procedure with desriable statistical properties and behaviors that better match the inferential goals of certain scientific studies, for example observational epidemiological studies that aim to estimate causal effects in the face of causal model uncertainty. We illustrate our approach on a data example from environmental epidemiology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18232&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Daniel Malinsky</name></author><category term="stat.ME" /><summary type="html">We study the data-driven selection of causal graphical models using constraint-based algorithms, which determine the existence or non-existence of edges (causal connections) in a graph based on testing a series of conditional independence hypotheses. In settings where the ultimate scientific goal is to use the selected graph to inform estimation of some causal effect of interest (e.g., by selecting a valid and sufficient set of adjustment variables), we argue that a “cautious” approach to graph selection should control the probability of falsely removing edges and prefer dense, rather than sparse, graphs. We propose a simple inversion of the usual conditional independence testing procedure: to remove an edge, test the null hypothesis of conditional association greater than some user-specified threshold, rather than the null of independence. This equivalence testing formulation to testing independence constraints leads to a procedure with desriable statistical properties and behaviors that better match the inferential goals of certain scientific studies, for example observational epidemiological studies that aim to estimate causal effects in the face of causal model uncertainty. We illustrate our approach on a data example from environmental epidemiology.</summary></entry><entry><title type="html">Accurate adaptive deep learning method for solving elliptic problems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Accurateadaptivedeeplearningmethodforsolvingellipticproblems.html" rel="alternate" type="text/html" title="Accurate adaptive deep learning method for solving elliptic problems" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Accurateadaptivedeeplearningmethodforsolvingellipticproblems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Accurateadaptivedeeplearningmethodforsolvingellipticproblems.html">&lt;p&gt;Deep learning method is of great importance in solving partial differential equations. In this paper, inspired by the failure-informed idea proposed by Gao et.al. (SIAM Journal on Scientific Computing 45(4)(2023)) and as an improvement, a new accurate adaptive deep learning method is proposed for solving elliptic problems, including the interface problems and the convection-dominated problems. Based on the failure probability framework, the piece-wise uniform distribution is used to approximate the optimal proposal distribution and an kernel-based method is proposed for efficient sampling. Together with the improved Levenberg-Marquardt optimization method, the proposed adaptive deep learning method shows great potential in improving solution accuracy. Numerical tests on the elliptic problems without interface conditions, on the elliptic interface problem, and on the convection-dominated problems demonstrate the effectiveness of the proposed method, as it reduces the relative errors by a factor varying from $10^2$ to $10^4$ for different cases.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18838&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jingyong Ying, Yaqi Xie, Jiao Li, Hongqiao Wang</name></author><category term="stat.CO" /><summary type="html">Deep learning method is of great importance in solving partial differential equations. In this paper, inspired by the failure-informed idea proposed by Gao et.al. (SIAM Journal on Scientific Computing 45(4)(2023)) and as an improvement, a new accurate adaptive deep learning method is proposed for solving elliptic problems, including the interface problems and the convection-dominated problems. Based on the failure probability framework, the piece-wise uniform distribution is used to approximate the optimal proposal distribution and an kernel-based method is proposed for efficient sampling. Together with the improved Levenberg-Marquardt optimization method, the proposed adaptive deep learning method shows great potential in improving solution accuracy. Numerical tests on the elliptic problems without interface conditions, on the elliptic interface problem, and on the convection-dominated problems demonstrate the effectiveness of the proposed method, as it reduces the relative errors by a factor varying from $10^2$ to $10^4$ for different cases.</summary></entry><entry><title type="html">Accurate and fast anomaly detection in industrial processes and IoT environments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AccurateandfastanomalydetectioninindustrialprocessesandIoTenvironments.html" rel="alternate" type="text/html" title="Accurate and fast anomaly detection in industrial processes and IoT environments" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AccurateandfastanomalydetectioninindustrialprocessesandIoTenvironments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AccurateandfastanomalydetectioninindustrialprocessesandIoTenvironments.html">&lt;p&gt;We present a novel, simple and widely applicable semi-supervised procedure for anomaly detection in industrial and IoT environments, SAnD (Simple Anomaly Detection). SAnD comprises 5 steps, each leveraging well-known statistical tools, namely; smoothing filters, variance inflation factors, the Mahalanobis distance, threshold selection algorithms and feature importance techniques. To our knowledge, SAnD is the first procedure that integrates these tools to identify anomalies and help decipher their putative causes. We show how each step contributes to tackling technical challenges that practitioners face when detecting anomalies in industrial contexts, where signals can be highly multicollinear, have unknown distributions, and intertwine short-lived noise with the long(er)-lived actual anomalies. The development of SAnD was motivated by a concrete case study from our industrial partner, which we use here to show its effectiveness. We also evaluate the performance of SAnD by comparing it with a selection of semi-supervised methods on public datasets from the literature on anomaly detection. We conclude that SAnD is effective, broadly applicable, and outperforms existing approaches in both anomaly detection and runtime.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17925&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Simone Tonini , Andrea Vandin , Francesca Chiaromonte , Daniele Licari , Fernando Barsacchi</name></author><category term="stat.AP" /><summary type="html">We present a novel, simple and widely applicable semi-supervised procedure for anomaly detection in industrial and IoT environments, SAnD (Simple Anomaly Detection). SAnD comprises 5 steps, each leveraging well-known statistical tools, namely; smoothing filters, variance inflation factors, the Mahalanobis distance, threshold selection algorithms and feature importance techniques. To our knowledge, SAnD is the first procedure that integrates these tools to identify anomalies and help decipher their putative causes. We show how each step contributes to tackling technical challenges that practitioners face when detecting anomalies in industrial contexts, where signals can be highly multicollinear, have unknown distributions, and intertwine short-lived noise with the long(er)-lived actual anomalies. The development of SAnD was motivated by a concrete case study from our industrial partner, which we use here to show its effectiveness. We also evaluate the performance of SAnD by comparing it with a selection of semi-supervised methods on public datasets from the literature on anomaly detection. We conclude that SAnD is effective, broadly applicable, and outperforms existing approaches in both anomaly detection and runtime.</summary></entry><entry><title type="html">Analyzing Taiwanese traffic patterns on consecutive holidays through forecast reconciliation and prediction-based anomaly detection techniques</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AnalyzingTaiwanesetrafficpatternsonconsecutiveholidaysthroughforecastreconciliationandpredictionbasedanomalydetectiontechniques.html" rel="alternate" type="text/html" title="Analyzing Taiwanese traffic patterns on consecutive holidays through forecast reconciliation and prediction-based anomaly detection techniques" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AnalyzingTaiwanesetrafficpatternsonconsecutiveholidaysthroughforecastreconciliationandpredictionbasedanomalydetectiontechniques</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AnalyzingTaiwanesetrafficpatternsonconsecutiveholidaysthroughforecastreconciliationandpredictionbasedanomalydetectiontechniques.html">&lt;p&gt;This study explores traffic patterns on Taiwanese highways during consecutive holidays and focuses on understanding Taiwanese highway traffic behavior. We propose a prediction-based detection method for finding highway traffic anomalies using reconciled ordinary least squares (OLS) forecasts and bootstrap prediction intervals. Two fundamental features of traffic flow time series – namely, seasonality and spatial autocorrelation – are captured by adding Fourier terms in OLS models, spatial aggregation (as a hierarchical structure mimicking the geographical division in regions, cities, and stations), and a reconciliation step. Our approach, although simple, is able to model complex traffic datasets with reasonable accuracy. Being based on OLS, it is efficient and permits avoiding the computational burden of more complex methods. Analyses of Taiwan’s consecutive holidays in 2019, 2020, and 2021 (73 days) showed strong variations in anomalies across different directions and highways. Specifically, we detected some areas and highways comprising a high number of traffic anomalies (north direction-central and southern regions-highways No. 1 and 3, south direction-southern region-highway No.3), and others with generally normal traffic (east and west direction). These results could provide important decision-support information to traffic authorities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.09537&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mahsa Ashouri, Frederick Kin Hing Phoa, Marzia A. Cremona</name></author><category term="stat.AP" /><summary type="html">This study explores traffic patterns on Taiwanese highways during consecutive holidays and focuses on understanding Taiwanese highway traffic behavior. We propose a prediction-based detection method for finding highway traffic anomalies using reconciled ordinary least squares (OLS) forecasts and bootstrap prediction intervals. Two fundamental features of traffic flow time series – namely, seasonality and spatial autocorrelation – are captured by adding Fourier terms in OLS models, spatial aggregation (as a hierarchical structure mimicking the geographical division in regions, cities, and stations), and a reconciliation step. Our approach, although simple, is able to model complex traffic datasets with reasonable accuracy. Being based on OLS, it is efficient and permits avoiding the computational burden of more complex methods. Analyses of Taiwan’s consecutive holidays in 2019, 2020, and 2021 (73 days) showed strong variations in anomalies across different directions and highways. Specifically, we detected some areas and highways comprising a high number of traffic anomalies (north direction-central and southern regions-highways No. 1 and 3, south direction-southern region-highway No.3), and others with generally normal traffic (east and west direction). These results could provide important decision-support information to traffic authorities.</summary></entry><entry><title type="html">An statistical analysis of COVID-19 intensive care unit bed occupancy data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AnstatisticalanalysisofCOVID19intensivecareunitbedoccupancydata.html" rel="alternate" type="text/html" title="An statistical analysis of COVID-19 intensive care unit bed occupancy data" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AnstatisticalanalysisofCOVID19intensivecareunitbedoccupancydata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/AnstatisticalanalysisofCOVID19intensivecareunitbedoccupancydata.html">&lt;p&gt;The COVID-19 pandemic has had far-reaching consequences, highlighting the urgency for explanatory and predictive tools to track infection rates and burden of care over time and space. However, the scarcity and inhomogeneity of data is a challenge. In this research we develop a robust framework for estimating and predicting the occupied beds of Intensive Care Units by presenting an innovative Small Area Estimation methodology based on the definition of mixed models with random regression coefficients. We applied it to estimate and predict the daily occupancy of Intensive Care Unit beds by COVID-19 in health areas of Castilla y Le&apos;on, from November 2020 to March 2022.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18493&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Naomi Diz-Rosales , María-José Lombardía , Domingo Morales</name></author><category term="stat.AP" /><summary type="html">The COVID-19 pandemic has had far-reaching consequences, highlighting the urgency for explanatory and predictive tools to track infection rates and burden of care over time and space. However, the scarcity and inhomogeneity of data is a challenge. In this research we develop a robust framework for estimating and predicting the occupied beds of Intensive Care Units by presenting an innovative Small Area Estimation methodology based on the definition of mixed models with random regression coefficients. We applied it to estimate and predict the daily occupancy of Intensive Care Unit beds by COVID-19 in health areas of Castilla y Le&apos;on, from November 2020 to March 2022.</summary></entry><entry><title type="html">A real-time digital twin of azimuthal thermoacoustic instabilities</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Arealtimedigitaltwinofazimuthalthermoacousticinstabilities.html" rel="alternate" type="text/html" title="A real-time digital twin of azimuthal thermoacoustic instabilities" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Arealtimedigitaltwinofazimuthalthermoacousticinstabilities</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Arealtimedigitaltwinofazimuthalthermoacousticinstabilities.html">&lt;p&gt;When they occur, azimuthal thermoacoustic oscillations can detrimentally affect the safe operation of gas turbines and aeroengines. We develop a real-time digital twin of azimuthal thermoacoustics of a hydrogen-based annular combustor. The digital twin seamlessly combines two sources of information about the system (i) a physics-based low-order model; and (ii) raw and sparse experimental data from microphones, which contain both aleatoric noise and turbulent fluctuations. First, we derive a low-order thermoacoustic model for azimuthal instabilities, which is deterministic. Second, we propose a real-time data assimilation framework to infer the acoustic pressure, the physical parameters, and the model and measurement biases simultaneously. This is the bias-regularized ensemble Kalman filter (r-EnKF), for which we find an analytical solution that solves the optimization problem. Third, we propose a reservoir computer, which infers both the model bias and measurement bias to close the assimilation equations. Fourth, we propose a real-time digital twin of the azimuthal thermoacoustic dynamics of a laboratory hydrogen-based annular combustor for a variety of equivalence ratios. We find that the real-time digital twin (i) autonomously predicts azimuthal dynamics, in contrast to bias-unregularized methods; (ii) uncovers the physical acoustic pressure from the raw data, i.e., it acts as a physics-based filter; (iii) is a time-varying parameter system, which generalizes existing models that have constant parameters, and capture only slow-varying variables. The digital twin generalizes to all equivalence ratios, which bridges the gap of existing models. This work opens new opportunities for real-time digital twinning of multi-physics problems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18793&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrea Nóvoa, Nicolas Noiray, James R. Dawson, Luca Magri</name></author><category term="stat.AP" /><summary type="html">When they occur, azimuthal thermoacoustic oscillations can detrimentally affect the safe operation of gas turbines and aeroengines. We develop a real-time digital twin of azimuthal thermoacoustics of a hydrogen-based annular combustor. The digital twin seamlessly combines two sources of information about the system (i) a physics-based low-order model; and (ii) raw and sparse experimental data from microphones, which contain both aleatoric noise and turbulent fluctuations. First, we derive a low-order thermoacoustic model for azimuthal instabilities, which is deterministic. Second, we propose a real-time data assimilation framework to infer the acoustic pressure, the physical parameters, and the model and measurement biases simultaneously. This is the bias-regularized ensemble Kalman filter (r-EnKF), for which we find an analytical solution that solves the optimization problem. Third, we propose a reservoir computer, which infers both the model bias and measurement bias to close the assimilation equations. Fourth, we propose a real-time digital twin of the azimuthal thermoacoustic dynamics of a laboratory hydrogen-based annular combustor for a variety of equivalence ratios. We find that the real-time digital twin (i) autonomously predicts azimuthal dynamics, in contrast to bias-unregularized methods; (ii) uncovers the physical acoustic pressure from the raw data, i.e., it acts as a physics-based filter; (iii) is a time-varying parameter system, which generalizes existing models that have constant parameters, and capture only slow-varying variables. The digital twin generalizes to all equivalence ratios, which bridges the gap of existing models. This work opens new opportunities for real-time digital twinning of multi-physics problems.</summary></entry><entry><title type="html">A switching state-space transmission model for tracking epidemics and assessing interventions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Aswitchingstatespacetransmissionmodelfortrackingepidemicsandassessinginterventions.html" rel="alternate" type="text/html" title="A switching state-space transmission model for tracking epidemics and assessing interventions" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Aswitchingstatespacetransmissionmodelfortrackingepidemicsandassessinginterventions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Aswitchingstatespacetransmissionmodelfortrackingepidemicsandassessinginterventions.html">&lt;p&gt;The effective control of infectious diseases relies on accurate assessment of the impact of interventions, which is often hindered by the complex dynamics of the spread of disease. A Beta-Dirichlet switching state-space transmission model is proposed to track underlying dynamics of disease and evaluate the effectiveness of interventions simultaneously. As time evolves, the switching mechanism introduced in the susceptible-exposed-infected-recovered (SEIR) model is able to capture the timing and magnitude of changes in the transmission rate due to the effectiveness of control measures. The implementation of this model is based on a particle Markov Chain Monte Carlo algorithm, which can estimate the time evolution of SEIR states, switching states, and high-dimensional parameters efficiently. The efficacy of the proposed model and estimation procedure are demonstrated through simulation studies. With a real-world application to British Columbia’s COVID-19 outbreak, the proposed switching state-space transmission model quantifies the reduction of transmission rate following interventions. The proposed model provides a promising tool to inform public health policies aimed at studying the underlying dynamics and evaluating the effectiveness of interventions during the spread of the disease.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.16138&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jingxue Feng, Liangliang Wang</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">The effective control of infectious diseases relies on accurate assessment of the impact of interventions, which is often hindered by the complex dynamics of the spread of disease. A Beta-Dirichlet switching state-space transmission model is proposed to track underlying dynamics of disease and evaluate the effectiveness of interventions simultaneously. As time evolves, the switching mechanism introduced in the susceptible-exposed-infected-recovered (SEIR) model is able to capture the timing and magnitude of changes in the transmission rate due to the effectiveness of control measures. The implementation of this model is based on a particle Markov Chain Monte Carlo algorithm, which can estimate the time evolution of SEIR states, switching states, and high-dimensional parameters efficiently. The efficacy of the proposed model and estimation procedure are demonstrated through simulation studies. With a real-world application to British Columbia’s COVID-19 outbreak, the proposed switching state-space transmission model quantifies the reduction of transmission rate following interventions. The proposed model provides a promising tool to inform public health policies aimed at studying the underlying dynamics and evaluating the effectiveness of interventions during the spread of the disease.</summary></entry><entry><title type="html">Bayesian Sparse Vector Autoregressive Switching Models with Application to Human Gesture Phase Segmentation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/BayesianSparseVectorAutoregressiveSwitchingModelswithApplicationtoHumanGesturePhaseSegmentation.html" rel="alternate" type="text/html" title="Bayesian Sparse Vector Autoregressive Switching Models with Application to Human Gesture Phase Segmentation" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/BayesianSparseVectorAutoregressiveSwitchingModelswithApplicationtoHumanGesturePhaseSegmentation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/BayesianSparseVectorAutoregressiveSwitchingModelswithApplicationtoHumanGesturePhaseSegmentation.html">&lt;p&gt;We propose a sparse vector autoregressive (VAR) hidden semi-Markov model (HSMM) for modeling temporal and contemporaneous (e.g. spatial) dependencies in multivariate nonstationary time series. The HSMM’s generic state distribution is embedded in a special transition matrix structure, facilitating efficient likelihood evaluations and arbitrary approximation accuracy. To promote sparsity of the VAR coefficients, we deploy an $l_1$-ball projection prior, which combines differentiability with a positive probability of obtaining exact zeros, achieving variable selection within each switching state. This also facilitates posterior estimation via Hamiltonian Monte Carlo (HMC). We further place non-local priors on the parameters of the HSMM dwell distribution improving the ability of Bayesian model selection to distinguish whether the data is better supported by the simpler hidden Markov model (HMM), or the more flexible HSMM. Our proposed methodology is illustrated via an application to human gesture phase segmentation based on sensor data, where we successfully identify and characterize the periods of rest and active gesturing, as well as the dynamical patterns involved in the gesture movements associated with each of these states.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2302.05347&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Beniamino Hadj-Amar, Jack Jewson, Marina Vannucci</name></author><category term="stat.AP" /><summary type="html">We propose a sparse vector autoregressive (VAR) hidden semi-Markov model (HSMM) for modeling temporal and contemporaneous (e.g. spatial) dependencies in multivariate nonstationary time series. The HSMM’s generic state distribution is embedded in a special transition matrix structure, facilitating efficient likelihood evaluations and arbitrary approximation accuracy. To promote sparsity of the VAR coefficients, we deploy an $l_1$-ball projection prior, which combines differentiability with a positive probability of obtaining exact zeros, achieving variable selection within each switching state. This also facilitates posterior estimation via Hamiltonian Monte Carlo (HMC). We further place non-local priors on the parameters of the HSMM dwell distribution improving the ability of Bayesian model selection to distinguish whether the data is better supported by the simpler hidden Markov model (HMM), or the more flexible HSMM. Our proposed methodology is illustrated via an application to human gesture phase segmentation based on sensor data, where we successfully identify and characterize the periods of rest and active gesturing, as well as the dynamical patterns involved in the gesture movements associated with each of these states.</summary></entry><entry><title type="html">Bayesian analysis of biomarker levels can predict time of recurrence of prostate cancer with strictly positive apparent Shannon information against an exponential attrition prior</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/BayesiananalysisofbiomarkerlevelscanpredicttimeofrecurrenceofprostatecancerwithstrictlypositiveapparentShannoninformationagainstanexponentialattritionprior.html" rel="alternate" type="text/html" title="Bayesian analysis of biomarker levels can predict time of recurrence of prostate cancer with strictly positive apparent Shannon information against an exponential attrition prior" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/BayesiananalysisofbiomarkerlevelscanpredicttimeofrecurrenceofprostatecancerwithstrictlypositiveapparentShannoninformationagainstanexponentialattritionprior</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/BayesiananalysisofbiomarkerlevelscanpredicttimeofrecurrenceofprostatecancerwithstrictlypositiveapparentShannoninformationagainstanexponentialattritionprior.html">&lt;p&gt;Shariat et al previously investigated the possibility of predicting, from preoperative biomarkers and clinical data, which of any pair of patients would suffer recurrence of prostate cancer first. We wished to establish the extent to which predictions of time of relapse from such a model could be improved upon using Bayesian methodology.
  The same dataset was reanalysed using a Bayesian skew-Student mixture model. Predictions were made of which of any pair of patients would relapse first and of the time of relapse. The benefit of using these biomarkers relative to predictions made without them, was measured by the apparent Shannon information, using as prior a simple exponential attrition model of relapse time independent of input variables.
  Using half the dataset for training and the other half for testing, predictions of relapse time from the strict Cox model gave $-\infty$ nepers of apparent Shannon information, (it predicts that relapse can only occur at times when patients in the training set relapsed). Deliberately smoothed predictions from the Cox model gave -0.001 (-0.131 to +0.120) nepers, while the Bayesian model gave +0.109 (+0.021 to +0.192) nepers (mean, 2.5 to 97.5 centiles), being positive with posterior probability 0.993 and beating the blurred Cox model with posterior probability 0.927.
  These predictions from the Bayesian model thus outperform those of the Cox model, but the overall yield of predictive information leaves scope for improvement of the range of biomarkers in use. The Bayesian model presented here is the first such model for prostate cancer to consider the variation of relapse hazard with biomarker concentrations to be smooth, as is intuitive. It is also the first model to be shown to provide more apparent Shannon information than the Cox model and the first to be shown to provide positive apparent information relative to an exponential prior.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17857&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roger Sewell, Elisabeth Crowe, Sharokh F. Shariat</name></author><category term="stat.AP" /><summary type="html">Shariat et al previously investigated the possibility of predicting, from preoperative biomarkers and clinical data, which of any pair of patients would suffer recurrence of prostate cancer first. We wished to establish the extent to which predictions of time of relapse from such a model could be improved upon using Bayesian methodology. The same dataset was reanalysed using a Bayesian skew-Student mixture model. Predictions were made of which of any pair of patients would relapse first and of the time of relapse. The benefit of using these biomarkers relative to predictions made without them, was measured by the apparent Shannon information, using as prior a simple exponential attrition model of relapse time independent of input variables. Using half the dataset for training and the other half for testing, predictions of relapse time from the strict Cox model gave $-\infty$ nepers of apparent Shannon information, (it predicts that relapse can only occur at times when patients in the training set relapsed). Deliberately smoothed predictions from the Cox model gave -0.001 (-0.131 to +0.120) nepers, while the Bayesian model gave +0.109 (+0.021 to +0.192) nepers (mean, 2.5 to 97.5 centiles), being positive with posterior probability 0.993 and beating the blurred Cox model with posterior probability 0.927. These predictions from the Bayesian model thus outperform those of the Cox model, but the overall yield of predictive information leaves scope for improvement of the range of biomarkers in use. The Bayesian model presented here is the first such model for prostate cancer to consider the variation of relapse hazard with biomarker concentrations to be smooth, as is intuitive. It is also the first model to be shown to provide more apparent Shannon information than the Cox model and the first to be shown to provide positive apparent information relative to an exponential prior.</summary></entry><entry><title type="html">Bridging Data Barriers among Participants: Assessing the Potential of Geoenergy through Federated Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/BridgingDataBarriersamongParticipantsAssessingthePotentialofGeoenergythroughFederatedLearning.html" rel="alternate" type="text/html" title="Bridging Data Barriers among Participants: Assessing the Potential of Geoenergy through Federated Learning" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/BridgingDataBarriersamongParticipantsAssessingthePotentialofGeoenergythroughFederatedLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/BridgingDataBarriersamongParticipantsAssessingthePotentialofGeoenergythroughFederatedLearning.html">&lt;p&gt;Machine learning algorithms emerge as a promising approach in energy fields, but its practical is hindered by data barriers, stemming from high collection costs and privacy concerns. This study introduces a novel federated learning (FL) framework based on XGBoost models, enabling safe collaborative modeling with accessible yet concealed data from multiple parties. Hyperparameter tuning of the models is achieved through Bayesian Optimization. To ascertain the merits of the proposed FL-XGBoost method, a comparative analysis is conducted between separate and centralized models to address a classical binary classification problem in geoenergy sector. The results reveal that the proposed FL framework strikes an optimal balance between privacy and accuracy. FL models demonstrate superior accuracy and generalization capabilities compared to separate models, particularly for participants with limited data or low correlation features and offers significant privacy benefits compared to centralized model. The aggregated optimization approach within the FL agreement proves effective in tuning hyperparameters. This study opens new avenues for assessing unconventional reservoirs through collaborative and privacy-preserving FL techniques.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18527&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Weike Peng, Jiaxin Gao, Yuntian Chen, Shengwei Wang</name></author><category term="stat.AP" /><summary type="html">Machine learning algorithms emerge as a promising approach in energy fields, but its practical is hindered by data barriers, stemming from high collection costs and privacy concerns. This study introduces a novel federated learning (FL) framework based on XGBoost models, enabling safe collaborative modeling with accessible yet concealed data from multiple parties. Hyperparameter tuning of the models is achieved through Bayesian Optimization. To ascertain the merits of the proposed FL-XGBoost method, a comparative analysis is conducted between separate and centralized models to address a classical binary classification problem in geoenergy sector. The results reveal that the proposed FL framework strikes an optimal balance between privacy and accuracy. FL models demonstrate superior accuracy and generalization capabilities compared to separate models, particularly for participants with limited data or low correlation features and offers significant privacy benefits compared to centralized model. The aggregated optimization approach within the FL agreement proves effective in tuning hyperparameters. This study opens new avenues for assessing unconventional reservoirs through collaborative and privacy-preserving FL techniques.</summary></entry><entry><title type="html">CVTN: Cross Variable and Temporal Integration for Time Series Forecasting</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/CVTNCrossVariableandTemporalIntegrationforTimeSeriesForecasting.html" rel="alternate" type="text/html" title="CVTN: Cross Variable and Temporal Integration for Time Series Forecasting" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/CVTNCrossVariableandTemporalIntegrationforTimeSeriesForecasting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/CVTNCrossVariableandTemporalIntegrationforTimeSeriesForecasting.html">&lt;p&gt;In multivariate time series forecasting, the Transformer architecture encounters two significant challenges: effectively mining features from historical sequences and avoiding overfitting during the learning of temporal dependencies. To tackle these challenges, this paper deconstructs time series forecasting into the learning of historical sequences and prediction sequences, introducing the Cross-Variable and Time Network (CVTN). This unique method divides multivariate time series forecasting into two phases: cross-variable learning for effectively mining fea tures from historical sequences, and cross-time learning to capture the temporal dependencies of prediction sequences. Separating these two phases helps avoid the impact of overfitting in cross-time learning on cross-variable learning. Exten sive experiments on various real-world datasets have confirmed its state-of-the-art (SOTA) performance. CVTN emphasizes three key dimensions in time series fore casting: the short-term and long-term nature of time series (locality and longevity), feature mining from both historical and prediction sequences, and the integration of cross-variable and cross-time learning. This approach not only advances the current state of time series forecasting but also provides a more comprehensive framework for future research in this field.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18730&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Han Zhou, Yuntian Chen</name></author><category term="stat.AP" /><summary type="html">In multivariate time series forecasting, the Transformer architecture encounters two significant challenges: effectively mining features from historical sequences and avoiding overfitting during the learning of temporal dependencies. To tackle these challenges, this paper deconstructs time series forecasting into the learning of historical sequences and prediction sequences, introducing the Cross-Variable and Time Network (CVTN). This unique method divides multivariate time series forecasting into two phases: cross-variable learning for effectively mining fea tures from historical sequences, and cross-time learning to capture the temporal dependencies of prediction sequences. Separating these two phases helps avoid the impact of overfitting in cross-time learning on cross-variable learning. Exten sive experiments on various real-world datasets have confirmed its state-of-the-art (SOTA) performance. CVTN emphasizes three key dimensions in time series fore casting: the short-term and long-term nature of time series (locality and longevity), feature mining from both historical and prediction sequences, and the integration of cross-variable and cross-time learning. This approach not only advances the current state of time series forecasting but also provides a more comprehensive framework for future research in this field.</summary></entry><entry><title type="html">Calibrar: an R package for fitting complex ecological models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/CalibraranRpackageforfittingcomplexecologicalmodels.html" rel="alternate" type="text/html" title="Calibrar: an R package for fitting complex ecological models" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/CalibraranRpackageforfittingcomplexecologicalmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/CalibraranRpackageforfittingcomplexecologicalmodels.html">&lt;p&gt;The fitting or parameter estimation of complex ecological models is a challenging optimisation task, with a notable lack of tools for fitting complex, long runtime or stochastic models. calibrar is an R package that is dedicated to the fitting of complex models to data. It is a generic tool that can be used for any type of model, especially those with non-differentiable objective functions and long runtime, including Individual Based Models. calibrar supports multiple phases and constrained optimisation, includes 18 optimisation algorithms, including derivative-based and heuristic ones. It supports any type of parallelization, the restart of interrupted optimisations for long runtime models and the combination of different optimisation methods during the multiple phases of the calibration. User-level expertise in R is necessary to handle calibration experiments with calibrar, but there is no need to modify the model’s code, which can be programmed in any language. It implements maximum likelihood estimation methods and automated construction of the objective function from simulated model outputs. For more experienced users, calibrar allows the implementation of user-defined objective functions. The package source code is fully accessible and can be installed directly from CRAN.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1603.03141&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ricardo Oliveros-Ramos, Yunne-Jai Shin</name></author><category term="stat.CO" /><summary type="html">The fitting or parameter estimation of complex ecological models is a challenging optimisation task, with a notable lack of tools for fitting complex, long runtime or stochastic models. calibrar is an R package that is dedicated to the fitting of complex models to data. It is a generic tool that can be used for any type of model, especially those with non-differentiable objective functions and long runtime, including Individual Based Models. calibrar supports multiple phases and constrained optimisation, includes 18 optimisation algorithms, including derivative-based and heuristic ones. It supports any type of parallelization, the restart of interrupted optimisations for long runtime models and the combination of different optimisation methods during the multiple phases of the calibration. User-level expertise in R is necessary to handle calibration experiments with calibrar, but there is no need to modify the model’s code, which can be programmed in any language. It implements maximum likelihood estimation methods and automated construction of the objective function from simulated model outputs. For more experienced users, calibrar allows the implementation of user-defined objective functions. The package source code is fully accessible and can be installed directly from CRAN.</summary></entry><entry><title type="html">Confidence Intervals for Error Rates in 1:1 Matching Tasks: Critical Statistical Analysis and Recommendations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ConfidenceIntervalsforErrorRatesin11MatchingTasksCriticalStatisticalAnalysisandRecommendations.html" rel="alternate" type="text/html" title="Confidence Intervals for Error Rates in 1:1 Matching Tasks: Critical Statistical Analysis and Recommendations" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ConfidenceIntervalsforErrorRatesin11MatchingTasksCriticalStatisticalAnalysisandRecommendations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ConfidenceIntervalsforErrorRatesin11MatchingTasksCriticalStatisticalAnalysisandRecommendations.html">&lt;p&gt;Matching algorithms are commonly used to predict matches between items in a collection. For example, in 1:1 face verification, a matching algorithm predicts whether two face images depict the same person. Accurately assessing the uncertainty of the error rates of such algorithms can be challenging when data are dependent and error rates are low, two aspects that have been often overlooked in the literature. In this work, we review methods for constructing confidence intervals for error rates in 1:1 matching tasks. We derive and examine the statistical properties of these methods, demonstrating how coverage and interval width vary with sample size, error rates, and degree of data dependence on both analysis and experiments with synthetic and real-world datasets. Based on our findings, we provide recommendations for best practices for constructing confidence intervals for error rates in 1:1 matching tasks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.01198&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Riccardo Fogliato, Pratik Patil, Pietro Perona</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Matching algorithms are commonly used to predict matches between items in a collection. For example, in 1:1 face verification, a matching algorithm predicts whether two face images depict the same person. Accurately assessing the uncertainty of the error rates of such algorithms can be challenging when data are dependent and error rates are low, two aspects that have been often overlooked in the literature. In this work, we review methods for constructing confidence intervals for error rates in 1:1 matching tasks. We derive and examine the statistical properties of these methods, demonstrating how coverage and interval width vary with sample size, error rates, and degree of data dependence on both analysis and experiments with synthetic and real-world datasets. Based on our findings, we provide recommendations for best practices for constructing confidence intervals for error rates in 1:1 matching tasks.</summary></entry><entry><title type="html">Conformal Prediction Sets for Populations of Graphs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ConformalPredictionSetsforPopulationsofGraphs.html" rel="alternate" type="text/html" title="Conformal Prediction Sets for Populations of Graphs" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ConformalPredictionSetsforPopulationsofGraphs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ConformalPredictionSetsforPopulationsofGraphs.html">&lt;p&gt;The analysis of data such as graphs has been gaining increasing attention in the past years. This is justified by the numerous applications in which they appear. Several methods are present to predict graphs, but much fewer to quantify the uncertainty of the prediction. The present work proposes an uncertainty quantification methodology for graphs, based on conformal prediction. The method works both for graphs with the same set of nodes (labelled graphs) and graphs with no clear correspondence between the set of nodes across the observed graphs (unlabelled graphs). The unlabelled case is dealt with the creation of prediction sets embedded in a quotient space. The proposed method does not rely on distributional assumptions, it achieves finite-sample validity, and it identifies interpretable prediction sets. To explore the features of this novel forecasting technique, we perform two simulation studies to show the methodology in both the labelled and the unlabelled case. We showcase the applicability of the method in analysing the performance of different teams during the FIFA 2018 football world championship via their player passing networks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18862&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anna Calissano, Matteo Fontana, Gianluca Zeni, Simone Vantini</name></author><category term="stat.ME" /><summary type="html">The analysis of data such as graphs has been gaining increasing attention in the past years. This is justified by the numerous applications in which they appear. Several methods are present to predict graphs, but much fewer to quantify the uncertainty of the prediction. The present work proposes an uncertainty quantification methodology for graphs, based on conformal prediction. The method works both for graphs with the same set of nodes (labelled graphs) and graphs with no clear correspondence between the set of nodes across the observed graphs (unlabelled graphs). The unlabelled case is dealt with the creation of prediction sets embedded in a quotient space. The proposed method does not rely on distributional assumptions, it achieves finite-sample validity, and it identifies interpretable prediction sets. To explore the features of this novel forecasting technique, we perform two simulation studies to show the methodology in both the labelled and the unlabelled case. We showcase the applicability of the method in analysing the performance of different teams during the FIFA 2018 football world championship via their player passing networks.</summary></entry><entry><title type="html">Conformal Ranked Retrieval</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ConformalRankedRetrieval.html" rel="alternate" type="text/html" title="Conformal Ranked Retrieval" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ConformalRankedRetrieval</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ConformalRankedRetrieval.html">&lt;p&gt;Given the wide adoption of ranked retrieval techniques in various information systems that significantly impact our daily lives, there is an increasing need to assess and address the uncertainty inherent in their predictions. This paper introduces a novel method using the conformal risk control framework to quantitatively measure and manage risks in the context of ranked retrieval problems. Our research focuses on a typical two-stage ranked retrieval problem, where the retrieval stage generates candidates for subsequent ranking. By carefully formulating the conformal risk for each stage, we have developed algorithms to effectively control these risks within their specified bounds. The efficacy of our proposed methods has been demonstrated through comprehensive experiments on three large-scale public datasets for ranked retrieval tasks, including the MSLR-WEB dataset, the Yahoo LTRC dataset and the MS MARCO dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17769&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yunpeng Xu, Wenge Guo, Zhi Wei</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Given the wide adoption of ranked retrieval techniques in various information systems that significantly impact our daily lives, there is an increasing need to assess and address the uncertainty inherent in their predictions. This paper introduces a novel method using the conformal risk control framework to quantitatively measure and manage risks in the context of ranked retrieval problems. Our research focuses on a typical two-stage ranked retrieval problem, where the retrieval stage generates candidates for subsequent ranking. By carefully formulating the conformal risk for each stage, we have developed algorithms to effectively control these risks within their specified bounds. The efficacy of our proposed methods has been demonstrated through comprehensive experiments on three large-scale public datasets for ranked retrieval tasks, including the MSLR-WEB dataset, the Yahoo LTRC dataset and the MS MARCO dataset.</summary></entry><entry><title type="html">Data Quality in Crowdsourcing and Spamming Behavior Detection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DataQualityinCrowdsourcingandSpammingBehaviorDetection.html" rel="alternate" type="text/html" title="Data Quality in Crowdsourcing and Spamming Behavior Detection" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DataQualityinCrowdsourcingandSpammingBehaviorDetection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DataQualityinCrowdsourcingandSpammingBehaviorDetection.html">&lt;p&gt;As crowdsourcing emerges as an efficient and cost-effective method for obtaining labels for machine learning datasets, it is important to assess the quality of crowd-provided data, so as to improve analysis performance and reduce biases in subsequent machine learning tasks. Given the lack of ground truth in most cases of crowdsourcing, we refer to data quality as annotators’ consistency and credibility. Unlike the simple scenarios where Kappa coefficient and intraclass correlation coefficient usually can apply, online crowdsourcing requires dealing with more complex situations. We introduce a systematic method for evaluating data quality and detecting spamming threats via variance decomposition, and we classify spammers into three categories based on their different behavioral patterns. A spammer index is proposed to assess entire data consistency and two metrics are developed to measure crowd worker’s credibility by utilizing the Markov chain and generalized random effects models. Furthermore, we showcase the practicality of our techniques and their advantages by applying them on a face verification task with both simulation and real-world data collected from two crowdsourcing platforms.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17582&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yang Ba, Michelle V. Mancenido, Erin K. Chiou, Rong Pan</name></author><category term="stat.AP" /><summary type="html">As crowdsourcing emerges as an efficient and cost-effective method for obtaining labels for machine learning datasets, it is important to assess the quality of crowd-provided data, so as to improve analysis performance and reduce biases in subsequent machine learning tasks. Given the lack of ground truth in most cases of crowdsourcing, we refer to data quality as annotators’ consistency and credibility. Unlike the simple scenarios where Kappa coefficient and intraclass correlation coefficient usually can apply, online crowdsourcing requires dealing with more complex situations. We introduce a systematic method for evaluating data quality and detecting spamming threats via variance decomposition, and we classify spammers into three categories based on their different behavioral patterns. A spammer index is proposed to assess entire data consistency and two metrics are developed to measure crowd worker’s credibility by utilizing the Markov chain and generalized random effects models. Furthermore, we showcase the practicality of our techniques and their advantages by applying them on a face verification task with both simulation and real-world data collected from two crowdsourcing platforms.</summary></entry><entry><title type="html">Decentralized Finance and Local Public Goods: A Bayesian Maximum Entropy Model of School District Spending in the U.S</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DecentralizedFinanceandLocalPublicGoodsABayesianMaximumEntropyModelofSchoolDistrictSpendingintheUS.html" rel="alternate" type="text/html" title="Decentralized Finance and Local Public Goods: A Bayesian Maximum Entropy Model of School District Spending in the U.S" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DecentralizedFinanceandLocalPublicGoodsABayesianMaximumEntropyModelofSchoolDistrictSpendingintheUS</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DecentralizedFinanceandLocalPublicGoodsABayesianMaximumEntropyModelofSchoolDistrictSpendingintheUS.html">&lt;p&gt;This paper investigates the distribution of public school expenditures across U.S. school districts using a bayesian maximum entropy model. Covering the period 2000-2016, I explore how inter-jurisdictional competition and household choice influence spending patterns within the public education sector, providing a novel empirical treatment of the Tiebout hypothesis within a statistical equilibrium framework. The analysis reveals that these expenditures are characterized by sharply peaked and positively skewed distributions, suggesting significant socioeconomic stratification. Employing Bayesian inference and Markov Chain Monte Carlo (MCMC) sampling, I fit these patterns into a statistical equilibrium model to elucidate the roles of competition, as well as household mobility and arbitrage in shaping the distribution of educational spending. The analysis reveals how the scale parameters associated with competition and household choice critically shape the equilibrium outcomes. The model and analysis offer a statistical basis for shaping policy measures intended to affect distributional outcomes in scenarios characterized by the decentralized provision of local public goods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17700&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Juan Melo</name></author><category term="stat.AP" /><summary type="html">This paper investigates the distribution of public school expenditures across U.S. school districts using a bayesian maximum entropy model. Covering the period 2000-2016, I explore how inter-jurisdictional competition and household choice influence spending patterns within the public education sector, providing a novel empirical treatment of the Tiebout hypothesis within a statistical equilibrium framework. The analysis reveals that these expenditures are characterized by sharply peaked and positively skewed distributions, suggesting significant socioeconomic stratification. Employing Bayesian inference and Markov Chain Monte Carlo (MCMC) sampling, I fit these patterns into a statistical equilibrium model to elucidate the roles of competition, as well as household mobility and arbitrage in shaping the distribution of educational spending. The analysis reveals how the scale parameters associated with competition and household choice critically shape the equilibrium outcomes. The model and analysis offer a statistical basis for shaping policy measures intended to affect distributional outcomes in scenarios characterized by the decentralized provision of local public goods.</summary></entry><entry><title type="html">Deep Neural Operator Driven Real Time Inference for Nuclear Systems to Enable Digital Twin Solutions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DeepNeuralOperatorDrivenRealTimeInferenceforNuclearSystemstoEnableDigitalTwinSolutions.html" rel="alternate" type="text/html" title="Deep Neural Operator Driven Real Time Inference for Nuclear Systems to Enable Digital Twin Solutions" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DeepNeuralOperatorDrivenRealTimeInferenceforNuclearSystemstoEnableDigitalTwinSolutions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DeepNeuralOperatorDrivenRealTimeInferenceforNuclearSystemstoEnableDigitalTwinSolutions.html">&lt;p&gt;This paper focuses on the feasibility of Deep Neural Operator (DeepONet) as a robust surrogate modeling method within the context of digital twin (DT) for nuclear energy systems. Through benchmarking and evaluation, this study showcases the generalizability and computational efficiency of DeepONet in solving a challenging particle transport problem. DeepONet also exhibits remarkable prediction accuracy and speed, outperforming traditional ML methods, making it a suitable algorithm for real-time DT inference. However, the application of DeepONet also reveals challenges related to optimal sensor placement and model evaluation, critical aspects of real-world implementation. Addressing these challenges will further enhance the method’s practicality and reliability. Overall, DeepONet presents a promising and transformative nuclear engineering research and applications tool. Its accurate prediction and computational efficiency capabilities can revolutionize DT systems, advancing nuclear engineering research. This study marks an important step towards harnessing the power of surrogate modeling techniques in critical engineering domains.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.07523&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kazuma Kobayashi, Syed Bahauddin Alam</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">This paper focuses on the feasibility of Deep Neural Operator (DeepONet) as a robust surrogate modeling method within the context of digital twin (DT) for nuclear energy systems. Through benchmarking and evaluation, this study showcases the generalizability and computational efficiency of DeepONet in solving a challenging particle transport problem. DeepONet also exhibits remarkable prediction accuracy and speed, outperforming traditional ML methods, making it a suitable algorithm for real-time DT inference. However, the application of DeepONet also reveals challenges related to optimal sensor placement and model evaluation, critical aspects of real-world implementation. Addressing these challenges will further enhance the method’s practicality and reliability. Overall, DeepONet presents a promising and transformative nuclear engineering research and applications tool. Its accurate prediction and computational efficiency capabilities can revolutionize DT systems, advancing nuclear engineering research. This study marks an important step towards harnessing the power of surrogate modeling techniques in critical engineering domains.</summary></entry><entry><title type="html">DeepVARMA: A Hybrid Deep Learning and VARMA Model for Chemical Industry Index Forecasting</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DeepVARMAAHybridDeepLearningandVARMAModelforChemicalIndustryIndexForecasting.html" rel="alternate" type="text/html" title="DeepVARMA: A Hybrid Deep Learning and VARMA Model for Chemical Industry Index Forecasting" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DeepVARMAAHybridDeepLearningandVARMAModelforChemicalIndustryIndexForecasting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DeepVARMAAHybridDeepLearningandVARMAModelforChemicalIndustryIndexForecasting.html">&lt;p&gt;Since the chemical industry index is one of the important indicators to measure the development of the chemical industry, forecasting it is critical for understanding the economic situation and trends of the industry. Taking the multivariable nonstationary series-synthetic material index as the main research object, this paper proposes a new prediction model: DeepVARMA, and its variants Deep-VARMA-re and DeepVARMA-en, which combine LSTM and VARMAX models. The new model firstly uses the deep learning model such as the LSTM remove the trends of the target time series and also learn the representation of endogenous variables, and then uses the VARMAX model to predict the detrended target time series with the embeddings of endogenous variables, and finally combines the trend learned by the LSTM and dependency learned by the VARMAX model to obtain the final predictive values. The experimental results show that (1) the new model achieves the best prediction accuracy by combining the LSTM encoding of the exogenous variables and the VARMAX model. (2) In multivariate non-stationary series prediction, DeepVARMA uses a phased processing strategy to show higher adaptability and accuracy compared to the traditional VARMA model as well as the machine learning models LSTM, RF and XGBoost. (3) Compared with smooth sequence prediction, the traditional VARMA and VARMAX models fluctuate more in predicting non-smooth sequences, while DeepVARMA shows more flexibility and robustness. This study provides more accurate tools and methods for future development and scientific decision-making in the chemical industry.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17615&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xiang Li, Hu Yang</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">Since the chemical industry index is one of the important indicators to measure the development of the chemical industry, forecasting it is critical for understanding the economic situation and trends of the industry. Taking the multivariable nonstationary series-synthetic material index as the main research object, this paper proposes a new prediction model: DeepVARMA, and its variants Deep-VARMA-re and DeepVARMA-en, which combine LSTM and VARMAX models. The new model firstly uses the deep learning model such as the LSTM remove the trends of the target time series and also learn the representation of endogenous variables, and then uses the VARMAX model to predict the detrended target time series with the embeddings of endogenous variables, and finally combines the trend learned by the LSTM and dependency learned by the VARMAX model to obtain the final predictive values. The experimental results show that (1) the new model achieves the best prediction accuracy by combining the LSTM encoding of the exogenous variables and the VARMAX model. (2) In multivariate non-stationary series prediction, DeepVARMA uses a phased processing strategy to show higher adaptability and accuracy compared to the traditional VARMA model as well as the machine learning models LSTM, RF and XGBoost. (3) Compared with smooth sequence prediction, the traditional VARMA and VARMAX models fluctuate more in predicting non-smooth sequences, while DeepVARMA shows more flexibility and robustness. This study provides more accurate tools and methods for future development and scientific decision-making in the chemical industry.</summary></entry><entry><title type="html">Detecting critical treatment effect bias in small subgroups</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Detectingcriticaltreatmenteffectbiasinsmallsubgroups.html" rel="alternate" type="text/html" title="Detecting critical treatment effect bias in small subgroups" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Detectingcriticaltreatmenteffectbiasinsmallsubgroups</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Detectingcriticaltreatmenteffectbiasinsmallsubgroups.html">&lt;p&gt;Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice. Observational studies, on the other hand, cover a broader patient population but are prone to various biases. Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial. We propose a novel strategy to benchmark observational studies beyond the average treatment effect. First, we design a statistical test for the null hypothesis that the treatment effects estimated from the two studies, conditioned on a set of relevant features, differ up to some tolerance. We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study. Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18905&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Piersilvio De Bartolomeis, Javier Abad, Konstantin Donhauser, Fanny Yang</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Randomized trials are considered the gold standard for making informed decisions in medicine, yet they often lack generalizability to the patient populations in clinical practice. Observational studies, on the other hand, cover a broader patient population but are prone to various biases. Thus, before using an observational study for decision-making, it is crucial to benchmark its treatment effect estimates against those derived from a randomized trial. We propose a novel strategy to benchmark observational studies beyond the average treatment effect. First, we design a statistical test for the null hypothesis that the treatment effects estimated from the two studies, conditioned on a set of relevant features, differ up to some tolerance. We then estimate an asymptotically valid lower bound on the maximum bias strength for any subgroup in the observational study. Finally, we validate our benchmarking strategy in a real-world setting and show that it leads to conclusions that align with established medical knowledge.</summary></entry><entry><title type="html">Diversity in the radiation-induced transcriptomic temporal response of mouse brain tissue regions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Diversityintheradiationinducedtranscriptomictemporalresponseofmousebraintissueregions.html" rel="alternate" type="text/html" title="Diversity in the radiation-induced transcriptomic temporal response of mouse brain tissue regions" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Diversityintheradiationinducedtranscriptomictemporalresponseofmousebraintissueregions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Diversityintheradiationinducedtranscriptomictemporalresponseofmousebraintissueregions.html">&lt;p&gt;A number of studies have indicated a potential association between prenatal exposure to radiation and late mental disabilities. This is believed to be due to long-term developmental changes and functional impairment of the central nervous system following radiation exposure during gestation. This study conducted a bioinformatics analysis on transcriptomic profiles from mouse brain tissue prenatally exposed to increasing doses of X-radiation. Gene expression levels were assessed in different brain regions (cortex, hippocampus, cerebellum) and collected at different time points (at 1 and 6 months after birth) for C57BL mice exposed at embryonic day E11 to varying doses of radiation (0, 0.1 and 1 Gy). This study aimed to elucidate the differences in response to radiation between different brain regions at different intervals after birth (1 and 6 months). The data was visualised using a two-dimensional Uniform Manifold Approximation and Projection (UMAP) projection, and the influence of the factors was investigated using analysis of variance (ANOVA). It was observed that gene expression was influenced by each factor (tissue, time, and dose), although to varying degrees. The gene expression trend within doses was compared for each tissue, as well as the significant pathways between tissues at different time intervals. Furthermore, in addition to radiation-responsive pathways, Cytoscape’s functional and network analyses revealed changes in various pathways related to cognition, which is consistent with previously published data [1] [2] [3], indicating late behavioural changes in animals prenatally exposed to radiation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18660&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Karolina Kulis, Sarah Baatout, Kevin Tabury, Joanna Polanska, Mohammed Abderrafi Benotmane</name></author><category term="stat.AP" /><summary type="html">A number of studies have indicated a potential association between prenatal exposure to radiation and late mental disabilities. This is believed to be due to long-term developmental changes and functional impairment of the central nervous system following radiation exposure during gestation. This study conducted a bioinformatics analysis on transcriptomic profiles from mouse brain tissue prenatally exposed to increasing doses of X-radiation. Gene expression levels were assessed in different brain regions (cortex, hippocampus, cerebellum) and collected at different time points (at 1 and 6 months after birth) for C57BL mice exposed at embryonic day E11 to varying doses of radiation (0, 0.1 and 1 Gy). This study aimed to elucidate the differences in response to radiation between different brain regions at different intervals after birth (1 and 6 months). The data was visualised using a two-dimensional Uniform Manifold Approximation and Projection (UMAP) projection, and the influence of the factors was investigated using analysis of variance (ANOVA). It was observed that gene expression was influenced by each factor (tissue, time, and dose), although to varying degrees. The gene expression trend within doses was compared for each tissue, as well as the significant pathways between tissues at different time intervals. Furthermore, in addition to radiation-responsive pathways, Cytoscape’s functional and network analyses revealed changes in various pathways related to cognition, which is consistent with previously published data [1] [2] [3], indicating late behavioural changes in animals prenatally exposed to radiation.</summary></entry><entry><title type="html">Doubly Adaptive Importance Sampling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DoublyAdaptiveImportanceSampling.html" rel="alternate" type="text/html" title="Doubly Adaptive Importance Sampling" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DoublyAdaptiveImportanceSampling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/DoublyAdaptiveImportanceSampling.html">&lt;p&gt;We propose an adaptive importance sampling scheme for Gaussian approximations of intractable posteriors. Optimization-based approximations like variational inference can be too inaccurate while existing Monte Carlo methods can be too slow. Therefore, we propose a hybrid where, at each iteration, the Monte Carlo effective sample size can be guaranteed at a fixed computational cost by interpolating between natural-gradient variational inference and importance sampling. The amount of damping in the updates adapts to the posterior and guarantees the effective sample size. Gaussianity enables the use of Stein’s lemma to obtain gradient-based optimization in the highly damped variational inference regime and a reduction of Monte Carlo error for undamped adaptive importance sampling. The result is a generic, embarrassingly parallel and adaptive posterior approximation method. Numerical studies on simulated and real data show its competitiveness with other, less general methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18556&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Willem van den Boom, Andrea Cremaschi, Alexandre H. Thiery</name></author><category term="stat.CO" /><summary type="html">We propose an adaptive importance sampling scheme for Gaussian approximations of intractable posteriors. Optimization-based approximations like variational inference can be too inaccurate while existing Monte Carlo methods can be too slow. Therefore, we propose a hybrid where, at each iteration, the Monte Carlo effective sample size can be guaranteed at a fixed computational cost by interpolating between natural-gradient variational inference and importance sampling. The amount of damping in the updates adapts to the posterior and guarantees the effective sample size. Gaussianity enables the use of Stein’s lemma to obtain gradient-based optimization in the highly damped variational inference regime and a reduction of Monte Carlo error for undamped adaptive importance sampling. The result is a generic, embarrassingly parallel and adaptive posterior approximation method. Numerical studies on simulated and real data show its competitiveness with other, less general methods.</summary></entry><entry><title type="html">Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/EmpiricalAnalysisofModelSelectionforHeterogeneousCausalEffectEstimation.html" rel="alternate" type="text/html" title="Empirical Analysis of Model Selection for Heterogeneous Causal Effect Estimation" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/EmpiricalAnalysisofModelSelectionforHeterogeneousCausalEffectEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/EmpiricalAnalysisofModelSelectionforHeterogeneousCausalEffectEstimation.html">&lt;p&gt;We study the problem of model selection in causal inference, specifically for conditional average treatment effect (CATE) estimation. Unlike machine learning, there is no perfect analogue of cross-validation for model selection as we do not observe the counterfactual potential outcomes. Towards this, a variety of surrogate metrics have been proposed for CATE model selection that use only observed data. However, we do not have a good understanding regarding their effectiveness due to limited comparisons in prior studies. We conduct an extensive empirical analysis to benchmark the surrogate model selection metrics introduced in the literature, as well as the novel ones introduced in this work. We ensure a fair comparison by tuning the hyperparameters associated with these metrics via AutoML, and provide more detailed trends by incorporating realistic datasets via generative modeling. Our analysis suggests novel model selection strategies based on careful hyperparameter selection of CATE estimators and causal ensembling.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.01939&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Divyat Mahajan, Ioannis Mitliagkas, Brady Neal, Vasilis Syrgkanis</name></author><category term="stat.ME" /><summary type="html">We study the problem of model selection in causal inference, specifically for conditional average treatment effect (CATE) estimation. Unlike machine learning, there is no perfect analogue of cross-validation for model selection as we do not observe the counterfactual potential outcomes. Towards this, a variety of surrogate metrics have been proposed for CATE model selection that use only observed data. However, we do not have a good understanding regarding their effectiveness due to limited comparisons in prior studies. We conduct an extensive empirical analysis to benchmark the surrogate model selection metrics introduced in the literature, as well as the novel ones introduced in this work. We ensure a fair comparison by tuning the hyperparameters associated with these metrics via AutoML, and provide more detailed trends by incorporating realistic datasets via generative modeling. Our analysis suggests novel model selection strategies based on careful hyperparameter selection of CATE estimators and causal ensembling.</summary></entry><entry><title type="html">Enhancing Uncertain Demand Prediction in Hospitals Using Simple and Advanced Machine Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/EnhancingUncertainDemandPredictioninHospitalsUsingSimpleandAdvancedMachineLearning.html" rel="alternate" type="text/html" title="Enhancing Uncertain Demand Prediction in Hospitals Using Simple and Advanced Machine Learning" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/EnhancingUncertainDemandPredictioninHospitalsUsingSimpleandAdvancedMachineLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/EnhancingUncertainDemandPredictioninHospitalsUsingSimpleandAdvancedMachineLearning.html">&lt;p&gt;Early and timely prediction of patient care demand not only affects effective resource allocation but also influences clinical decision-making as well as patient experience. Accurately predicting patient care demand, however, is a ubiquitous challenge for hospitals across the world due, in part, to the demand’s time-varying temporal variability, and, in part, to the difficulty in modelling trends in advance. To address this issue, here, we develop two methods, a relatively simple time-vary linear model, and a more advanced neural network model. The former forecasts patient arrivals hourly over a week based on factors such as day of the week and previous 7-day arrival patterns. The latter leverages a long short-term memory (LSTM) model, capturing non-linear relationships between past data and a three-day forecasting window. We evaluate the predictive capabilities of the two proposed approaches compared to two na&quot;ive approaches - a reduced-rank vector autoregressive (VAR) model and the TBATS model. Using patient care demand data from Rambam Medical Center in Israel, our results show that both proposed models effectively capture hourly variations of patient demand. Additionally, the linear model is more explainable thanks to its simple architecture, whereas, by accurately modelling weekly seasonal trends, the LSTM model delivers lower prediction errors. Taken together, our explorations suggest the utility of machine learning in predicting time-varying patient care demand; additionally, it is possible to predict patient care demand with good accuracy (around 4 patients) three days or a week in advance using machine learning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18670&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Annie Hu, Samuel Stockman, Xun Wu, Richard Wood, Bangdong Zhi, Oliver Y. Chén</name></author><category term="stat.AP" /><summary type="html">Early and timely prediction of patient care demand not only affects effective resource allocation but also influences clinical decision-making as well as patient experience. Accurately predicting patient care demand, however, is a ubiquitous challenge for hospitals across the world due, in part, to the demand’s time-varying temporal variability, and, in part, to the difficulty in modelling trends in advance. To address this issue, here, we develop two methods, a relatively simple time-vary linear model, and a more advanced neural network model. The former forecasts patient arrivals hourly over a week based on factors such as day of the week and previous 7-day arrival patterns. The latter leverages a long short-term memory (LSTM) model, capturing non-linear relationships between past data and a three-day forecasting window. We evaluate the predictive capabilities of the two proposed approaches compared to two na&quot;ive approaches - a reduced-rank vector autoregressive (VAR) model and the TBATS model. Using patient care demand data from Rambam Medical Center in Israel, our results show that both proposed models effectively capture hourly variations of patient demand. Additionally, the linear model is more explainable thanks to its simple architecture, whereas, by accurately modelling weekly seasonal trends, the LSTM model delivers lower prediction errors. Taken together, our explorations suggest the utility of machine learning in predicting time-varying patient care demand; additionally, it is possible to predict patient care demand with good accuracy (around 4 patients) three days or a week in advance using machine learning.</summary></entry><entry><title type="html">Estimating the Sampling Distribution of Posterior Decision Summaries in Bayesian Clinical Trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/EstimatingtheSamplingDistributionofPosteriorDecisionSummariesinBayesianClinicalTrials.html" rel="alternate" type="text/html" title="Estimating the Sampling Distribution of Posterior Decision Summaries in Bayesian Clinical Trials" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/EstimatingtheSamplingDistributionofPosteriorDecisionSummariesinBayesianClinicalTrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/EstimatingtheSamplingDistributionofPosteriorDecisionSummariesinBayesianClinicalTrials.html">&lt;p&gt;Bayesian inference and the use of posterior or posterior predictive probabilities for decision making have become increasingly popular in clinical trials. The current practice in Bayesian clinical trials relies on a hybrid Bayesian-frequentist approach where the design and decision criteria are assessed with respect to frequentist operating characteristics such as power and type I error rate conditioning on a given set of parameters. These operating characteristics are commonly obtained via simulation studies. The utility of Bayesian measures, such as ``assurance”, that incorporate uncertainty about model parameters in estimating the probabilities of various decisions in trials has been demonstrated recently. However, the computational burden remains an obstacle toward wider use of such criteria. In this article, we propose methodology which utilizes large sample theory of the posterior distribution to define parametric models for the sampling distribution of the posterior summaries used for decision making. The parameters of these models are estimated using a small number of simulation scenarios, thereby refining these models to capture the sampling distribution for small to moderate sample size. The proposed approach toward the assessment of conditional and marginal operating characteristics and sample size determination can be considered as simulation-assisted rather than simulation-based. It enables formal incorporation of uncertainty about the trial assumptions via a design prior and significantly reduces the computational burden for the design of Bayesian trials in general.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.09151&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shirin Golchi, James Willard</name></author><category term="stat.ME" /><summary type="html">Bayesian inference and the use of posterior or posterior predictive probabilities for decision making have become increasingly popular in clinical trials. The current practice in Bayesian clinical trials relies on a hybrid Bayesian-frequentist approach where the design and decision criteria are assessed with respect to frequentist operating characteristics such as power and type I error rate conditioning on a given set of parameters. These operating characteristics are commonly obtained via simulation studies. The utility of Bayesian measures, such as ``assurance”, that incorporate uncertainty about model parameters in estimating the probabilities of various decisions in trials has been demonstrated recently. However, the computational burden remains an obstacle toward wider use of such criteria. In this article, we propose methodology which utilizes large sample theory of the posterior distribution to define parametric models for the sampling distribution of the posterior summaries used for decision making. The parameters of these models are estimated using a small number of simulation scenarios, thereby refining these models to capture the sampling distribution for small to moderate sample size. The proposed approach toward the assessment of conditional and marginal operating characteristics and sample size determination can be considered as simulation-assisted rather than simulation-based. It enables formal incorporation of uncertainty about the trial assumptions via a design prior and significantly reduces the computational burden for the design of Bayesian trials in general.</summary></entry><entry><title type="html">Estimation of uncertainties in the density driven flow in fractured porous media using MLMC</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/EstimationofuncertaintiesinthedensitydrivenflowinfracturedporousmediausingMLMC.html" rel="alternate" type="text/html" title="Estimation of uncertainties in the density driven flow in fractured porous media using MLMC" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/EstimationofuncertaintiesinthedensitydrivenflowinfracturedporousmediausingMLMC</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/EstimationofuncertaintiesinthedensitydrivenflowinfracturedporousmediausingMLMC.html">&lt;p&gt;We use the Multi Level Monte Carlo method to estimate uncertainties in a Henry-like salt water intrusion problem with a fracture. The flow is induced by the variation of the density of the fluid phase, which depends on the mass fraction of salt. We assume that the fracture has a known fixed location but an uncertain aperture. Other input uncertainties are the porosity and permeability fields and the recharge. In our setting, porosity and permeability vary spatially and recharge is time-dependent. For each realisation of these uncertain parameters, the evolution of the mass fraction and pressure fields is modelled by a system of non-linear and time-dependent PDEs with a jump of the solution at the fracture. The uncertainties propagate into the distribution of the salt concentration, which is an important characteristic of the quality of water resources. We show that the multilevel Monte Carlo (MLMC) method is able to reduce the overall computational cost compared to classical Monte Carlo methods. This is achieved by balancing discretisation and statistical errors. Multiple scenarios are evaluated at different spatial and temporal mesh levels. The deterministic solver ug4 is run in parallel to calculate all stochastic scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18003&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Dmitry Logashenko, Alexander Litvinenko, Raul Tempone, Gabriel Wittum</name></author><category term="stat.CO" /><summary type="html">We use the Multi Level Monte Carlo method to estimate uncertainties in a Henry-like salt water intrusion problem with a fracture. The flow is induced by the variation of the density of the fluid phase, which depends on the mass fraction of salt. We assume that the fracture has a known fixed location but an uncertain aperture. Other input uncertainties are the porosity and permeability fields and the recharge. In our setting, porosity and permeability vary spatially and recharge is time-dependent. For each realisation of these uncertain parameters, the evolution of the mass fraction and pressure fields is modelled by a system of non-linear and time-dependent PDEs with a jump of the solution at the fracture. The uncertainties propagate into the distribution of the salt concentration, which is an important characteristic of the quality of water resources. We show that the multilevel Monte Carlo (MLMC) method is able to reduce the overall computational cost compared to classical Monte Carlo methods. This is achieved by balancing discretisation and statistical errors. Multiple scenarios are evaluated at different spatial and temporal mesh levels. The deterministic solver ug4 is run in parallel to calculate all stochastic scenarios.</summary></entry><entry><title type="html">Exit Spillovers of Foreign-invested Enterprises in Shenzhen’s Electronics Manufacturing Industry</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ExitSpilloversofForeigninvestedEnterprisesinShenzhensElectronicsManufacturingIndustry.html" rel="alternate" type="text/html" title="Exit Spillovers of Foreign-invested Enterprises in Shenzhen’s Electronics Manufacturing Industry" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ExitSpilloversofForeigninvestedEnterprisesinShenzhensElectronicsManufacturingIndustry</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ExitSpilloversofForeigninvestedEnterprisesinShenzhensElectronicsManufacturingIndustry.html">&lt;p&gt;Neighborhood characteristics have been broadly studied with different firm behaviors, e.g. birth, entry, expansion, and survival, except for firm exit. Using a novel dataset of foreign-invested enterprises operating in Shenzhen’s electronics manufacturing industry from 2017 to 2021, I investigate the spillover effects of firm exits on other firms in the vicinity, from both the industry group and the industry class level. Significant neighborhood effects are identified for the industry group level, but not the industry class level.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18009&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hanqiao Zhang</name></author><category term="stat.AP" /><summary type="html">Neighborhood characteristics have been broadly studied with different firm behaviors, e.g. birth, entry, expansion, and survival, except for firm exit. Using a novel dataset of foreign-invested enterprises operating in Shenzhen’s electronics manufacturing industry from 2017 to 2021, I investigate the spillover effects of firm exits on other firms in the vicinity, from both the industry group and the industry class level. Significant neighborhood effects are identified for the industry group level, but not the industry class level.</summary></entry><entry><title type="html">Explainable, Interpretable &amp;amp; Trustworthy AI for Intelligent Digital Twin: Case Study on Remaining Useful Life</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ExplainableInterpretableTrustworthyAIforIntelligentDigitalTwinCaseStudyonRemainingUsefulLife.html" rel="alternate" type="text/html" title="Explainable, Interpretable &amp;amp; Trustworthy AI for Intelligent Digital Twin: Case Study on Remaining Useful Life" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ExplainableInterpretableTrustworthyAIforIntelligentDigitalTwinCaseStudyonRemainingUsefulLife</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ExplainableInterpretableTrustworthyAIforIntelligentDigitalTwinCaseStudyonRemainingUsefulLife.html">&lt;p&gt;Artificial intelligence (AI) and Machine learning (ML) are increasingly used in energy and engineering systems, but these models must be fair, unbiased, and explainable. It is critical to have confidence in AI’s trustworthiness. ML techniques have been useful in predicting important parameters and in improving model performance. However, for these AI techniques to be useful for making decisions, they need to be audited, accounted for, and easy to understand. Therefore, the use of explainable AI (XAI) and interpretable machine learning (IML) is crucial for the accurate prediction of prognostics, such as remaining useful life (RUL), in a digital twin system, to make it intelligent while ensuring that the AI model is transparent in its decision-making processes and that the predictions it generates can be understood and trusted by users. By using AI that is explainable, interpretable, and trustworthy, intelligent digital twin systems can make more accurate predictions of RUL, leading to better maintenance and repair planning, and ultimately, improved system performance. The objective of this paper is to explain the ideas of XAI and IML and to justify the important role of AI/ML in the digital twin framework and components, which requires XAI to understand the prediction better. This paper explains the importance of XAI and IML in both local and global aspects to ensure the use of trustworthy AI/ML applications for RUL prediction. We used the RUL prediction for the XAI and IML studies and leveraged the integrated Python toolbox for interpretable machine learning~(PiML).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.06676&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kazuma Kobayashi, Syed Bahauddin Alam</name></author><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Artificial intelligence (AI) and Machine learning (ML) are increasingly used in energy and engineering systems, but these models must be fair, unbiased, and explainable. It is critical to have confidence in AI’s trustworthiness. ML techniques have been useful in predicting important parameters and in improving model performance. However, for these AI techniques to be useful for making decisions, they need to be audited, accounted for, and easy to understand. Therefore, the use of explainable AI (XAI) and interpretable machine learning (IML) is crucial for the accurate prediction of prognostics, such as remaining useful life (RUL), in a digital twin system, to make it intelligent while ensuring that the AI model is transparent in its decision-making processes and that the predictions it generates can be understood and trusted by users. By using AI that is explainable, interpretable, and trustworthy, intelligent digital twin systems can make more accurate predictions of RUL, leading to better maintenance and repair planning, and ultimately, improved system performance. The objective of this paper is to explain the ideas of XAI and IML and to justify the important role of AI/ML in the digital twin framework and components, which requires XAI to understand the prediction better. This paper explains the importance of XAI and IML in both local and global aspects to ensure the use of trustworthy AI/ML applications for RUL prediction. We used the RUL prediction for the XAI and IML studies and leveraged the integrated Python toolbox for interpretable machine learning~(PiML).</summary></entry><entry><title type="html">Expressing and visualizing model uncertainty in Bayesian variable selection using Cartesian credible sets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ExpressingandvisualizingmodeluncertaintyinBayesianvariableselectionusingCartesiancrediblesets.html" rel="alternate" type="text/html" title="Expressing and visualizing model uncertainty in Bayesian variable selection using Cartesian credible sets" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ExpressingandvisualizingmodeluncertaintyinBayesianvariableselectionusingCartesiancrediblesets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ExpressingandvisualizingmodeluncertaintyinBayesianvariableselectionusingCartesiancrediblesets.html">&lt;p&gt;Modern regression applications can involve hundreds or thousands of variables which motivates the use of variable selection methods. Bayesian variable selection defines a posterior distribution on the possible subsets of the variables (which are usually termed models) to express uncertainty about which variables are strongly linked to the response. This can be used to provide Bayesian model averaged predictions or inference, and to understand the relative importance of different variables. However, there has been little work on meaningful representations of this uncertainty beyond first order summaries. We introduce Cartesian credible sets to address this gap. The elements of these sets are formed by concatenating sub-models defined on each block of a partition of the variables. Investigating these sub-models allow us to understand whether the models in the Cartesian credible set always/never/sometimes include a particular variable or group of variables and provide a useful summary of model uncertainty. We introduce methods to find these sets that emphasize ease of understanding. The potential of the method is illustrated on regression problems with both small and large numbers of variables.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.12323&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>J. E. Griffin</name></author><category term="stat.ME" /><summary type="html">Modern regression applications can involve hundreds or thousands of variables which motivates the use of variable selection methods. Bayesian variable selection defines a posterior distribution on the possible subsets of the variables (which are usually termed models) to express uncertainty about which variables are strongly linked to the response. This can be used to provide Bayesian model averaged predictions or inference, and to understand the relative importance of different variables. However, there has been little work on meaningful representations of this uncertainty beyond first order summaries. We introduce Cartesian credible sets to address this gap. The elements of these sets are formed by concatenating sub-models defined on each block of a partition of the variables. Investigating these sub-models allow us to understand whether the models in the Cartesian credible set always/never/sometimes include a particular variable or group of variables and provide a useful summary of model uncertainty. We introduce methods to find these sets that emphasize ease of understanding. The potential of the method is illustrated on regression problems with both small and large numbers of variables.</summary></entry><entry><title type="html">Fully Synthetic Data for Complex Surveys</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/FullySyntheticDataforComplexSurveys.html" rel="alternate" type="text/html" title="Fully Synthetic Data for Complex Surveys" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/FullySyntheticDataforComplexSurveys</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/FullySyntheticDataforComplexSurveys.html">&lt;p&gt;When seeking to release public use files for confidential data, statistical agencies can generate fully synthetic data. We propose an approach for making fully synthetic data from surveys collected with complex sampling designs. Our approach adheres to the general strategy proposed by Rubin (1993). Specifically, we generate pseudo-populations by applying the weighted finite population Bayesian bootstrap to account for survey weights, take simple random samples from those pseudo-populations, estimate synthesis models using these simple random samples, and release simulated data drawn from the models as public use files. To facilitate variance estimation, we use the framework of multiple imputation with two data generation strategies. In the first, we generate multiple data sets from each simple random sample. In the second, we generate a single synthetic data set from each simple random sample. We present multiple imputation combining rules for each setting. We illustrate the repeated sampling properties of the combining rules via simulation studies, including comparisons with synthetic data generation based on pseudo-likelihood methods. We apply the proposed methods to a subset of data from the American Community Survey.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.09115&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shirley Mathur, Yajuan Si, Jerome P. Reiter</name></author><category term="stat.ME" /><summary type="html">When seeking to release public use files for confidential data, statistical agencies can generate fully synthetic data. We propose an approach for making fully synthetic data from surveys collected with complex sampling designs. Our approach adheres to the general strategy proposed by Rubin (1993). Specifically, we generate pseudo-populations by applying the weighted finite population Bayesian bootstrap to account for survey weights, take simple random samples from those pseudo-populations, estimate synthesis models using these simple random samples, and release simulated data drawn from the models as public use files. To facilitate variance estimation, we use the framework of multiple imputation with two data generation strategies. In the first, we generate multiple data sets from each simple random sample. In the second, we generate a single synthetic data set from each simple random sample. We present multiple imputation combining rules for each setting. We illustrate the repeated sampling properties of the combining rules via simulation studies, including comparisons with synthetic data generation based on pseudo-likelihood methods. We apply the proposed methods to a subset of data from the American Community Survey.</summary></entry><entry><title type="html">Global Sensitivity and Domain-Selective Testing for Functional-Valued Responses: An Application to Climate Economy Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/GlobalSensitivityandDomainSelectiveTestingforFunctionalValuedResponsesAnApplicationtoClimateEconomyModels.html" rel="alternate" type="text/html" title="Global Sensitivity and Domain-Selective Testing for Functional-Valued Responses: An Application to Climate Economy Models" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/GlobalSensitivityandDomainSelectiveTestingforFunctionalValuedResponsesAnApplicationtoClimateEconomyModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/GlobalSensitivityandDomainSelectiveTestingforFunctionalValuedResponsesAnApplicationtoClimateEconomyModels.html">&lt;p&gt;Understanding the dynamics and evolution of climate change and associated uncertainties is key for designing robust policy actions. Computer models are key tools in this scientific effort, which have now reached a high level of sophistication and complexity. Model auditing is needed in order to better understand their results, and to deal with the fact that such models are increasingly opaque with respect to their inner workings. Current techniques such as Global Sensitivity Analysis (GSA) are limited to dealing either with multivariate outputs, stochastic ones, or finite-change inputs. This limits their applicability to time-varying variables such as future pathways of greenhouse gases. To provide additional semantics in the analysis of a model ensemble, we provide an extension of GSA methodologies tackling the case of stochastic functional outputs with finite change inputs. To deal with finite change inputs and functional outputs, we propose an extension of currently available GSA methodologies while we deal with the stochastic part by introducing a novel, domain-selective inferential technique for sensitivity indices. Our method is explored via a simulation study that shows its robustness and efficacy in detecting sensitivity patterns. We apply it to real world data, where its capabilities can provide to practitioners and policymakers additional information about the time dynamics of sensitivity patterns, as well as information about robustness.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.13850&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matteo Fontana, Massimo Tavoni, Simone Vantini</name></author><category term="stat.ME" /><summary type="html">Understanding the dynamics and evolution of climate change and associated uncertainties is key for designing robust policy actions. Computer models are key tools in this scientific effort, which have now reached a high level of sophistication and complexity. Model auditing is needed in order to better understand their results, and to deal with the fact that such models are increasingly opaque with respect to their inner workings. Current techniques such as Global Sensitivity Analysis (GSA) are limited to dealing either with multivariate outputs, stochastic ones, or finite-change inputs. This limits their applicability to time-varying variables such as future pathways of greenhouse gases. To provide additional semantics in the analysis of a model ensemble, we provide an extension of GSA methodologies tackling the case of stochastic functional outputs with finite change inputs. To deal with finite change inputs and functional outputs, we propose an extension of currently available GSA methodologies while we deal with the stochastic part by introducing a novel, domain-selective inferential technique for sensitivity indices. Our method is explored via a simulation study that shows its robustness and efficacy in detecting sensitivity patterns. We apply it to real world data, where its capabilities can provide to practitioners and policymakers additional information about the time dynamics of sensitivity patterns, as well as information about robustness.</summary></entry><entry><title type="html">High-Dimensional Single-Index Models: Link Estimation and Marginal Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/HighDimensionalSingleIndexModelsLinkEstimationandMarginalInference.html" rel="alternate" type="text/html" title="High-Dimensional Single-Index Models: Link Estimation and Marginal Inference" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/HighDimensionalSingleIndexModelsLinkEstimationandMarginalInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/HighDimensionalSingleIndexModelsLinkEstimationandMarginalInference.html">&lt;p&gt;This study proposes a novel method for estimation and hypothesis testing in high-dimensional single-index models. We address a common scenario where the sample size and the dimension of regression coefficients are large and comparable. Unlike traditional approaches, which often overlook the estimation of the unknown link function, we introduce a new method for link function estimation. Leveraging the information from the estimated link function, we propose more efficient estimators that are better aligned with the underlying model. Furthermore, we rigorously establish the asymptotic normality of each coordinate of the estimator. This provides a valid construction of confidence intervals and $p$-values for any finite collection of coordinates. Numerical experiments validate our theoretical results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17812&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kazuma Sawaya, Yoshimasa Uematsu, Masaaki Imaizumi</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This study proposes a novel method for estimation and hypothesis testing in high-dimensional single-index models. We address a common scenario where the sample size and the dimension of regression coefficients are large and comparable. Unlike traditional approaches, which often overlook the estimation of the unknown link function, we introduce a new method for link function estimation. Leveraging the information from the estimated link function, we propose more efficient estimators that are better aligned with the underlying model. Furthermore, we rigorously establish the asymptotic normality of each coordinate of the estimator. This provides a valid construction of confidence intervals and $p$-values for any finite collection of coordinates. Numerical experiments validate our theoretical results.</summary></entry><entry><title type="html">Implicit Generative Prior for Bayesian Neural Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ImplicitGenerativePriorforBayesianNeuralNetworks.html" rel="alternate" type="text/html" title="Implicit Generative Prior for Bayesian Neural Networks" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ImplicitGenerativePriorforBayesianNeuralNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ImplicitGenerativePriorforBayesianNeuralNetworks.html">&lt;p&gt;Predictive uncertainty quantification is crucial for reliable decision-making in various applied domains. Bayesian neural networks offer a powerful framework for this task. However, defining meaningful priors and ensuring computational efficiency remain significant challenges, especially for complex real-world applications. This paper addresses these challenges by proposing a novel neural adaptive empirical Bayes (NA-EB) framework. NA-EB leverages a class of implicit generative priors derived from low-dimensional distributions. This allows for efficient handling of complex data structures and effective capture of underlying relationships in real-world datasets. The proposed NA-EB framework combines variational inference with a gradient ascent algorithm. This enables simultaneous hyperparameter selection and approximation of the posterior distribution, leading to improved computational efficiency. We establish the theoretical foundation of the framework through posterior and classification consistency. We demonstrate the practical applications of our framework through extensive evaluations on a variety of tasks, including the two-spiral problem, regression, 10 UCI datasets, and image classification tasks on both MNIST and CIFAR-10 datasets. The results of our experiments highlight the superiority of our proposed framework over existing methods, such as sparse variational Bayesian and generative models, in terms of prediction accuracy and uncertainty quantification.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18008&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yijia Liu, Xiao Wang</name></author><category term="stat.AP" /><summary type="html">Predictive uncertainty quantification is crucial for reliable decision-making in various applied domains. Bayesian neural networks offer a powerful framework for this task. However, defining meaningful priors and ensuring computational efficiency remain significant challenges, especially for complex real-world applications. This paper addresses these challenges by proposing a novel neural adaptive empirical Bayes (NA-EB) framework. NA-EB leverages a class of implicit generative priors derived from low-dimensional distributions. This allows for efficient handling of complex data structures and effective capture of underlying relationships in real-world datasets. The proposed NA-EB framework combines variational inference with a gradient ascent algorithm. This enables simultaneous hyperparameter selection and approximation of the posterior distribution, leading to improved computational efficiency. We establish the theoretical foundation of the framework through posterior and classification consistency. We demonstrate the practical applications of our framework through extensive evaluations on a variety of tasks, including the two-spiral problem, regression, 10 UCI datasets, and image classification tasks on both MNIST and CIFAR-10 datasets. The results of our experiments highlight the superiority of our proposed framework over existing methods, such as sparse variational Bayesian and generative models, in terms of prediction accuracy and uncertainty quantification.</summary></entry><entry><title type="html">Improved generalization with deep neural operators for engineering systems: Path towards digital twin</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ImprovedgeneralizationwithdeepneuraloperatorsforengineeringsystemsPathtowardsdigitaltwin.html" rel="alternate" type="text/html" title="Improved generalization with deep neural operators for engineering systems: Path towards digital twin" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ImprovedgeneralizationwithdeepneuraloperatorsforengineeringsystemsPathtowardsdigitaltwin</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ImprovedgeneralizationwithdeepneuraloperatorsforengineeringsystemsPathtowardsdigitaltwin.html">&lt;p&gt;Neural Operator Networks (ONets) represent a novel advancement in machine learning algorithms, offering a robust and generalizable alternative for approximating partial differential equations (PDEs) solutions. Unlike traditional Neural Networks (NN), which directly approximate functions, ONets specialize in approximating mathematical operators, enhancing their efficacy in addressing complex PDEs. In this work, we evaluate the capabilities of Deep Operator Networks (DeepONets), an ONets implementation using a branch/trunk architecture. Three test cases are studied: a system of ODEs, a general diffusion system, and the convection/diffusion Burgers equation. It is demonstrated that DeepONets can accurately learn the solution operators, achieving prediction accuracy scores above 0.96 for the ODE and diffusion problems over the observed domain while achieving zero shot (without retraining) capability. More importantly, when evaluated on unseen scenarios (zero shot feature), the trained models exhibit excellent generalization ability. This underscores ONets vital niche for surrogate modeling and digital twin development across physical systems. While convection-diffusion poses a greater challenge, the results confirm the promise of ONets and motivate further enhancements to the DeepONet algorithm. This work represents an important step towards unlocking the potential of digital twins through robust and generalizable surrogates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.06701&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kazuma Kobayashi, James Daniell, Syed Bahauddin Alam</name></author><category term="stat.AP," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">Neural Operator Networks (ONets) represent a novel advancement in machine learning algorithms, offering a robust and generalizable alternative for approximating partial differential equations (PDEs) solutions. Unlike traditional Neural Networks (NN), which directly approximate functions, ONets specialize in approximating mathematical operators, enhancing their efficacy in addressing complex PDEs. In this work, we evaluate the capabilities of Deep Operator Networks (DeepONets), an ONets implementation using a branch/trunk architecture. Three test cases are studied: a system of ODEs, a general diffusion system, and the convection/diffusion Burgers equation. It is demonstrated that DeepONets can accurately learn the solution operators, achieving prediction accuracy scores above 0.96 for the ODE and diffusion problems over the observed domain while achieving zero shot (without retraining) capability. More importantly, when evaluated on unseen scenarios (zero shot feature), the trained models exhibit excellent generalization ability. This underscores ONets vital niche for surrogate modeling and digital twin development across physical systems. While convection-diffusion poses a greater challenge, the results confirm the promise of ONets and motivate further enhancements to the DeepONet algorithm. This work represents an important step towards unlocking the potential of digital twins through robust and generalizable surrogates.</summary></entry><entry><title type="html">Inference for the panel ARMA-GARCH model when both $N$ and $T$ are large</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/InferenceforthepanelARMAGARCHmodelwhenbothNandTarelarge.html" rel="alternate" type="text/html" title="Inference for the panel ARMA-GARCH model when both $N$ and $T$ are large" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/InferenceforthepanelARMAGARCHmodelwhenbothNandTarelarge</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/InferenceforthepanelARMAGARCHmodelwhenbothNandTarelarge.html">&lt;p&gt;We propose a panel ARMA-GARCH model to capture the dynamics of large panel data with $N$ individuals over $T$ time periods. For this model, we provide a two-step estimation procedure to estimate the ARMA parameters and GARCH parameters stepwisely. Under some regular conditions, we show that all of the proposed estimators are asymptotically normal with the convergence rate $(NT)^{-1/2}$, and they have the asymptotic biases when both $N$ and $T$ diverge to infinity at the same rate. Particularly, we find that the asymptotic biases result from the fixed effect, estimation effect, and unobservable initial values. To correct the biases, we further propose the bias-corrected version of estimators by using either the analytical asymptotics or jackknife method. Our asymptotic results are based on a new central limit theorem for the linear-quadratic form in the martingale difference sequence, when the weight matrix is uniformly bounded in row and column. Simulations and one real example are given to demonstrate the usefulness of our panel ARMA-GARCH model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18377&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bing Su, Ke Zhu</name></author><category term="stat.ME" /><summary type="html">We propose a panel ARMA-GARCH model to capture the dynamics of large panel data with $N$ individuals over $T$ time periods. For this model, we provide a two-step estimation procedure to estimate the ARMA parameters and GARCH parameters stepwisely. Under some regular conditions, we show that all of the proposed estimators are asymptotically normal with the convergence rate $(NT)^{-1/2}$, and they have the asymptotic biases when both $N$ and $T$ diverge to infinity at the same rate. Particularly, we find that the asymptotic biases result from the fixed effect, estimation effect, and unobservable initial values. To correct the biases, we further propose the bias-corrected version of estimators by using either the analytical asymptotics or jackknife method. Our asymptotic results are based on a new central limit theorem for the linear-quadratic form in the martingale difference sequence, when the weight matrix is uniformly bounded in row and column. Simulations and one real example are given to demonstrate the usefulness of our panel ARMA-GARCH model.</summary></entry><entry><title type="html">Likelihood Based Inference in Fully and Partially Observed Exponential Family Graphical Models with Intractable Normalizing Constants</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/LikelihoodBasedInferenceinFullyandPartiallyObservedExponentialFamilyGraphicalModelswithIntractableNormalizingConstants.html" rel="alternate" type="text/html" title="Likelihood Based Inference in Fully and Partially Observed Exponential Family Graphical Models with Intractable Normalizing Constants" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/LikelihoodBasedInferenceinFullyandPartiallyObservedExponentialFamilyGraphicalModelswithIntractableNormalizingConstants</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/LikelihoodBasedInferenceinFullyandPartiallyObservedExponentialFamilyGraphicalModelswithIntractableNormalizingConstants.html">&lt;p&gt;Probabilistic graphical models that encode an underlying Markov random field are fundamental building blocks of generative modeling to learn latent representations in modern multivariate data sets with complex dependency structures. Among these, the exponential family graphical models are especially popular, given their fairly well-understood statistical properties and computational scalability to high-dimensional data based on pseudo-likelihood methods. These models have been successfully applied in many fields, such as the Ising model in statistical physics and count graphical models in genomics. Another strand of models allows some nodes to be latent, so as to allow the marginal distribution of the observable nodes to depart from exponential family to capture more complex dependence. These approaches form the basis of generative models in artificial intelligence, such as the Boltzmann machines and their restricted versions. A fundamental barrier to likelihood-based (i.e., both maximum likelihood and fully Bayesian) inference in both fully and partially observed cases is the intractability of the likelihood. The usual workaround is via adopting pseudo-likelihood based approaches, following the pioneering work of Besag (1974). The goal of this paper is to demonstrate that full likelihood based analysis of these models is feasible in a computationally efficient manner. The chief innovation lies in using a technique of Geyer (1991) to estimate the intractable normalizing constant, as well as its gradient, for intractable graphical models. Extensive numerical results, supporting theory and comparisons with pseudo-likelihood based approaches demonstrate the applicability of the proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17763&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yujie Chen, Anindya Bhadra, Antik Chakraborty</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">Probabilistic graphical models that encode an underlying Markov random field are fundamental building blocks of generative modeling to learn latent representations in modern multivariate data sets with complex dependency structures. Among these, the exponential family graphical models are especially popular, given their fairly well-understood statistical properties and computational scalability to high-dimensional data based on pseudo-likelihood methods. These models have been successfully applied in many fields, such as the Ising model in statistical physics and count graphical models in genomics. Another strand of models allows some nodes to be latent, so as to allow the marginal distribution of the observable nodes to depart from exponential family to capture more complex dependence. These approaches form the basis of generative models in artificial intelligence, such as the Boltzmann machines and their restricted versions. A fundamental barrier to likelihood-based (i.e., both maximum likelihood and fully Bayesian) inference in both fully and partially observed cases is the intractability of the likelihood. The usual workaround is via adopting pseudo-likelihood based approaches, following the pioneering work of Besag (1974). The goal of this paper is to demonstrate that full likelihood based analysis of these models is feasible in a computationally efficient manner. The chief innovation lies in using a technique of Geyer (1991) to estimate the intractable normalizing constant, as well as its gradient, for intractable graphical models. Extensive numerical results, supporting theory and comparisons with pseudo-likelihood based approaches demonstrate the applicability of the proposed method.</summary></entry><entry><title type="html">Mahalanobis balancing: a multivariate perspective on approximate covariate balancing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Mahalanobisbalancingamultivariateperspectiveonapproximatecovariatebalancing.html" rel="alternate" type="text/html" title="Mahalanobis balancing: a multivariate perspective on approximate covariate balancing" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Mahalanobisbalancingamultivariateperspectiveonapproximatecovariatebalancing</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Mahalanobisbalancingamultivariateperspectiveonapproximatecovariatebalancing.html">&lt;p&gt;In the past decade, various exact balancing-based weighting methods were introduced to the causal inference literature. Exact balancing alleviates the extreme weight and model misspecification issues that may incur when one implements inverse probability weighting. It eliminates covariate imbalance by imposing balancing constraints in an optimization problem. The optimization problem can nevertheless be infeasible when there is bad overlap between the covariate distributions in the treated and control groups or when the covariates are high-dimensional. Recently, approximate balancing was proposed as an alternative balancing framework, which resolves the feasibility issue by using inequality moment constraints instead. However, it can be difficult to select the threshold parameters when the number of constraints is large. Moreover, moment constraints may not fully capture the discrepancy of covariate distributions. In this paper, we propose Mahalanobis balancing, which approximately balances covariate distributions from a multivariate perspective. We use a quadratic constraint to control overall imbalance with a single threshold parameter, which can be tuned by a simple selection procedure. We show that the dual problem of Mahalanobis balancing is an l_2 norm-based regularized regression problem, and establish interesting connection to propensity score models. We further generalize Mahalanobis balancing to the high-dimensional scenario. We derive asymptotic properties and make extensive comparisons with existing balancing methods in the numerical studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2204.13439&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yimin Dai, Ying Yan</name></author><category term="stat.ME" /><summary type="html">In the past decade, various exact balancing-based weighting methods were introduced to the causal inference literature. Exact balancing alleviates the extreme weight and model misspecification issues that may incur when one implements inverse probability weighting. It eliminates covariate imbalance by imposing balancing constraints in an optimization problem. The optimization problem can nevertheless be infeasible when there is bad overlap between the covariate distributions in the treated and control groups or when the covariates are high-dimensional. Recently, approximate balancing was proposed as an alternative balancing framework, which resolves the feasibility issue by using inequality moment constraints instead. However, it can be difficult to select the threshold parameters when the number of constraints is large. Moreover, moment constraints may not fully capture the discrepancy of covariate distributions. In this paper, we propose Mahalanobis balancing, which approximately balances covariate distributions from a multivariate perspective. We use a quadratic constraint to control overall imbalance with a single threshold parameter, which can be tuned by a simple selection procedure. We show that the dual problem of Mahalanobis balancing is an l_2 norm-based regularized regression problem, and establish interesting connection to propensity score models. We further generalize Mahalanobis balancing to the high-dimensional scenario. We derive asymptotic properties and make extensive comparisons with existing balancing methods in the numerical studies.</summary></entry><entry><title type="html">Manipulating a Continuous Instrumental Variable in an Observational Study of Premature Babies: Algorithm, Partial Identification Bounds, and Inference under Randomization and Biased Randomization Assumptions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ManipulatingaContinuousInstrumentalVariableinanObservationalStudyofPrematureBabiesAlgorithmPartialIdentificationBoundsandInferenceunderRandomizationandBiasedRandomizationAssumptions.html" rel="alternate" type="text/html" title="Manipulating a Continuous Instrumental Variable in an Observational Study of Premature Babies: Algorithm, Partial Identification Bounds, and Inference under Randomization and Biased Randomization Assumptions" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ManipulatingaContinuousInstrumentalVariableinanObservationalStudyofPrematureBabiesAlgorithmPartialIdentificationBoundsandInferenceunderRandomizationandBiasedRandomizationAssumptions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ManipulatingaContinuousInstrumentalVariableinanObservationalStudyofPrematureBabiesAlgorithmPartialIdentificationBoundsandInferenceunderRandomizationandBiasedRandomizationAssumptions.html">&lt;p&gt;Regionalization of intensive care for premature babies refers to a triage system of mothers with high-risk pregnancies to hospitals of varied capabilities based on risks faced by infants. Due to the limited capacity of high-level hospitals, which are equipped with advanced expertise to provide critical care, understanding the effect of delivering premature babies at such hospitals on infant mortality for different subgroups of high-risk mothers could facilitate the design of an efficient perinatal regionalization system. Towards answering this question, Baiocchi et al. (2010) proposed to strengthen an excess-travel-time-based, continuous instrumental variable (IV) in an IV-based, matched-pair design by switching focus to a smaller cohort amenable to being paired with a larger separation in the IV dose. Three elements changed with the strengthened IV: the study cohort, compliance rate and latent complier subgroup. Here, we introduce a non-bipartite, template matching algorithm that embeds data into a target, pair-randomized encouragement trial which maintains fidelity to the original study cohort while strengthening the IV. We then study randomization-based and IV-dependent, biased-randomization-based inference of partial identification bounds for the sample average treatment effect (SATE) in an IV-based matched pair design, which deviates from the usual effect ratio estimand in that the SATE is agnostic to the IV and who is matched to whom, although a strengthened IV design could narrow the partial identification bounds. Based on our proposed strengthened-IV design, we found that delivering at a high-level NICU reduced preterm babies’ mortality rate compared to a low-level NICU for $81,766 \times 2 = 163,532$ mothers and their preterm babies and the effect appeared to be minimal among non-black, low-risk mothers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17734&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhe Chen, Min Haeng Cho, Bo Zhang</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Regionalization of intensive care for premature babies refers to a triage system of mothers with high-risk pregnancies to hospitals of varied capabilities based on risks faced by infants. Due to the limited capacity of high-level hospitals, which are equipped with advanced expertise to provide critical care, understanding the effect of delivering premature babies at such hospitals on infant mortality for different subgroups of high-risk mothers could facilitate the design of an efficient perinatal regionalization system. Towards answering this question, Baiocchi et al. (2010) proposed to strengthen an excess-travel-time-based, continuous instrumental variable (IV) in an IV-based, matched-pair design by switching focus to a smaller cohort amenable to being paired with a larger separation in the IV dose. Three elements changed with the strengthened IV: the study cohort, compliance rate and latent complier subgroup. Here, we introduce a non-bipartite, template matching algorithm that embeds data into a target, pair-randomized encouragement trial which maintains fidelity to the original study cohort while strengthening the IV. We then study randomization-based and IV-dependent, biased-randomization-based inference of partial identification bounds for the sample average treatment effect (SATE) in an IV-based matched pair design, which deviates from the usual effect ratio estimand in that the SATE is agnostic to the IV and who is matched to whom, although a strengthened IV design could narrow the partial identification bounds. Based on our proposed strengthened-IV design, we found that delivering at a high-level NICU reduced preterm babies’ mortality rate compared to a low-level NICU for $81,766 \times 2 = 163,532$ mothers and their preterm babies and the effect appeared to be minimal among non-black, low-risk mothers.</summary></entry><entry><title type="html">Neutral Pivoting: Strong Bias Correction for Shared Information</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/NeutralPivotingStrongBiasCorrectionforSharedInformation.html" rel="alternate" type="text/html" title="Neutral Pivoting: Strong Bias Correction for Shared Information" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/NeutralPivotingStrongBiasCorrectionforSharedInformation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/NeutralPivotingStrongBiasCorrectionforSharedInformation.html">&lt;p&gt;In the absence of historical data for use as forecasting inputs, decision makers often ask a panel of judges to predict the outcome of interest, leveraging the wisdom of the crowd (Surowiecki 2005). Even if the crowd is large and skilled, shared information can bias the simple mean of judges’ estimates. Addressing the issue of bias, Palley and Soll (2019) introduces a novel approach called pivoting. Pivoting can take several forms, most notably the powerful and reliable minimal pivot. We build on the intuition of the minimal pivot and propose a more aggressive bias correction known as the neutral pivot. The neutral pivot achieves the largest bias correction of its class that both avoids the need to directly estimate crowd composition or skill and maintains a smaller expected squared error than the simple mean for all considered settings. Empirical assessments on real datasets confirm the effectiveness of the neutral pivot compared to current methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17737&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joseph Rilling</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">In the absence of historical data for use as forecasting inputs, decision makers often ask a panel of judges to predict the outcome of interest, leveraging the wisdom of the crowd (Surowiecki 2005). Even if the crowd is large and skilled, shared information can bias the simple mean of judges’ estimates. Addressing the issue of bias, Palley and Soll (2019) introduces a novel approach called pivoting. Pivoting can take several forms, most notably the powerful and reliable minimal pivot. We build on the intuition of the minimal pivot and propose a more aggressive bias correction known as the neutral pivot. The neutral pivot achieves the largest bias correction of its class that both avoids the need to directly estimate crowd composition or skill and maintains a smaller expected squared error than the simple mean for all considered settings. Empirical assessments on real datasets confirm the effectiveness of the neutral pivot compared to current methods.</summary></entry><entry><title type="html">Neyman-Pearson Multi-class Classification via Cost-sensitive Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/NeymanPearsonMulticlassClassificationviaCostsensitiveLearning.html" rel="alternate" type="text/html" title="Neyman-Pearson Multi-class Classification via Cost-sensitive Learning" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/NeymanPearsonMulticlassClassificationviaCostsensitiveLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/NeymanPearsonMulticlassClassificationviaCostsensitiveLearning.html">&lt;p&gt;Most existing classification methods aim to minimize the overall misclassification error rate. However, in applications such as loan default prediction, different types of errors can have varying consequences. To address this asymmetry issue, two popular paradigms have been developed: the Neyman-Pearson (NP) paradigm and the cost-sensitive (CS) paradigm. Previous studies on the NP paradigm have primarily focused on the binary case, while the multi-class NP problem poses a greater challenge due to its unknown feasibility. In this work, we tackle the multi-class NP problem by establishing a connection with the CS problem via strong duality and propose two algorithms. We extend the concept of NP oracle inequalities, crucial in binary classifications, to NP oracle properties in the multi-class context. Our algorithms satisfy these NP oracle properties under certain conditions. Furthermore, we develop practical algorithms to assess the feasibility and strong duality in multi-class NP problems, which can offer practitioners the landscape of a multi-class NP problem with various target error levels. Simulations and real data studies validate the effectiveness of our algorithms. To our knowledge, this is the first study to address the multi-class NP problem with theoretical guarantees. The proposed algorithms have been implemented in the R package \texttt{npcs}, which is available on CRAN.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2111.04597&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ye Tian, Yang Feng</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Most existing classification methods aim to minimize the overall misclassification error rate. However, in applications such as loan default prediction, different types of errors can have varying consequences. To address this asymmetry issue, two popular paradigms have been developed: the Neyman-Pearson (NP) paradigm and the cost-sensitive (CS) paradigm. Previous studies on the NP paradigm have primarily focused on the binary case, while the multi-class NP problem poses a greater challenge due to its unknown feasibility. In this work, we tackle the multi-class NP problem by establishing a connection with the CS problem via strong duality and propose two algorithms. We extend the concept of NP oracle inequalities, crucial in binary classifications, to NP oracle properties in the multi-class context. Our algorithms satisfy these NP oracle properties under certain conditions. Furthermore, we develop practical algorithms to assess the feasibility and strong duality in multi-class NP problems, which can offer practitioners the landscape of a multi-class NP problem with various target error levels. Simulations and real data studies validate the effectiveness of our algorithms. To our knowledge, this is the first study to address the multi-class NP problem with theoretical guarantees. The proposed algorithms have been implemented in the R package \texttt{npcs}, which is available on CRAN.</summary></entry><entry><title type="html">On new omnibus tests of uniformity on the hypersphere</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Onnewomnibustestsofuniformityonthehypersphere.html" rel="alternate" type="text/html" title="On new omnibus tests of uniformity on the hypersphere" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Onnewomnibustestsofuniformityonthehypersphere</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Onnewomnibustestsofuniformityonthehypersphere.html">&lt;p&gt;Two new omnibus tests of uniformity for data on the hypersphere are proposed. The new test statistics exploit closed-form expressions for orthogonal polynomials, feature tuning parameters, and are related to a ``smooth maximum’’ function and the Poisson kernel. We obtain exact moments of the test statistics under uniformity and rotationally symmetric alternatives, and give their null asymptotic distributions. We consider approximate oracle tuning parameters that maximize the power of the tests against known generic alternatives and provide tests that estimate oracle parameters through cross-validated procedures while maintaining the significance level. Numerical experiments explore the effectiveness of null asymptotic distributions and the accuracy of inexpensive approximations of exact null distributions. A simulation study compares the powers of the new tests with other tests of the Sobolev class, showing the benefits of the former. The proposed tests are applied to the study of the (seemingly uniform) nursing times of wild polar bears.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.04519&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alberto Fernández-de-Marcos, Eduardo García-Portugués</name></author><category term="stat.ME" /><summary type="html">Two new omnibus tests of uniformity for data on the hypersphere are proposed. The new test statistics exploit closed-form expressions for orthogonal polynomials, feature tuning parameters, and are related to a ``smooth maximum’’ function and the Poisson kernel. We obtain exact moments of the test statistics under uniformity and rotationally symmetric alternatives, and give their null asymptotic distributions. We consider approximate oracle tuning parameters that maximize the power of the tests against known generic alternatives and provide tests that estimate oracle parameters through cross-validated procedures while maintaining the significance level. Numerical experiments explore the effectiveness of null asymptotic distributions and the accuracy of inexpensive approximations of exact null distributions. A simulation study compares the powers of the new tests with other tests of the Sobolev class, showing the benefits of the former. The proposed tests are applied to the study of the (seemingly uniform) nursing times of wild polar bears.</summary></entry><entry><title type="html">On the Stability of General Bayesian Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/OntheStabilityofGeneralBayesianInference.html" rel="alternate" type="text/html" title="On the Stability of General Bayesian Inference" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/OntheStabilityofGeneralBayesianInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/OntheStabilityofGeneralBayesianInference.html">&lt;p&gt;We study the stability of posterior predictive inferences to the specification of the likelihood model and perturbations of the data generating process. In modern big data analyses, useful broad structural judgements may be elicited from the decision-maker but a level of interpolation is required to arrive at a likelihood model. As a result, an often computationally convenient canonical form is used in place of the decision-maker’s true beliefs. Equally, in practice, observational datasets often contain unforeseen heterogeneities and recording errors and therefore do not necessarily correspond to how the process was idealised by the decision-maker. Acknowledging such imprecisions, a faithful Bayesian analysis should ideally be stable across reasonable equivalence classes of such inputs. We are able to guarantee that traditional Bayesian updating provides stability across only a very strict class of likelihood models and data generating processes, requiring the decision-maker to elicit their beliefs and understand how the data was generated with an unreasonable degree of accuracy. On the other hand, a generalised Bayesian alternative using the $\beta$-divergence loss function is shown to be stable across practical and interpretable neighbourhoods, providing assurances that posterior inferences are not overly dependent on accidentally introduced spurious specifications or data collection errors. We illustrate this in linear regression, binary classification, and mixture modelling examples, showing that stable updating does not compromise the ability to learn about the data generating process. These stability results provide a compelling justification for using generalised Bayes to facilitate inference under simplified canonical models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.13701&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jack Jewson, Jim Q. Smith, Chris Holmes</name></author><category term="stat.ME" /><summary type="html">We study the stability of posterior predictive inferences to the specification of the likelihood model and perturbations of the data generating process. In modern big data analyses, useful broad structural judgements may be elicited from the decision-maker but a level of interpolation is required to arrive at a likelihood model. As a result, an often computationally convenient canonical form is used in place of the decision-maker’s true beliefs. Equally, in practice, observational datasets often contain unforeseen heterogeneities and recording errors and therefore do not necessarily correspond to how the process was idealised by the decision-maker. Acknowledging such imprecisions, a faithful Bayesian analysis should ideally be stable across reasonable equivalence classes of such inputs. We are able to guarantee that traditional Bayesian updating provides stability across only a very strict class of likelihood models and data generating processes, requiring the decision-maker to elicit their beliefs and understand how the data was generated with an unreasonable degree of accuracy. On the other hand, a generalised Bayesian alternative using the $\beta$-divergence loss function is shown to be stable across practical and interpretable neighbourhoods, providing assurances that posterior inferences are not overly dependent on accidentally introduced spurious specifications or data collection errors. We illustrate this in linear regression, binary classification, and mixture modelling examples, showing that stable updating does not compromise the ability to learn about the data generating process. These stability results provide a compelling justification for using generalised Bayes to facilitate inference under simplified canonical models.</summary></entry><entry><title type="html">Out-of-distribution generalization under random, dense distributional shifts</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Outofdistributiongeneralizationunderrandomdensedistributionalshifts.html" rel="alternate" type="text/html" title="Out-of-distribution generalization under random, dense distributional shifts" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Outofdistributiongeneralizationunderrandomdensedistributionalshifts</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Outofdistributiongeneralizationunderrandomdensedistributionalshifts.html">&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Many existing approaches for estimating parameters in settings with distributional shifts operate under an invariance assumption. For example, under covariate shift, it is assumed that p(y&lt;/td&gt;
      &lt;td&gt;x) remains invariant. We refer to such distribution shifts as sparse, since they may be substantial but affect only a part of the data generating system. In contrast, in various real-world settings, shifts might be dense. More specifically, these dense distributional shifts may arise through numerous small and random changes in the population and environment. First, we will discuss empirical evidence for such random dense distributional shifts and explain why commonly used models for distribution shifts-including adversarial approaches-may not be appropriate under these conditions. Then, we will develop tools to infer parameters and make predictions for partially observed, shifted distributions. Finally, we will apply the framework to several real-world data sets and discuss diagnostics to evaluate the fit of the distributional uncertainty model.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18370&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yujin Jeong, Dominik Rothenhäusler</name></author><category term="stat.ME" /><summary type="html">Many existing approaches for estimating parameters in settings with distributional shifts operate under an invariance assumption. For example, under covariate shift, it is assumed that p(y x) remains invariant. We refer to such distribution shifts as sparse, since they may be substantial but affect only a part of the data generating system. In contrast, in various real-world settings, shifts might be dense. More specifically, these dense distributional shifts may arise through numerous small and random changes in the population and environment. First, we will discuss empirical evidence for such random dense distributional shifts and explain why commonly used models for distribution shifts-including adversarial approaches-may not be appropriate under these conditions. Then, we will develop tools to infer parameters and make predictions for partially observed, shifted distributions. Finally, we will apply the framework to several real-world data sets and discuss diagnostics to evaluate the fit of the distributional uncertainty model.</summary></entry><entry><title type="html">PWEXP: An R Package Using Piecewise Exponential Model for Study Design and Event/Timeline Prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/PWEXPAnRPackageUsingPiecewiseExponentialModelforStudyDesignandEventTimelinePrediction.html" rel="alternate" type="text/html" title="PWEXP: An R Package Using Piecewise Exponential Model for Study Design and Event/Timeline Prediction" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/PWEXPAnRPackageUsingPiecewiseExponentialModelforStudyDesignandEventTimelinePrediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/PWEXPAnRPackageUsingPiecewiseExponentialModelforStudyDesignandEventTimelinePrediction.html">&lt;p&gt;Parametric assumptions such as exponential distribution are commonly used in clinical trial design and analysis. However, violation of distribution assumptions can introduce biases in sample size and power calculations. Piecewise exponential (PWE) hazard model partitions the hazard function into segments each with constant hazards and is easy for interpretation and computation. Due to its piecewise property, PWE can fit a wide range of survival curves and accurately predict the future number of events and analysis time in event-driven clinical trials, thus enabling more flexible and reliable study designs. Compared with other existing approaches, the PWE model provides a superior balance of flexibility and robustness in model fitting and prediction. The proposed PWEXP package is designed for estimating and predicting PWE hazard models for right-censored data. By utilizing well-established criteria such as AIC, BIC, and cross-validation log-likelihood, the PWEXP package chooses the optimal number of change-points and determines the optimal position of change-points. With its particular goodness-of-fit, the PWEXP provides accurate and robust hazard estimation, which can be used for reliable power calculation at study design and timeline prediction at study conduct. The package also offers visualization functions to facilitate the interpretation of survival curve fitting results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17772&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tianchen Xu, Rachael Wen</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Parametric assumptions such as exponential distribution are commonly used in clinical trial design and analysis. However, violation of distribution assumptions can introduce biases in sample size and power calculations. Piecewise exponential (PWE) hazard model partitions the hazard function into segments each with constant hazards and is easy for interpretation and computation. Due to its piecewise property, PWE can fit a wide range of survival curves and accurately predict the future number of events and analysis time in event-driven clinical trials, thus enabling more flexible and reliable study designs. Compared with other existing approaches, the PWE model provides a superior balance of flexibility and robustness in model fitting and prediction. The proposed PWEXP package is designed for estimating and predicting PWE hazard models for right-censored data. By utilizing well-established criteria such as AIC, BIC, and cross-validation log-likelihood, the PWEXP package chooses the optimal number of change-points and determines the optimal position of change-points. With its particular goodness-of-fit, the PWEXP provides accurate and robust hazard estimation, which can be used for reliable power calculation at study design and timeline prediction at study conduct. The package also offers visualization functions to facilitate the interpretation of survival curve fitting results.</summary></entry><entry><title type="html">Physics-constrained robust learning of open-form partial differential equations from limited and noisy data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Physicsconstrainedrobustlearningofopenformpartialdifferentialequationsfromlimitedandnoisydata.html" rel="alternate" type="text/html" title="Physics-constrained robust learning of open-form partial differential equations from limited and noisy data" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Physicsconstrainedrobustlearningofopenformpartialdifferentialequationsfromlimitedandnoisydata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Physicsconstrainedrobustlearningofopenformpartialdifferentialequationsfromlimitedandnoisydata.html">&lt;p&gt;Unveiling the underlying governing equations of nonlinear dynamic systems remains a significant challenge. Insufficient prior knowledge hinders the determination of an accurate candidate library, while noisy observations lead to imprecise evaluations, which in turn result in redundant function terms or erroneous equations. This study proposes a framework to robustly uncover open-form partial differential equations (PDEs) from limited and noisy data. The framework operates through two alternating update processes: discovering and embedding. The discovering phase employs symbolic representation and a novel reinforcement learning (RL)-guided hybrid PDE generator to efficiently produce diverse open-form PDEs with tree structures. A neural network-based predictive model fits the system response and serves as the reward evaluator for the generated PDEs. PDEs with higher rewards are utilized to iteratively optimize the generator via the RL strategy and the best-performing PDE is selected by a parameter-free stability metric. The embedding phase integrates the initially identified PDE from the discovering process as a physical constraint into the predictive model for robust training. The traversal of PDE trees automates the construction of the computational graph and the embedding process without human intervention. Numerical experiments demonstrate our framework’s capability to uncover governing equations from nonlinear dynamic systems with limited and highly noisy data and outperform other physics-informed neural network-based discovery methods. This work opens new potential for exploring real-world systems with limited understanding.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.07672&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mengge Du, Yuntian Chen, Longfeng Nie, Siyu Lou, Dongxiao Zhang</name></author><category term="stat.AP" /><summary type="html">Unveiling the underlying governing equations of nonlinear dynamic systems remains a significant challenge. Insufficient prior knowledge hinders the determination of an accurate candidate library, while noisy observations lead to imprecise evaluations, which in turn result in redundant function terms or erroneous equations. This study proposes a framework to robustly uncover open-form partial differential equations (PDEs) from limited and noisy data. The framework operates through two alternating update processes: discovering and embedding. The discovering phase employs symbolic representation and a novel reinforcement learning (RL)-guided hybrid PDE generator to efficiently produce diverse open-form PDEs with tree structures. A neural network-based predictive model fits the system response and serves as the reward evaluator for the generated PDEs. PDEs with higher rewards are utilized to iteratively optimize the generator via the RL strategy and the best-performing PDE is selected by a parameter-free stability metric. The embedding phase integrates the initially identified PDE from the discovering process as a physical constraint into the predictive model for robust training. The traversal of PDE trees automates the construction of the computational graph and the embedding process without human intervention. Numerical experiments demonstrate our framework’s capability to uncover governing equations from nonlinear dynamic systems with limited and highly noisy data and outperform other physics-informed neural network-based discovery methods. This work opens new potential for exploring real-world systems with limited understanding.</summary></entry><entry><title type="html">Pièces de viole des Cinq Livres and their statistical signatures: the musical work of Marin Marais and Jordi Savall</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Pi%C3%A8cesdevioledesCinqLivresandtheirstatisticalsignaturesthemusicalworkofMarinMaraisandJordiSavall.html" rel="alternate" type="text/html" title="Pièces de viole des Cinq Livres and their statistical signatures: the musical work of Marin Marais and Jordi Savall" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Pi%C3%A8cesdevioledesCinqLivresandtheirstatisticalsignaturesthemusicalworkofMarinMaraisandJordiSavall</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Pi%C3%A8cesdevioledesCinqLivresandtheirstatisticalsignaturesthemusicalworkofMarinMaraisandJordiSavall.html">&lt;p&gt;This study analyzes the spectrum of audio signals related to the work of “Pi`eces de viole des Cinq Livres” based on the collaborative work between Marin Marais and Jordi Savall for the underlying musical information. In particular, we explore the identification of possible statistical signatures related to this musical work. Based on the complex systems approach, we compute the spectrum of audio signals, analyze and identify their best-fit statistical distributions, and plot their relative frequencies using the scientific pitch notation. Findings suggest that the collection of frequency components related to the spectrum of each of the books that form this audio work show highly skewed and associated statistical distributions. Therefore, the most frequent statistical distribution that best describes the collection of these audio data and may be associated with a singular statistical signature is the exponential.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18355&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Igor Lugo, Martha G. Alatriste-Contreras</name></author><category term="stat.AP" /><summary type="html">This study analyzes the spectrum of audio signals related to the work of “Pi`eces de viole des Cinq Livres” based on the collaborative work between Marin Marais and Jordi Savall for the underlying musical information. In particular, we explore the identification of possible statistical signatures related to this musical work. Based on the complex systems approach, we compute the spectrum of audio signals, analyze and identify their best-fit statistical distributions, and plot their relative frequencies using the scientific pitch notation. Findings suggest that the collection of frequency components related to the spectrum of each of the books that form this audio work show highly skewed and associated statistical distributions. Therefore, the most frequent statistical distribution that best describes the collection of these audio data and may be associated with a singular statistical signature is the exponential.</summary></entry><entry><title type="html">Randomization-based confidence intervals for the local average treatment effect</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Randomizationbasedconfidenceintervalsforthelocalaveragetreatmenteffect.html" rel="alternate" type="text/html" title="Randomization-based confidence intervals for the local average treatment effect" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Randomizationbasedconfidenceintervalsforthelocalaveragetreatmenteffect</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Randomizationbasedconfidenceintervalsforthelocalaveragetreatmenteffect.html">&lt;p&gt;We consider the problem of generating confidence intervals in randomized experiments with noncompliance. We show that a refinement of a randomization-based procedure proposed by Imbens and Rosenbaum (2005) has desirable properties. Namely, we show that using a studentized Anderson-Rubin-type statistic as a test statistic yields confidence intervals that are finite-sample exact under treatment effect homogeneity, and remain asymptotically valid for the Local Average Treatment Effect when the treatment effect is heterogeneous. We provide a uniform analysis of this procedure.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18786&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>P. M. Aronow, Haoge Chang, Patrick Lopatto</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We consider the problem of generating confidence intervals in randomized experiments with noncompliance. We show that a refinement of a randomization-based procedure proposed by Imbens and Rosenbaum (2005) has desirable properties. Namely, we show that using a studentized Anderson-Rubin-type statistic as a test statistic yields confidence intervals that are finite-sample exact under treatment effect homogeneity, and remain asymptotically valid for the Local Average Treatment Effect when the treatment effect is heterogeneous. We provide a uniform analysis of this procedure.</summary></entry><entry><title type="html">Regularised Spectral Estimation for High-Dimensional Point Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/RegularisedSpectralEstimationforHighDimensionalPointProcesses.html" rel="alternate" type="text/html" title="Regularised Spectral Estimation for High-Dimensional Point Processes" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/RegularisedSpectralEstimationforHighDimensionalPointProcesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/RegularisedSpectralEstimationforHighDimensionalPointProcesses.html">&lt;p&gt;Advances in modern technology have enabled the simultaneous recording of neural spiking activity, which statistically can be represented by a multivariate point process. We characterise the second order structure of this process via the spectral density matrix, a frequency domain equivalent of the covariance matrix. In the context of neuronal analysis, statistics based on the spectral density matrix can be used to infer connectivity in the brain network between individual neurons. However, the high-dimensional nature of spike train data mean that it is often difficult, or at times impossible, to compute these statistics. In this work, we discuss the importance of regularisation-based methods for spectral estimation, and propose novel methodology for use in the point process setting. We establish asymptotic properties for our proposed estimators and evaluate their performance on synthetic data simulated from multivariate Hawkes processes. Finally, we apply our methodology to neuroscience spike train data in order to illustrate its ability to infer brain connectivity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.12908&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carla Pinkney, Carolina Euan, Alex Gibberd, Ali Shojaie</name></author><category term="stat.ME" /><summary type="html">Advances in modern technology have enabled the simultaneous recording of neural spiking activity, which statistically can be represented by a multivariate point process. We characterise the second order structure of this process via the spectral density matrix, a frequency domain equivalent of the covariance matrix. In the context of neuronal analysis, statistics based on the spectral density matrix can be used to infer connectivity in the brain network between individual neurons. However, the high-dimensional nature of spike train data mean that it is often difficult, or at times impossible, to compute these statistics. In this work, we discuss the importance of regularisation-based methods for spectral estimation, and propose novel methodology for use in the point process setting. We establish asymptotic properties for our proposed estimators and evaluate their performance on synthetic data simulated from multivariate Hawkes processes. Finally, we apply our methodology to neuroscience spike train data in order to illustrate its ability to infer brain connectivity.</summary></entry><entry><title type="html">Riemannian Laplace Approximation with the Fisher Metric</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/RiemannianLaplaceApproximationwiththeFisherMetric.html" rel="alternate" type="text/html" title="Riemannian Laplace Approximation with the Fisher Metric" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/RiemannianLaplaceApproximationwiththeFisherMetric</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/RiemannianLaplaceApproximationwiththeFisherMetric.html">&lt;p&gt;Laplace’s method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties depend heavily on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the method, and demonstrating practical improvements in a range of experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.02766&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hanlin Yu, Marcelo Hartmann, Bernardo Williams, Mark Girolami, Arto Klami</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Laplace’s method approximates a target density with a Gaussian distribution at its mode. It is computationally efficient and asymptotically exact for Bayesian inference due to the Bernstein-von Mises theorem, but for complex targets and finite-data posteriors it is often too crude an approximation. A recent generalization of the Laplace Approximation transforms the Gaussian approximation according to a chosen Riemannian geometry providing a richer approximation family, while still retaining computational efficiency. However, as shown here, its properties depend heavily on the chosen metric, indeed the metric adopted in previous work results in approximations that are overly narrow as well as being biased even at the limit of infinite data. We correct this shortcoming by developing the approximation family further, deriving two alternative variants that are exact at the limit of infinite data, extending the theoretical analysis of the method, and demonstrating practical improvements in a range of experiments.</summary></entry><entry><title type="html">Robust Bayesian Inference for Berkson and Classical Measurement Error Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/RobustBayesianInferenceforBerksonandClassicalMeasurementErrorModels.html" rel="alternate" type="text/html" title="Robust Bayesian Inference for Berkson and Classical Measurement Error Models" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/RobustBayesianInferenceforBerksonandClassicalMeasurementErrorModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/RobustBayesianInferenceforBerksonandClassicalMeasurementErrorModels.html">&lt;p&gt;Measurement error occurs when a covariate influencing a response variable is corrupted by noise. This can lead to misleading inference outcomes, particularly in problems where accurately estimating the relationship between covariates and response variables is crucial, such as causal effect estimation. Existing methods for dealing with measurement error often rely on strong assumptions such as knowledge of the error distribution or its variance and availability of replicated measurements of the covariates. We propose a Bayesian Nonparametric Learning framework that is robust to mismeasured covariates, does not require the preceding assumptions, and can incorporate prior beliefs about the error distribution. This approach gives rise to a general framework that is suitable for both Classical and Berkson error models via the appropriate specification of the prior centering measure of a Dirichlet Process (DP). Moreover, it offers flexibility in the choice of loss function depending on the type of regression model. We provide bounds on the generalization error based on the Maximum Mean Discrepancy (MMD) loss which allows for generalization to non-Gaussian distributed errors and nonlinear covariate-response relationships. We showcase the effectiveness of the proposed framework versus prior art in real-world problems containing either Berkson or Classical measurement errors.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.01468&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Charita Dellaporta, Theodoros Damoulas</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Measurement error occurs when a covariate influencing a response variable is corrupted by noise. This can lead to misleading inference outcomes, particularly in problems where accurately estimating the relationship between covariates and response variables is crucial, such as causal effect estimation. Existing methods for dealing with measurement error often rely on strong assumptions such as knowledge of the error distribution or its variance and availability of replicated measurements of the covariates. We propose a Bayesian Nonparametric Learning framework that is robust to mismeasured covariates, does not require the preceding assumptions, and can incorporate prior beliefs about the error distribution. This approach gives rise to a general framework that is suitable for both Classical and Berkson error models via the appropriate specification of the prior centering measure of a Dirichlet Process (DP). Moreover, it offers flexibility in the choice of loss function depending on the type of regression model. We provide bounds on the generalization error based on the Maximum Mean Discrepancy (MMD) loss which allows for generalization to non-Gaussian distributed errors and nonlinear covariate-response relationships. We showcase the effectiveness of the proposed framework versus prior art in real-world problems containing either Berkson or Classical measurement errors.</summary></entry><entry><title type="html">Semiparametric causal mediation analysis in cluster-randomized experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Semiparametriccausalmediationanalysisinclusterrandomizedexperiments.html" rel="alternate" type="text/html" title="Semiparametric causal mediation analysis in cluster-randomized experiments" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Semiparametriccausalmediationanalysisinclusterrandomizedexperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Semiparametriccausalmediationanalysisinclusterrandomizedexperiments.html">&lt;p&gt;In cluster-randomized experiments, there is emerging interest in exploring the causal mechanism in which a cluster-level treatment affects the outcome through an intermediate outcome. Despite an extensive development of causal mediation methods in the past decade, only a few exceptions have been considered in assessing causal mediation in cluster-randomized studies, all of which depend on parametric model-based estimators. In this article, we develop the formal semiparametric efficiency theory to motivate several doubly-robust methods for addressing several mediation effect estimands corresponding to both the cluster-average and the individual-level treatment effects in cluster-randomized experiments–the natural indirect effect, natural direct effect, and spillover mediation effect. We derive the efficient influence function for each mediation effect, and carefully parameterize each efficient influence function to motivate practical strategies for operationalizing each estimator. We consider both parametric working models and data-adaptive machine learners to estimate the nuisance functions, and obtain semiparametric efficient causal mediation estimators in the latter case. Our methods are illustrated via extensive simulations and two completed cluster-randomized experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18256&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chao Cheng, Fan Li</name></author><category term="stat.ME" /><summary type="html">In cluster-randomized experiments, there is emerging interest in exploring the causal mechanism in which a cluster-level treatment affects the outcome through an intermediate outcome. Despite an extensive development of causal mediation methods in the past decade, only a few exceptions have been considered in assessing causal mediation in cluster-randomized studies, all of which depend on parametric model-based estimators. In this article, we develop the formal semiparametric efficiency theory to motivate several doubly-robust methods for addressing several mediation effect estimands corresponding to both the cluster-average and the individual-level treatment effects in cluster-randomized experiments–the natural indirect effect, natural direct effect, and spillover mediation effect. We derive the efficient influence function for each mediation effect, and carefully parameterize each efficient influence function to motivate practical strategies for operationalizing each estimator. We consider both parametric working models and data-adaptive machine learners to estimate the nuisance functions, and obtain semiparametric efficient causal mediation estimators in the latter case. Our methods are illustrated via extensive simulations and two completed cluster-randomized experiments.</summary></entry><entry><title type="html">Semiparametric fiducial inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Semiparametricfiducialinference.html" rel="alternate" type="text/html" title="Semiparametric fiducial inference" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Semiparametricfiducialinference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Semiparametricfiducialinference.html">&lt;p&gt;R. A. Fisher introduced the concept of fiducial as a potential replacement for the Bayesian posterior distribution in the 1930s. During the past century, fiducial approaches have been explored in various parametric and nonparametric settings. However, to the best of our knowledge, no fiducial inference has been developed in the realm of semiparametric statistics. In this paper, we propose a novel fiducial approach for semiparametric models. To streamline our presentation, we use the Cox proportional hazards model, which is the most popular model for the analysis of survival data, as a running example. Other models and extensions are also discussed. In our experiments, we find our method to perform well especially in situations when the maximum likelihood estimator fails.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18779&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yifan Cui, Jan Hannig, Paul Edlefsen</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">R. A. Fisher introduced the concept of fiducial as a potential replacement for the Bayesian posterior distribution in the 1930s. During the past century, fiducial approaches have been explored in various parametric and nonparametric settings. However, to the best of our knowledge, no fiducial inference has been developed in the realm of semiparametric statistics. In this paper, we propose a novel fiducial approach for semiparametric models. To streamline our presentation, we use the Cox proportional hazards model, which is the most popular model for the analysis of survival data, as a running example. Other models and extensions are also discussed. In our experiments, we find our method to perform well especially in situations when the maximum likelihood estimator fails.</summary></entry><entry><title type="html">Semiparametric mean and variance joint models with Laplace link functions for count time series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/SemiparametricmeanandvariancejointmodelswithLaplacelinkfunctionsforcounttimeseries.html" rel="alternate" type="text/html" title="Semiparametric mean and variance joint models with Laplace link functions for count time series" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/SemiparametricmeanandvariancejointmodelswithLaplacelinkfunctionsforcounttimeseries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/SemiparametricmeanandvariancejointmodelswithLaplacelinkfunctionsforcounttimeseries.html">&lt;p&gt;Count time series data are frequently analyzed by modeling their conditional means and the conditional variance is often considered to be a deterministic function of the corresponding conditional mean and is not typically modeled independently. We propose a semiparametric mean and variance joint model, called random rounded count-valued generalized autoregressive conditional heteroskedastic (RRC-GARCH) model, to address this limitation. The RRC-GARCH model and its variations allow for the joint modeling of both the conditional mean and variance and offer a flexible framework for capturing various mean-variance structures (MVSs). One main feature of this model is its ability to accommodate negative values for regression coefficients and autocorrelation functions. The autocorrelation structure of the RRC-GARCH model using the proposed Laplace link functions with nonnegative regression coefficients is the same as that of an autoregressive moving-average (ARMA) process. For the new model, the stationarity and ergodicity are established and the consistency and asymptotic normality of the conditional least squares estimator are proved. Model selection criteria are proposed to evaluate the RRC-GARCH models. The performance of the RRC-GARCH model is assessed through analyses of both simulated and real data sets. The results indicate that the model can effectively capture the MVS of count time series data and generate accurate forecast means and variances.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18421&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tianqing Liu, Xiaohui Yuan</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Count time series data are frequently analyzed by modeling their conditional means and the conditional variance is often considered to be a deterministic function of the corresponding conditional mean and is not typically modeled independently. We propose a semiparametric mean and variance joint model, called random rounded count-valued generalized autoregressive conditional heteroskedastic (RRC-GARCH) model, to address this limitation. The RRC-GARCH model and its variations allow for the joint modeling of both the conditional mean and variance and offer a flexible framework for capturing various mean-variance structures (MVSs). One main feature of this model is its ability to accommodate negative values for regression coefficients and autocorrelation functions. The autocorrelation structure of the RRC-GARCH model using the proposed Laplace link functions with nonnegative regression coefficients is the same as that of an autoregressive moving-average (ARMA) process. For the new model, the stationarity and ergodicity are established and the consistency and asymptotic normality of the conditional least squares estimator are proved. Model selection criteria are proposed to evaluate the RRC-GARCH models. The performance of the RRC-GARCH model is assessed through analyses of both simulated and real data sets. The results indicate that the model can effectively capture the MVS of count time series data and generate accurate forecast means and variances.</summary></entry><entry><title type="html">Sensitivity Analysis for Linear Estimators</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/SensitivityAnalysisforLinearEstimators.html" rel="alternate" type="text/html" title="Sensitivity Analysis for Linear Estimators" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/SensitivityAnalysisforLinearEstimators</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/SensitivityAnalysisforLinearEstimators.html">&lt;p&gt;We propose a novel sensitivity analysis framework for linear estimators with identification failures that can be viewed as seeing the wrong outcome distribution. Our approach measures the degree of identification failure through the change in measure between the observed distribution and a hypothetical target distribution that would identify the causal parameter of interest. The framework yields a sensitivity analysis that generalizes existing bounds for Average Potential Outcome (APO), Regression Discontinuity (RD), and instrumental variables (IV) exclusion failure designs. Our partial identification results extend results from the APO context to allow even unbounded likelihood ratios. Our proposed sensitivity analysis consistently estimates sharp bounds under plausible conditions and estimates valid bounds under mild conditions. We find that our method performs well in simulations even when targeting a discontinuous and nearly infinite bound.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.06305&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jacob Dorn, Luther Yap</name></author><category term="stat.ME" /><summary type="html">We propose a novel sensitivity analysis framework for linear estimators with identification failures that can be viewed as seeing the wrong outcome distribution. Our approach measures the degree of identification failure through the change in measure between the observed distribution and a hypothetical target distribution that would identify the causal parameter of interest. The framework yields a sensitivity analysis that generalizes existing bounds for Average Potential Outcome (APO), Regression Discontinuity (RD), and instrumental variables (IV) exclusion failure designs. Our partial identification results extend results from the APO context to allow even unbounded likelihood ratios. Our proposed sensitivity analysis consistently estimates sharp bounds under plausible conditions and estimates valid bounds under mild conditions. We find that our method performs well in simulations even when targeting a discontinuous and nearly infinite bound.</summary></entry><entry><title type="html">Sequential model confidence</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Sequentialmodelconfidence.html" rel="alternate" type="text/html" title="Sequential model confidence" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Sequentialmodelconfidence</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Sequentialmodelconfidence.html">&lt;p&gt;In most prediction and estimation situations, scientists consider various statistical models for the same problem, and naturally want to select amongst the best. Hansen et al. (2011) provide a powerful solution to this problem by the so-called model confidence set, a subset of the original set of available models that contains the best models with a given level of confidence. Importantly, model confidence sets respect the underlying selection uncertainty by being flexible in size. However, they presuppose a fixed sample size which stands in contrast to the fact that model selection and forecast evaluation are inherently sequential tasks where we successively collect new data and where the decision to continue or conclude a study may depend on the previous outcomes. In this article, we extend model confidence sets sequentially over time by relying on sequential testing methods. Recently, e-processes and confidence sequences have been introduced as new, safe methods for assessing statistical evidence. Sequential model confidence sets allow to continuously monitor the models’ performances and come with time-uniform, nonasymptotic coverage guarantees.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18678&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sebastian Arnold, Georgios Gavrilopoulos, Benedikt Schulz, Johanna Ziegel</name></author><category term="stat.ME" /><summary type="html">In most prediction and estimation situations, scientists consider various statistical models for the same problem, and naturally want to select amongst the best. Hansen et al. (2011) provide a powerful solution to this problem by the so-called model confidence set, a subset of the original set of available models that contains the best models with a given level of confidence. Importantly, model confidence sets respect the underlying selection uncertainty by being flexible in size. However, they presuppose a fixed sample size which stands in contrast to the fact that model selection and forecast evaluation are inherently sequential tasks where we successively collect new data and where the decision to continue or conclude a study may depend on the previous outcomes. In this article, we extend model confidence sets sequentially over time by relying on sequential testing methods. Recently, e-processes and confidence sequences have been introduced as new, safe methods for assessing statistical evidence. Sequential model confidence sets allow to continuously monitor the models’ performances and come with time-uniform, nonasymptotic coverage guarantees.</summary></entry><entry><title type="html">Sequential monitoring for explosive volatility regimes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Sequentialmonitoringforexplosivevolatilityregimes.html" rel="alternate" type="text/html" title="Sequential monitoring for explosive volatility regimes" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Sequentialmonitoringforexplosivevolatilityregimes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Sequentialmonitoringforexplosivevolatilityregimes.html">&lt;p&gt;In this paper, we develop two families of sequential monitoring procedure to (timely) detect changes in a GARCH(1,1) model. Whilst our methodologies can be applied for the general analysis of changepoints in GARCH(1,1) sequences, they are in particular designed to detect changes from stationarity to explosivity or vice versa, thus allowing to check for volatility bubbles. Our statistics can be applied irrespective of whether the historical sample is stationary or not, and indeed without prior knowledge of the regime of the observations before and after the break. In particular, we construct our detectors as the CUSUM process of the quasi-Fisher scores of the log likelihood function. In order to ensure timely detection, we then construct our boundary function (exceeding which would indicate a break) by including a weighting sequence which is designed to shorten the detection delay in the presence of a changepoint. We consider two types of weights: a lighter set of weights, which ensures timely detection in the presence of changes occurring early, but not too early after the end of the historical sample; and a heavier set of weights, called Renyi weights which is designed to ensure timely detection in the presence of changepoints occurring very early in the monitoring horizon. In both cases, we derive the limiting distribution of the detection delays, indicating the expected delay for each set of weights. Our theoretical results are validated via a comprehensive set of simulations, and an empirical application to daily returns of individual stocks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17885&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lajos Horvath, Lorenzo Trapani, Shixuan Wang</name></author><category term="stat.ME" /><summary type="html">In this paper, we develop two families of sequential monitoring procedure to (timely) detect changes in a GARCH(1,1) model. Whilst our methodologies can be applied for the general analysis of changepoints in GARCH(1,1) sequences, they are in particular designed to detect changes from stationarity to explosivity or vice versa, thus allowing to check for volatility bubbles. Our statistics can be applied irrespective of whether the historical sample is stationary or not, and indeed without prior knowledge of the regime of the observations before and after the break. In particular, we construct our detectors as the CUSUM process of the quasi-Fisher scores of the log likelihood function. In order to ensure timely detection, we then construct our boundary function (exceeding which would indicate a break) by including a weighting sequence which is designed to shorten the detection delay in the presence of a changepoint. We consider two types of weights: a lighter set of weights, which ensures timely detection in the presence of changes occurring early, but not too early after the end of the historical sample; and a heavier set of weights, called Renyi weights which is designed to ensure timely detection in the presence of changepoints occurring very early in the monitoring horizon. In both cases, we derive the limiting distribution of the detection delays, indicating the expected delay for each set of weights. Our theoretical results are validated via a comprehensive set of simulations, and an empirical application to daily returns of individual stocks.</summary></entry><entry><title type="html">Singular-value statistics of directed random graphs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Singularvaluestatisticsofdirectedrandomgraphs.html" rel="alternate" type="text/html" title="Singular-value statistics of directed random graphs" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Singularvaluestatisticsofdirectedrandomgraphs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Singularvaluestatisticsofdirectedrandomgraphs.html">&lt;p&gt;Singular-value statistics (SVS) has been recently presented as a random matrix theory tool able to properly characterize non-Hermitian random matrix ensembles [PRX Quantum {\bf 4}, 040312 (2023)]. Here, we perform a numerical study of the SVS of the non-Hermitian adjacency matrices $\mathbf{A}$ of directed random graphs, where $\mathbf{A}$ are members of diluted real Ginibre ensembles. We consider two models of directed random graphs: Erd&quot;os-R&apos;enyi graphs and random regular graphs. Specifically, we focus on the ratio $r$ between nearest neighbor singular values and the minimum singular value $\lambda_{min}$. We show that $\langle r \rangle$ (where $\langle \cdot \rangle$ represents ensemble average) can effectively characterize the transition between mostly isolated vertices to almost complete graphs, while the probability density function of $\lambda_{min}$ can clearly distinguish between different graph models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18259&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>J. A. Mendez-Bermudez, R. Aguilar-Sanchez</name></author><category term="stat.AP" /><summary type="html">Singular-value statistics (SVS) has been recently presented as a random matrix theory tool able to properly characterize non-Hermitian random matrix ensembles [PRX Quantum {\bf 4}, 040312 (2023)]. Here, we perform a numerical study of the SVS of the non-Hermitian adjacency matrices $\mathbf{A}$ of directed random graphs, where $\mathbf{A}$ are members of diluted real Ginibre ensembles. We consider two models of directed random graphs: Erd&quot;os-R&apos;enyi graphs and random regular graphs. Specifically, we focus on the ratio $r$ between nearest neighbor singular values and the minimum singular value $\lambda_{min}$. We show that $\langle r \rangle$ (where $\langle \cdot \rangle$ represents ensemble average) can effectively characterize the transition between mostly isolated vertices to almost complete graphs, while the probability density function of $\lambda_{min}$ can clearly distinguish between different graph models.</summary></entry><entry><title type="html">Subsampling Error in Stochastic Gradient Langevin Diffusions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/SubsamplingErrorinStochasticGradientLangevinDiffusions.html" rel="alternate" type="text/html" title="Subsampling Error in Stochastic Gradient Langevin Diffusions" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/SubsamplingErrorinStochasticGradientLangevinDiffusions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/SubsamplingErrorinStochasticGradientLangevinDiffusions.html">&lt;p&gt;The Stochastic Gradient Langevin Dynamics (SGLD) are popularly used to approximate Bayesian posterior distributions in statistical learning procedures with large-scale data. As opposed to many usual Markov chain Monte Carlo (MCMC) algorithms, SGLD is not stationary with respect to the posterior distribution; two sources of error appear: The first error is introduced by an Euler–Maruyama discretisation of a Langevin diffusion process, the second error comes from the data subsampling that enables its use in large-scale data settings. In this work, we consider an idealised version of SGLD to analyse the method’s pure subsampling error that we then see as a best-case error for diffusion-based subsampling MCMC methods. Indeed, we introduce and study the Stochastic Gradient Langevin Diffusion (SGLDiff), a continuous-time Markov process that follows the Langevin diffusion corresponding to a data subset and switches this data subset after exponential waiting times. There, we show the exponential ergodicity of SLGDiff and that the Wasserstein distance between the posterior and the limiting distribution of SGLDiff is bounded above by a fractional power of the mean waiting time. We bring our results into context with other analyses of SGLD.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.13882&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kexin Jin, Chenguang Liu, Jonas Latz</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">The Stochastic Gradient Langevin Dynamics (SGLD) are popularly used to approximate Bayesian posterior distributions in statistical learning procedures with large-scale data. As opposed to many usual Markov chain Monte Carlo (MCMC) algorithms, SGLD is not stationary with respect to the posterior distribution; two sources of error appear: The first error is introduced by an Euler–Maruyama discretisation of a Langevin diffusion process, the second error comes from the data subsampling that enables its use in large-scale data settings. In this work, we consider an idealised version of SGLD to analyse the method’s pure subsampling error that we then see as a best-case error for diffusion-based subsampling MCMC methods. Indeed, we introduce and study the Stochastic Gradient Langevin Diffusion (SGLDiff), a continuous-time Markov process that follows the Langevin diffusion corresponding to a data subset and switches this data subset after exponential waiting times. There, we show the exponential ergodicity of SLGDiff and that the Wasserstein distance between the posterior and the limiting distribution of SGLDiff is bounded above by a fractional power of the mean waiting time. We bring our results into context with other analyses of SGLD.</summary></entry><entry><title type="html">Switching Models of Oscillatory Networks Greatly Improve Inference of Dynamic Functional Connectivity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/SwitchingModelsofOscillatoryNetworksGreatlyImproveInferenceofDynamicFunctionalConnectivity.html" rel="alternate" type="text/html" title="Switching Models of Oscillatory Networks Greatly Improve Inference of Dynamic Functional Connectivity" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/SwitchingModelsofOscillatoryNetworksGreatlyImproveInferenceofDynamicFunctionalConnectivity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/SwitchingModelsofOscillatoryNetworksGreatlyImproveInferenceofDynamicFunctionalConnectivity.html">&lt;p&gt;Functional brain networks can change rapidly as a function of stimuli or cognitive shifts. Tracking dynamic functional connectivity is particularly challenging as it requires estimating the structure of the network at each moment as well as how it is shifting through time. In this paper, we describe a general modeling framework and a set of specific models that provides substantially increased statistical power for estimating rhythmic dynamic networks, based on the assumption that for a particular experiment or task, the network state at any moment is chosen from a discrete set of possible network modes. Each model is comprised of three components: (1) a set of latent switching states that represent transitions between the expression of each network mode; (2) a set of latent oscillators, each characterized by an estimated mean oscillation frequency and an instantaneous phase and amplitude at each time point; and (3) an observation model that relates the observed activity at each electrode to a linear combination of the latent oscillators. We develop an expectation-maximization procedure to estimate the network structure for each switching state and the probability of each state being expressed at each moment. We conduct a set of simulation studies to illustrate the application of these models and quantify their statistical power, even in the face of model misspecification.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18854&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wan-Chi Hsin, Uri T. Eden, Emily P. Stephen</name></author><category term="stat.ME" /><summary type="html">Functional brain networks can change rapidly as a function of stimuli or cognitive shifts. Tracking dynamic functional connectivity is particularly challenging as it requires estimating the structure of the network at each moment as well as how it is shifting through time. In this paper, we describe a general modeling framework and a set of specific models that provides substantially increased statistical power for estimating rhythmic dynamic networks, based on the assumption that for a particular experiment or task, the network state at any moment is chosen from a discrete set of possible network modes. Each model is comprised of three components: (1) a set of latent switching states that represent transitions between the expression of each network mode; (2) a set of latent oscillators, each characterized by an estimated mean oscillation frequency and an instantaneous phase and amplitude at each time point; and (3) an observation model that relates the observed activity at each electrode to a linear combination of the latent oscillators. We develop an expectation-maximization procedure to estimate the network structure for each switching state and the probability of each state being expressed at each moment. We conduct a set of simulation studies to illustrate the application of these models and quantify their statistical power, even in the face of model misspecification.</summary></entry><entry><title type="html">Testing for no effect in regression problems: a permutation approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Testingfornoeffectinregressionproblemsapermutationapproach.html" rel="alternate" type="text/html" title="Testing for no effect in regression problems: a permutation approach" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Testingfornoeffectinregressionproblemsapermutationapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Testingfornoeffectinregressionproblemsapermutationapproach.html">&lt;p&gt;Often the question arises whether $Y$ can be predicted based on $X$ using a certain model. Especially for highly flexible models such as neural networks one may ask whether a seemingly good prediction is actually better than fitting pure noise or whether it has to be attributed to the flexibility of the model. This paper proposes a rigorous permutation test to assess whether the prediction is better than the prediction of pure noise. The test avoids any sample splitting and is based instead on generating new pairings of $(X_i,Y_j)$. It introduces a new formulation of the null hypothesis and rigorous justification for the test, which distinguishes it from previous literature. The theoretical findings are applied both to simulated data and to sensor data of tennis serves in an experimental context. The simulation study underscores how the available information affects the test. It shows that the less informative the predictors, the lower the probability of rejecting the null hypothesis of fitting pure noise and emphasizes that detecting weaker dependence between variables requires a sufficient sample size.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.02685&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Micha{\l} Ciszewski, Jakob Söhl, Ton Leenen, Bart van Trigt, Geurt Jongbloed</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Often the question arises whether $Y$ can be predicted based on $X$ using a certain model. Especially for highly flexible models such as neural networks one may ask whether a seemingly good prediction is actually better than fitting pure noise or whether it has to be attributed to the flexibility of the model. This paper proposes a rigorous permutation test to assess whether the prediction is better than the prediction of pure noise. The test avoids any sample splitting and is based instead on generating new pairings of $(X_i,Y_j)$. It introduces a new formulation of the null hypothesis and rigorous justification for the test, which distinguishes it from previous literature. The theoretical findings are applied both to simulated data and to sensor data of tennis serves in an experimental context. The simulation study underscores how the available information affects the test. It shows that the less informative the predictors, the lower the probability of rejecting the null hypothesis of fitting pure noise and emphasizes that detecting weaker dependence between variables requires a sufficient sample size.</summary></entry><entry><title type="html">Testing for similarity of dose response in multi-regional clinical trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Testingforsimilarityofdoseresponseinmultiregionalclinicaltrials.html" rel="alternate" type="text/html" title="Testing for similarity of dose response in multi-regional clinical trials" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Testingforsimilarityofdoseresponseinmultiregionalclinicaltrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Testingforsimilarityofdoseresponseinmultiregionalclinicaltrials.html">&lt;p&gt;This paper addresses the problem of deciding whether the dose response relationships between subgroups and the full population in a multi-regional trial are similar to each other. Similarity is measured in terms of the maximal deviation between the dose response curves. We consider a parametric framework and develop two powerful bootstrap tests for the similarity between the dose response curves of one subgroup and the full population, and for the similarity between the dose response curves of several subgroups and the full population. We prove the validity of the tests, investigate the finite sample properties by means of a simulation study and finally illustrate the methodology in a case study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17682&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Holger Dette, Lukas Koletzko, Frank Bretz</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This paper addresses the problem of deciding whether the dose response relationships between subgroups and the full population in a multi-regional trial are similar to each other. Similarity is measured in terms of the maximal deviation between the dose response curves. We consider a parametric framework and develop two powerful bootstrap tests for the similarity between the dose response curves of one subgroup and the full population, and for the similarity between the dose response curves of several subgroups and the full population. We prove the validity of the tests, investigate the finite sample properties by means of a simulation study and finally illustrate the methodology in a case study.</summary></entry><entry><title type="html">Thinking inside the bounds: Improved error distributions for indifference point data analysis and simulation via beta regression using common discounting functions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ThinkinginsidetheboundsImprovederrordistributionsforindifferencepointdataanalysisandsimulationviabetaregressionusingcommondiscountingfunctions.html" rel="alternate" type="text/html" title="Thinking inside the bounds: Improved error distributions for indifference point data analysis and simulation via beta regression using common discounting functions" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ThinkinginsidetheboundsImprovederrordistributionsforindifferencepointdataanalysisandsimulationviabetaregressionusingcommondiscountingfunctions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/ThinkinginsidetheboundsImprovederrordistributionsforindifferencepointdataanalysisandsimulationviabetaregressionusingcommondiscountingfunctions.html">&lt;p&gt;Standard nonlinear regression is commonly used when modeling indifference points due to its ability to closely follow observed data, resulting in a good model fit. However, standard nonlinear regression currently lacks a reasonable distribution-based framework for indifference points, which limits its ability to adequately describe the inherent variability in the data. Software commonly assumes data follow a normal distribution with constant variance. However, typical indifference points do not follow a normal distribution or exhibit constant variance. To address these limitations, this paper introduces a class of nonlinear beta regression models that offers excellent fit to discounting data and enhances simulation-based approaches. This beta regression model can accommodate popular discounting functions. This work proposes three specific advances. First, our model automatically captures non-constant variance as a function of delay. Second, our model improves simulation-based approaches since it obeys the natural boundaries of observable data, unlike the ordinary assumption of normal residuals and constant variance. Finally, we introduce a scale-location-truncation trick that allows beta regression to accommodate observed values of zero and one. A comparison between beta regression and standard nonlinear regression reveals close agreement in the estimated discounting rate k obtained from both methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18000&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mingang Kim, Mikhail N. Koffarnus, Christopher T Franck</name></author><category term="stat.ME" /><summary type="html">Standard nonlinear regression is commonly used when modeling indifference points due to its ability to closely follow observed data, resulting in a good model fit. However, standard nonlinear regression currently lacks a reasonable distribution-based framework for indifference points, which limits its ability to adequately describe the inherent variability in the data. Software commonly assumes data follow a normal distribution with constant variance. However, typical indifference points do not follow a normal distribution or exhibit constant variance. To address these limitations, this paper introduces a class of nonlinear beta regression models that offers excellent fit to discounting data and enhances simulation-based approaches. This beta regression model can accommodate popular discounting functions. This work proposes three specific advances. First, our model automatically captures non-constant variance as a function of delay. Second, our model improves simulation-based approaches since it obeys the natural boundaries of observable data, unlike the ordinary assumption of normal residuals and constant variance. Finally, we introduce a scale-location-truncation trick that allows beta regression to accommodate observed values of zero and one. A comparison between beta regression and standard nonlinear regression reveals close agreement in the estimated discounting rate k obtained from both methods.</summary></entry><entry><title type="html">Tucker tensor factor models: matricization and mode-wise PCA estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/TuckertensorfactormodelsmatricizationandmodewisePCAestimation.html" rel="alternate" type="text/html" title="Tucker tensor factor models: matricization and mode-wise PCA estimation" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/TuckertensorfactormodelsmatricizationandmodewisePCAestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/TuckertensorfactormodelsmatricizationandmodewisePCAestimation.html">&lt;p&gt;High-dimensional, higher-order tensor data are gaining prominence in a variety of fields, including but not limited to computer vision and network analysis. Tensor factor models, induced from noisy versions of tensor decompositions or factorizations, are natural potent instruments to study a collection of tensor-variate objects that may be dependent or independent. However, it is still in the early stage of developing statistical inferential theories for the estimation of various low-rank structures, which are customary to play the role of signals of tensor factor models. In this paper, we attempt to ``decode” the estimation of a higher-order tensor factor model by leveraging tensor matricization. Specifically, we recast it into mode-wise traditional high-dimensional vector/fiber factor models, enabling the deployment of conventional principal components analysis (PCA) for estimation. Demonstrated by the Tucker tensor factor model (TuTFaM), which is induced from the noisy version of the widely-used Tucker decomposition, we summarize that estimations on signal components are essentially mode-wise PCA techniques, and the involvement of projection and iteration will enhance the signal-to-noise ratio to various extent. We establish the inferential theory of the proposed estimators, conduct rich simulation experiments, and illustrate how the proposed estimations can work in tensor reconstruction, and clustering for independent video and dependent economic datasets, respectively.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2206.02508&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xu Zhang, Guodong Li, Catherine C. Liu, Jianhua Guo</name></author><category term="stat.ME" /><summary type="html">High-dimensional, higher-order tensor data are gaining prominence in a variety of fields, including but not limited to computer vision and network analysis. Tensor factor models, induced from noisy versions of tensor decompositions or factorizations, are natural potent instruments to study a collection of tensor-variate objects that may be dependent or independent. However, it is still in the early stage of developing statistical inferential theories for the estimation of various low-rank structures, which are customary to play the role of signals of tensor factor models. In this paper, we attempt to ``decode” the estimation of a higher-order tensor factor model by leveraging tensor matricization. Specifically, we recast it into mode-wise traditional high-dimensional vector/fiber factor models, enabling the deployment of conventional principal components analysis (PCA) for estimation. Demonstrated by the Tucker tensor factor model (TuTFaM), which is induced from the noisy version of the widely-used Tucker decomposition, we summarize that estimations on signal components are essentially mode-wise PCA techniques, and the involvement of projection and iteration will enhance the signal-to-noise ratio to various extent. We establish the inferential theory of the proposed estimators, conduct rich simulation experiments, and illustrate how the proposed estimations can work in tensor reconstruction, and clustering for independent video and dependent economic datasets, respectively.</summary></entry><entry><title type="html">Two-way Homogeneity Pursuit for Quantile Network Vector Autoregression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/TwowayHomogeneityPursuitforQuantileNetworkVectorAutoregression.html" rel="alternate" type="text/html" title="Two-way Homogeneity Pursuit for Quantile Network Vector Autoregression" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/TwowayHomogeneityPursuitforQuantileNetworkVectorAutoregression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/TwowayHomogeneityPursuitforQuantileNetworkVectorAutoregression.html">&lt;p&gt;While the Vector Autoregression (VAR) model has received extensive attention for modelling complex time series, quantile VAR analysis remains relatively underexplored for high-dimensional time series data. To address this disparity, we introduce a two-way grouped network quantile (TGNQ) autoregression model for time series collected on large-scale networks, known for their significant heterogeneous and directional interactions among nodes. Our proposed model simultaneously conducts node clustering and model estimation to balance complexity and interpretability. To account for the directional influence among network nodes, each network node is assigned two latent group memberships that can be consistently estimated using our proposed estimation procedure. Theoretical analysis demonstrates the consistency of membership and parameter estimators even with an overspecified number of groups. With the correct group specification, estimated parameters are proven to be asymptotically normal, enabling valid statistical inferences. Moreover, we propose a quantile information criterion for consistently selecting the number of groups. Simulation studies show promising finite sample performance, and we apply the methodology to analyze connectedness and risk spillover effects among Chinese A-share stocks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18732&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wenyang Liu, Ganggang Xu, Jianqing Fan, Xuening Zhu</name></author><category term="stat.ME" /><summary type="html">While the Vector Autoregression (VAR) model has received extensive attention for modelling complex time series, quantile VAR analysis remains relatively underexplored for high-dimensional time series data. To address this disparity, we introduce a two-way grouped network quantile (TGNQ) autoregression model for time series collected on large-scale networks, known for their significant heterogeneous and directional interactions among nodes. Our proposed model simultaneously conducts node clustering and model estimation to balance complexity and interpretability. To account for the directional influence among network nodes, each network node is assigned two latent group memberships that can be consistently estimated using our proposed estimation procedure. Theoretical analysis demonstrates the consistency of membership and parameter estimators even with an overspecified number of groups. With the correct group specification, estimated parameters are proven to be asymptotically normal, enabling valid statistical inferences. Moreover, we propose a quantile information criterion for consistently selecting the number of groups. Simulation studies show promising finite sample performance, and we apply the methodology to analyze connectedness and risk spillover effects among Chinese A-share stocks.</summary></entry><entry><title type="html">Uncertainty quantification for iterative algorithms in linear models with application to early stopping</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Uncertaintyquantificationforiterativealgorithmsinlinearmodelswithapplicationtoearlystopping.html" rel="alternate" type="text/html" title="Uncertainty quantification for iterative algorithms in linear models with application to early stopping" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Uncertaintyquantificationforiterativealgorithmsinlinearmodelswithapplicationtoearlystopping</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Uncertaintyquantificationforiterativealgorithmsinlinearmodelswithapplicationtoearlystopping.html">&lt;p&gt;This paper investigates the iterates $\hbb^1,\dots,\hbb^T$ obtained from iterative algorithms in high-dimensional linear regression problems, in the regime where the feature dimension $p$ is comparable with the sample size $n$, i.e., $p \asymp n$. The analysis and proposed estimators are applicable to Gradient Descent (GD), proximal GD and their accelerated variants such as Fast Iterative Soft-Thresholding (FISTA). The paper proposes novel estimators for the generalization error of the iterate $\hbb^t$ for any fixed iteration $t$ along the trajectory. These estimators are proved to be $\sqrt n$-consistent under Gaussian designs. Applications to early-stopping are provided: when the generalization error of the iterates is a U-shape function of the iteration $t$, the estimates allow to select from the data an iteration $\hat t$ that achieves the smallest generalization error along the trajectory. Additionally, we provide a technique for developing debiasing corrections and valid confidence intervals for the components of the true coefficient vector from the iterate $\hbb^t$ at any finite iteration $t$. Extensive simulations on synthetic data illustrate the theoretical results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17856&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Pierre C. Bellec, Kai Tan</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This paper investigates the iterates $\hbb^1,\dots,\hbb^T$ obtained from iterative algorithms in high-dimensional linear regression problems, in the regime where the feature dimension $p$ is comparable with the sample size $n$, i.e., $p \asymp n$. The analysis and proposed estimators are applicable to Gradient Descent (GD), proximal GD and their accelerated variants such as Fast Iterative Soft-Thresholding (FISTA). The paper proposes novel estimators for the generalization error of the iterate $\hbb^t$ for any fixed iteration $t$ along the trajectory. These estimators are proved to be $\sqrt n$-consistent under Gaussian designs. Applications to early-stopping are provided: when the generalization error of the iterates is a U-shape function of the iteration $t$, the estimates allow to select from the data an iteration $\hat t$ that achieves the smallest generalization error along the trajectory. Additionally, we provide a technique for developing debiasing corrections and valid confidence intervals for the components of the true coefficient vector from the iterate $\hbb^t$ at any finite iteration $t$. Extensive simulations on synthetic data illustrate the theoretical results.</summary></entry><entry><title type="html">Universal Cold RNA Phase Transitions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/UniversalColdRNAPhaseTransitions.html" rel="alternate" type="text/html" title="Universal Cold RNA Phase Transitions" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/UniversalColdRNAPhaseTransitions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/UniversalColdRNAPhaseTransitions.html">&lt;p&gt;RNA’s diversity of structures and functions impacts all life forms since primordia. We use calorimetric force spectroscopy to investigate RNA folding landscapes in previously unexplored low-temperature conditions. We find that Watson-Crick RNA hairpins, the most basic secondary structure elements, undergo a glass-like transition below $\mathbf{T_G\sim 20 ^{\circ}}$C where the heat capacity abruptly changes and the RNA folds into a diversity of misfolded structures. We hypothesize that an altered RNA biochemistry, determined by sequence-independent ribose-water interactions, outweighs sequence-dependent base pairing. The ubiquitous ribose-water interactions lead to universal RNA phase transitions below $\mathbf{T_G}$, such as maximum stability at $\mathbf{T_S\sim 5 ^{\circ}}$C where water density is maximum, and cold denaturation at $\mathbf{T_C\sim-50^{\circ}}$C. RNA cold biochemistry may have a profound impact on RNA function and evolution.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.15352&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Paolo Rissone, Aurelien Severino, Isabel Pastor, Felix Ritort</name></author><category term="stat.ME" /><summary type="html">RNA’s diversity of structures and functions impacts all life forms since primordia. We use calorimetric force spectroscopy to investigate RNA folding landscapes in previously unexplored low-temperature conditions. We find that Watson-Crick RNA hairpins, the most basic secondary structure elements, undergo a glass-like transition below $\mathbf{T_G\sim 20 ^{\circ}}$C where the heat capacity abruptly changes and the RNA folds into a diversity of misfolded structures. We hypothesize that an altered RNA biochemistry, determined by sequence-independent ribose-water interactions, outweighs sequence-dependent base pairing. The ubiquitous ribose-water interactions lead to universal RNA phase transitions below $\mathbf{T_G}$, such as maximum stability at $\mathbf{T_S\sim 5 ^{\circ}}$C where water density is maximum, and cold denaturation at $\mathbf{T_C\sim-50^{\circ}}$C. RNA cold biochemistry may have a profound impact on RNA function and evolution.</summary></entry><entry><title type="html">Using Exponential Histograms to Approximate the Quantiles of Heavy- and Light-Tailed Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/UsingExponentialHistogramstoApproximatetheQuantilesofHeavyandLightTailedData.html" rel="alternate" type="text/html" title="Using Exponential Histograms to Approximate the Quantiles of Heavy- and Light-Tailed Data" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/UsingExponentialHistogramstoApproximatetheQuantilesofHeavyandLightTailedData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/UsingExponentialHistogramstoApproximatetheQuantilesofHeavyandLightTailedData.html">&lt;p&gt;Exponential histograms, with bins of the form $\left{ \left(\rho^{k-1},\rho^{k}\right]\right} _{k\in\mathbb{Z}}$, for $\rho&amp;gt;1$, straightforwardly summarize the quantiles of streaming data sets (Masson et al. 2019). While they guarantee the relative accuracy of their estimates, they appear to use only $\log n$ values to summarize $n$ inputs. We study four aspects of exponential histograms – size, accuracy, occupancy, and largest gap size – when inputs are i.i.d. $\mathrm{Exp}\left(\lambda\right)$ or i.i.d. $\mathrm{Pareto}\left(\nu,\beta\right)$, taking $\mathrm{Exp}\left(\lambda\right)$ (or, $\mathrm{Pareto}\left(\nu,\beta\right)$) to represent all light- (or, heavy-) tailed distributions. We show that, in these settings, size grows like $\log n$ and takes on a Gumbel distribution as $n$ grows large. We bound the missing mass to the right of the histogram and the mass of its final bin and show that occupancy grows apace with size. Finally, we approximate the size of the largest number of consecutive, empty bins. Our study gives a deeper and broader view of this low-memory approach to quantile estimation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18024&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Philip T. Labo</name></author><category term="stat.AP," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Exponential histograms, with bins of the form $\left{ \left(\rho^{k-1},\rho^{k}\right]\right} _{k\in\mathbb{Z}}$, for $\rho&amp;gt;1$, straightforwardly summarize the quantiles of streaming data sets (Masson et al. 2019). While they guarantee the relative accuracy of their estimates, they appear to use only $\log n$ values to summarize $n$ inputs. We study four aspects of exponential histograms – size, accuracy, occupancy, and largest gap size – when inputs are i.i.d. $\mathrm{Exp}\left(\lambda\right)$ or i.i.d. $\mathrm{Pareto}\left(\nu,\beta\right)$, taking $\mathrm{Exp}\left(\lambda\right)$ (or, $\mathrm{Pareto}\left(\nu,\beta\right)$) to represent all light- (or, heavy-) tailed distributions. We show that, in these settings, size grows like $\log n$ and takes on a Gumbel distribution as $n$ grows large. We bound the missing mass to the right of the histogram and the mass of its final bin and show that occupancy grows apace with size. Finally, we approximate the size of the largest number of consecutive, empty bins. Our study gives a deeper and broader view of this low-memory approach to quantile estimation.</summary></entry><entry><title type="html">Using Pre-training and Interaction Modeling for ancestry-specific disease prediction in UK Biobank</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/UsingPretrainingandInteractionModelingforancestryspecificdiseasepredictioninUKBiobank.html" rel="alternate" type="text/html" title="Using Pre-training and Interaction Modeling for ancestry-specific disease prediction in UK Biobank" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/UsingPretrainingandInteractionModelingforancestryspecificdiseasepredictioninUKBiobank</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/UsingPretrainingandInteractionModelingforancestryspecificdiseasepredictioninUKBiobank.html">&lt;p&gt;Recent genome-wide association studies (GWAS) have uncovered the genetic basis of complex traits, but show an under-representation of non-European descent individuals, underscoring a critical gap in genetic research. Here, we assess whether we can improve disease prediction across diverse ancestries using multiomic data. We evaluate the performance of Group-LASSO INTERaction-NET (glinternet) and pretrained lasso in disease prediction focusing on diverse ancestries in the UK Biobank. Models were trained on data from White British and other ancestries and validated across a cohort of over 96,000 individuals for 8 diseases. Out of 96 models trained, we report 16 with statistically significant incremental predictive performance in terms of ROC-AUC scores. These findings suggest that advanced statistical methods that borrow information across multiple ancestries may improve disease risk prediction, but with limited benefit.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17626&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thomas Le Menestrel, Erin Craig, Robert Tibshirani, Trevor Hastie, Manuel Rivas</name></author><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Recent genome-wide association studies (GWAS) have uncovered the genetic basis of complex traits, but show an under-representation of non-European descent individuals, underscoring a critical gap in genetic research. Here, we assess whether we can improve disease prediction across diverse ancestries using multiomic data. We evaluate the performance of Group-LASSO INTERaction-NET (glinternet) and pretrained lasso in disease prediction focusing on diverse ancestries in the UK Biobank. Models were trained on data from White British and other ancestries and validated across a cohort of over 96,000 individuals for 8 diseases. Out of 96 models trained, we report 16 with statistically significant incremental predictive performance in terms of ROC-AUC scores. These findings suggest that advanced statistical methods that borrow information across multiple ancestries may improve disease risk prediction, but with limited benefit.</summary></entry><entry><title type="html">VT-MRF-SPF: Variable Target Markov Random Field Scalable Particle Filter</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/VTMRFSPFVariableTargetMarkovRandomFieldScalableParticleFilter.html" rel="alternate" type="text/html" title="VT-MRF-SPF: Variable Target Markov Random Field Scalable Particle Filter" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/VTMRFSPFVariableTargetMarkovRandomFieldScalableParticleFilter</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/VTMRFSPFVariableTargetMarkovRandomFieldScalableParticleFilter.html">&lt;p&gt;Markov random fields (MRFs) are invaluable tools across diverse fields, and spatiotemporal MRFs (STMRFs) amplify their effectiveness by integrating spatial and temporal dimensions. However, modeling spatiotemporal data introduces additional hurdles, including dynamic spatial dimensions and partial observations, prevalent in scenarios like disease spread analysis and environmental monitoring. Tracking high-dimensional targets with complex spatiotemporal interactions over extended periods poses significant challenges in accuracy, efficiency, and computational feasibility. To tackle these obstacles, we introduce the variable target MRF scalable particle filter (VT-MRF-SPF), a fully online learning algorithm designed for high-dimensional target tracking over STMRFs with varying dimensions under partial observation. We rigorously guarantee algorithm performance, explicitly indicating overcoming the curse of dimensionality. Additionally, we provide practical guidelines for tuning graphical parameters, leading to superior performance in extensive examinations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18857&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ning Ning</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Markov random fields (MRFs) are invaluable tools across diverse fields, and spatiotemporal MRFs (STMRFs) amplify their effectiveness by integrating spatial and temporal dimensions. However, modeling spatiotemporal data introduces additional hurdles, including dynamic spatial dimensions and partial observations, prevalent in scenarios like disease spread analysis and environmental monitoring. Tracking high-dimensional targets with complex spatiotemporal interactions over extended periods poses significant challenges in accuracy, efficiency, and computational feasibility. To tackle these obstacles, we introduce the variable target MRF scalable particle filter (VT-MRF-SPF), a fully online learning algorithm designed for high-dimensional target tracking over STMRFs with varying dimensions under partial observation. We rigorously guarantee algorithm performance, explicitly indicating overcoming the curse of dimensionality. Additionally, we provide practical guidelines for tuning graphical parameters, leading to superior performance in extensive examinations.</summary></entry><entry><title type="html">What Hides behind Unfairness? Exploring Dynamics Fairness in Reinforcement Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/WhatHidesbehindUnfairnessExploringDynamicsFairnessinReinforcementLearning.html" rel="alternate" type="text/html" title="What Hides behind Unfairness? Exploring Dynamics Fairness in Reinforcement Learning" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/WhatHidesbehindUnfairnessExploringDynamicsFairnessinReinforcementLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/WhatHidesbehindUnfairnessExploringDynamicsFairnessinReinforcementLearning.html">&lt;p&gt;In sequential decision-making problems involving sensitive attributes like race and gender, reinforcement learning (RL) agents must carefully consider long-term fairness while maximizing returns. Recent works have proposed many different types of fairness notions, but how unfairness arises in RL problems remains unclear. In this paper, we address this gap in the literature by investigating the sources of inequality through a causal lens. We first analyse the causal relationships governing the data generation process and decompose the effect of sensitive attributes on long-term well-being into distinct components. We then introduce a novel notion called dynamics fairness, which explicitly captures the inequality stemming from environmental dynamics, distinguishing it from those induced by decision-making or inherited from the past. This notion requires evaluating the expected changes in the next state and the reward induced by changing the value of the sensitive attribute while holding everything else constant. To quantitatively evaluate this counterfactual concept, we derive identification formulas that allow us to obtain reliable estimations from data. Extensive experiments demonstrate the effectiveness of the proposed techniques in explaining, detecting, and reducing inequality in reinforcement learning. We publicly release code at https://github.com/familyld/InsightFair.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.10942&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhihong Deng, Jing Jiang, Guodong Long, Chengqi Zhang</name></author><category term="stat.ME" /><summary type="html">In sequential decision-making problems involving sensitive attributes like race and gender, reinforcement learning (RL) agents must carefully consider long-term fairness while maximizing returns. Recent works have proposed many different types of fairness notions, but how unfairness arises in RL problems remains unclear. In this paper, we address this gap in the literature by investigating the sources of inequality through a causal lens. We first analyse the causal relationships governing the data generation process and decompose the effect of sensitive attributes on long-term well-being into distinct components. We then introduce a novel notion called dynamics fairness, which explicitly captures the inequality stemming from environmental dynamics, distinguishing it from those induced by decision-making or inherited from the past. This notion requires evaluating the expected changes in the next state and the reward induced by changing the value of the sensitive attribute while holding everything else constant. To quantitatively evaluate this counterfactual concept, we derive identification formulas that allow us to obtain reliable estimations from data. Extensive experiments demonstrate the effectiveness of the proposed techniques in explaining, detecting, and reducing inequality in reinforcement learning. We publicly release code at https://github.com/familyld/InsightFair.</summary></entry><entry><title type="html">Why You Should Not Trust Interpretations in Machine Learning: Adversarial Attacks on Partial Dependence Plots</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/WhyYouShouldNotTrustInterpretationsinMachineLearningAdversarialAttacksonPartialDependencePlots.html" rel="alternate" type="text/html" title="Why You Should Not Trust Interpretations in Machine Learning: Adversarial Attacks on Partial Dependence Plots" /><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/WhyYouShouldNotTrustInterpretationsinMachineLearningAdversarialAttacksonPartialDependencePlots</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/WhyYouShouldNotTrustInterpretationsinMachineLearningAdversarialAttacksonPartialDependencePlots.html">&lt;p&gt;The adoption of artificial intelligence (AI) across industries has led to the widespread use of complex black-box models and interpretation tools for decision making. This paper proposes an adversarial framework to uncover the vulnerability of permutation-based interpretation methods for machine learning tasks, with a particular focus on partial dependence (PD) plots. This adversarial framework modifies the original black box model to manipulate its predictions for instances in the extrapolation domain. As a result, it produces deceptive PD plots that can conceal discriminatory behaviors while preserving most of the original model’s predictions. This framework can produce multiple fooled PD plots via a single model. By using real-world datasets including an auto insurance claims dataset and COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) dataset, our results show that it is possible to intentionally hide the discriminatory behavior of a predictor and make the black-box model appear neutral through interpretation tools like PD plots while retaining almost all the predictions of the original black-box model. Managerial insights for regulators and practitioners are provided based on the findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.18702&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xi Xin, Fei Huang, Giles Hooker</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">The adoption of artificial intelligence (AI) across industries has led to the widespread use of complex black-box models and interpretation tools for decision making. This paper proposes an adversarial framework to uncover the vulnerability of permutation-based interpretation methods for machine learning tasks, with a particular focus on partial dependence (PD) plots. This adversarial framework modifies the original black box model to manipulate its predictions for instances in the extrapolation domain. As a result, it produces deceptive PD plots that can conceal discriminatory behaviors while preserving most of the original model’s predictions. This framework can produce multiple fooled PD plots via a single model. By using real-world datasets including an auto insurance claims dataset and COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) dataset, our results show that it is possible to intentionally hide the discriminatory behavior of a predictor and make the black-box model appear neutral through interpretation tools like PD plots while retaining almost all the predictions of the original black-box model. Managerial insights for regulators and practitioners are provided based on the findings.</summary></entry></feed>