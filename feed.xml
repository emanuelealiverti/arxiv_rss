<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-28T07:15:11+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Bayesian joint model for mediation analysis with matrix-valued mediators</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ABayesianjointmodelformediationanalysiswithmatrixvaluedmediators.html" rel="alternate" type="text/html" title="A Bayesian joint model for mediation analysis with matrix-valued mediators" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ABayesianjointmodelformediationanalysiswithmatrixvaluedmediators</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ABayesianjointmodelformediationanalysiswithmatrixvaluedmediators.html">&lt;p&gt;Unscheduled treatment interruptions may lead to reduced quality of care in radiation therapy (RT). Identifying the RT prescription dose effects on the outcome of treatment interruptions, mediated through doses distributed into different organs-at-risk (OARs), can inform future treatment planning. The radiation exposure to OARs can be summarized by a matrix of dose-volume histograms (DVH) for each patient. Although various methods for high-dimensional mediation analysis have been proposed recently, few studies investigated how matrix-valued data can be treated as mediators. In this paper, we propose a novel Bayesian joint mediation model for high-dimensional matrix-valued mediators. In this joint model, latent features are extracted from the matrix-valued data through an adaptation of probabilistic multilinear principal components analysis (MPCA), retaining the inherent matrix structure. We derive and implement a Gibbs sampling algorithm to jointly estimate all model parameters, and introduce a Varimax rotation method to identify active indicators of mediation among the matrix-valued data. Our simulation study finds that the proposed joint model has higher efficiency in estimating causal decomposition effects compared to an alternative two-step method, and demonstrates that the mediation effects can be identified and visualized in the matrix form. We apply the method to study the effect of prescription dose on treatment interruptions in anal canal cancer patients.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.00803&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zijin Liu, Zhihui Liu, Ali Hosni, John Kim, Bei Jiang, Olli Saarela</name></author><category term="stat.ME" /><summary type="html">Unscheduled treatment interruptions may lead to reduced quality of care in radiation therapy (RT). Identifying the RT prescription dose effects on the outcome of treatment interruptions, mediated through doses distributed into different organs-at-risk (OARs), can inform future treatment planning. The radiation exposure to OARs can be summarized by a matrix of dose-volume histograms (DVH) for each patient. Although various methods for high-dimensional mediation analysis have been proposed recently, few studies investigated how matrix-valued data can be treated as mediators. In this paper, we propose a novel Bayesian joint mediation model for high-dimensional matrix-valued mediators. In this joint model, latent features are extracted from the matrix-valued data through an adaptation of probabilistic multilinear principal components analysis (MPCA), retaining the inherent matrix structure. We derive and implement a Gibbs sampling algorithm to jointly estimate all model parameters, and introduce a Varimax rotation method to identify active indicators of mediation among the matrix-valued data. Our simulation study finds that the proposed joint model has higher efficiency in estimating causal decomposition effects compared to an alternative two-step method, and demonstrates that the mediation effects can be identified and visualized in the matrix form. We apply the method to study the effect of prescription dose on treatment interruptions in anal canal cancer patients.</summary></entry><entry><title type="html">Analysis of Full-scale Riser Responses in Field Conditions Based on Gaussian Mixture Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/AnalysisofFullscaleRiserResponsesinFieldConditionsBasedonGaussianMixtureModel.html" rel="alternate" type="text/html" title="Analysis of Full-scale Riser Responses in Field Conditions Based on Gaussian Mixture Model" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/AnalysisofFullscaleRiserResponsesinFieldConditionsBasedonGaussianMixtureModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/AnalysisofFullscaleRiserResponsesinFieldConditionsBasedonGaussianMixtureModel.html">&lt;p&gt;Offshore slender marine structures experience complex and combined load conditions from waves, current and vessel motions that may result in both wave frequency and vortex shedding response patterns. Field measurements often consist of records of environmental conditions and riser responses, typically with 30-minute intervals. These data can be represented in a high-dimensional parameter space. However, it is difficult to visualize and understand the structural responses, as they are affected by many of these parameters. It becomes easier to identify trends and key parameters if the measurements with the same characteristics can be grouped together. Cluster analysis is an unsupervised learning method, which groups the data based on their relative distance, density of the data space, intervals, or statistical distributions. In the present study, a Gaussian mixture model guided by domain knowledge has been applied to analyze field measurements. Using the 242 measurement events of the Helland-Hansen riser, it is demonstrated that riser responses can be grouped into 12 clusters by the identification of key environmental parameters. This results in an improved understanding of complex structure responses. Furthermore, the cluster results are valuable for evaluating the riser response prediction accuracy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18611&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jie Wu, S{\o}lve Eidnes, Jingzhe Jin, Halvor Lie, Decao Yin, Elizabeth Passano, Svein S{\ae}vik, Signe Riemer-Sorensen</name></author><category term="stat.AP" /><summary type="html">Offshore slender marine structures experience complex and combined load conditions from waves, current and vessel motions that may result in both wave frequency and vortex shedding response patterns. Field measurements often consist of records of environmental conditions and riser responses, typically with 30-minute intervals. These data can be represented in a high-dimensional parameter space. However, it is difficult to visualize and understand the structural responses, as they are affected by many of these parameters. It becomes easier to identify trends and key parameters if the measurements with the same characteristics can be grouped together. Cluster analysis is an unsupervised learning method, which groups the data based on their relative distance, density of the data space, intervals, or statistical distributions. In the present study, a Gaussian mixture model guided by domain knowledge has been applied to analyze field measurements. Using the 242 measurement events of the Helland-Hansen riser, it is demonstrated that riser responses can be grouped into 12 clusters by the identification of key environmental parameters. This results in an improved understanding of complex structure responses. Furthermore, the cluster results are valuable for evaluating the riser response prediction accuracy.</summary></entry><entry><title type="html">Asymptotic equivalence of Principal Components and Quasi Maximum Likelihood estimators in Large Approximate Factor Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/AsymptoticequivalenceofPrincipalComponentsandQuasiMaximumLikelihoodestimatorsinLargeApproximateFactorModels.html" rel="alternate" type="text/html" title="Asymptotic equivalence of Principal Components and Quasi Maximum Likelihood estimators in Large Approximate Factor Models" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/AsymptoticequivalenceofPrincipalComponentsandQuasiMaximumLikelihoodestimatorsinLargeApproximateFactorModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/AsymptoticequivalenceofPrincipalComponentsandQuasiMaximumLikelihoodestimatorsinLargeApproximateFactorModels.html">&lt;p&gt;This paper investigates the properties of Quasi Maximum Likelihood estimation of an approximate factor model for an $n$-dimensional vector of stationary time series. We prove that the factor loadings estimated by Quasi Maximum Likelihood are asymptotically equivalent, as $n\to\infty$, to those estimated via Principal Components. Both estimators are, in turn, also asymptotically equivalent, as $n\to\infty$, to the unfeasible Ordinary Least Squares estimator we would have if the factors were observed. We also show that the usual sandwich form of the asymptotic covariance matrix of the Quasi Maximum Likelihood estimator is asymptotically equivalent to the simpler asymptotic covariance matrix of the unfeasible Ordinary Least Squares. All these results hold in the general case in which the idiosyncratic components are cross-sectionally heteroskedastic, as well as serially and cross-sectionally weakly correlated. The intuition behind these results is that as $n\to\infty$ the factors can be considered as observed, thus showing that factor models enjoy a blessing of dimensionality.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.09864&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matteo Barigozzi</name></author><category term="stat.ME" /><summary type="html">This paper investigates the properties of Quasi Maximum Likelihood estimation of an approximate factor model for an $n$-dimensional vector of stationary time series. We prove that the factor loadings estimated by Quasi Maximum Likelihood are asymptotically equivalent, as $n\to\infty$, to those estimated via Principal Components. Both estimators are, in turn, also asymptotically equivalent, as $n\to\infty$, to the unfeasible Ordinary Least Squares estimator we would have if the factors were observed. We also show that the usual sandwich form of the asymptotic covariance matrix of the Quasi Maximum Likelihood estimator is asymptotically equivalent to the simpler asymptotic covariance matrix of the unfeasible Ordinary Least Squares. All these results hold in the general case in which the idiosyncratic components are cross-sectionally heteroskedastic, as well as serially and cross-sectionally weakly correlated. The intuition behind these results is that as $n\to\infty$ the factors can be considered as observed, thus showing that factor models enjoy a blessing of dimensionality.</summary></entry><entry><title type="html">Bayesian Hierarchical Modeling and Inference for Mechanistic Systems in Industrial Hygiene</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BayesianHierarchicalModelingandInferenceforMechanisticSystemsinIndustrialHygiene.html" rel="alternate" type="text/html" title="Bayesian Hierarchical Modeling and Inference for Mechanistic Systems in Industrial Hygiene" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BayesianHierarchicalModelingandInferenceforMechanisticSystemsinIndustrialHygiene</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BayesianHierarchicalModelingandInferenceforMechanisticSystemsinIndustrialHygiene.html">&lt;p&gt;A series of experiments in stationary and moving passenger rail cars were conducted to measure removal rates of particles in the size ranges of SARS-CoV-2 viral aerosols, and the air changes per hour provided by existing and modified air handling systems. Such methods for exposure assessments are customarily based on mechanistic models derived from physical laws of particle movement that are deterministic and do not account for measurement errors inherent in data collection. The resulting analysis compromises on reliably learning about mechanistic factors such as ventilation rates, aerosol generation rates and filtration efficiencies from field measurements. This manuscript develops a Bayesian state space modeling framework that synthesizes information from the mechanistic system as well as the field data. We derive a stochastic model from finite difference approximations of differential equations explaining particle concentrations. Our inferential framework trains the mechanistic system using the field measurements from the chamber experiments and delivers reliable estimates of the underlying physical process with fully model-based uncertainty quantification. Our application falls within the realm of Bayesian “melding” of mechanistic and statistical models and is of significant relevance to industrial hygienists and public health researchers working on assessment of exposure to viral aerosols in rail car fleets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.00450&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Soumyakanti Pan, Darpan Das, Gurumurthy Ramachandran, Sudipto Banerjee</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">A series of experiments in stationary and moving passenger rail cars were conducted to measure removal rates of particles in the size ranges of SARS-CoV-2 viral aerosols, and the air changes per hour provided by existing and modified air handling systems. Such methods for exposure assessments are customarily based on mechanistic models derived from physical laws of particle movement that are deterministic and do not account for measurement errors inherent in data collection. The resulting analysis compromises on reliably learning about mechanistic factors such as ventilation rates, aerosol generation rates and filtration efficiencies from field measurements. This manuscript develops a Bayesian state space modeling framework that synthesizes information from the mechanistic system as well as the field data. We derive a stochastic model from finite difference approximations of differential equations explaining particle concentrations. Our inferential framework trains the mechanistic system using the field measurements from the chamber experiments and delivers reliable estimates of the underlying physical process with fully model-based uncertainty quantification. Our application falls within the realm of Bayesian “melding” of mechanistic and statistical models and is of significant relevance to industrial hygienists and public health researchers working on assessment of exposure to viral aerosols in rail car fleets.</summary></entry><entry><title type="html">Bayesian Inference for Stochastic Predictions of Non-Gaussian Systems with Applications in Climate Change</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BayesianInferenceforStochasticPredictionsofNonGaussianSystemswithApplicationsinClimateChange.html" rel="alternate" type="text/html" title="Bayesian Inference for Stochastic Predictions of Non-Gaussian Systems with Applications in Climate Change" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BayesianInferenceforStochasticPredictionsofNonGaussianSystemswithApplicationsinClimateChange</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BayesianInferenceforStochasticPredictionsofNonGaussianSystemswithApplicationsinClimateChange.html">&lt;p&gt;Climate change poses significant challenges for accurate climate modeling due to the complexity and variability of non-Gaussian climate systems. To address the complexities of non-Gaussian systems in climate modeling, this thesis proposes a Bayesian framework utilizing the Unscented Kalman Filter (UKF), Ensemble Kalman Filter (EnKF), and Unscented Particle Filter (UPF) for one-dimensional and two-dimensional stochastic climate models, evaluated with real-world temperature and sea level data. We study these methods under varying conditions, including measurement noise, sample sizes, and observed and hidden variables, to highlight their respective advantages and limitations. Our findings reveal that merely increasing data is insufficient for accurate predictions; instead, selecting appropriate methods is crucial. This research provides insights into issues related to information barrier, curse of dimensionality, prediction variability, and measurement noise quantification, thereby enhancing the application of these techniques in real-world climate scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18606&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yunjin Tong</name></author><category term="stat.AP" /><summary type="html">Climate change poses significant challenges for accurate climate modeling due to the complexity and variability of non-Gaussian climate systems. To address the complexities of non-Gaussian systems in climate modeling, this thesis proposes a Bayesian framework utilizing the Unscented Kalman Filter (UKF), Ensemble Kalman Filter (EnKF), and Unscented Particle Filter (UPF) for one-dimensional and two-dimensional stochastic climate models, evaluated with real-world temperature and sea level data. We study these methods under varying conditions, including measurement noise, sample sizes, and observed and hidden variables, to highlight their respective advantages and limitations. Our findings reveal that merely increasing data is insufficient for accurate predictions; instead, selecting appropriate methods is crucial. This research provides insights into issues related to information barrier, curse of dimensionality, prediction variability, and measurement noise quantification, thereby enhancing the application of these techniques in real-world climate scenarios.</summary></entry><entry><title type="html">Bayesian identification of nonseparable Hamiltonians with multiplicative noise using deep learning and reduced-order modeling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BayesianidentificationofnonseparableHamiltonianswithmultiplicativenoiseusingdeeplearningandreducedordermodeling.html" rel="alternate" type="text/html" title="Bayesian identification of nonseparable Hamiltonians with multiplicative noise using deep learning and reduced-order modeling" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BayesianidentificationofnonseparableHamiltonianswithmultiplicativenoiseusingdeeplearningandreducedordermodeling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BayesianidentificationofnonseparableHamiltonianswithmultiplicativenoiseusingdeeplearningandreducedordermodeling.html">&lt;p&gt;This paper presents a structure-preserving Bayesian approach for learning nonseparable Hamiltonian systems using stochastic dynamic models allowing for statistically-dependent, vector-valued additive and multiplicative measurement noise. The approach is comprised of three main facets. First, we derive a Gaussian filter for a statistically-dependent, vector-valued, additive and multiplicative noise model that is needed to evaluate the likelihood within the Bayesian posterior. Second, we develop a novel algorithm for cost-effective application of Bayesian system identification to high-dimensional systems. Third, we demonstrate how structure-preserving methods can be incorporated into the proposed framework, using nonseparable Hamiltonians as an illustrative system class. We assess the method’s performance based on the forecasting accuracy of a model estimated from-single trajectory data. We compare the Bayesian method to a state-of-the-art machine learning method on a canonical nonseparable Hamiltonian model and a chaotic double pendulum model with small, noisy training datasets. The results show that using the Bayesian posterior as a training objective can yield upwards of 724 times improvement in Hamiltonian mean squared error using training data with up to 10% multiplicative noise compared to a standard training objective. Lastly, we demonstrate the utility of the novel algorithm for parameter estimation of a 64-dimensional model of the spatially-discretized nonlinear Schr&quot;odinger equation with data corrupted by up to 20% multiplicative noise.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.12476&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nicholas Galioto, Harsh Sharma, Boris Kramer, Alex Arkady Gorodetsky</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">This paper presents a structure-preserving Bayesian approach for learning nonseparable Hamiltonian systems using stochastic dynamic models allowing for statistically-dependent, vector-valued additive and multiplicative measurement noise. The approach is comprised of three main facets. First, we derive a Gaussian filter for a statistically-dependent, vector-valued, additive and multiplicative noise model that is needed to evaluate the likelihood within the Bayesian posterior. Second, we develop a novel algorithm for cost-effective application of Bayesian system identification to high-dimensional systems. Third, we demonstrate how structure-preserving methods can be incorporated into the proposed framework, using nonseparable Hamiltonians as an illustrative system class. We assess the method’s performance based on the forecasting accuracy of a model estimated from-single trajectory data. We compare the Bayesian method to a state-of-the-art machine learning method on a canonical nonseparable Hamiltonian model and a chaotic double pendulum model with small, noisy training datasets. The results show that using the Bayesian posterior as a training objective can yield upwards of 724 times improvement in Hamiltonian mean squared error using training data with up to 10% multiplicative noise compared to a standard training objective. Lastly, we demonstrate the utility of the novel algorithm for parameter estimation of a 64-dimensional model of the spatially-discretized nonlinear Schr&quot;odinger equation with data corrupted by up to 20% multiplicative noise.</summary></entry><entry><title type="html">Bayesian inference: More than Bayes’s theorem</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BayesianinferenceMorethanBayesstheorem.html" rel="alternate" type="text/html" title="Bayesian inference: More than Bayes’s theorem" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BayesianinferenceMorethanBayesstheorem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BayesianinferenceMorethanBayesstheorem.html">&lt;p&gt;Bayesian inference gets its name from &lt;em&gt;Bayes’s theorem&lt;/em&gt;, expressing posterior probabilities for hypotheses about a data generating process as the (normalized) product of prior probabilities and a likelihood function. But Bayesian inference uses all of probability theory, not just Bayes’s theorem. Many hypotheses of scientific interest are &lt;em&gt;composite hypotheses&lt;/em&gt;, with the strength of evidence for the hypothesis dependent on knowledge about auxiliary factors, such as the values of nuisance parameters (e.g., uncertain background rates or calibration factors). Many important capabilities of Bayesian methods arise from use of the law of total probability, which instructs analysts to compute probabilities for composite hypotheses by &lt;em&gt;marginalization&lt;/em&gt; over auxiliary factors. This tutorial targets relative newcomers to Bayesian inference, aiming to complement tutorials that focus on Bayes’s theorem and how priors modulate likelihoods. The emphasis here is on marginalization over parameter spaces – both how it is the foundation for important capabilities, and how it may motivate caution when parameter spaces are large. Topics covered include the difference between likelihood and probability, understanding the impact of priors beyond merely shifting the maximum likelihood estimate, and the role of marginalization in accounting for uncertainty in nuisance parameters, systematic error, and model misspecification.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18905&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thomas J. Loredo, Robert L. Wolpert</name></author><category term="stat.ME" /><summary type="html">Bayesian inference gets its name from Bayes’s theorem, expressing posterior probabilities for hypotheses about a data generating process as the (normalized) product of prior probabilities and a likelihood function. But Bayesian inference uses all of probability theory, not just Bayes’s theorem. Many hypotheses of scientific interest are composite hypotheses, with the strength of evidence for the hypothesis dependent on knowledge about auxiliary factors, such as the values of nuisance parameters (e.g., uncertain background rates or calibration factors). Many important capabilities of Bayesian methods arise from use of the law of total probability, which instructs analysts to compute probabilities for composite hypotheses by marginalization over auxiliary factors. This tutorial targets relative newcomers to Bayesian inference, aiming to complement tutorials that focus on Bayes’s theorem and how priors modulate likelihoods. The emphasis here is on marginalization over parameter spaces – both how it is the foundation for important capabilities, and how it may motivate caution when parameter spaces are large. Topics covered include the difference between likelihood and probability, understanding the impact of priors beyond merely shifting the maximum likelihood estimate, and the role of marginalization in accounting for uncertainty in nuisance parameters, systematic error, and model misspecification.</summary></entry><entry><title type="html">Benchmarking M6 Competitors: An Analysis of Financial Metrics and Discussion of Incentives</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BenchmarkingM6CompetitorsAnAnalysisofFinancialMetricsandDiscussionofIncentives.html" rel="alternate" type="text/html" title="Benchmarking M6 Competitors: An Analysis of Financial Metrics and Discussion of Incentives" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BenchmarkingM6CompetitorsAnAnalysisofFinancialMetricsandDiscussionofIncentives</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/BenchmarkingM6CompetitorsAnAnalysisofFinancialMetricsandDiscussionofIncentives.html">&lt;p&gt;The M6 Competition assessed the performance of competitors using a ranked probability score and an information ratio (IR). While these metrics do well at picking the winners in the competition, crucial questions remain for investors with longer-term incentives. To address these questions, we compare the competitors’ performance to a number of conventional (long-only) and alternative indices using standard industry metrics. We apply factor models to the competitors’ returns and show the difficulty for any competitor to demonstrate a statistically significant value-add above industry-standard benchmarks within the short timeframe of the competition. We also uncover that most competitors generated lower risk-adjusted returns and lower maximum drawdowns than randomly selected portfolios, and that most competitors could not generate significant out-performance in raw returns. We further introduce two new strategies by picking the competitors with the best (Superstars) and worst (Superlosers) recent performance and show that it is challenging to identify skill amongst investment managers. Overall, our findings highlight the difference in incentives for competitors over professional investors, where the upside of winning the competition dwarfs the potential downside of not winning to maximize fees over an extended period of time.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19105&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matthew J. Schneider, Rufus Rankin, Prabir Burman, Alexander Aue</name></author><category term="stat.AP" /><summary type="html">The M6 Competition assessed the performance of competitors using a ranked probability score and an information ratio (IR). While these metrics do well at picking the winners in the competition, crucial questions remain for investors with longer-term incentives. To address these questions, we compare the competitors’ performance to a number of conventional (long-only) and alternative indices using standard industry metrics. We apply factor models to the competitors’ returns and show the difficulty for any competitor to demonstrate a statistically significant value-add above industry-standard benchmarks within the short timeframe of the competition. We also uncover that most competitors generated lower risk-adjusted returns and lower maximum drawdowns than randomly selected portfolios, and that most competitors could not generate significant out-performance in raw returns. We further introduce two new strategies by picking the competitors with the best (Superstars) and worst (Superlosers) recent performance and show that it is challenging to identify skill amongst investment managers. Overall, our findings highlight the difference in incentives for competitors over professional investors, where the upside of winning the competition dwarfs the potential downside of not winning to maximize fees over an extended period of time.</summary></entry><entry><title type="html">Causal Message Passing: A Method for Experiments with Unknown and General Network Interference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/CausalMessagePassingAMethodforExperimentswithUnknownandGeneralNetworkInterference.html" rel="alternate" type="text/html" title="Causal Message Passing: A Method for Experiments with Unknown and General Network Interference" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/CausalMessagePassingAMethodforExperimentswithUnknownandGeneralNetworkInterference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/CausalMessagePassingAMethodforExperimentswithUnknownandGeneralNetworkInterference.html">&lt;p&gt;Randomized experiments are a powerful methodology for data-driven evaluation of decisions or interventions. Yet, their validity may be undermined by network interference. This occurs when the treatment of one unit impacts not only its outcome but also that of connected units, biasing traditional treatment effect estimations. Our study introduces a new framework to accommodate complex and unknown network interference, moving beyond specialized models in the existing literature. Our framework, termed causal message-passing, is grounded in high-dimensional approximate message passing methodology. It is tailored for multi-period experiments and is particularly effective in settings with many units and prevalent network interference. The framework models causal effects as a dynamic process where a treated unit’s impact propagates through the network via neighboring units until equilibrium is reached. This approach allows us to approximate the dynamics of potential outcomes over time, enabling the extraction of valuable information before treatment effects reach equilibrium. Utilizing causal message-passing, we introduce a practical algorithm to estimate the total treatment effect, defined as the impact observed when all units are treated compared to the scenario where no unit receives treatment. We demonstrate the effectiveness of this approach across five numerical scenarios, each characterized by a distinct interference structure.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.08340&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sadegh Shirani, Mohsen Bayati</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Randomized experiments are a powerful methodology for data-driven evaluation of decisions or interventions. Yet, their validity may be undermined by network interference. This occurs when the treatment of one unit impacts not only its outcome but also that of connected units, biasing traditional treatment effect estimations. Our study introduces a new framework to accommodate complex and unknown network interference, moving beyond specialized models in the existing literature. Our framework, termed causal message-passing, is grounded in high-dimensional approximate message passing methodology. It is tailored for multi-period experiments and is particularly effective in settings with many units and prevalent network interference. The framework models causal effects as a dynamic process where a treated unit’s impact propagates through the network via neighboring units until equilibrium is reached. This approach allows us to approximate the dynamics of potential outcomes over time, enabling the extraction of valuable information before treatment effects reach equilibrium. Utilizing causal message-passing, we introduce a practical algorithm to estimate the total treatment effect, defined as the impact observed when all units are treated compared to the scenario where no unit receives treatment. We demonstrate the effectiveness of this approach across five numerical scenarios, each characterized by a distinct interference structure.</summary></entry><entry><title type="html">Comparing Lasso and Adaptive Lasso in High-Dimensional Data: A Genetic Survival Analysis in Triple-Negative Breast Cancer</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ComparingLassoandAdaptiveLassoinHighDimensionalDataAGeneticSurvivalAnalysisinTripleNegativeBreastCancer.html" rel="alternate" type="text/html" title="Comparing Lasso and Adaptive Lasso in High-Dimensional Data: A Genetic Survival Analysis in Triple-Negative Breast Cancer" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ComparingLassoandAdaptiveLassoinHighDimensionalDataAGeneticSurvivalAnalysisinTripleNegativeBreastCancer</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ComparingLassoandAdaptiveLassoinHighDimensionalDataAGeneticSurvivalAnalysisinTripleNegativeBreastCancer.html">&lt;p&gt;This study aims to evaluate the performance of Cox regression with lasso penalty and adaptive lasso penalty in high-dimensional settings. Variable selection methods are necessary in this context to reduce dimensionality and make the problem feasible. Several weight calculation procedures for adaptive lasso are proposed to determine if they offer an improvement over lasso, as adaptive lasso addresses its inherent bias. These proposed weights are based on principal component analysis, ridge regression, univariate Cox regressions and random survival forest (RSF). The proposals are evaluated in simulated datasets.
  A real application of these methodologies in the context of genomic data is also carried out. The study consists of determining the variables, clinical and genetic, that influence the survival of patients with triple-negative breast cancer (TNBC), which is a type breast cancer with low survival rates due to its aggressive nature.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19213&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Pilar González-Barquero , Rosa E. Lillo , Álvaro Méndez-Civieta</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">This study aims to evaluate the performance of Cox regression with lasso penalty and adaptive lasso penalty in high-dimensional settings. Variable selection methods are necessary in this context to reduce dimensionality and make the problem feasible. Several weight calculation procedures for adaptive lasso are proposed to determine if they offer an improvement over lasso, as adaptive lasso addresses its inherent bias. These proposed weights are based on principal component analysis, ridge regression, univariate Cox regressions and random survival forest (RSF). The proposals are evaluated in simulated datasets. A real application of these methodologies in the context of genomic data is also carried out. The study consists of determining the variables, clinical and genetic, that influence the survival of patients with triple-negative breast cancer (TNBC), which is a type breast cancer with low survival rates due to its aggressive nature.</summary></entry><entry><title type="html">Confidence interval estimation of mixed oil length with conditional diffusion model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Confidenceintervalestimationofmixedoillengthwithconditionaldiffusionmodel.html" rel="alternate" type="text/html" title="Confidence interval estimation of mixed oil length with conditional diffusion model" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Confidenceintervalestimationofmixedoillengthwithconditionaldiffusionmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Confidenceintervalestimationofmixedoillengthwithconditionaldiffusionmodel.html">&lt;p&gt;Accurately estimating the mixed oil length plays a big role in the economic benefit for oil pipeline network. While various proposed methods have tried to predict the mixed oil length, they often exhibit an extremely high probability (around 50\%) of underestimating it. This is attributed to their failure to consider the statistical variability inherent in the estimated length of mixed oil. To address such issues, we propose to use the conditional diffusion model to learn the distribution of the mixed oil length given pipeline features. Subsequently, we design a confidence interval estimation for the length of the mixed oil based on the pseudo-samples generated by the learned diffusion model. To our knowledge, we are the first to present an estimation scheme for confidence interval of the oil-mixing length that considers statistical variability, thereby reducing the possibility of underestimating it. When employing the upper bound of the interval as a reference for excluding the mixed oil, the probability of underestimation can be as minimal as 5\%, a substantial reduction compared to 50\%. Furthermore, utilizing the mean of the generated pseudo samples as the estimator for the mixed oil length enhances prediction accuracy by at least 10\% compared to commonly used methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18603&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yanfeng Yang, Lihong Zhang, Ziqi Chen, Miaomiao Yu, Lei Chen</name></author><category term="stat.AP" /><summary type="html">Accurately estimating the mixed oil length plays a big role in the economic benefit for oil pipeline network. While various proposed methods have tried to predict the mixed oil length, they often exhibit an extremely high probability (around 50\%) of underestimating it. This is attributed to their failure to consider the statistical variability inherent in the estimated length of mixed oil. To address such issues, we propose to use the conditional diffusion model to learn the distribution of the mixed oil length given pipeline features. Subsequently, we design a confidence interval estimation for the length of the mixed oil based on the pseudo-samples generated by the learned diffusion model. To our knowledge, we are the first to present an estimation scheme for confidence interval of the oil-mixing length that considers statistical variability, thereby reducing the possibility of underestimating it. When employing the upper bound of the interval as a reference for excluding the mixed oil, the probability of underestimation can be as minimal as 5\%, a substantial reduction compared to 50\%. Furthermore, utilizing the mean of the generated pseudo samples as the estimator for the mixed oil length enhances prediction accuracy by at least 10\% compared to commonly used methods.</summary></entry><entry><title type="html">Credit Ratings: Heterogeneous Effect on Capital Structure</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/CreditRatingsHeterogeneousEffectonCapitalStructure.html" rel="alternate" type="text/html" title="Credit Ratings: Heterogeneous Effect on Capital Structure" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/CreditRatingsHeterogeneousEffectonCapitalStructure</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/CreditRatingsHeterogeneousEffectonCapitalStructure.html">&lt;p&gt;Why do companies choose particular capital structures? A compelling answer to this question remains elusive despite extensive research. In this article, we use double machine learning to examine the heterogeneous causal effect of credit ratings on leverage. Taking advantage of the flexibility of random forests within the double machine learning framework, we model the relationship between variables associated with leverage and credit ratings without imposing strong assumptions about their functional form. This approach also allows for data-driven variable selection from a large set of individual company characteristics, supporting valid causal inference. We report three findings: First, credit ratings causally affect the leverage ratio. Having a rating, as opposed to having none, increases leverage by approximately 7 to 9 percentage points, or 30\% to 40\% relative to the sample mean leverage. However, this result comes with an important caveat, captured in our second finding: the effect is highly heterogeneous and varies depending on the specific rating. For AAA and AA ratings, the effect is negative, reducing leverage by about 5 percentage points. For A and BBB ratings, the effect is approximately zero. From BB ratings onwards, the effect becomes positive, exceeding 10 percentage points. Third, contrary to what the second finding might imply at first glance, the change from no effect to a positive effect does not occur abruptly at the boundary between investment and speculative grade ratings. Rather, it is gradual, taking place across the granular rating notches (“+/-“) within the BBB and BB categories.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18936&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Helmut Wasserbacher, Martin Spindler</name></author><category term="stat.AP" /><summary type="html">Why do companies choose particular capital structures? A compelling answer to this question remains elusive despite extensive research. In this article, we use double machine learning to examine the heterogeneous causal effect of credit ratings on leverage. Taking advantage of the flexibility of random forests within the double machine learning framework, we model the relationship between variables associated with leverage and credit ratings without imposing strong assumptions about their functional form. This approach also allows for data-driven variable selection from a large set of individual company characteristics, supporting valid causal inference. We report three findings: First, credit ratings causally affect the leverage ratio. Having a rating, as opposed to having none, increases leverage by approximately 7 to 9 percentage points, or 30\% to 40\% relative to the sample mean leverage. However, this result comes with an important caveat, captured in our second finding: the effect is highly heterogeneous and varies depending on the specific rating. For AAA and AA ratings, the effect is negative, reducing leverage by about 5 percentage points. For A and BBB ratings, the effect is approximately zero. From BB ratings onwards, the effect becomes positive, exceeding 10 percentage points. Third, contrary to what the second finding might imply at first glance, the change from no effect to a positive effect does not occur abruptly at the boundary between investment and speculative grade ratings. Rather, it is gradual, taking place across the granular rating notches (“+/-“) within the BBB and BB categories.</summary></entry><entry><title type="html">Cutting Feedback in Misspecified Copula Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/CuttingFeedbackinMisspecifiedCopulaModels.html" rel="alternate" type="text/html" title="Cutting Feedback in Misspecified Copula Models" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/CuttingFeedbackinMisspecifiedCopulaModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/CuttingFeedbackinMisspecifiedCopulaModels.html">&lt;p&gt;In copula models the marginal distributions and copula function are specified separately. We treat these as two modules in a modular Bayesian inference framework, and propose conducting modified Bayesian inference by “cutting feedback”. Cutting feedback limits the influence of potentially misspecified modules in posterior inference. We consider two types of cuts. The first limits the influence of a misspecified copula on inference for the marginals, which is a Bayesian analogue of the popular Inference for Margins (IFM) estimator. The second limits the influence of misspecified marginals on inference for the copula parameters by using a pseudo likelihood of the ranks to define the cut model. We establish that if only one of the modules is misspecified, then the appropriate cut posterior gives accurate uncertainty quantification asymptotically for the parameters in the other module. Computation of the cut posteriors is difficult, and new variational inference methods to do so are proposed. The efficacy of the new methodology is demonstrated using both simulated data and a substantive multivariate time series application from macroeconomic forecasting. In the latter, cutting feedback from misspecified marginals to a 1096 dimension copula improves posterior inference and predictive accuracy greatly, compared to conventional Bayesian inference.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.03521&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael Stanley Smith, Weichang Yu, David J. Nott, David Frazier</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In copula models the marginal distributions and copula function are specified separately. We treat these as two modules in a modular Bayesian inference framework, and propose conducting modified Bayesian inference by “cutting feedback”. Cutting feedback limits the influence of potentially misspecified modules in posterior inference. We consider two types of cuts. The first limits the influence of a misspecified copula on inference for the marginals, which is a Bayesian analogue of the popular Inference for Margins (IFM) estimator. The second limits the influence of misspecified marginals on inference for the copula parameters by using a pseudo likelihood of the ranks to define the cut model. We establish that if only one of the modules is misspecified, then the appropriate cut posterior gives accurate uncertainty quantification asymptotically for the parameters in the other module. Computation of the cut posteriors is difficult, and new variational inference methods to do so are proposed. The efficacy of the new methodology is demonstrated using both simulated data and a substantive multivariate time series application from macroeconomic forecasting. In the latter, cutting feedback from misspecified marginals to a 1096 dimension copula improves posterior inference and predictive accuracy greatly, compared to conventional Bayesian inference.</summary></entry><entry><title type="html">Data Sketching and Stacking: A Confluence of Two Strategies for Predictive Inference in Gaussian Process Regressions with High-Dimensional Features</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/DataSketchingandStackingAConfluenceofTwoStrategiesforPredictiveInferenceinGaussianProcessRegressionswithHighDimensionalFeatures.html" rel="alternate" type="text/html" title="Data Sketching and Stacking: A Confluence of Two Strategies for Predictive Inference in Gaussian Process Regressions with High-Dimensional Features" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/DataSketchingandStackingAConfluenceofTwoStrategiesforPredictiveInferenceinGaussianProcessRegressionswithHighDimensionalFeatures</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/DataSketchingandStackingAConfluenceofTwoStrategiesforPredictiveInferenceinGaussianProcessRegressionswithHighDimensionalFeatures.html">&lt;p&gt;This article focuses on drawing computationally-efficient predictive inference from Gaussian process (GP) regressions with a large number of features when the response is conditionally independent of the features given the projection to a noisy low dimensional manifold. Bayesian estimation of the regression relationship using Markov Chain Monte Carlo and subsequent predictive inference is computationally prohibitive and may lead to inferential inaccuracies since accurate variable selection is essentially impossible in such high-dimensional GP regressions. As an alternative, this article proposes a strategy to sketch the high-dimensional feature vector with a carefully constructed sketching matrix, before fitting a GP with the scalar outcome and the sketched feature vector to draw predictive inference. The analysis is performed in parallel with many different sketching matrices and smoothing parameters in different processors, and the predictive inferences are combined using Bayesian predictive stacking. Since posterior predictive distribution in each processor is analytically tractable, the algorithm allows bypassing the robustness issues due to convergence and mixing of MCMC chains, leading to fast implementation with very large number of features. Simulation studies show superior performance of the proposed approach with a wide variety of competitors. The approach outperforms competitors in drawing point prediction with predictive uncertainties of outdoor air pollution from satellite images.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18681&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Samuel Gailliot, Rajarshi Guhaniyogi, Roger D. Peng</name></author><category term="stat.ME" /><summary type="html">This article focuses on drawing computationally-efficient predictive inference from Gaussian process (GP) regressions with a large number of features when the response is conditionally independent of the features given the projection to a noisy low dimensional manifold. Bayesian estimation of the regression relationship using Markov Chain Monte Carlo and subsequent predictive inference is computationally prohibitive and may lead to inferential inaccuracies since accurate variable selection is essentially impossible in such high-dimensional GP regressions. As an alternative, this article proposes a strategy to sketch the high-dimensional feature vector with a carefully constructed sketching matrix, before fitting a GP with the scalar outcome and the sketched feature vector to draw predictive inference. The analysis is performed in parallel with many different sketching matrices and smoothing parameters in different processors, and the predictive inferences are combined using Bayesian predictive stacking. Since posterior predictive distribution in each processor is analytically tractable, the algorithm allows bypassing the robustness issues due to convergence and mixing of MCMC chains, leading to fast implementation with very large number of features. Simulation studies show superior performance of the proposed approach with a wide variety of competitors. The approach outperforms competitors in drawing point prediction with predictive uncertainties of outdoor air pollution from satellite images.</summary></entry><entry><title type="html">Eliciting prior information from clinical trials via calibrated Bayes factor</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ElicitingpriorinformationfromclinicaltrialsviacalibratedBayesfactor.html" rel="alternate" type="text/html" title="Eliciting prior information from clinical trials via calibrated Bayes factor" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ElicitingpriorinformationfromclinicaltrialsviacalibratedBayesfactor</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ElicitingpriorinformationfromclinicaltrialsviacalibratedBayesfactor.html">&lt;p&gt;In the Bayesian framework power prior distributions are increasingly adopted in clinical trials and similar studies to incorporate external and past information, typically to inform the parameter associated to a treatment effect. Their use is particularly effective in scenarios with small sample sizes and where robust prior information is actually available. A crucial component of this methodology is represented by its weight parameter, which controls the volume of historical information incorporated into the current analysis. This parameter can be considered as either fixed or random. Although various strategies exist for its determination, eliciting the prior distribution of the weight parameter according to a full Bayesian approach remains a challenge. In general, this parameter should be carefully selected to accurately reflect the available prior information without dominating the posterior inferential conclusions. To this aim, we propose a novel method for eliciting the prior distribution of the weight parameter through a simulation-based calibrated Bayes factor procedure. This approach allows for the prior distribution to be updated based on the strength of evidence provided by the data: The goal is to facilitate the integration of historical data when it aligns with current information and to limit it when discrepancies arise in terms, for instance, of prior-data conflicts. The performance of the proposed method is tested through simulation studies and applied to real data from clinical trials.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19346&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roberto Macrì Demartino, Leonardo Egidi, Nicola Torelli, Ioannis Ntzoufras</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In the Bayesian framework power prior distributions are increasingly adopted in clinical trials and similar studies to incorporate external and past information, typically to inform the parameter associated to a treatment effect. Their use is particularly effective in scenarios with small sample sizes and where robust prior information is actually available. A crucial component of this methodology is represented by its weight parameter, which controls the volume of historical information incorporated into the current analysis. This parameter can be considered as either fixed or random. Although various strategies exist for its determination, eliciting the prior distribution of the weight parameter according to a full Bayesian approach remains a challenge. In general, this parameter should be carefully selected to accurately reflect the available prior information without dominating the posterior inferential conclusions. To this aim, we propose a novel method for eliciting the prior distribution of the weight parameter through a simulation-based calibrated Bayes factor procedure. This approach allows for the prior distribution to be updated based on the strength of evidence provided by the data: The goal is to facilitate the integration of historical data when it aligns with current information and to limit it when discrepancies arise in terms, for instance, of prior-data conflicts. The performance of the proposed method is tested through simulation studies and applied to real data from clinical trials.</summary></entry><entry><title type="html">Exact confidence intervals for functions of parameters in the k-sample multinomial problem</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Exactconfidenceintervalsforfunctionsofparametersintheksamplemultinomialproblem.html" rel="alternate" type="text/html" title="Exact confidence intervals for functions of parameters in the k-sample multinomial problem" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Exactconfidenceintervalsforfunctionsofparametersintheksamplemultinomialproblem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Exactconfidenceintervalsforfunctionsofparametersintheksamplemultinomialproblem.html">&lt;p&gt;When the target of inference is a real-valued function of probability parameters in the k-sample multinomial problem, variance estimation may be challenging. In small samples, methods like the nonparametric bootstrap or delta method may perform poorly. We propose a novel general method in this setting for computing exact p-values and confidence intervals which means that type I error rates are correctly bounded and confidence intervals have at least nominal coverage at all sample sizes. Our method is applicable to any real-valued function of multinomial probabilities, accommodating an arbitrary number of samples with varying category counts. We describe the method and provide an implementation of it in R, with some computational optimization to ensure broad applicability. Simulations demonstrate our method’s ability to maintain correct coverage rates in settings where the nonparametric bootstrap fails.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19141&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael C Sachs, Erin E Gabriel, Michael P Fay</name></author><category term="stat.CO" /><summary type="html">When the target of inference is a real-valued function of probability parameters in the k-sample multinomial problem, variance estimation may be challenging. In small samples, methods like the nonparametric bootstrap or delta method may perform poorly. We propose a novel general method in this setting for computing exact p-values and confidence intervals which means that type I error rates are correctly bounded and confidence intervals have at least nominal coverage at all sample sizes. Our method is applicable to any real-valued function of multinomial probabilities, accommodating an arbitrary number of samples with varying category counts. We describe the method and provide an implementation of it in R, with some computational optimization to ensure broad applicability. Simulations demonstrate our method’s ability to maintain correct coverage rates in settings where the nonparametric bootstrap fails.</summary></entry><entry><title type="html">Full Information Linked ICA: addressing missing data problem in multimodal fusion</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/FullInformationLinkedICAaddressingmissingdataprobleminmultimodalfusion.html" rel="alternate" type="text/html" title="Full Information Linked ICA: addressing missing data problem in multimodal fusion" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/FullInformationLinkedICAaddressingmissingdataprobleminmultimodalfusion</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/FullInformationLinkedICAaddressingmissingdataprobleminmultimodalfusion.html">&lt;p&gt;Recent advances in multimodal imaging acquisition techniques have allowed us to measure different aspects of brain structure and function. Multimodal fusion, such as linked independent component analysis (LICA), is popularly used to integrate complementary information. However, it has suffered from missing data, commonly occurring in neuroimaging data. Therefore, in this paper, we propose a Full Information LICA algorithm (FI-LICA) to handle the missing data problem during multimodal fusion under the LICA framework. Built upon complete cases, our method employs the principle of full information and utilizes all available information to recover the missing latent information. Our simulation experiments showed the ideal performance of FI-LICA compared to current practices. Further, we applied FI-LICA to multimodal data from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) study, showcasing better performance in classifying current diagnosis and in predicting the AD transition of participants with mild cognitive impairment (MCI), thereby highlighting the practical utility of our proposed method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18829&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ruiyang Li, F. DuBois Bowman, Seonjoo Lee</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Recent advances in multimodal imaging acquisition techniques have allowed us to measure different aspects of brain structure and function. Multimodal fusion, such as linked independent component analysis (LICA), is popularly used to integrate complementary information. However, it has suffered from missing data, commonly occurring in neuroimaging data. Therefore, in this paper, we propose a Full Information LICA algorithm (FI-LICA) to handle the missing data problem during multimodal fusion under the LICA framework. Built upon complete cases, our method employs the principle of full information and utilizes all available information to recover the missing latent information. Our simulation experiments showed the ideal performance of FI-LICA compared to current practices. Further, we applied FI-LICA to multimodal data from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) study, showcasing better performance in classifying current diagnosis and in predicting the AD transition of participants with mild cognitive impairment (MCI), thereby highlighting the practical utility of our proposed method.</summary></entry><entry><title type="html">Functional Adaptive Double-Sparsity Estimator for Functional Linear Regression Model with Multiple Functional Covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/FunctionalAdaptiveDoubleSparsityEstimatorforFunctionalLinearRegressionModelwithMultipleFunctionalCovariates.html" rel="alternate" type="text/html" title="Functional Adaptive Double-Sparsity Estimator for Functional Linear Regression Model with Multiple Functional Covariates" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/FunctionalAdaptiveDoubleSparsityEstimatorforFunctionalLinearRegressionModelwithMultipleFunctionalCovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/FunctionalAdaptiveDoubleSparsityEstimatorforFunctionalLinearRegressionModelwithMultipleFunctionalCovariates.html">&lt;p&gt;Sensor devices have been increasingly used in engineering and health studies recently, and the captured multi-dimensional activity and vital sign signals can be studied in association with health outcomes to inform public health. The common approach is the scalar-on-function regression model, in which health outcomes are the scalar responses while high-dimensional sensor signals are the functional covariates, but how to effectively interpret results becomes difficult. In this study, we propose a new Functional Adaptive Double-Sparsity (FadDoS) estimator based on functional regularization of sparse group lasso with multiple functional predictors, which can achieve global sparsity via functional variable selection and local sparsity via zero-subinterval identification within coefficient functions. We prove that the FadDoS estimator converges at a bounded rate and satisfies the oracle property under mild conditions. Extensive simulation studies confirm the theoretical properties and exhibit excellent performances compared to existing approaches. Application to a Kinect sensor study that utilized an advanced motion sensing device tracking human multiple joint movements and conducted among community-dwelling elderly demonstrates how the FadDoS estimator can effectively characterize the detailed association between joint movements and physical health assessments. The proposed method is not only effective in Kinect sensor analysis but also applicable to broader fields, where multi-dimensional sensor signals are collected simultaneously, to expand the use of sensor devices in health studies and facilitate sensor data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.10054&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Cheng Cao, Jiguo Cao, Hailiang Wang, Kwok-Leung Tsui, Xinyue Li</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Sensor devices have been increasingly used in engineering and health studies recently, and the captured multi-dimensional activity and vital sign signals can be studied in association with health outcomes to inform public health. The common approach is the scalar-on-function regression model, in which health outcomes are the scalar responses while high-dimensional sensor signals are the functional covariates, but how to effectively interpret results becomes difficult. In this study, we propose a new Functional Adaptive Double-Sparsity (FadDoS) estimator based on functional regularization of sparse group lasso with multiple functional predictors, which can achieve global sparsity via functional variable selection and local sparsity via zero-subinterval identification within coefficient functions. We prove that the FadDoS estimator converges at a bounded rate and satisfies the oracle property under mild conditions. Extensive simulation studies confirm the theoretical properties and exhibit excellent performances compared to existing approaches. Application to a Kinect sensor study that utilized an advanced motion sensing device tracking human multiple joint movements and conducted among community-dwelling elderly demonstrates how the FadDoS estimator can effectively characterize the detailed association between joint movements and physical health assessments. The proposed method is not only effective in Kinect sensor analysis but also applicable to broader fields, where multi-dimensional sensor signals are collected simultaneously, to expand the use of sensor devices in health studies and facilitate sensor data analysis.</summary></entry><entry><title type="html">Functional knockoffs selection with applications to functional data analysis in high dimensions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Functionalknockoffsselectionwithapplicationstofunctionaldataanalysisinhighdimensions.html" rel="alternate" type="text/html" title="Functional knockoffs selection with applications to functional data analysis in high dimensions" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Functionalknockoffsselectionwithapplicationstofunctionaldataanalysisinhighdimensions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Functionalknockoffsselectionwithapplicationstofunctionaldataanalysisinhighdimensions.html">&lt;p&gt;The knockoffs is a recently proposed powerful framework that effectively controls the false discovery rate (FDR) for variable selection. However, none of the existing knockoff solutions are directly suited to handle multivariate or high-dimensional functional data, which has become increasingly prevalent in various scientific applications. In this paper, we propose a novel functional model-X knockoffs selection framework tailored to sparse high-dimensional functional models, and show that our proposal can achieve the effective FDR control for any sample size. Furthermore, we illustrate the proposed functional model-X knockoffs selection procedure along with the associated theoretical guarantees for both FDR control and asymptotic power using examples of commonly adopted functional linear additive regression models and the functional graphical model. In the construction of functional knockoffs, we integrate essential components including the correlation operator matrix, the Karhunen-Lo`eve expansion, and semidefinite programming, and develop executable algorithms. We demonstrate the superiority of our proposed methods over the competitors through both extensive simulations and the analysis of two brain imaging datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18189&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinghao Qiao, Mingya Long, Qizhai Li</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">The knockoffs is a recently proposed powerful framework that effectively controls the false discovery rate (FDR) for variable selection. However, none of the existing knockoff solutions are directly suited to handle multivariate or high-dimensional functional data, which has become increasingly prevalent in various scientific applications. In this paper, we propose a novel functional model-X knockoffs selection framework tailored to sparse high-dimensional functional models, and show that our proposal can achieve the effective FDR control for any sample size. Furthermore, we illustrate the proposed functional model-X knockoffs selection procedure along with the associated theoretical guarantees for both FDR control and asymptotic power using examples of commonly adopted functional linear additive regression models and the functional graphical model. In the construction of functional knockoffs, we integrate essential components including the correlation operator matrix, the Karhunen-Lo`eve expansion, and semidefinite programming, and develop executable algorithms. We demonstrate the superiority of our proposed methods over the competitors through both extensive simulations and the analysis of two brain imaging datasets.</summary></entry><entry><title type="html">Gratia: An R package for exploring generalized additive models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/GratiaAnRpackageforexploringgeneralizedadditivemodels.html" rel="alternate" type="text/html" title="Gratia: An R package for exploring generalized additive models" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/GratiaAnRpackageforexploringgeneralizedadditivemodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/GratiaAnRpackageforexploringgeneralizedadditivemodels.html">&lt;p&gt;Generalized additive models (GAMs, Hastie &amp;amp; Tibshirani, 1990; Wood, 2017) are an extension of the generalized linear model that allows the effects of covariates to be modelled as smooth functions. GAMs are increasingly used in many areas of science (e.g. Pedersen, Miller, Simpson, &amp;amp; Ross, 2019; Simpson, 2018) because the smooth functions allow nonlinear relationships between covariates and the response to be learned from the data through the use of penalized splines. Within the R (R Core Team, 2024) ecosystem, Simon Wood’s mgcv package (Wood, 2017) is widely used to fit GAMs and is a Recommended package that ships with R as part of the default install. A growing number of other R packages build upon mgcv, for example as an engine to fit specialised models not handled by mgcv itself (e.g. GJMR, Marra &amp;amp; Radice, 2023), or to make use of the wide range of splines available in mgcv (e.g. brms, B&quot;urkner, 2017).
  The gratia package builds upon mgcv by providing functions that make working with GAMs easier. gratia takes a tidy approach (Wickham, 2014) providing ggplot2 (Wickham, 2016) replacements for mgcv’s base graphics-based plots, functions for model diagnostics and exploration of fitted models, and a family of functions for drawing samples from the posterior distribution of a fitted GAM. Additional functionality is provided to facilitate the teaching and understanding of GAMs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19082&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Gavin L. Simpson</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Generalized additive models (GAMs, Hastie &amp;amp; Tibshirani, 1990; Wood, 2017) are an extension of the generalized linear model that allows the effects of covariates to be modelled as smooth functions. GAMs are increasingly used in many areas of science (e.g. Pedersen, Miller, Simpson, &amp;amp; Ross, 2019; Simpson, 2018) because the smooth functions allow nonlinear relationships between covariates and the response to be learned from the data through the use of penalized splines. Within the R (R Core Team, 2024) ecosystem, Simon Wood’s mgcv package (Wood, 2017) is widely used to fit GAMs and is a Recommended package that ships with R as part of the default install. A growing number of other R packages build upon mgcv, for example as an engine to fit specialised models not handled by mgcv itself (e.g. GJMR, Marra &amp;amp; Radice, 2023), or to make use of the wide range of splines available in mgcv (e.g. brms, B&quot;urkner, 2017). The gratia package builds upon mgcv by providing functions that make working with GAMs easier. gratia takes a tidy approach (Wickham, 2014) providing ggplot2 (Wickham, 2016) replacements for mgcv’s base graphics-based plots, functions for model diagnostics and exploration of fitted models, and a family of functions for drawing samples from the posterior distribution of a fitted GAM. Additional functionality is provided to facilitate the teaching and understanding of GAMs.</summary></entry><entry><title type="html">Heavy tails and negative correlation in a binomial model for sports matches: applications to curling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Heavytailsandnegativecorrelationinabinomialmodelforsportsmatchesapplicationstocurling.html" rel="alternate" type="text/html" title="Heavy tails and negative correlation in a binomial model for sports matches: applications to curling" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Heavytailsandnegativecorrelationinabinomialmodelforsportsmatchesapplicationstocurling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Heavytailsandnegativecorrelationinabinomialmodelforsportsmatchesapplicationstocurling.html">&lt;p&gt;A binomial model for sports matches is developed making use of the maximum possible score $n$ in a game. In contrast to previous approaches the scores of the two teams are negatively correlated, abstracting from a scenario whereby teams cancel each other out. When $n$ is known, analytical results are possible via a Gaussian approximation. Model calibration is obtained via generalized linear modelling, enabling elementary econometric and strategic analysis to be performed. Inter alia this includes quantifying the Last Stone First End effect, analogous to the home-field advantage found in conventional sports. When $n$ is unknown the model behaviour is richer and leads to heavy-tailed non-Gaussian behaviour. We present an approximate analysis of this case based on the Variance Gamma distribution.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18601&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>John Fry, Mark Austin, Silvio Fanzon</name></author><category term="stat.AP" /><summary type="html">A binomial model for sports matches is developed making use of the maximum possible score $n$ in a game. In contrast to previous approaches the scores of the two teams are negatively correlated, abstracting from a scenario whereby teams cancel each other out. When $n$ is known, analytical results are possible via a Gaussian approximation. Model calibration is obtained via generalized linear modelling, enabling elementary econometric and strategic analysis to be performed. Inter alia this includes quantifying the Last Stone First End effect, analogous to the home-field advantage found in conventional sports. When $n$ is unknown the model behaviour is richer and leads to heavy-tailed non-Gaussian behaviour. We present an approximate analysis of this case based on the Variance Gamma distribution.</summary></entry><entry><title type="html">How to build your latent Markov model – the role of time and space</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/HowtobuildyourlatentMarkovmodeltheroleoftimeandspace.html" rel="alternate" type="text/html" title="How to build your latent Markov model – the role of time and space" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/HowtobuildyourlatentMarkovmodeltheroleoftimeandspace</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/HowtobuildyourlatentMarkovmodeltheroleoftimeandspace.html">&lt;p&gt;Statistical models that involve latent Markovian state processes have become immensely popular tools for analysing time series and other sequential data. However, the plethora of model formulations, the inconsistent use of terminology, and the various inferential approaches and software packages can be overwhelming to practitioners, especially when they are new to this area. With this review-like paper, we thus aim to provide guidance for both statisticians and practitioners working with latent Markov models by offering a unifying view on what otherwise are often considered separate model classes, from hidden Markov models over state-space models to Markov-modulated Poisson processes. In particular, we provide a roadmap for identifying a suitable latent Markov model formulation given the data to be analysed. Furthermore, we emphasise that it is key to applied work with any of these model classes to understand how recursive techniques exploiting the models’ dependence structure can be used for inference. The R package LaMa adapts this unified view and provides an easy-to-use framework for very fast (C++ based) evaluation of the likelihood of any of the models discussed in this paper, allowing users to tailor a latent Markov model to their data using a Lego-type approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19157&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sina Mews, Jan-Ole Koslik, Roland Langrock</name></author><category term="stat.ME" /><summary type="html">Statistical models that involve latent Markovian state processes have become immensely popular tools for analysing time series and other sequential data. However, the plethora of model formulations, the inconsistent use of terminology, and the various inferential approaches and software packages can be overwhelming to practitioners, especially when they are new to this area. With this review-like paper, we thus aim to provide guidance for both statisticians and practitioners working with latent Markov models by offering a unifying view on what otherwise are often considered separate model classes, from hidden Markov models over state-space models to Markov-modulated Poisson processes. In particular, we provide a roadmap for identifying a suitable latent Markov model formulation given the data to be analysed. Furthermore, we emphasise that it is key to applied work with any of these model classes to understand how recursive techniques exploiting the models’ dependence structure can be used for inference. The R package LaMa adapts this unified view and provides an easy-to-use framework for very fast (C++ based) evaluation of the likelihood of any of the models discussed in this paper, allowing users to tailor a latent Markov model to their data using a Lego-type approach.</summary></entry><entry><title type="html">JAXbind: Bind any function to JAX</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/JAXbindBindanyfunctiontoJAX.html" rel="alternate" type="text/html" title="JAXbind: Bind any function to JAX" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/JAXbindBindanyfunctiontoJAX</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/JAXbindBindanyfunctiontoJAX.html">&lt;p&gt;JAX is widely used in machine learning and scientific computing, the latter of which often relies on existing high-performance code that we would ideally like to incorporate into JAX. Reimplementing the existing code in JAX is often impractical and the existing interface in JAX for binding custom code either limits the user to a single Jacobian product or requires deep knowledge of JAX and its C++ backend for general Jacobian products. With JAXbind we drastically reduce the effort required to bind custom functions implemented in other programming languages with full support for Jacobian-vector products and vector-Jacobian products to JAX. Specifically, JAXbind provides an easy-to-use Python interface for defining custom, so-called JAX primitives. Via JAXbind, any function callable from Python can be exposed as a JAX primitive. JAXbind allows a user to interface the JAX function transformation engine with custom derivatives and batching rules, enabling all JAX transformations for the custom primitive.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.08847&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jakob Roth, Martin Reinecke, Gordian Edenhofer</name></author><category term="stat.CO" /><summary type="html">JAX is widely used in machine learning and scientific computing, the latter of which often relies on existing high-performance code that we would ideally like to incorporate into JAX. Reimplementing the existing code in JAX is often impractical and the existing interface in JAX for binding custom code either limits the user to a single Jacobian product or requires deep knowledge of JAX and its C++ backend for general Jacobian products. With JAXbind we drastically reduce the effort required to bind custom functions implemented in other programming languages with full support for Jacobian-vector products and vector-Jacobian products to JAX. Specifically, JAXbind provides an easy-to-use Python interface for defining custom, so-called JAX primitives. Via JAXbind, any function callable from Python can be exposed as a JAX primitive. JAXbind allows a user to interface the JAX function transformation engine with custom derivatives and batching rules, enabling all JAX transformations for the custom primitive.</summary></entry><entry><title type="html">Length Optimization in Conformal Prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/LengthOptimizationinConformalPrediction.html" rel="alternate" type="text/html" title="Length Optimization in Conformal Prediction" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/LengthOptimizationinConformalPrediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/LengthOptimizationinConformalPrediction.html">&lt;p&gt;Conditional validity and length efficiency are two crucial aspects of conformal prediction (CP). Achieving conditional validity ensures accurate uncertainty quantification for data subpopulations, while proper length efficiency ensures that the prediction sets remain informative and non-trivial. Despite significant efforts to address each of these issues individually, a principled framework that reconciles these two objectives has been missing in the CP literature. In this paper, we develop Conformal Prediction with Length-Optimization (CPL) - a novel framework that constructs prediction sets with (near-) optimal length while ensuring conditional validity under various classes of covariate shifts, including the key cases of marginal and group-conditional coverage. In the infinite sample regime, we provide strong duality results which indicate that CPL achieves conditional validity and length optimality. In the finite sample regime, we show that CPL constructs conditionally valid prediction sets. Our extensive empirical evaluations demonstrate the superior prediction set size performance of CPL compared to state-of-the-art methods across diverse real-world and synthetic datasets in classification, regression, and text-related settings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18814&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shayan Kiyani, George Pappas, Hamed Hassani</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Conditional validity and length efficiency are two crucial aspects of conformal prediction (CP). Achieving conditional validity ensures accurate uncertainty quantification for data subpopulations, while proper length efficiency ensures that the prediction sets remain informative and non-trivial. Despite significant efforts to address each of these issues individually, a principled framework that reconciles these two objectives has been missing in the CP literature. In this paper, we develop Conformal Prediction with Length-Optimization (CPL) - a novel framework that constructs prediction sets with (near-) optimal length while ensuring conditional validity under various classes of covariate shifts, including the key cases of marginal and group-conditional coverage. In the infinite sample regime, we provide strong duality results which indicate that CPL achieves conditional validity and length optimality. In the finite sample regime, we show that CPL constructs conditionally valid prediction sets. Our extensive empirical evaluations demonstrate the superior prediction set size performance of CPL compared to state-of-the-art methods across diverse real-world and synthetic datasets in classification, regression, and text-related settings.</summary></entry><entry><title type="html">Lithium-Ion Battery System Health Monitoring and Fault Analysis from Field Data Using Gaussian Processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/LithiumIonBatterySystemHealthMonitoringandFaultAnalysisfromFieldDataUsingGaussianProcesses.html" rel="alternate" type="text/html" title="Lithium-Ion Battery System Health Monitoring and Fault Analysis from Field Data Using Gaussian Processes" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/LithiumIonBatterySystemHealthMonitoringandFaultAnalysisfromFieldDataUsingGaussianProcesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/LithiumIonBatterySystemHealthMonitoringandFaultAnalysisfromFieldDataUsingGaussianProcesses.html">&lt;p&gt;Health monitoring, fault analysis, and detection are critical for the safe and sustainable operation of battery systems. We apply Gaussian process resistance models on lithium iron phosphate battery field data to effectively separate the time-dependent and operating point-dependent resistance. The data set contains 29 battery systems returned to the manufacturer for warranty, each with eight cells in series, totaling 232 cells and 131 million data rows. We develop probabilistic fault detection rules using recursive spatiotemporal Gaussian processes. These processes allow the quick processing of over a million data points, enabling advanced online monitoring and furthering the understanding of battery pack failure in the field. The analysis underlines that often, only a single cell shows abnormal behavior or a knee point, consistent with weakest-link failure for cells connected in series, amplified by local resistive heating. The results further the understanding of how batteries degrade and fail in the field and demonstrate the potential of efficient online monitoring based on data. We open-source the code and publish the large data set upon completion of the review of this article.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19015&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joachim Schaeffer, Eric Lenz, Duncan Gulla, Martin Z. Bazant, Richard D. Braatz, Rolf Findeisen</name></author><category term="stat.AP" /><summary type="html">Health monitoring, fault analysis, and detection are critical for the safe and sustainable operation of battery systems. We apply Gaussian process resistance models on lithium iron phosphate battery field data to effectively separate the time-dependent and operating point-dependent resistance. The data set contains 29 battery systems returned to the manufacturer for warranty, each with eight cells in series, totaling 232 cells and 131 million data rows. We develop probabilistic fault detection rules using recursive spatiotemporal Gaussian processes. These processes allow the quick processing of over a million data points, enabling advanced online monitoring and furthering the understanding of battery pack failure in the field. The analysis underlines that often, only a single cell shows abnormal behavior or a knee point, consistent with weakest-link failure for cells connected in series, amplified by local resistive heating. The results further the understanding of how batteries degrade and fail in the field and demonstrate the potential of efficient online monitoring based on data. We open-source the code and publish the large data set upon completion of the review of this article.</summary></entry><entry><title type="html">Mixture priors for replication studies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Mixturepriorsforreplicationstudies.html" rel="alternate" type="text/html" title="Mixture priors for replication studies" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Mixturepriorsforreplicationstudies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Mixturepriorsforreplicationstudies.html">&lt;p&gt;Replication of scientific studies is important for assessing the credibility of their results. However, there is no consensus on how to quantify the extent to which a replication study replicates an original result. We propose a novel Bayesian approach based on mixture priors. The idea is to use a mixture of the posterior distribution based on the original study and a non-informative distribution as the prior for the analysis of the replication study. The mixture weight then determines the extent to which the original and replication data are pooled.
  Two distinct strategies are presented: one with fixed mixture weights, and one that introduces uncertainty by assigning a prior distribution to the mixture weight itself. Furthermore, it is shown how within this framework Bayes factors can be used for formal testing of scientific hypotheses, such as tests regarding the presence or absence of an effect. To showcase the practical application of the methodology, we analyze data from three replication studies. Our findings suggest that mixture priors are a valuable and intuitive alternative to other Bayesian methods for analyzing replication studies, such as hierarchical models and power priors. We provide the free and open source R package repmix that implements the proposed methodology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19152&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roberto Macrì Demartino, Leonardo Egidi, Leonhard Held, Samuel Pawel</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Replication of scientific studies is important for assessing the credibility of their results. However, there is no consensus on how to quantify the extent to which a replication study replicates an original result. We propose a novel Bayesian approach based on mixture priors. The idea is to use a mixture of the posterior distribution based on the original study and a non-informative distribution as the prior for the analysis of the replication study. The mixture weight then determines the extent to which the original and replication data are pooled. Two distinct strategies are presented: one with fixed mixture weights, and one that introduces uncertainty by assigning a prior distribution to the mixture weight itself. Furthermore, it is shown how within this framework Bayes factors can be used for formal testing of scientific hypotheses, such as tests regarding the presence or absence of an effect. To showcase the practical application of the methodology, we analyze data from three replication studies. Our findings suggest that mixture priors are a valuable and intuitive alternative to other Bayesian methods for analyzing replication studies, such as hierarchical models and power priors. We provide the free and open source R package repmix that implements the proposed methodology.</summary></entry><entry><title type="html">MultiObjMatch: Matching with Optimal Tradeoffs between Multiple Objectives in R</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/MultiObjMatchMatchingwithOptimalTradeoffsbetweenMultipleObjectivesinR.html" rel="alternate" type="text/html" title="MultiObjMatch: Matching with Optimal Tradeoffs between Multiple Objectives in R" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/MultiObjMatchMatchingwithOptimalTradeoffsbetweenMultipleObjectivesinR</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/MultiObjMatchMatchingwithOptimalTradeoffsbetweenMultipleObjectivesinR.html">&lt;p&gt;In an observational study, matching aims to create many small sets of similar treated and control units from initial samples that may differ substantially in order to permit more credible causal inferences. The problem of constructing matched sets may be formulated as an optimization problem, but it can be challenging to specify a single objective function that adequately captures all the design considerations at work. One solution, proposed by \citet{pimentel2019optimal} is to explore a family of matched designs that are Pareto optimal for multiple objective functions. We present an R package, \href{https://github.com/ShichaoHan/MultiObjMatch}{\texttt{MultiObjMatch}}, that implements this multi-objective matching strategy using a network flow algorithm for several common design goals: marginal balance on important covariates, size of the matched sample, and average within-pair multivariate distances. We demonstrate the package’s flexibility in exploring user-defined tradeoffs of interest via two case studies, a reanalysis of the canonical National Supported Work dataset and a novel analysis of a clinical dataset to estimate the impact of diabetic kidney disease on hospitalization costs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18819&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shichao Han, Samuel D. Pimentel</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In an observational study, matching aims to create many small sets of similar treated and control units from initial samples that may differ substantially in order to permit more credible causal inferences. The problem of constructing matched sets may be formulated as an optimization problem, but it can be challenging to specify a single objective function that adequately captures all the design considerations at work. One solution, proposed by \citet{pimentel2019optimal} is to explore a family of matched designs that are Pareto optimal for multiple objective functions. We present an R package, \href{https://github.com/ShichaoHan/MultiObjMatch}{\texttt{MultiObjMatch}}, that implements this multi-objective matching strategy using a network flow algorithm for several common design goals: marginal balance on important covariates, size of the matched sample, and average within-pair multivariate distances. We demonstrate the package’s flexibility in exploring user-defined tradeoffs of interest via two case studies, a reanalysis of the canonical National Supported Work dataset and a novel analysis of a clinical dataset to estimate the impact of diabetic kidney disease on hospitalization costs.</summary></entry><entry><title type="html">Multi-level Phenotypic Models of Cardiovascular Disease and Obstructive Sleep Apnea Comorbidities: A Longitudinal Wisconsin Sleep Cohort Study</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/MultilevelPhenotypicModelsofCardiovascularDiseaseandObstructiveSleepApneaComorbiditiesALongitudinalWisconsinSleepCohortStudy.html" rel="alternate" type="text/html" title="Multi-level Phenotypic Models of Cardiovascular Disease and Obstructive Sleep Apnea Comorbidities: A Longitudinal Wisconsin Sleep Cohort Study" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/MultilevelPhenotypicModelsofCardiovascularDiseaseandObstructiveSleepApneaComorbiditiesALongitudinalWisconsinSleepCohortStudy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/MultilevelPhenotypicModelsofCardiovascularDiseaseandObstructiveSleepApneaComorbiditiesALongitudinalWisconsinSleepCohortStudy.html">&lt;p&gt;Cardiovascular diseases (CVDs) are notably prevalent among patients with obstructive sleep apnea (OSA), posing unique challenges in predicting CVD progression due to the intricate interactions of comorbidities. Traditional models typically lack the necessary dynamic and longitudinal scope to accurately forecast CVD trajectories in OSA patients. This study introduces a novel multi-level phenotypic model to analyze the progression and interplay of these conditions over time, utilizing data from the Wisconsin Sleep Cohort, which includes 1,123 participants followed for decades. Our methodology comprises three advanced steps: (1) Conducting feature importance analysis through tree-based models to underscore critical predictive variables like total cholesterol, low-density lipoprotein (LDL), and diabetes. (2) Developing a logistic mixed-effects model (LGMM) to track longitudinal transitions and pinpoint significant factors, which displayed a diagnostic accuracy of 0.9556. (3) Implementing t-distributed Stochastic Neighbor Embedding (t-SNE) alongside Gaussian Mixture Models (GMM) to segment patient data into distinct phenotypic clusters that reflect varied risk profiles and disease progression pathways. This phenotypic clustering revealed two main groups, with one showing a markedly increased risk of major adverse cardiovascular events (MACEs), underscored by the significant predictive role of nocturnal hypoxia and sympathetic nervous system activity from sleep data. Analysis of transitions and trajectories with t-SNE and GMM highlighted different progression rates within the cohort, with one cluster progressing more slowly towards severe CVD states than the other. This study offers a comprehensive understanding of the dynamic relationship between CVD and OSA, providing valuable tools for predicting disease onset and tailoring treatment approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18602&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Duy Nguyen, Ca Hoang, Phat K. Huynh, Tien Truong, Dang Nguyen, Abhay Sharma, Trung Q. Le</name></author><category term="stat.AP," /><category term="stat.CO" /><summary type="html">Cardiovascular diseases (CVDs) are notably prevalent among patients with obstructive sleep apnea (OSA), posing unique challenges in predicting CVD progression due to the intricate interactions of comorbidities. Traditional models typically lack the necessary dynamic and longitudinal scope to accurately forecast CVD trajectories in OSA patients. This study introduces a novel multi-level phenotypic model to analyze the progression and interplay of these conditions over time, utilizing data from the Wisconsin Sleep Cohort, which includes 1,123 participants followed for decades. Our methodology comprises three advanced steps: (1) Conducting feature importance analysis through tree-based models to underscore critical predictive variables like total cholesterol, low-density lipoprotein (LDL), and diabetes. (2) Developing a logistic mixed-effects model (LGMM) to track longitudinal transitions and pinpoint significant factors, which displayed a diagnostic accuracy of 0.9556. (3) Implementing t-distributed Stochastic Neighbor Embedding (t-SNE) alongside Gaussian Mixture Models (GMM) to segment patient data into distinct phenotypic clusters that reflect varied risk profiles and disease progression pathways. This phenotypic clustering revealed two main groups, with one showing a markedly increased risk of major adverse cardiovascular events (MACEs), underscored by the significant predictive role of nocturnal hypoxia and sympathetic nervous system activity from sleep data. Analysis of transitions and trajectories with t-SNE and GMM highlighted different progression rates within the cohort, with one cluster progressing more slowly towards severe CVD states than the other. This study offers a comprehensive understanding of the dynamic relationship between CVD and OSA, providing valuable tools for predicting disease onset and tailoring treatment approaches.</summary></entry><entry><title type="html">Nonlinear Multivariate Function-on-function Regression with Variable Selection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/NonlinearMultivariateFunctiononfunctionRegressionwithVariableSelection.html" rel="alternate" type="text/html" title="Nonlinear Multivariate Function-on-function Regression with Variable Selection" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/NonlinearMultivariateFunctiononfunctionRegressionwithVariableSelection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/NonlinearMultivariateFunctiononfunctionRegressionwithVariableSelection.html">&lt;p&gt;This paper proposes a multivariate nonlinear function-on-function regression model, which allows both the response and the covariates can be multi-dimensional functions. The model is built upon the multivariate functional reproducing kernel Hilbert space (RKHS) theory. It predicts the response function by linearly combining each covariate function in their respective functional RKHS, and extends the representation theorem to accommodate model estimation. Further variable selection is proposed by adding the lasso penalty to the coefficients of the kernel functions. A block coordinate descent algorithm is proposed for model estimation, and several theoretical properties are discussed. Finally, we evaluate the efficacy of our proposed model using simulation data and a real-case dataset in meteorology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19021&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xu Haijie, Zhang Chen</name></author><category term="stat.ME" /><summary type="html">This paper proposes a multivariate nonlinear function-on-function regression model, which allows both the response and the covariates can be multi-dimensional functions. The model is built upon the multivariate functional reproducing kernel Hilbert space (RKHS) theory. It predicts the response function by linearly combining each covariate function in their respective functional RKHS, and extends the representation theorem to accommodate model estimation. Further variable selection is proposed by adding the lasso penalty to the coefficients of the kernel functions. A block coordinate descent algorithm is proposed for model estimation, and several theoretical properties are discussed. Finally, we evaluate the efficacy of our proposed model using simulation data and a real-case dataset in meteorology.</summary></entry><entry><title type="html">Nonparametric Strategy Test</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/NonparametricStrategyTest.html" rel="alternate" type="text/html" title="Nonparametric Strategy Test" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/NonparametricStrategyTest</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/NonparametricStrategyTest.html">&lt;p&gt;We present a nonparametric statistical test for determining whether an agent is following a given mixed strategy in a repeated strategic-form game given samples of the agent’s play. This involves two components: determining whether the agent’s frequencies of pure strategies are sufficiently close to the target frequencies, and determining whether the pure strategies selected are independent between different game iterations. Our integrated test involves applying a chi-squared goodness of fit test for the first component and a generalized Wald-Wolfowitz runs test for the second component. The results from both tests are combined using Bonferroni correction to produce a complete test for a given significance level $\alpha.$ We applied the test to publicly available data of human rock-paper-scissors play. The data consists of 50 iterations of play for 500 human players. We test with a null hypothesis that the players are following a uniform random strategy independently at each game iteration. Using a significance level of $\alpha = 0.05$, we conclude that 305 (61%) of the subjects are following the target strategy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.10695&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sam Ganzfried</name></author><category term="stat.ME" /><summary type="html">We present a nonparametric statistical test for determining whether an agent is following a given mixed strategy in a repeated strategic-form game given samples of the agent’s play. This involves two components: determining whether the agent’s frequencies of pure strategies are sufficiently close to the target frequencies, and determining whether the pure strategies selected are independent between different game iterations. Our integrated test involves applying a chi-squared goodness of fit test for the first component and a generalized Wald-Wolfowitz runs test for the second component. The results from both tests are combined using Bonferroni correction to produce a complete test for a given significance level $\alpha.$ We applied the test to publicly available data of human rock-paper-scissors play. The data consists of 50 iterations of play for 500 human players. We test with a null hypothesis that the players are following a uniform random strategy independently at each game iteration. Using a significance level of $\alpha = 0.05$, we conclude that 305 (61%) of the subjects are following the target strategy.</summary></entry><entry><title type="html">On scalable ARMA models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/OnscalableARMAmodels.html" rel="alternate" type="text/html" title="On scalable ARMA models" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/OnscalableARMAmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/OnscalableARMAmodels.html">&lt;p&gt;This paper considers both the least squares and quasi-maximum likelihood estimation for the recently proposed scalable ARMA model, a parametric infinite-order vector AR model, and their asymptotic normality is also established. It makes feasible the inference on this computationally efficient model, especially for economic and financial time series. An efficient block coordinate descent algorithm is further introduced to search for estimates, and a Bayesian information criterion with selection consistency is suggested for model selection. Simulation experiments are conducted to illustrate their finite sample performance, and a real application on six macroeconomic indicators illustrates the usefulness of the proposed methodology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.12825&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuchang Lin, Wenyu Li, Qianqian Zhu, Guodong Li</name></author><category term="stat.ME" /><summary type="html">This paper considers both the least squares and quasi-maximum likelihood estimation for the recently proposed scalable ARMA model, a parametric infinite-order vector AR model, and their asymptotic normality is also established. It makes feasible the inference on this computationally efficient model, especially for economic and financial time series. An efficient block coordinate descent algorithm is further introduced to search for estimates, and a Bayesian information criterion with selection consistency is suggested for model selection. Simulation experiments are conducted to illustrate their finite sample performance, and a real application on six macroeconomic indicators illustrates the usefulness of the proposed methodology.</summary></entry><entry><title type="html">Review of Quasi-Randomization Approaches for Estimation from Non-probability Samples</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ReviewofQuasiRandomizationApproachesforEstimationfromNonprobabilitySamples.html" rel="alternate" type="text/html" title="Review of Quasi-Randomization Approaches for Estimation from Non-probability Samples" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ReviewofQuasiRandomizationApproachesforEstimationfromNonprobabilitySamples</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ReviewofQuasiRandomizationApproachesforEstimationfromNonprobabilitySamples.html">&lt;p&gt;The recent proliferation of computers and the internet have opened new opportunities for collecting and processing data. However, such data are often obtained without a well-planned probability survey design. Such non-probability based samples cannot be automatically regarded as representative of the population of interest. Several classes of methods for estimation and inferences from non-probability samples have been developed in recent years. The quasi-randomization methods assume that non-probability sample selection is governed by an underlying latent random mechanism. The basic idea is to use information collected from a probability (“reference”) sample to uncover latent non-probability survey participation probabilities (also known as “propensity scores”) and use them in estimation of target finite population parameters. In this paper, we review and compare theoretical properties of recently developed methods of estimation survey participation probabilities and study their relative performances in simulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.05383&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vladislav Beresovsky, Julie Gershunskaya, Terrance D. Savitsky</name></author><category term="stat.AP" /><summary type="html">The recent proliferation of computers and the internet have opened new opportunities for collecting and processing data. However, such data are often obtained without a well-planned probability survey design. Such non-probability based samples cannot be automatically regarded as representative of the population of interest. Several classes of methods for estimation and inferences from non-probability samples have been developed in recent years. The quasi-randomization methods assume that non-probability sample selection is governed by an underlying latent random mechanism. The basic idea is to use information collected from a probability (“reference”) sample to uncover latent non-probability survey participation probabilities (also known as “propensity scores”) and use them in estimation of target finite population parameters. In this paper, we review and compare theoretical properties of recently developed methods of estimation survey participation probabilities and study their relative performances in simulations.</summary></entry><entry><title type="html">Robust Distributed Learning of Functional Data From Simulators through Data Sketching</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/RobustDistributedLearningofFunctionalDataFromSimulatorsthroughDataSketching.html" rel="alternate" type="text/html" title="Robust Distributed Learning of Functional Data From Simulators through Data Sketching" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/RobustDistributedLearningofFunctionalDataFromSimulatorsthroughDataSketching</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/RobustDistributedLearningofFunctionalDataFromSimulatorsthroughDataSketching.html">&lt;p&gt;In environmental studies, realistic simulations are essential for understanding complex systems. Statistical emulation with Gaussian processes (GPs) in functional data models have become a standard tool for this purpose. Traditional centralized processing of such models requires substantial computational and storage resources, leading to emerging distributed Bayesian learning algorithms that partition data into shards for distributed computations. However, concerns about the sensitivity of distributed inference to shard selection arise. Instead of using data shards, our approach employs multiple random matrices to create random linear projections, or sketches, of the dataset. Posterior inference on functional data models is conducted using random data sketches on various machines in parallel. These individual inferences are combined across machines at a central server. The aggregation of inference across random matrices makes our approach resilient to the selection of data sketches, resulting in robust distributed Bayesian learning. An important advantage is its ability to maintain the privacy of sampling units, as random sketches prevent the recovery of raw data. We highlight the significance of our approach through simulation examples and showcase the performance of our approach as an emulator using surrogates of the Sea, Lake, and Overland Surges from Hurricanes (SLOSH) simulator - an important simulator for government agencies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18751&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>R. Jacob Andros, Rajarshi Guhaniyogi, Devin Francom, Donatella Pasqualini</name></author><category term="stat.AP" /><summary type="html">In environmental studies, realistic simulations are essential for understanding complex systems. Statistical emulation with Gaussian processes (GPs) in functional data models have become a standard tool for this purpose. Traditional centralized processing of such models requires substantial computational and storage resources, leading to emerging distributed Bayesian learning algorithms that partition data into shards for distributed computations. However, concerns about the sensitivity of distributed inference to shard selection arise. Instead of using data shards, our approach employs multiple random matrices to create random linear projections, or sketches, of the dataset. Posterior inference on functional data models is conducted using random data sketches on various machines in parallel. These individual inferences are combined across machines at a central server. The aggregation of inference across random matrices makes our approach resilient to the selection of data sketches, resulting in robust distributed Bayesian learning. An important advantage is its ability to maintain the privacy of sampling units, as random sketches prevent the recovery of raw data. We highlight the significance of our approach through simulation examples and showcase the performance of our approach as an emulator using surrogates of the Sea, Lake, and Overland Surges from Hurricanes (SLOSH) simulator - an important simulator for government agencies.</summary></entry><entry><title type="html">Semi-Parametric Inference for Doubly Stochastic Spatial Point Processes: An Approximate Penalized Poisson Likelihood Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/SemiParametricInferenceforDoublyStochasticSpatialPointProcessesAnApproximatePenalizedPoissonLikelihoodApproach.html" rel="alternate" type="text/html" title="Semi-Parametric Inference for Doubly Stochastic Spatial Point Processes: An Approximate Penalized Poisson Likelihood Approach" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/SemiParametricInferenceforDoublyStochasticSpatialPointProcessesAnApproximatePenalizedPoissonLikelihoodApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/SemiParametricInferenceforDoublyStochasticSpatialPointProcessesAnApproximatePenalizedPoissonLikelihoodApproach.html">&lt;p&gt;Doubly-stochastic point processes model the occurrence of events over a spatial domain as an inhomogeneous Poisson process conditioned on the realization of a random intensity function. They are flexible tools for capturing spatial heterogeneity and dependence. However, existing implementations of doubly-stochastic spatial models are computationally demanding, often have limited theoretical guarantee, and/or rely on restrictive assumptions. We propose a penalized regression method for estimating covariate effects in doubly-stochastic point processes that is computationally efficient and does not require a parametric form or stationarity of the underlying intensity. Our approach is based on an approximate (discrete and deterministic) formulation of the true (continuous and stochastic) intensity function. We show that consistency and asymptotic normality of the covariate effect estimates can be achieved despite the model misspecification, and develop a covariance estimator that leads to a valid, albeit conservative, statistical inference procedure. A simulation study shows the validity of our approach under less restrictive assumptions on the data generating mechanism, and an application to Seattle crime data demonstrates better prediction accuracy compared with existing alternatives.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.06756&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Si Cheng, Jon Wakefield, Ali Shojaie</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">Doubly-stochastic point processes model the occurrence of events over a spatial domain as an inhomogeneous Poisson process conditioned on the realization of a random intensity function. They are flexible tools for capturing spatial heterogeneity and dependence. However, existing implementations of doubly-stochastic spatial models are computationally demanding, often have limited theoretical guarantee, and/or rely on restrictive assumptions. We propose a penalized regression method for estimating covariate effects in doubly-stochastic point processes that is computationally efficient and does not require a parametric form or stationarity of the underlying intensity. Our approach is based on an approximate (discrete and deterministic) formulation of the true (continuous and stochastic) intensity function. We show that consistency and asymptotic normality of the covariate effect estimates can be achieved despite the model misspecification, and develop a covariance estimator that leads to a valid, albeit conservative, statistical inference procedure. A simulation study shows the validity of our approach under less restrictive assumptions on the data generating mechanism, and an application to Seattle crime data demonstrates better prediction accuracy compared with existing alternatives.</summary></entry><entry><title type="html">Shrinkage MMSE estimators of covariances beyond the zero-mean and stationary variance assumptions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ShrinkageMMSEestimatorsofcovariancesbeyondthezeromeanandstationaryvarianceassumptions.html" rel="alternate" type="text/html" title="Shrinkage MMSE estimators of covariances beyond the zero-mean and stationary variance assumptions" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ShrinkageMMSEestimatorsofcovariancesbeyondthezeromeanandstationaryvarianceassumptions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ShrinkageMMSEestimatorsofcovariancesbeyondthezeromeanandstationaryvarianceassumptions.html">&lt;p&gt;We tackle covariance estimation in low-sample scenarios, employing a structured covariance matrix with shrinkage methods. These involve convexly combining a low-bias/high-variance empirical estimate with a biased regularization estimator, striking a bias-variance trade-off. Literature provides optimal settings of the regularization amount through risk minimization between the true covariance and its shrunk counterpart. Such estimators were derived for zero-mean statistics with i.i.d. diagonal regularization matrices accounting for the average sample variance solely. We extend these results to regularization matrices accounting for the sample variances both for centered and non-centered samples. In the latter case, the empirical estimate of the true mean is incorporated into our shrinkage estimators. Introducing confidence weights into the statistics also enhance estimator robustness against outliers. We compare our estimators to other shrinkage methods both on numerical simulations and on real data to solve a detection problem in astronomy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.07104&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Olivier Flasseur, Eric Thiébaut, Loïc Denis, Maud Langlois</name></author><category term="stat.ME" /><summary type="html">We tackle covariance estimation in low-sample scenarios, employing a structured covariance matrix with shrinkage methods. These involve convexly combining a low-bias/high-variance empirical estimate with a biased regularization estimator, striking a bias-variance trade-off. Literature provides optimal settings of the regularization amount through risk minimization between the true covariance and its shrunk counterpart. Such estimators were derived for zero-mean statistics with i.i.d. diagonal regularization matrices accounting for the average sample variance solely. We extend these results to regularization matrices accounting for the sample variances both for centered and non-centered samples. In the latter case, the empirical estimate of the true mean is incorporated into our shrinkage estimators. Introducing confidence weights into the statistics also enhance estimator robustness against outliers. We compare our estimators to other shrinkage methods both on numerical simulations and on real data to solve a detection problem in astronomy.</summary></entry><entry><title type="html">Similarities among top one day batters: physics-based quantification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Similaritiesamongtoponedaybattersphysicsbasedquantification.html" rel="alternate" type="text/html" title="Similarities among top one day batters: physics-based quantification" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Similaritiesamongtoponedaybattersphysicsbasedquantification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Similaritiesamongtoponedaybattersphysicsbasedquantification.html">&lt;p&gt;Assessment of the performance of a player in any sport is very much needed to determine the ranking of players and make a solid team with the best players. Besides these, fans, journalists, sports persons, and sports councils often analyse the performances of current and retired players to identify the best players of all time. Here, we study the performance of all-time top batters in one-day cricket using physics-based statistical methods. The batters are selected in this study who possess either higher total runs or a high number of centuries. It is found that the total runs increases linearly with the innings number at the later stage of the batter carrier, and the runs rate estimated from the linear regression analysis also increases linearly with the average runs. The probability of non-scoring innings is found to be a negligibly small number (i.e., $\leq 0.1$ ) for each batter. Furthermore, based on innings-wise runs, we have computed the six-dimensional probability distribution vector for each player. Two components of the probability distribution vector vary linearly with average runs. The component representing the probability of scoring runs less than 50 linearly decreases with the average runs. In contrast, the probability of scoring runs greater than or equal to 100 and less than 150 linearly increases with the average runs. We have also estimated the entropy to assess the diversity of a player. Interestingly, the entropy varies linearly with the average runs, giving rise to two clusters corresponding to the old and recent players. Furthermore, the angle between two probability vectors is calculated for each pair of players to measure the similarities among the players. It is found that some of the players are almost identical to each other.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18617&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Dipak Patra</name></author><category term="cond-mat.stat-mech," /><category term="stat.AP" /><summary type="html">Assessment of the performance of a player in any sport is very much needed to determine the ranking of players and make a solid team with the best players. Besides these, fans, journalists, sports persons, and sports councils often analyse the performances of current and retired players to identify the best players of all time. Here, we study the performance of all-time top batters in one-day cricket using physics-based statistical methods. The batters are selected in this study who possess either higher total runs or a high number of centuries. It is found that the total runs increases linearly with the innings number at the later stage of the batter carrier, and the runs rate estimated from the linear regression analysis also increases linearly with the average runs. The probability of non-scoring innings is found to be a negligibly small number (i.e., $\leq 0.1$ ) for each batter. Furthermore, based on innings-wise runs, we have computed the six-dimensional probability distribution vector for each player. Two components of the probability distribution vector vary linearly with average runs. The component representing the probability of scoring runs less than 50 linearly decreases with the average runs. In contrast, the probability of scoring runs greater than or equal to 100 and less than 150 linearly increases with the average runs. We have also estimated the entropy to assess the diversity of a player. Interestingly, the entropy varies linearly with the average runs, giving rise to two clusters corresponding to the old and recent players. Furthermore, the angle between two probability vectors is calculated for each pair of players to measure the similarities among the players. It is found that some of the players are almost identical to each other.</summary></entry><entry><title type="html">Stable Differentiable Causal Discovery</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/StableDifferentiableCausalDiscovery.html" rel="alternate" type="text/html" title="Stable Differentiable Causal Discovery" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/StableDifferentiableCausalDiscovery</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/StableDifferentiableCausalDiscovery.html">&lt;p&gt;Inferring causal relationships as directed acyclic graphs (DAGs) is an important but challenging problem. Differentiable Causal Discovery (DCD) is a promising approach to this problem, framing the search as a continuous optimization. But existing DCD methods are numerically unstable, with poor performance beyond tens of variables. In this paper, we propose Stable Differentiable Causal Discovery (SDCD), a new method that improves previous DCD methods in two ways: (1) It employs an alternative constraint for acyclicity; this constraint is more stable, both theoretically and empirically, and fast to compute. (2) It uses a training procedure tailored for sparse causal graphs, which are common in real-world scenarios. We first derive SDCD and prove its stability and correctness. We then evaluate it with both observational and interventional data and on both small-scale and large-scale settings. We find that SDCD outperforms existing methods in both convergence speed and accuracy and can scale to thousands of variables. We provide code at https://github.com/azizilab/sdcd.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.10263&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Achille Nazaret, Justin Hong, Elham Azizi, David Blei</name></author><category term="stat.ME" /><summary type="html">Inferring causal relationships as directed acyclic graphs (DAGs) is an important but challenging problem. Differentiable Causal Discovery (DCD) is a promising approach to this problem, framing the search as a continuous optimization. But existing DCD methods are numerically unstable, with poor performance beyond tens of variables. In this paper, we propose Stable Differentiable Causal Discovery (SDCD), a new method that improves previous DCD methods in two ways: (1) It employs an alternative constraint for acyclicity; this constraint is more stable, both theoretically and empirically, and fast to compute. (2) It uses a training procedure tailored for sparse causal graphs, which are common in real-world scenarios. We first derive SDCD and prove its stability and correctness. We then evaluate it with both observational and interventional data and on both small-scale and large-scale settings. We find that SDCD outperforms existing methods in both convergence speed and accuracy and can scale to thousands of variables. We provide code at https://github.com/azizilab/sdcd.</summary></entry><entry><title type="html">Stochastic Gradient Piecewise Deterministic Monte Carlo Samplers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/StochasticGradientPiecewiseDeterministicMonteCarloSamplers.html" rel="alternate" type="text/html" title="Stochastic Gradient Piecewise Deterministic Monte Carlo Samplers" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/StochasticGradientPiecewiseDeterministicMonteCarloSamplers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/StochasticGradientPiecewiseDeterministicMonteCarloSamplers.html">&lt;p&gt;Recent work has suggested using Monte Carlo methods based on piecewise deterministic Markov processes (PDMPs) to sample from target distributions of interest. PDMPs are non-reversible continuous-time processes endowed with momentum, and hence can mix better than standard reversible MCMC samplers. Furthermore, they can incorporate exact sub-sampling schemes which only require access to a single (randomly selected) data point at each iteration, yet without introducing bias to the algorithm’s stationary distribution. However, the range of models for which PDMPs can be used, particularly with sub-sampling, is limited. We propose approximate simulation of PDMPs with sub-sampling for scalable sampling from posterior distributions. The approximation takes the form of an Euler approximation to the true PDMP dynamics, and involves using an estimate of the gradient of the log-posterior based on a data sub-sample. We thus call this class of algorithms stochastic-gradient PDMPs. Importantly, the trajectories of stochastic-gradient PDMPs are continuous and can leverage recent ideas for sampling from measures with continuous and atomic components. We show these methods are easy to implement, present results on their approximation error and demonstrate numerically that this class of algorithms has similar efficiency to, but is more robust than, stochastic gradient Langevin dynamics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19051&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Paul Fearnhead, Sebastiano Grazzi, Chris Nemeth, Gareth O. Roberts</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Recent work has suggested using Monte Carlo methods based on piecewise deterministic Markov processes (PDMPs) to sample from target distributions of interest. PDMPs are non-reversible continuous-time processes endowed with momentum, and hence can mix better than standard reversible MCMC samplers. Furthermore, they can incorporate exact sub-sampling schemes which only require access to a single (randomly selected) data point at each iteration, yet without introducing bias to the algorithm’s stationary distribution. However, the range of models for which PDMPs can be used, particularly with sub-sampling, is limited. We propose approximate simulation of PDMPs with sub-sampling for scalable sampling from posterior distributions. The approximation takes the form of an Euler approximation to the true PDMP dynamics, and involves using an estimate of the gradient of the log-posterior based on a data sub-sample. We thus call this class of algorithms stochastic-gradient PDMPs. Importantly, the trajectories of stochastic-gradient PDMPs are continuous and can leverage recent ideas for sampling from measures with continuous and atomic components. We show these methods are easy to implement, present results on their approximation error and demonstrate numerically that this class of algorithms has similar efficiency to, but is more robust than, stochastic gradient Langevin dynamics.</summary></entry><entry><title type="html">Structured prior distributions for the covariance matrix in latent factor models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Structuredpriordistributionsforthecovariancematrixinlatentfactormodels.html" rel="alternate" type="text/html" title="Structured prior distributions for the covariance matrix in latent factor models" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Structuredpriordistributionsforthecovariancematrixinlatentfactormodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Structuredpriordistributionsforthecovariancematrixinlatentfactormodels.html">&lt;p&gt;Factor models are widely used for dimension reduction in the analysis of multivariate data. This is achieved through decomposition of a p x p covariance matrix into the sum of two components. Through a latent factor representation, they can be interpreted as a diagonal matrix of idiosyncratic variances and a shared variation matrix, that is, the product of a p x k factor loadings matrix and its transpose. If k « p, this defines a parsimonious factorisation of the covariance matrix. Historically, little attention has been paid to incorporating prior information in Bayesian analyses using factor models where, at best, the prior for the factor loadings is order invariant. In this work, a class of structured priors is developed that can encode ideas of dependence structure about the shared variation matrix. The construction allows data-informed shrinkage towards sensible parametric structures while also facilitating inference over the number of factors. Using an unconstrained reparameterisation of stationary vector autoregressions, the methodology is extended to stationary dynamic factor models. For computational inference, parameter-expanded Markov chain Monte Carlo samplers are proposed, including an efficient adaptive Gibbs sampler. Two substantive applications showcase the scope of the methodology and its inferential benefits.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.07831&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sarah Elizabeth Heaps, Ian Hyla Jermyn</name></author><category term="stat.ME" /><summary type="html">Factor models are widely used for dimension reduction in the analysis of multivariate data. This is achieved through decomposition of a p x p covariance matrix into the sum of two components. Through a latent factor representation, they can be interpreted as a diagonal matrix of idiosyncratic variances and a shared variation matrix, that is, the product of a p x k factor loadings matrix and its transpose. If k « p, this defines a parsimonious factorisation of the covariance matrix. Historically, little attention has been paid to incorporating prior information in Bayesian analyses using factor models where, at best, the prior for the factor loadings is order invariant. In this work, a class of structured priors is developed that can encode ideas of dependence structure about the shared variation matrix. The construction allows data-informed shrinkage towards sensible parametric structures while also facilitating inference over the number of factors. Using an unconstrained reparameterisation of stationary vector autoregressions, the methodology is extended to stationary dynamic factor models. For computational inference, parameter-expanded Markov chain Monte Carlo samplers are proposed, including an efficient adaptive Gibbs sampler. Two substantive applications showcase the scope of the methodology and its inferential benefits.</summary></entry><entry><title type="html">The Bayesian Infinitesimal Jackknife for Variance</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/TheBayesianInfinitesimalJackknifeforVariance.html" rel="alternate" type="text/html" title="The Bayesian Infinitesimal Jackknife for Variance" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/TheBayesianInfinitesimalJackknifeforVariance</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/TheBayesianInfinitesimalJackknifeforVariance.html">&lt;p&gt;The frequentist variability of Bayesian posterior expectations can provide meaningful measures of uncertainty even when models are misspecified. Classical methods to asymptotically approximate the frequentist covariance of Bayesian estimators such as the Laplace approximation and the nonparametric bootstrap can be practically inconvenient, since the Laplace approximation may require an intractable integral to compute the marginal log posterior, and the bootstrap requires computing the posterior for many different bootstrap datasets. We develop and explore the infinitesimal jackknife (IJ), an alternative method for computing asymptotic frequentist covariance of smooth functionals of exchangeable data, which is based on the “influence function” of robust statistics. We show that the influence function for posterior expectations has the form of a simple posterior covariance, and that the IJ covariance estimate is, in turn, easily computed from a single set of posterior samples. Under conditions similar to those required for a Bayesian central limit theorem to apply, we prove that the corresponding IJ covariance estimate is asymptotically equivalent to the Laplace approximation and the bootstrap. In the presence of nuisance parameters that may not obey a central limit theorem, we argue using a von Mises expansion that the IJ covariance is inconsistent, but can remain a good approximation to the limiting frequentist variance. We demonstrate the accuracy and computational benefits of the IJ covariance estimates with simulated and real-world experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.06466&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ryan Giordano, Tamara Broderick</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">The frequentist variability of Bayesian posterior expectations can provide meaningful measures of uncertainty even when models are misspecified. Classical methods to asymptotically approximate the frequentist covariance of Bayesian estimators such as the Laplace approximation and the nonparametric bootstrap can be practically inconvenient, since the Laplace approximation may require an intractable integral to compute the marginal log posterior, and the bootstrap requires computing the posterior for many different bootstrap datasets. We develop and explore the infinitesimal jackknife (IJ), an alternative method for computing asymptotic frequentist covariance of smooth functionals of exchangeable data, which is based on the “influence function” of robust statistics. We show that the influence function for posterior expectations has the form of a simple posterior covariance, and that the IJ covariance estimate is, in turn, easily computed from a single set of posterior samples. Under conditions similar to those required for a Bayesian central limit theorem to apply, we prove that the corresponding IJ covariance estimate is asymptotically equivalent to the Laplace approximation and the bootstrap. In the presence of nuisance parameters that may not obey a central limit theorem, we argue using a von Mises expansion that the IJ covariance is inconsistent, but can remain a good approximation to the limiting frequentist variance. We demonstrate the accuracy and computational benefits of the IJ covariance estimates with simulated and real-world experiments.</summary></entry><entry><title type="html">The myth of declining competitive balance in the UEFA Champions League group stage</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ThemythofdecliningcompetitivebalanceintheUEFAChampionsLeaguegroupstage.html" rel="alternate" type="text/html" title="The myth of declining competitive balance in the UEFA Champions League group stage" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ThemythofdecliningcompetitivebalanceintheUEFAChampionsLeaguegroupstage</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ThemythofdecliningcompetitivebalanceintheUEFAChampionsLeaguegroupstage.html">&lt;p&gt;According to previous studies, competitive balance has significantly declined in the UEFA Champions League group stage over the recent decades. Our paper introduces six alternative indices for measuring ex ante and ex post competitive balance in order to explore the robustness of these results. The ex ante measures are based on Elo ratings, while the ex post measures compare the group ranking to reasonable benchmarks. We find no evidence of any trend in the competitive balance of the UEFA Champions League group stage between the 2003/04 and 2023/24 seasons.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.19222&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>László Csató, Dóra Gréta Petróczy</name></author><category term="stat.AP" /><summary type="html">According to previous studies, competitive balance has significantly declined in the UEFA Champions League group stage over the recent decades. Our paper introduces six alternative indices for measuring ex ante and ex post competitive balance in order to explore the robustness of these results. The ex ante measures are based on Elo ratings, while the ex post measures compare the group ranking to reasonable benchmarks. We find no evidence of any trend in the competitive balance of the UEFA Champions League group stage between the 2003/04 and 2023/24 seasons.</summary></entry><entry><title type="html">Unbiased least squares regression via averaged stochastic gradient descent</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Unbiasedleastsquaresregressionviaaveragedstochasticgradientdescent.html" rel="alternate" type="text/html" title="Unbiased least squares regression via averaged stochastic gradient descent" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Unbiasedleastsquaresregressionviaaveragedstochasticgradientdescent</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/Unbiasedleastsquaresregressionviaaveragedstochasticgradientdescent.html">&lt;p&gt;We consider an on-line least squares regression problem with optimal solution $\theta^&lt;em&gt;$ and Hessian matrix H, and study a time-average stochastic gradient descent estimator of $\theta^&lt;/em&gt;$. For $k\ge2$, we provide an unbiased estimator of $\theta^&lt;em&gt;$ that is a modification of the time-average estimator, runs with an expected number of time-steps of order k, with O(1/k) expected excess risk. The constant behind the O notation depends on parameters of the regression and is a poly-logarithmic function of the smallest eigenvalue of H. We provide both a biased and unbiased estimator of the expected excess risk of the time-average estimator and of its unbiased counterpart, without requiring knowledge of either H or $\theta^&lt;/em&gt;$. We describe an “average-start” version of our estimators with similar properties. Our approach is based on randomized multilevel Monte Carlo. Our numerical experiments confirm our theoretical findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.18623&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nabil Kahalé</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">We consider an on-line least squares regression problem with optimal solution $\theta^$ and Hessian matrix H, and study a time-average stochastic gradient descent estimator of $\theta^$. For $k\ge2$, we provide an unbiased estimator of $\theta^$ that is a modification of the time-average estimator, runs with an expected number of time-steps of order k, with O(1/k) expected excess risk. The constant behind the O notation depends on parameters of the regression and is a poly-logarithmic function of the smallest eigenvalue of H. We provide both a biased and unbiased estimator of the expected excess risk of the time-average estimator and of its unbiased counterpart, without requiring knowledge of either H or $\theta^$. We describe an “average-start” version of our estimators with similar properties. Our approach is based on randomized multilevel Monte Carlo. Our numerical experiments confirm our theoretical findings.</summary></entry><entry><title type="html">X-Vine Models for Multivariate Extremes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/XVineModelsforMultivariateExtremes.html" rel="alternate" type="text/html" title="X-Vine Models for Multivariate Extremes" /><published>2024-06-28T00:00:00+00:00</published><updated>2024-06-28T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/XVineModelsforMultivariateExtremes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/XVineModelsforMultivariateExtremes.html">&lt;p&gt;Regular vine sequences permit the organisation of variables in a random vector along a sequence of trees. Regular vine models have become greatly popular in dependence modelling as a way to combine arbitrary bivariate copulas into higher-dimensional ones, offering flexibility, parsimony, and tractability. In this project, we use regular vine structures to decompose and construct the exponent measure density of a multivariate extreme value distribution, or, equivalently, the tail copula density. Although these densities pose theoretical challenges due to their infinite mass, their homogeneity property offers simplifications. The theory sheds new light on existing parametric families and facilitates the construction of new ones, called X-vines. Computations proceed via recursive formulas in terms of bivariate model components. We develop simulation algorithms for X-vine multivariate Pareto distributions as well as methods for parameter estimation and model selection on the basis of threshold exceedances. The methods are illustrated by Monte Carlo experiments and a case study on US flight delay data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.15205&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anna Kiriliouk, Jeongjin Lee, Johan Segers</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Regular vine sequences permit the organisation of variables in a random vector along a sequence of trees. Regular vine models have become greatly popular in dependence modelling as a way to combine arbitrary bivariate copulas into higher-dimensional ones, offering flexibility, parsimony, and tractability. In this project, we use regular vine structures to decompose and construct the exponent measure density of a multivariate extreme value distribution, or, equivalently, the tail copula density. Although these densities pose theoretical challenges due to their infinite mass, their homogeneity property offers simplifications. The theory sheds new light on existing parametric families and facilitates the construction of new ones, called X-vines. Computations proceed via recursive formulas in terms of bivariate model components. We develop simulation algorithms for X-vine multivariate Pareto distributions as well as methods for parameter estimation and model selection on the basis of threshold exceedances. The methods are illustrated by Monte Carlo experiments and a case study on US flight delay data.</summary></entry></feed>