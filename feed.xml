<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-06T07:13:29+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">ALCM: Autonomous LLM-Augmented Causal Discovery Framework</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/ALCMAutonomousLLMAugmentedCausalDiscoveryFramework.html" rel="alternate" type="text/html" title="ALCM: Autonomous LLM-Augmented Causal Discovery Framework" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/ALCMAutonomousLLMAugmentedCausalDiscoveryFramework</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/ALCMAutonomousLLMAugmentedCausalDiscoveryFramework.html">&lt;p&gt;To perform effective causal inference in high-dimensional datasets, initiating the process with causal discovery is imperative, wherein a causal graph is generated based on observational data. However, obtaining a complete and accurate causal graph poses a formidable challenge, recognized as an NP-hard problem. Recently, the advent of Large Language Models (LLMs) has ushered in a new era, indicating their emergent capabilities and widespread applicability in facilitating causal reasoning across diverse domains, such as medicine, finance, and science. The expansive knowledge base of LLMs holds the potential to elevate the field of causal reasoning by offering interpretability, making inferences, generalizability, and uncovering novel causal structures. In this paper, we introduce a new framework, named Autonomous LLM-Augmented Causal Discovery Framework (ALCM), to synergize data-driven causal discovery algorithms and LLMs, automating the generation of a more resilient, accurate, and explicable causal graph. The ALCM consists of three integral components: causal structure learning, causal wrapper, and LLM-driven causal refiner. These components autonomously collaborate within a dynamic environment to address causal discovery questions and deliver plausible causal graphs. We evaluate the ALCM framework by implementing two demonstrations on seven well-known datasets. Experimental results demonstrate that ALCM outperforms existing LLM methods and conventional data-driven causal reasoning mechanisms. This study not only shows the effectiveness of the ALCM but also underscores new research directions in leveraging the causal reasoning capabilities of LLMs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01744&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elahe Khatibi, Mahyar Abbasian, Zhongqi Yang, Iman Azimi, Amir M. Rahmani</name></author><category term="stat.ME" /><summary type="html">To perform effective causal inference in high-dimensional datasets, initiating the process with causal discovery is imperative, wherein a causal graph is generated based on observational data. However, obtaining a complete and accurate causal graph poses a formidable challenge, recognized as an NP-hard problem. Recently, the advent of Large Language Models (LLMs) has ushered in a new era, indicating their emergent capabilities and widespread applicability in facilitating causal reasoning across diverse domains, such as medicine, finance, and science. The expansive knowledge base of LLMs holds the potential to elevate the field of causal reasoning by offering interpretability, making inferences, generalizability, and uncovering novel causal structures. In this paper, we introduce a new framework, named Autonomous LLM-Augmented Causal Discovery Framework (ALCM), to synergize data-driven causal discovery algorithms and LLMs, automating the generation of a more resilient, accurate, and explicable causal graph. The ALCM consists of three integral components: causal structure learning, causal wrapper, and LLM-driven causal refiner. These components autonomously collaborate within a dynamic environment to address causal discovery questions and deliver plausible causal graphs. We evaluate the ALCM framework by implementing two demonstrations on seven well-known datasets. Experimental results demonstrate that ALCM outperforms existing LLM methods and conventional data-driven causal reasoning mechanisms. This study not only shows the effectiveness of the ALCM but also underscores new research directions in leveraging the causal reasoning capabilities of LLMs.</summary></entry><entry><title type="html">A comparison of regression models for static and dynamic prediction of a prognostic outcome during admission in electronic health care records</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Acomparisonofregressionmodelsforstaticanddynamicpredictionofaprognosticoutcomeduringadmissioninelectronichealthcarerecords.html" rel="alternate" type="text/html" title="A comparison of regression models for static and dynamic prediction of a prognostic outcome during admission in electronic health care records" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Acomparisonofregressionmodelsforstaticanddynamicpredictionofaprognosticoutcomeduringadmissioninelectronichealthcarerecords</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Acomparisonofregressionmodelsforstaticanddynamicpredictionofaprognosticoutcomeduringadmissioninelectronichealthcarerecords.html">&lt;p&gt;Objective Hospitals register information in the electronic health records (EHR) continuously until discharge or death. As such, there is no censoring for in-hospital outcomes. We aimed to compare different dynamic regression modeling approaches to predict central line-associated bloodstream infections (CLABSI) in EHR while accounting for competing events precluding CLABSI. Materials and Methods We analyzed data from 30,862 catheter episodes at University Hospitals Leuven from 2012 and 2013 to predict 7-day risk of CLABSI. Competing events are discharge and death. Static models at catheter onset included logistic, multinomial logistic, Cox, cause-specific hazard, and Fine-Gray regression. Dynamic models updated predictions daily up to 30 days after catheter onset (i.e. landmarks 0 to 30 days), and included landmark supermodel extensions of the static models, separate Fine-Gray models per landmark time, and regularized multi-task learning (RMTL). Model performance was assessed using 100 random 2:1 train-test splits. Results The Cox model performed worst of all static models in terms of area under the receiver operating characteristic curve (AUC) and calibration. Dynamic landmark supermodels reached peak AUCs between 0.741-0.747 at landmark 5. The Cox landmark supermodel had the worst AUCs (&amp;lt;=0.731) and calibration up to landmark 7. Separate Fine-Gray models per landmark performed worst for later landmarks, when the number of patients at risk was low. Discussion and Conclusion Categorical and time-to-event approaches had similar performance in the static and dynamic settings, except Cox models. Ignoring competing risks caused problems for risk prediction in the time-to-event framework (Cox), but not in the categorical framework (logistic regression).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01986&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shan Gao, Elena Albu, Hein Putter, Pieter Stijnen, Frank Rademakers, Veerle Cossey, Yves Debaveye, Christel Janssens, Ben Van Calster, Laure Wynants</name></author><category term="stat.AP" /><summary type="html">Objective Hospitals register information in the electronic health records (EHR) continuously until discharge or death. As such, there is no censoring for in-hospital outcomes. We aimed to compare different dynamic regression modeling approaches to predict central line-associated bloodstream infections (CLABSI) in EHR while accounting for competing events precluding CLABSI. Materials and Methods We analyzed data from 30,862 catheter episodes at University Hospitals Leuven from 2012 and 2013 to predict 7-day risk of CLABSI. Competing events are discharge and death. Static models at catheter onset included logistic, multinomial logistic, Cox, cause-specific hazard, and Fine-Gray regression. Dynamic models updated predictions daily up to 30 days after catheter onset (i.e. landmarks 0 to 30 days), and included landmark supermodel extensions of the static models, separate Fine-Gray models per landmark time, and regularized multi-task learning (RMTL). Model performance was assessed using 100 random 2:1 train-test splits. Results The Cox model performed worst of all static models in terms of area under the receiver operating characteristic curve (AUC) and calibration. Dynamic landmark supermodels reached peak AUCs between 0.741-0.747 at landmark 5. The Cox landmark supermodel had the worst AUCs (&amp;lt;=0.731) and calibration up to landmark 7. Separate Fine-Gray models per landmark performed worst for later landmarks, when the number of patients at risk was low. Discussion and Conclusion Categorical and time-to-event approaches had similar performance in the static and dynamic settings, except Cox models. Ignoring competing risks caused problems for risk prediction in the time-to-event framework (Cox), but not in the categorical framework (logistic regression).</summary></entry><entry><title type="html">A geometric power analysis for general log-linear models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Ageometricpoweranalysisforgeneralloglinearmodels.html" rel="alternate" type="text/html" title="A geometric power analysis for general log-linear models" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Ageometricpoweranalysisforgeneralloglinearmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Ageometricpoweranalysisforgeneralloglinearmodels.html">&lt;p&gt;Log-linear models are widely used to express the association in multivariate frequency data on contingency tables. The paper focuses on the power analysis for testing the goodness-of-fit hypothesis for this model type. Conventionally, for the power-related sample size calculations a deviation from the null hypothesis (effect size) is specified by means of the chi-square goodness-of-fit index. It is argued that the odds ratio is a more natural measure of effect size, with the advantage of having a data-relevant interpretation. Therefore, a class of log-affine models that are specified by odds ratios whose values deviate from those of the null by a small amount can be chosen as an alternative. Being expressed as sets of constraints on odds ratios, both hypotheses are represented by smooth surfaces in the probability simplex, and thus, the power analysis can be given a geometric interpretation as well. A concept of geometric power is introduced and a Monte-Carlo algorithm for its estimation is proposed. The framework is applied to the power analysis of goodness-of-fit in the context of multinomial sampling. An iterative scaling procedure for generating distributions from a log-affine model is described and its convergence is proved. To illustrate, the geometric power analysis is carried out for data from a clinical study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.10271&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anna Klimova</name></author><category term="stat.ME" /><summary type="html">Log-linear models are widely used to express the association in multivariate frequency data on contingency tables. The paper focuses on the power analysis for testing the goodness-of-fit hypothesis for this model type. Conventionally, for the power-related sample size calculations a deviation from the null hypothesis (effect size) is specified by means of the chi-square goodness-of-fit index. It is argued that the odds ratio is a more natural measure of effect size, with the advantage of having a data-relevant interpretation. Therefore, a class of log-affine models that are specified by odds ratios whose values deviate from those of the null by a small amount can be chosen as an alternative. Being expressed as sets of constraints on odds ratios, both hypotheses are represented by smooth surfaces in the probability simplex, and thus, the power analysis can be given a geometric interpretation as well. A concept of geometric power is introduced and a Monte-Carlo algorithm for its estimation is proposed. The framework is applied to the power analysis of goodness-of-fit in the context of multinomial sampling. An iterative scaling procedure for generating distributions from a log-affine model is described and its convergence is proved. To illustrate, the geometric power analysis is carried out for data from a clinical study.</summary></entry><entry><title type="html">Bayesian Learning of Clinically Meaningful Sepsis Phenotypes in Northern Tanzania</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/BayesianLearningofClinicallyMeaningfulSepsisPhenotypesinNorthernTanzania.html" rel="alternate" type="text/html" title="Bayesian Learning of Clinically Meaningful Sepsis Phenotypes in Northern Tanzania" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/BayesianLearningofClinicallyMeaningfulSepsisPhenotypesinNorthernTanzania</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/BayesianLearningofClinicallyMeaningfulSepsisPhenotypesinNorthernTanzania.html">&lt;p&gt;Sepsis is a life-threatening condition caused by a dysregulated host response to infection. Recently, researchers have hypothesized that sepsis consists of a heterogeneous spectrum of distinct subtypes, motivating several studies to identify clusters of sepsis patients that correspond to subtypes, with the long-term goal of using these clusters to design subtype-specific treatments. Therefore, clinicians rely on clusters having a concrete medical interpretation, usually corresponding to clinically meaningful regions of the sample space that have a concrete implication to practitioners. In this article, we propose Clustering Around Meaningful Regions (CLAMR), a Bayesian clustering approach that explicitly models the medical interpretation of each cluster center. CLAMR favors clusterings that can be summarized via meaningful feature values, leading to medically significant sepsis patient clusters. We also provide details on measuring the effect of each feature on the clustering using Bayesian hypothesis tests, so one can assess what features are relevant for cluster interpretation. Our focus is on clustering sepsis patients from Moshi, Tanzania, where patients are younger and the prevalence of HIV infection is higher than in previous sepsis subtyping cohorts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01746&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexander Dombowsky, David B. Dunson, Deng B. Madut, Matthew P. Rubach, Amy H. Herring</name></author><category term="stat.AP" /><summary type="html">Sepsis is a life-threatening condition caused by a dysregulated host response to infection. Recently, researchers have hypothesized that sepsis consists of a heterogeneous spectrum of distinct subtypes, motivating several studies to identify clusters of sepsis patients that correspond to subtypes, with the long-term goal of using these clusters to design subtype-specific treatments. Therefore, clinicians rely on clusters having a concrete medical interpretation, usually corresponding to clinically meaningful regions of the sample space that have a concrete implication to practitioners. In this article, we propose Clustering Around Meaningful Regions (CLAMR), a Bayesian clustering approach that explicitly models the medical interpretation of each cluster center. CLAMR favors clusterings that can be summarized via meaningful feature values, leading to medically significant sepsis patient clusters. We also provide details on measuring the effect of each feature on the clustering using Bayesian hypothesis tests, so one can assess what features are relevant for cluster interpretation. Our focus is on clustering sepsis patients from Moshi, Tanzania, where patients are younger and the prevalence of HIV infection is higher than in previous sepsis subtyping cohorts.</summary></entry><entry><title type="html">Causal Discovery Under Local Privacy</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/CausalDiscoveryUnderLocalPrivacy.html" rel="alternate" type="text/html" title="Causal Discovery Under Local Privacy" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/CausalDiscoveryUnderLocalPrivacy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/CausalDiscoveryUnderLocalPrivacy.html">&lt;p&gt;Differential privacy is a widely adopted framework designed to safeguard the sensitive information of data providers within a data set. It is based on the application of controlled noise at the interface between the server that stores and processes the data, and the data consumers. Local differential privacy is a variant that allows data providers to apply the privatization mechanism themselves on their data individually. Therefore it provides protection also in contexts in which the server, or even the data collector, cannot be trusted. The introduction of noise, however, inevitably affects the utility of the data, particularly by distorting the correlations between individual data components. This distortion can prove detrimental to tasks such as causal discovery. In this paper, we consider various well-known locally differentially private mechanisms and compare the trade-off between the privacy they provide, and the accuracy of the causal structure produced by algorithms for causal learning when applied to data obfuscated by these mechanisms. Our analysis yields valuable insights for selecting appropriate local differentially private protocols for causal discovery tasks. We foresee that our findings will aid researchers and practitioners in conducting locally private causal discovery.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.04037&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rūta Binkytė, Carlos Pinzón, Szilvia Lestyán, Kangsoo Jung, Héber H. Arcolezi, Catuscia Palamidessi</name></author><category term="stat.ME" /><summary type="html">Differential privacy is a widely adopted framework designed to safeguard the sensitive information of data providers within a data set. It is based on the application of controlled noise at the interface between the server that stores and processes the data, and the data consumers. Local differential privacy is a variant that allows data providers to apply the privatization mechanism themselves on their data individually. Therefore it provides protection also in contexts in which the server, or even the data collector, cannot be trusted. The introduction of noise, however, inevitably affects the utility of the data, particularly by distorting the correlations between individual data components. This distortion can prove detrimental to tasks such as causal discovery. In this paper, we consider various well-known locally differentially private mechanisms and compare the trade-off between the privacy they provide, and the accuracy of the causal structure produced by algorithms for causal learning when applied to data obfuscated by these mechanisms. Our analysis yields valuable insights for selecting appropriate local differentially private protocols for causal discovery tasks. We foresee that our findings will aid researchers and practitioners in conducting locally private causal discovery.</summary></entry><entry><title type="html">Causal Effects in Matching Mechanisms with Strategically Reported Preferences</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/CausalEffectsinMatchingMechanismswithStrategicallyReportedPreferences.html" rel="alternate" type="text/html" title="Causal Effects in Matching Mechanisms with Strategically Reported Preferences" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/CausalEffectsinMatchingMechanismswithStrategicallyReportedPreferences</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/CausalEffectsinMatchingMechanismswithStrategicallyReportedPreferences.html">&lt;p&gt;A growing number of central authorities use assignment mechanisms to allocate students to schools in a way that reflects student preferences and school priorities. However, most real-world mechanisms incentivize students to strategically misreport their preferences. In this paper, we provide an approach for identifying the causal effects of school assignment on future outcomes that accounts for strategic misreporting. Misreporting may invalidate existing point-identification approaches, and we derive sharp bounds for causal effects that are robust to strategic behavior. Our approach applies to any mechanism as long as there exist placement scores and cutoffs that characterize that mechanism’s allocation rule. We use data from a deferred acceptance mechanism that assigns students to more than 1,000 university-major combinations in Chile. Matching theory predicts that students’ behavior in Chile should be strategic because they can list only up to eight options, and we find empirical evidence consistent with such behavior. Our bounds are informative enough to reveal significant heterogeneity in graduation success with respect to preferences and school assignment.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.14282&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marinho Bertanha, Margaux Luflade, Ismael Mourifié</name></author><category term="stat.ME" /><summary type="html">A growing number of central authorities use assignment mechanisms to allocate students to schools in a way that reflects student preferences and school priorities. However, most real-world mechanisms incentivize students to strategically misreport their preferences. In this paper, we provide an approach for identifying the causal effects of school assignment on future outcomes that accounts for strategic misreporting. Misreporting may invalidate existing point-identification approaches, and we derive sharp bounds for causal effects that are robust to strategic behavior. Our approach applies to any mechanism as long as there exist placement scores and cutoffs that characterize that mechanism’s allocation rule. We use data from a deferred acceptance mechanism that assigns students to more than 1,000 university-major combinations in Chile. Matching theory predicts that students’ behavior in Chile should be strategic because they can list only up to eight options, and we find empirical evidence consistent with such behavior. Our bounds are informative enough to reveal significant heterogeneity in graduation success with respect to preferences and school assignment.</summary></entry><entry><title type="html">Confidence regions for a persistence diagram of a single image with one or more loops</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Confidenceregionsforapersistencediagramofasingleimagewithoneormoreloops.html" rel="alternate" type="text/html" title="Confidence regions for a persistence diagram of a single image with one or more loops" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Confidenceregionsforapersistencediagramofasingleimagewithoneormoreloops</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Confidenceregionsforapersistencediagramofasingleimagewithoneormoreloops.html">&lt;p&gt;Topological data analysis (TDA) uses persistent homology to quantify loops and higher-dimensional holes in data, making it particularly relevant for examining the characteristics of images of cells in the field of cell biology. In the context of a cell injury, as time progresses, a wound in the form of a ring emerges in the cell image and then gradually vanishes. Performing statistical inference on this ring-like pattern in a single image is challenging due to the absence of repeated samples. In this paper, we develop a novel framework leveraging TDA to estimate underlying structures within individual images and quantify associated uncertainties through confidence regions. Our proposed method partitions the image into the background and the damaged cell regions. Then pixels within the affected cell region are used to establish confidence regions in the space of persistence diagrams (topological summary statistics). The method establishes estimates on the persistence diagrams which correct the bias of traditional TDA approaches. A simulation study is conducted to evaluate the coverage probabilities of the proposed confidence regions in comparison to an alternative approach is proposed in this paper. We also illustrate our methodology by a real-world example provided by cell repair.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01651&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Susan Glenn, Jessi Cisewski-Kehe, Jun Zhu, William M. Bement</name></author><category term="stat.ME" /><summary type="html">Topological data analysis (TDA) uses persistent homology to quantify loops and higher-dimensional holes in data, making it particularly relevant for examining the characteristics of images of cells in the field of cell biology. In the context of a cell injury, as time progresses, a wound in the form of a ring emerges in the cell image and then gradually vanishes. Performing statistical inference on this ring-like pattern in a single image is challenging due to the absence of repeated samples. In this paper, we develop a novel framework leveraging TDA to estimate underlying structures within individual images and quantify associated uncertainties through confidence regions. Our proposed method partitions the image into the background and the damaged cell regions. Then pixels within the affected cell region are used to establish confidence regions in the space of persistence diagrams (topological summary statistics). The method establishes estimates on the persistence diagrams which correct the bias of traditional TDA approaches. A simulation study is conducted to evaluate the coverage probabilities of the proposed confidence regions in comparison to an alternative approach is proposed in this paper. We also illustrate our methodology by a real-world example provided by cell repair.</summary></entry><entry><title type="html">Efficient spline orthogonal basis for representation of density functions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Efficientsplineorthogonalbasisforrepresentationofdensityfunctions.html" rel="alternate" type="text/html" title="Efficient spline orthogonal basis for representation of density functions" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Efficientsplineorthogonalbasisforrepresentationofdensityfunctions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Efficientsplineorthogonalbasisforrepresentationofdensityfunctions.html">&lt;p&gt;Probability density functions form a specific class of functional data objects with intrinsic properties of scale invariance and relative scale characterized by the unit integral constraint. The Bayes spaces methodology respects their specific nature, and the centred log-ratio transformation enables processing such functional data in the standard Lebesgue space of square-integrable functions. As the data representing densities are frequently observed in their discrete form, the focus has been on their spline representation. Therefore, the crucial step in the approximation is to construct a proper spline basis reflecting their specific properties. Since the centred log-ratio transformation forms a subspace of functions with a zero integral constraint, the standard $B$-spline basis is no longer suitable. Recently, a new spline basis incorporating this zero integral property, called $Z!B$-splines, was developed. However, this basis does not possess the orthogonal property which is beneficial from computational and application point of view. As a result of this paper, we describe an efficient method for constructing an orthogonal $Z!B$-splines basis, called $Z!B$-splinets. The advantages of the $Z!B$-splinet approach are foremost a computational efficiency and locality of basis supports that is desirable for data interpretability, e.g. in the context of functional principal component analysis. The proposed approach is demonstrated on an empirical demographic dataset.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02231&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jana Burkotová, Ivana Pavlů, Hiba Nassar, Jitka Machalová, Karel Hron</name></author><category term="stat.ME" /><summary type="html">Probability density functions form a specific class of functional data objects with intrinsic properties of scale invariance and relative scale characterized by the unit integral constraint. The Bayes spaces methodology respects their specific nature, and the centred log-ratio transformation enables processing such functional data in the standard Lebesgue space of square-integrable functions. As the data representing densities are frequently observed in their discrete form, the focus has been on their spline representation. Therefore, the crucial step in the approximation is to construct a proper spline basis reflecting their specific properties. Since the centred log-ratio transformation forms a subspace of functions with a zero integral constraint, the standard $B$-spline basis is no longer suitable. Recently, a new spline basis incorporating this zero integral property, called $Z!B$-splines, was developed. However, this basis does not possess the orthogonal property which is beneficial from computational and application point of view. As a result of this paper, we describe an efficient method for constructing an orthogonal $Z!B$-splines basis, called $Z!B$-splinets. The advantages of the $Z!B$-splinet approach are foremost a computational efficiency and locality of basis supports that is desirable for data interpretability, e.g. in the context of functional principal component analysis. The proposed approach is demonstrated on an empirical demographic dataset.</summary></entry><entry><title type="html">Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/FairRiskControlAGeneralizedFrameworkforCalibratingMultigroupFairnessRisks.html" rel="alternate" type="text/html" title="Fair Risk Control: A Generalized Framework for Calibrating Multi-group Fairness Risks" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/FairRiskControlAGeneralizedFrameworkforCalibratingMultigroupFairnessRisks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/FairRiskControlAGeneralizedFrameworkforCalibratingMultigroupFairnessRisks.html">&lt;p&gt;This paper introduces a framework for post-processing machine learning models so that their predictions satisfy multi-group fairness guarantees. Based on the celebrated notion of multicalibration, we introduce $(\mathbf{s},\mathcal{G}, \alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for multi-dimensional mappings $\mathbf{s}$, constraint set $\mathcal{G}$, and a pre-specified threshold level $\alpha$. We propose associated algorithms to achieve this notion in general settings. This framework is then applied to diverse scenarios encompassing different fairness concerns, including false negative rate control in image segmentation, prediction set conditional uncertainty quantification in hierarchical classification, and de-biased text generation in language models. We conduct numerical studies on several datasets and tasks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02225&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lujing Zhang, Aaron Roth, Linjun Zhang</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">This paper introduces a framework for post-processing machine learning models so that their predictions satisfy multi-group fairness guarantees. Based on the celebrated notion of multicalibration, we introduce $(\mathbf{s},\mathcal{G}, \alpha)-$GMC (Generalized Multi-Dimensional Multicalibration) for multi-dimensional mappings $\mathbf{s}$, constraint set $\mathcal{G}$, and a pre-specified threshold level $\alpha$. We propose associated algorithms to achieve this notion in general settings. This framework is then applied to diverse scenarios encompassing different fairness concerns, including false negative rate control in image segmentation, prediction set conditional uncertainty quantification in hierarchical classification, and de-biased text generation in language models. We conduct numerical studies on several datasets and tasks.</summary></entry><entry><title type="html">Handling missing data when estimating causal effects with Targeted Maximum Likelihood Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/HandlingmissingdatawhenestimatingcausaleffectswithTargetedMaximumLikelihoodEstimation.html" rel="alternate" type="text/html" title="Handling missing data when estimating causal effects with Targeted Maximum Likelihood Estimation" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/HandlingmissingdatawhenestimatingcausaleffectswithTargetedMaximumLikelihoodEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/HandlingmissingdatawhenestimatingcausaleffectswithTargetedMaximumLikelihoodEstimation.html">&lt;p&gt;Targeted Maximum Likelihood Estimation (TMLE) is increasingly used for doubly robust causal inference, but how missing data should be handled when using TMLE with data-adaptive approaches is unclear. Based on the Victorian Adolescent Health Cohort Study, we conducted a simulation study to evaluate eight missing data methods in this context: complete-case analysis, extended TMLE incorporating outcome-missingness model, missing covariate missing indicator method, five multiple imputation (MI) approaches using parametric or machine-learning models. Six scenarios were considered, varying in exposure/outcome generation models (presence of confounder-confounder interactions) and missingness mechanisms (whether outcome influenced missingness in other variables and presence of interaction/non-linear terms in missingness models). Complete-case analysis and extended TMLE had small biases when outcome did not influence missingness in other variables. Parametric MI without interactions had large bias when exposure/outcome generation models included interactions. Parametric MI including interactions performed best in bias and variance reduction across all settings, except when missingness models included a non-linear term. When choosing a method to handle missing data in the context of TMLE, researchers must consider the missingness mechanism and, for MI, compatibility with the analysis method. In many settings, a parametric MI approach that incorporates interactions and non-linearities is expected to perform well.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.05274&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>S. Ghazaleh Dashti, Katherine J. Lee, Julie A. Simpson, Ian R. White, John B. Carlin, Margarita Moreno-Betancur</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Targeted Maximum Likelihood Estimation (TMLE) is increasingly used for doubly robust causal inference, but how missing data should be handled when using TMLE with data-adaptive approaches is unclear. Based on the Victorian Adolescent Health Cohort Study, we conducted a simulation study to evaluate eight missing data methods in this context: complete-case analysis, extended TMLE incorporating outcome-missingness model, missing covariate missing indicator method, five multiple imputation (MI) approaches using parametric or machine-learning models. Six scenarios were considered, varying in exposure/outcome generation models (presence of confounder-confounder interactions) and missingness mechanisms (whether outcome influenced missingness in other variables and presence of interaction/non-linear terms in missingness models). Complete-case analysis and extended TMLE had small biases when outcome did not influence missingness in other variables. Parametric MI without interactions had large bias when exposure/outcome generation models included interactions. Parametric MI including interactions performed best in bias and variance reduction across all settings, except when missingness models included a non-linear term. When choosing a method to handle missing data in the context of TMLE, researchers must consider the missingness mechanism and, for MI, compatibility with the analysis method. In many settings, a parametric MI approach that incorporates interactions and non-linearities is expected to perform well.</summary></entry><entry><title type="html">Identification of SNPs in genomes using GRAMEP, an alignment-free method based on the Principle of Maximum Entropy</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/IdentificationofSNPsingenomesusingGRAMEPanalignmentfreemethodbasedonthePrincipleofMaximumEntropy.html" rel="alternate" type="text/html" title="Identification of SNPs in genomes using GRAMEP, an alignment-free method based on the Principle of Maximum Entropy" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/IdentificationofSNPsingenomesusingGRAMEPanalignmentfreemethodbasedonthePrincipleofMaximumEntropy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/IdentificationofSNPsingenomesusingGRAMEPanalignmentfreemethodbasedonthePrincipleofMaximumEntropy.html">&lt;p&gt;Advances in high throughput sequencing technologies provide a large number of genomes to be analyzed, so computational methodologies play a crucial role in analyzing and extracting knowledge from the data generated. Investigating genomic mutations is critical because of their impact on chromosomal evolution, genetic disorders, and diseases. It is common to adopt aligning sequences for analyzing genomic variations, however, this approach can be computationally expensive and potentially arbitrary in scenarios with large datasets. Here, we present a novel method for identifying single nucleotide polymorphisms (SNPs) in DNA sequences from assembled genomes. This method uses the principle of maximum entropy to select the most informative k-mers specific to the variant under investigation. The use of this informative k-mer set enables the detection of variant-specific mutations in comparison to a reference sequence. In addition, our method offers the possibility of classifying novel sequences with no need for organism-specific information. GRAMEP demonstrated high accuracy in both in silico simulations and analyses of real viral genomes, including Dengue, HIV, and SARS-CoV-2. Our approach maintained accurate SARS-CoV-2 variant identification while demonstrating a lower computational cost compared to the gold-standard statistical tools. The source code for this proof-of-concept implementation is freely available at https://github.com/omatheuspimenta/GRAMEP.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01715&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matheus Henrique Pimenta-Zanon, André Yoshiaki Kashiwabara, André Luís Laforga Vanzela, Fabricio Martins Lopes</name></author><category term="stat.AP" /><summary type="html">Advances in high throughput sequencing technologies provide a large number of genomes to be analyzed, so computational methodologies play a crucial role in analyzing and extracting knowledge from the data generated. Investigating genomic mutations is critical because of their impact on chromosomal evolution, genetic disorders, and diseases. It is common to adopt aligning sequences for analyzing genomic variations, however, this approach can be computationally expensive and potentially arbitrary in scenarios with large datasets. Here, we present a novel method for identifying single nucleotide polymorphisms (SNPs) in DNA sequences from assembled genomes. This method uses the principle of maximum entropy to select the most informative k-mers specific to the variant under investigation. The use of this informative k-mer set enables the detection of variant-specific mutations in comparison to a reference sequence. In addition, our method offers the possibility of classifying novel sequences with no need for organism-specific information. GRAMEP demonstrated high accuracy in both in silico simulations and analyses of real viral genomes, including Dengue, HIV, and SARS-CoV-2. Our approach maintained accurate SARS-CoV-2 variant identification while demonstrating a lower computational cost compared to the gold-standard statistical tools. The source code for this proof-of-concept implementation is freely available at https://github.com/omatheuspimenta/GRAMEP.</summary></entry><entry><title type="html">Improved distance correlation estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Improveddistancecorrelationestimation.html" rel="alternate" type="text/html" title="Improved distance correlation estimation" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Improveddistancecorrelationestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Improveddistancecorrelationestimation.html">&lt;p&gt;Distance correlation is a novel class of multivariate dependence measure, taking positive values between 0 and 1, and applicable to random vectors of arbitrary dimensions, not necessarily equal. It offers several advantages over the well-known Pearson correlation coefficient, the most important is that distance correlation equals zero if and only if the random vectors are independent.
  There are two different estimators of the distance correlation available in the literature. The first one, proposed by Sz&apos;ekely et al. (2007), is based on an asymptotically unbiased estimator of the distance covariance which turns out to be a V-statistic. The second one builds on an unbiased estimator of the distance covariance proposed in Sz&apos;ekely et al. (2014), proved to be an U-statistic by Sz&apos;ekely and Huo (2016). This study evaluates their efficiency (mean squared error) and compares computational times for both methods under different dependence structures. Under conditions of independence or near-independence, the V-estimates are biased, while the U-estimator frequently cannot be computed due to negative values. To address this challenge, a convex linear combination of the former estimators is proposed and studied, yielding good results regardless of the level of dependence.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01958&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Blanca E. Monroy-Castillo, M. A,  Jácome, Ricardo Cao</name></author><category term="stat.CO," /><category term="stat.TH" /><summary type="html">Distance correlation is a novel class of multivariate dependence measure, taking positive values between 0 and 1, and applicable to random vectors of arbitrary dimensions, not necessarily equal. It offers several advantages over the well-known Pearson correlation coefficient, the most important is that distance correlation equals zero if and only if the random vectors are independent. There are two different estimators of the distance correlation available in the literature. The first one, proposed by Sz&apos;ekely et al. (2007), is based on an asymptotically unbiased estimator of the distance covariance which turns out to be a V-statistic. The second one builds on an unbiased estimator of the distance covariance proposed in Sz&apos;ekely et al. (2014), proved to be an U-statistic by Sz&apos;ekely and Huo (2016). This study evaluates their efficiency (mean squared error) and compares computational times for both methods under different dependence structures. Under conditions of independence or near-independence, the V-estimates are biased, while the U-estimator frequently cannot be computed due to negative values. To address this challenge, a convex linear combination of the former estimators is proposed and studied, yielding good results regardless of the level of dependence.</summary></entry><entry><title type="html">Joint distribution of rises, falls, and number of runs in random sequences</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Jointdistributionofrisesfallsandnumberofrunsinrandomsequences.html" rel="alternate" type="text/html" title="Joint distribution of rises, falls, and number of runs in random sequences" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Jointdistributionofrisesfallsandnumberofrunsinrandomsequences</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Jointdistributionofrisesfallsandnumberofrunsinrandomsequences.html">&lt;p&gt;By using the matrix formulation of the two-step approach to the distributions of runs, a recursive relation and an explicit expression are derived for the generating function of the joint distribution of rises and falls for multivariate random sequences in terms of generating functions of individual letters, from which the generating functions of the joint distribution of rises, falls, and number of runs are obtained. An explicit formula for the joint distribution of rises and falls with arbitrary specification is also obtained.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01748&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yong Kong</name></author><category term="stat.AP" /><summary type="html">By using the matrix formulation of the two-step approach to the distributions of runs, a recursive relation and an explicit expression are derived for the generating function of the joint distribution of rises and falls for multivariate random sequences in terms of generating functions of individual letters, from which the generating functions of the joint distribution of rises, falls, and number of runs are obtained. An explicit formula for the joint distribution of rises and falls with arbitrary specification is also obtained.</summary></entry><entry><title type="html">Minimax Regret Learning for Data with Heterogeneous Subgroups</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/MinimaxRegretLearningforDatawithHeterogeneousSubgroups.html" rel="alternate" type="text/html" title="Minimax Regret Learning for Data with Heterogeneous Subgroups" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/MinimaxRegretLearningforDatawithHeterogeneousSubgroups</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/MinimaxRegretLearningforDatawithHeterogeneousSubgroups.html">&lt;p&gt;Modern complex datasets often consist of various sub-populations. To develop robust and generalizable methods in the presence of sub-population heterogeneity, it is important to guarantee a uniform learning performance instead of an average one. In many applications, prior information is often available on which sub-population or group the data points belong to. Given the observed groups of data, we develop a min-max-regret (MMR) learning framework for general supervised learning, which targets to minimize the worst-group regret. Motivated from the regret-based decision theoretic framework, the proposed MMR is distinguished from the value-based or risk-based robust learning methods in the existing literature. The regret criterion features several robustness and invariance properties simultaneously. In terms of generalizability, we develop the theoretical guarantee for the worst-case regret over a super-population of the meta data, which incorporates the observed sub-populations, their mixtures, as well as other unseen sub-populations that could be approximated by the observed ones. We demonstrate the effectiveness of our method through extensive simulation studies and an application to kidney transplantation data from hundreds of transplant centers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01709&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Weibin Mo, Weijing Tang, Songkai Xue, Yufeng Liu, Ji Zhu</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Modern complex datasets often consist of various sub-populations. To develop robust and generalizable methods in the presence of sub-population heterogeneity, it is important to guarantee a uniform learning performance instead of an average one. In many applications, prior information is often available on which sub-population or group the data points belong to. Given the observed groups of data, we develop a min-max-regret (MMR) learning framework for general supervised learning, which targets to minimize the worst-group regret. Motivated from the regret-based decision theoretic framework, the proposed MMR is distinguished from the value-based or risk-based robust learning methods in the existing literature. The regret criterion features several robustness and invariance properties simultaneously. In terms of generalizability, we develop the theoretical guarantee for the worst-case regret over a super-population of the meta data, which incorporates the observed sub-populations, their mixtures, as well as other unseen sub-populations that could be approximated by the observed ones. We demonstrate the effectiveness of our method through extensive simulation studies and an application to kidney transplantation data from hundreds of transplant centers.</summary></entry><entry><title type="html">Nonparametric classification with missing data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Nonparametricclassificationwithmissingdata.html" rel="alternate" type="text/html" title="Nonparametric classification with missing data" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Nonparametricclassificationwithmissingdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/Nonparametricclassificationwithmissingdata.html">&lt;p&gt;We introduce a new nonparametric framework for classification problems in the presence of missing data. The key aspect of our framework is that the regression function decomposes into an anova-type sum of orthogonal functions, of which some (or even many) may be zero. Working under a general missingness setting, which allows features to be missing not at random, our main goal is to derive the minimax rate for the excess risk in this problem. In addition to the decomposition property, the rate depends on parameters that control the tail behaviour of the marginal feature distributions, the smoothness of the regression function and a margin condition. The ambient data dimension does not appear in the minimax rate, which can therefore be faster than in the classical nonparametric setting. We further propose a new method, called the Hard-thresholding Anova Missing data (HAM) classifier, based on a careful combination of a k-nearest neighbour algorithm and a thresholding step. The HAM classifier attains the minimax rate up to polylogarithmic factors and numerical experiments further illustrate its utility.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.11672&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Torben Sell, Thomas B. Berrett, Timothy I. Cannings</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We introduce a new nonparametric framework for classification problems in the presence of missing data. The key aspect of our framework is that the regression function decomposes into an anova-type sum of orthogonal functions, of which some (or even many) may be zero. Working under a general missingness setting, which allows features to be missing not at random, our main goal is to derive the minimax rate for the excess risk in this problem. In addition to the decomposition property, the rate depends on parameters that control the tail behaviour of the marginal feature distributions, the smoothness of the regression function and a margin condition. The ambient data dimension does not appear in the minimax rate, which can therefore be faster than in the classical nonparametric setting. We further propose a new method, called the Hard-thresholding Anova Missing data (HAM) classifier, based on a careful combination of a k-nearest neighbour algorithm and a thresholding step. The HAM classifier attains the minimax rate up to polylogarithmic factors and numerical experiments further illustrate its utility.</summary></entry><entry><title type="html">Predictive Decision Synthesis for Portfolios: Betting on Better Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/PredictiveDecisionSynthesisforPortfoliosBettingonBetterModels.html" rel="alternate" type="text/html" title="Predictive Decision Synthesis for Portfolios: Betting on Better Models" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/PredictiveDecisionSynthesisforPortfoliosBettingonBetterModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/PredictiveDecisionSynthesisforPortfoliosBettingonBetterModels.html">&lt;p&gt;We discuss and develop Bayesian dynamic modelling and predictive decision synthesis for portfolio analysis. The context involves model uncertainty with a set of candidate models for financial time series with main foci in sequential learning, forecasting, and recursive decisions for portfolio reinvestments. The foundational perspective of Bayesian predictive decision synthesis (BPDS) defines novel, operational analysis and resulting predictive and decision outcomes. A detailed case study of BPDS in financial forecasting of international exchange rate time series and portfolio rebalancing, with resulting BPDS-based decision outcomes compared to traditional Bayesian analysis, exemplifies and highlights the practical advances achievable under the expanded, subjective Bayesian approach that BPDS defines.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01598&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emily Tallman, Mike West</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">We discuss and develop Bayesian dynamic modelling and predictive decision synthesis for portfolio analysis. The context involves model uncertainty with a set of candidate models for financial time series with main foci in sequential learning, forecasting, and recursive decisions for portfolio reinvestments. The foundational perspective of Bayesian predictive decision synthesis (BPDS) defines novel, operational analysis and resulting predictive and decision outcomes. A detailed case study of BPDS in financial forecasting of international exchange rate time series and portfolio rebalancing, with resulting BPDS-based decision outcomes compared to traditional Bayesian analysis, exemplifies and highlights the practical advances achievable under the expanded, subjective Bayesian approach that BPDS defines.</summary></entry><entry><title type="html">Quantifying the Causal Effect of Financial Literacy Courses on Financial Health</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/QuantifyingtheCausalEffectofFinancialLiteracyCoursesonFinancialHealth.html" rel="alternate" type="text/html" title="Quantifying the Causal Effect of Financial Literacy Courses on Financial Health" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/QuantifyingtheCausalEffectofFinancialLiteracyCoursesonFinancialHealth</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/QuantifyingtheCausalEffectofFinancialLiteracyCoursesonFinancialHealth.html">&lt;p&gt;In this study, we investigate the causal effect of financial literacy education on a composite financial health score constructed from 17 self-reported financial health and distress metrics ranging from spending habits to confidence in ability to repay debt to day-to-day financial skill. Leveraging data from the 2021 National Financial Capability Study, we find a significant and positive average treatment effect of financial literacy education on financial health. To test the robustness of this effect, we utilize a variety of causal estimators (Generalized Lin’s estimator, 1:1 propensity matching, IPW, and AIPW) and conduct sensitivity analysis using alternate health outcome scoring and varying caliper strengths. Our results are robust to these changes. The robust positive effect of financial literacy education on financial health found here motivates financial education for all individuals and holds implications for policymakers seeking to address the worsening debt problem in the U.S, though the relatively small magnitude of effect demands further research by experts in the domain of financial health.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01789&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Daniel Frees, Arnav Gangal, Charles Shaviro</name></author><category term="stat.AP" /><summary type="html">In this study, we investigate the causal effect of financial literacy education on a composite financial health score constructed from 17 self-reported financial health and distress metrics ranging from spending habits to confidence in ability to repay debt to day-to-day financial skill. Leveraging data from the 2021 National Financial Capability Study, we find a significant and positive average treatment effect of financial literacy education on financial health. To test the robustness of this effect, we utilize a variety of causal estimators (Generalized Lin’s estimator, 1:1 propensity matching, IPW, and AIPW) and conduct sensitivity analysis using alternate health outcome scoring and varying caliper strengths. Our results are robust to these changes. The robust positive effect of financial literacy education on financial health found here motivates financial education for all individuals and holds implications for policymakers seeking to address the worsening debt problem in the U.S, though the relatively small magnitude of effect demands further research by experts in the domain of financial health.</summary></entry><entry><title type="html">Sample-efficient neural likelihood-free Bayesian inference of implicit HMMs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/SampleefficientneurallikelihoodfreeBayesianinferenceofimplicitHMMs.html" rel="alternate" type="text/html" title="Sample-efficient neural likelihood-free Bayesian inference of implicit HMMs" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/SampleefficientneurallikelihoodfreeBayesianinferenceofimplicitHMMs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/SampleefficientneurallikelihoodfreeBayesianinferenceofimplicitHMMs.html">&lt;p&gt;Likelihood-free inference methods based on neural conditional density estimation were shown to drastically reduce the simulation burden in comparison to classical methods such as ABC. When applied in the context of any latent variable model, such as a Hidden Markov model (HMM), these methods are designed to only estimate the parameters, rather than the joint distribution of the parameters and the hidden states. Naive application of these methods to a HMM, ignoring the inference of this joint posterior distribution, will thus produce an inaccurate estimate of the posterior predictive distribution, in turn hampering the assessment of goodness-of-fit. To rectify this problem, we propose a novel, sample-efficient likelihood-free method for estimating the high-dimensional hidden states of an implicit HMM. Our approach relies on learning directly the intractable posterior distribution of the hidden states, using an autoregressive-flow, by exploiting the Markov property. Upon evaluating our approach on some implicit HMMs, we found that the quality of the estimates retrieved using our method is comparable to what can be achieved using a much more computationally expensive SMC algorithm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01737&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sanmitra Ghosh, Paul J. Birrell, Daniela De Angelis</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Likelihood-free inference methods based on neural conditional density estimation were shown to drastically reduce the simulation burden in comparison to classical methods such as ABC. When applied in the context of any latent variable model, such as a Hidden Markov model (HMM), these methods are designed to only estimate the parameters, rather than the joint distribution of the parameters and the hidden states. Naive application of these methods to a HMM, ignoring the inference of this joint posterior distribution, will thus produce an inaccurate estimate of the posterior predictive distribution, in turn hampering the assessment of goodness-of-fit. To rectify this problem, we propose a novel, sample-efficient likelihood-free method for estimating the high-dimensional hidden states of an implicit HMM. Our approach relies on learning directly the intractable posterior distribution of the hidden states, using an autoregressive-flow, by exploiting the Markov property. Upon evaluating our approach on some implicit HMMs, we found that the quality of the estimates retrieved using our method is comparable to what can be achieved using a much more computationally expensive SMC algorithm.</summary></entry><entry><title type="html">Sensitivity analysis for matching on high-dimensional predictors: A case study of racial disparity in US mortality</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/SensitivityanalysisformatchingonhighdimensionalpredictorsAcasestudyofracialdisparityinUSmortality.html" rel="alternate" type="text/html" title="Sensitivity analysis for matching on high-dimensional predictors: A case study of racial disparity in US mortality" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/SensitivityanalysisformatchingonhighdimensionalpredictorsAcasestudyofracialdisparityinUSmortality</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/SensitivityanalysisformatchingonhighdimensionalpredictorsAcasestudyofracialdisparityinUSmortality.html">&lt;p&gt;Matching on a low dimensional vector of scalar covariates consists of constructing groups of individuals in which each individual in a group is within a pre-specified distance from an individual in another group. However, matching in high dimensional spaces is more challenging because the distance can be sensitive to implementation details, caliper width, and measurement error of observations. To partially address these problems, we propose to use extensive sensitivity analyses and identify the main sources of variation and bias. We illustrate these concepts by examining the racial disparity in all-cause mortality in the US using the National Health and Nutrition Examination Survey (NHANES 2003-2006). In particular, we match African Americans to Caucasian Americans on age, gender, BMI and objectively measured physical activity (PA). PA is measured every minute using accelerometers for up to seven days and then transformed into an empirical distribution of all of the minute-level observations. The Wasserstein metric is used as the measure of distance between these participant-specific distributions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01694&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marina Hernandez, Ciprian Crainiceanu</name></author><category term="stat.AP" /><summary type="html">Matching on a low dimensional vector of scalar covariates consists of constructing groups of individuals in which each individual in a group is within a pre-specified distance from an individual in another group. However, matching in high dimensional spaces is more challenging because the distance can be sensitive to implementation details, caliper width, and measurement error of observations. To partially address these problems, we propose to use extensive sensitivity analyses and identify the main sources of variation and bias. We illustrate these concepts by examining the racial disparity in all-cause mortality in the US using the National Health and Nutrition Examination Survey (NHANES 2003-2006). In particular, we match African Americans to Caucasian Americans on age, gender, BMI and objectively measured physical activity (PA). PA is measured every minute using accelerometers for up to seven days and then transformed into an empirical distribution of all of the minute-level observations. The Wasserstein metric is used as the measure of distance between these participant-specific distributions.</summary></entry><entry><title type="html">Stable Distillation and High-Dimensional Hypothesis Testing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/StableDistillationandHighDimensionalHypothesisTesting.html" rel="alternate" type="text/html" title="Stable Distillation and High-Dimensional Hypothesis Testing" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/StableDistillationandHighDimensionalHypothesisTesting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/StableDistillationandHighDimensionalHypothesisTesting.html">&lt;p&gt;While powerful methods have been developed for high-dimensional hypothesis testing assuming orthogonal parameters, current approaches struggle to generalize to the more common non-orthogonal case. We propose Stable Distillation (SD), a simple paradigm for iteratively extracting independent pieces of information from observed data, assuming a parametric model. When applied to hypothesis testing for large regression models, SD orthogonalizes the effect estimates of non-orthogonal predictors by judiciously introducing noise into the observed outcomes vector, yielding mutually independent p-values across predictors. Simulations and a real regression example using US campaign contributions show that SD yields a scalable approach for non-orthogonal designs that exceeds or matches the power of existing methods against sparse alternatives. While we only present explicit SD algorithms for hypothesis testing in ordinary least squares and logistic regression, we provide general guidance for deriving and improving the power of SD procedures.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.12539&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ryan Christ, Ira Hall, David Steinsaltz</name></author><category term="stat.ME" /><summary type="html">While powerful methods have been developed for high-dimensional hypothesis testing assuming orthogonal parameters, current approaches struggle to generalize to the more common non-orthogonal case. We propose Stable Distillation (SD), a simple paradigm for iteratively extracting independent pieces of information from observed data, assuming a parametric model. When applied to hypothesis testing for large regression models, SD orthogonalizes the effect estimates of non-orthogonal predictors by judiciously introducing noise into the observed outcomes vector, yielding mutually independent p-values across predictors. Simulations and a real regression example using US campaign contributions show that SD yields a scalable approach for non-orthogonal designs that exceeds or matches the power of existing methods against sparse alternatives. While we only present explicit SD algorithms for hypothesis testing in ordinary least squares and logistic regression, we provide general guidance for deriving and improving the power of SD procedures.</summary></entry><entry><title type="html">Testing for an Explosive Bubble using High-Frequency Volatility</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/TestingforanExplosiveBubbleusingHighFrequencyVolatility.html" rel="alternate" type="text/html" title="Testing for an Explosive Bubble using High-Frequency Volatility" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/TestingforanExplosiveBubbleusingHighFrequencyVolatility</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/TestingforanExplosiveBubbleusingHighFrequencyVolatility.html">&lt;p&gt;Based on a continuous-time stochastic volatility model with a linear drift, we develop a test for explosive behavior in financial asset prices at a low frequency when prices are sampled at a higher frequency. The test exploits the volatility information in the high-frequency data. The method consists of devolatizing log-asset price increments with realized volatility measures and performing a supremum-type recursive Dickey-Fuller test on the devolatized sample. The proposed test has a nuisance-parameter-free asymptotic distribution and is easy to implement. We study the size and power properties of the test in Monte Carlo simulations. A real-time date-stamping strategy based on the devolatized sample is proposed for the origination and conclusion dates of the explosive regime. Conditions under which the real-time date-stamping strategy is consistent are established. The test and the date-stamping strategy are applied to study explosive behavior in cryptocurrency and stock markets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.02087&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>H. Peter Boswijk, Jun Yu, Yang Zu</name></author><category term="stat.ME" /><summary type="html">Based on a continuous-time stochastic volatility model with a linear drift, we develop a test for explosive behavior in financial asset prices at a low frequency when prices are sampled at a higher frequency. The test exploits the volatility information in the high-frequency data. The method consists of devolatizing log-asset price increments with realized volatility measures and performing a supremum-type recursive Dickey-Fuller test on the devolatized sample. The proposed test has a nuisance-parameter-free asymptotic distribution and is easy to implement. We study the size and power properties of the test in Monte Carlo simulations. A real-time date-stamping strategy based on the devolatized sample is proposed for the origination and conclusion dates of the explosive regime. Conditions under which the real-time date-stamping strategy is consistent are established. The test and the date-stamping strategy are applied to study explosive behavior in cryptocurrency and stock markets.</summary></entry><entry><title type="html">The m-th Longest Runs of Multivariate Random Sequences</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/ThemthLongestRunsofMultivariateRandomSequences.html" rel="alternate" type="text/html" title="The m-th Longest Runs of Multivariate Random Sequences" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/ThemthLongestRunsofMultivariateRandomSequences</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/ThemthLongestRunsofMultivariateRandomSequences.html">&lt;p&gt;The distributions of the $m$-th longest runs of multivariate random sequences are considered. For random sequences made up of $k$ kinds of letters, the lengths of the runs are sorted in two ways to give two definitions of run length ordering. In one definition, the lengths of the runs are sorted separately for each letter type. In the second definition, the lengths of all the runs are sorted together. Exact formulas are developed for the distributions of the m-th longest runs for both definitions. The derivations are based on a two-step method that is applicable to various other runs-related distributions, such as joint distributions of several letter types and multiple run lengths of a single letter type.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01747&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yong Kong</name></author><category term="stat.AP" /><summary type="html">The distributions of the $m$-th longest runs of multivariate random sequences are considered. For random sequences made up of $k$ kinds of letters, the lengths of the runs are sorted in two ways to give two definitions of run length ordering. In one definition, the lengths of the runs are sorted separately for each letter type. In the second definition, the lengths of all the runs are sorted together. Exact formulas are developed for the distributions of the m-th longest runs for both definitions. The derivations are based on a two-step method that is applicable to various other runs-related distributions, such as joint distributions of several letter types and multiple run lengths of a single letter type.</summary></entry><entry><title type="html">Unifying and extending Precision Recall metrics for assessing generative models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/UnifyingandextendingPrecisionRecallmetricsforassessinggenerativemodels.html" rel="alternate" type="text/html" title="Unifying and extending Precision Recall metrics for assessing generative models" /><published>2024-05-06T00:00:00+00:00</published><updated>2024-05-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/UnifyingandextendingPrecisionRecallmetricsforassessinggenerativemodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/06/UnifyingandextendingPrecisionRecallmetricsforassessinggenerativemodels.html">&lt;p&gt;With the recent success of generative models in image and text, the evaluation of generative models has gained a lot of attention. Whereas most generative models are compared in terms of scalar values such as Frechet Inception Distance (FID) or Inception Score (IS), in the last years (Sajjadi et al., 2018) proposed a definition of precision-recall curve to characterize the closeness of two distributions. Since then, various approaches to precision and recall have seen the light (Kynkaanniemi et al., 2019; Naeem et al., 2020; Park &amp;amp; Kim, 2023). They center their attention on the extreme values of precision and recall, but apart from this fact, their ties are elusive. In this paper, we unify most of these approaches under the same umbrella, relying on the work of (Simon et al., 2019). Doing so, we were able not only to recover entire curves, but also to expose the sources of the accounted pitfalls of the concerned metrics. We also provide consistency results that go well beyond the ones presented in the corresponding literature. Last, we study the different behaviors of the curves obtained experimentally.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01611&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Benjamin Sykes, Loic Simon, Julien Rabin</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">With the recent success of generative models in image and text, the evaluation of generative models has gained a lot of attention. Whereas most generative models are compared in terms of scalar values such as Frechet Inception Distance (FID) or Inception Score (IS), in the last years (Sajjadi et al., 2018) proposed a definition of precision-recall curve to characterize the closeness of two distributions. Since then, various approaches to precision and recall have seen the light (Kynkaanniemi et al., 2019; Naeem et al., 2020; Park &amp;amp; Kim, 2023). They center their attention on the extreme values of precision and recall, but apart from this fact, their ties are elusive. In this paper, we unify most of these approaches under the same umbrella, relying on the work of (Simon et al., 2019). Doing so, we were able not only to recover entire curves, but also to expose the sources of the accounted pitfalls of the concerned metrics. We also provide consistency results that go well beyond the ones presented in the corresponding literature. Last, we study the different behaviors of the curves obtained experimentally.</summary></entry></feed>