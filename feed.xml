<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-13T07:14:03+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">ASF-YOLO: A Novel YOLO Model with Attentional Scale Sequence Fusion for Cell Instance Segmentation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/ASFYOLOANovelYOLOModelwithAttentionalScaleSequenceFusionforCellInstanceSegmentation.html" rel="alternate" type="text/html" title="ASF-YOLO: A Novel YOLO Model with Attentional Scale Sequence Fusion for Cell Instance Segmentation" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/ASFYOLOANovelYOLOModelwithAttentionalScaleSequenceFusionforCellInstanceSegmentation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/ASFYOLOANovelYOLOModelwithAttentionalScaleSequenceFusionforCellInstanceSegmentation.html">&lt;p&gt;We propose a novel Attentional Scale Sequence Fusion based You Only Look Once (YOLO) framework (ASF-YOLO) which combines spatial and scale features for accurate and fast cell instance segmentation. Built on the YOLO segmentation framework, we employ the Scale Sequence Feature Fusion (SSFF) module to enhance the multi-scale information extraction capability of the network, and the Triple Feature Encoder (TFE) module to fuse feature maps of different scales to increase detailed information. We further introduce a Channel and Position Attention Mechanism (CPAM) to integrate both the SSFF and TPE modules, which focus on informative channels and spatial position-related small objects for improved detection and segmentation performance. Experimental validations on two cell datasets show remarkable segmentation accuracy and speed of the proposed ASF-YOLO model. It achieves a box mAP of 0.91, mask mAP of 0.887, and an inference speed of 47.3 FPS on the 2018 Data Science Bowl dataset, outperforming the state-of-the-art methods. The source code is available at https://github.com/mkang315/ASF-YOLO.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.06458&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ming Kang, Chee-Ming Ting, Fung Fung Ting, RaphaÃ«l C. -W. Phan</name></author><category term="stat.AP" /><summary type="html">We propose a novel Attentional Scale Sequence Fusion based You Only Look Once (YOLO) framework (ASF-YOLO) which combines spatial and scale features for accurate and fast cell instance segmentation. Built on the YOLO segmentation framework, we employ the Scale Sequence Feature Fusion (SSFF) module to enhance the multi-scale information extraction capability of the network, and the Triple Feature Encoder (TFE) module to fuse feature maps of different scales to increase detailed information. We further introduce a Channel and Position Attention Mechanism (CPAM) to integrate both the SSFF and TPE modules, which focus on informative channels and spatial position-related small objects for improved detection and segmentation performance. Experimental validations on two cell datasets show remarkable segmentation accuracy and speed of the proposed ASF-YOLO model. It achieves a box mAP of 0.91, mask mAP of 0.887, and an inference speed of 47.3 FPS on the 2018 Data Science Bowl dataset, outperforming the state-of-the-art methods. The source code is available at https://github.com/mkang315/ASF-YOLO.</summary></entry><entry><title type="html">A Stochastic-Geometrical Framework for Object Pose Estimation based on Mixture Models Avoiding the Correspondence Problem</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/AStochasticGeometricalFrameworkforObjectPoseEstimationbasedonMixtureModelsAvoidingtheCorrespondenceProblem.html" rel="alternate" type="text/html" title="A Stochastic-Geometrical Framework for Object Pose Estimation based on Mixture Models Avoiding the Correspondence Problem" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/AStochasticGeometricalFrameworkforObjectPoseEstimationbasedonMixtureModelsAvoidingtheCorrespondenceProblem</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/AStochasticGeometricalFrameworkforObjectPoseEstimationbasedonMixtureModelsAvoidingtheCorrespondenceProblem.html">&lt;p&gt;Background: Pose estimation of rigid objects is a practical challenge in optical metrology and computer vision. This paper presents a novel stochastic-geometrical modeling framework for object pose estimation based on observing multiple feature points.
  Methods: This framework utilizes mixture models for feature point densities in object space and for interpreting real measurements. Advantages are the avoidance to resolve individual feature correspondences and to incorporate correct stochastic dependencies in multi-view applications. First, the general modeling framework is presented, second, a general algorithm for pose estimation is derived, and third, two example models (camera and lateration setup) are presented.
  Results: Numerical experiments show the effectiveness of this modeling and general algorithm by presenting four simulation scenarios for three observation systems, including the dependence on measurement resolution, object deformations and measurement noise. Probabilistic modeling utilizing mixture models shows the potential for accurate and robust pose estimations while avoiding the correspondence problem.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.18107&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wolfgang Hoegele</name></author><category term="stat.AP" /><summary type="html">Background: Pose estimation of rigid objects is a practical challenge in optical metrology and computer vision. This paper presents a novel stochastic-geometrical modeling framework for object pose estimation based on observing multiple feature points. Methods: This framework utilizes mixture models for feature point densities in object space and for interpreting real measurements. Advantages are the avoidance to resolve individual feature correspondences and to incorporate correct stochastic dependencies in multi-view applications. First, the general modeling framework is presented, second, a general algorithm for pose estimation is derived, and third, two example models (camera and lateration setup) are presented. Results: Numerical experiments show the effectiveness of this modeling and general algorithm by presenting four simulation scenarios for three observation systems, including the dependence on measurement resolution, object deformations and measurement noise. Probabilistic modeling utilizing mixture models shows the potential for accurate and robust pose estimations while avoiding the correspondence problem.</summary></entry><entry><title type="html">Accounting for selection biases in population analyses: equivalence of the in-likelihood and post-processing approaches</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Accountingforselectionbiasesinpopulationanalysesequivalenceoftheinlikelihoodandpostprocessingapproaches.html" rel="alternate" type="text/html" title="Accounting for selection biases in population analyses: equivalence of the in-likelihood and post-processing approaches" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Accountingforselectionbiasesinpopulationanalysesequivalenceoftheinlikelihoodandpostprocessingapproaches</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Accountingforselectionbiasesinpopulationanalysesequivalenceoftheinlikelihoodandpostprocessingapproaches.html">&lt;p&gt;In this paper I show the equivalence, under appropriate assumptions, of two alternative methods to account for the presence of selection biases (also called selection effects) in population studies: one is to include the selection effects in the likelihood directly; the other follows the procedure of first inferring the observed distribution and then removing selection effects a posteriori. Moreover, I investigate a potential bias allegedly induced by the latter approach: I show that this procedure, if applied under the appropriate assumptions, does not produce the aforementioned bias.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06366&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Stefano Rinaldi</name></author><category term="stat.ME" /><summary type="html">In this paper I show the equivalence, under appropriate assumptions, of two alternative methods to account for the presence of selection biases (also called selection effects) in population studies: one is to include the selection effects in the likelihood directly; the other follows the procedure of first inferring the observed distribution and then removing selection effects a posteriori. Moreover, I investigate a potential bias allegedly induced by the latter approach: I show that this procedure, if applied under the appropriate assumptions, does not produce the aforementioned bias.</summary></entry><entry><title type="html">Analysis of Active/Inactive Patterns in the NHANES Data using Generalized Multilevel Functional Principal Component Analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/AnalysisofActiveInactivePatternsintheNHANESDatausingGeneralizedMultilevelFunctionalPrincipalComponentAnalysis.html" rel="alternate" type="text/html" title="Analysis of Active/Inactive Patterns in the NHANES Data using Generalized Multilevel Functional Principal Component Analysis" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/AnalysisofActiveInactivePatternsintheNHANESDatausingGeneralizedMultilevelFunctionalPrincipalComponentAnalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/AnalysisofActiveInactivePatternsintheNHANESDatausingGeneralizedMultilevelFunctionalPrincipalComponentAnalysis.html">&lt;p&gt;Between 2011 and 2014 NHANES collected objectively measured physical activity data using wrist-worn accelerometers for tens of thousands of individuals for up to seven days. Here we analyze the minute-level indicators of being active, which can be viewed as binary (because there is an active indicator at every minute), multilevel (because there are multiple days of data for each study participant), functional (because within-day data can be viewed as a function of time) data. To extract within- and between-participant directions of variation in the data, we introduce Generalized Multilevel Functional Principal Component Analysis (GM-FPCA), an approach based on the dimension reduction of the linear predictor. Scores associated with specific patterns of activity are shown to be strongly associated with time to death. Extensive simulation studies indicate that GM-FPCA provides accurate estimation of model parameters, is computationally stable, and is scalable in the number of study participants, visits, and observations within visits. R code for implementing the method is provided.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.14054&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinkai Zhou, Julia Wrobel, Ciprian M. Crainiceanu, Andrew Leroux</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Between 2011 and 2014 NHANES collected objectively measured physical activity data using wrist-worn accelerometers for tens of thousands of individuals for up to seven days. Here we analyze the minute-level indicators of being active, which can be viewed as binary (because there is an active indicator at every minute), multilevel (because there are multiple days of data for each study participant), functional (because within-day data can be viewed as a function of time) data. To extract within- and between-participant directions of variation in the data, we introduce Generalized Multilevel Functional Principal Component Analysis (GM-FPCA), an approach based on the dimension reduction of the linear predictor. Scores associated with specific patterns of activity are shown to be strongly associated with time to death. Extensive simulation studies indicate that GM-FPCA provides accurate estimation of model parameters, is computationally stable, and is scalable in the number of study participants, visits, and observations within visits. R code for implementing the method is provided.</summary></entry><entry><title type="html">A statistical reconstruction algorithm for positronium lifetime imaging using time-of-flight positron emission tomography</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Astatisticalreconstructionalgorithmforpositroniumlifetimeimagingusingtimeofflightpositronemissiontomography.html" rel="alternate" type="text/html" title="A statistical reconstruction algorithm for positronium lifetime imaging using time-of-flight positron emission tomography" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Astatisticalreconstructionalgorithmforpositroniumlifetimeimagingusingtimeofflightpositronemissiontomography</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Astatisticalreconstructionalgorithmforpositroniumlifetimeimagingusingtimeofflightpositronemissiontomography.html">&lt;p&gt;Positron emission tomography (PET) is an important modality for diagnosing diseases such as cancer and Alzheimerâs disease, capable of revealing the uptake of radiolabeled molecules that target specific pathological markers of the diseases. Recently, positronium lifetime imaging (PLI) that adds to traditional PET the ability to explore properties of the tissue microenvironment beyond tracer uptake has been demonstrated with time-of-flight (TOF) PET and the use of non-pure positron emitters. However, achieving accurate reconstruction of lifetime images from data acquired by systems having a finite TOF resolution still presents a challenge. This paper focuses on the two-dimensional PLI, introducing a maximum likelihood estimation (MLE) method that employs an exponentially modified Gaussian (EMG) probability distribution that describes the positronium lifetime data produced by TOF PET. We evaluate the performance of our EMG-based MLE method against \st{traditional} approaches using exponential likelihood functions and penalized surrogate methods. Results from computer-simulated data reveal that the proposed EMG-MLE method can yield quantitatively accurate lifetime images. We also demonstrate that the proposed MLE formulation can be extended to handle PLI data containing multiple positron populations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2206.06463&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hsin-Hsiung Huang, Zheyuan Zhu, Slun Booppasiri, Zhuo Chen, Shuo Pang, Chien-Min Kao</name></author><category term="stat.AP" /><summary type="html">Positron emission tomography (PET) is an important modality for diagnosing diseases such as cancer and Alzheimerâs disease, capable of revealing the uptake of radiolabeled molecules that target specific pathological markers of the diseases. Recently, positronium lifetime imaging (PLI) that adds to traditional PET the ability to explore properties of the tissue microenvironment beyond tracer uptake has been demonstrated with time-of-flight (TOF) PET and the use of non-pure positron emitters. However, achieving accurate reconstruction of lifetime images from data acquired by systems having a finite TOF resolution still presents a challenge. This paper focuses on the two-dimensional PLI, introducing a maximum likelihood estimation (MLE) method that employs an exponentially modified Gaussian (EMG) probability distribution that describes the positronium lifetime data produced by TOF PET. We evaluate the performance of our EMG-based MLE method against \st{traditional} approaches using exponential likelihood functions and penalized surrogate methods. Results from computer-simulated data reveal that the proposed EMG-MLE method can yield quantitatively accurate lifetime images. We also demonstrate that the proposed MLE formulation can be extended to handle PLI data containing multiple positron populations.</summary></entry><entry><title type="html">Balancing Specialization and Adaptation in a Transforming Scientific Landscape</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/BalancingSpecializationandAdaptationinaTransformingScientificLandscape.html" rel="alternate" type="text/html" title="Balancing Specialization and Adaptation in a Transforming Scientific Landscape" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/BalancingSpecializationandAdaptationinaTransformingScientificLandscape</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/BalancingSpecializationandAdaptationinaTransformingScientificLandscape.html">&lt;p&gt;How do scientists navigate between the need to capitalize on their prior knowledge through specialization, and the urge to adapt to evolving research opportunities? Drawing from diverse perspectives on adaptation, including cultural evolution, this paper proposes an unsupervised Bayesian approach motivated by Optimal Transport of the evolution of scientistsâ research portfolios in response to transformations in their field. The model relies on $186,162$ scientific abstracts and authorship data to evaluate the influence of intellectual, social, and institutional resources on scientistsâ trajectories within a cohort of $2\,195$ high-energy physicists between 2000 and 2019. Using Inverse Optimal Transport, the reallocation of research efforts is shown to be shaped by learning costs, thus enhancing the utility of the scientific capital disseminated among scientists. Two dimensions of social capital, namely âdiversityâ and âpowerâ, have opposite associations with the magnitude of change in scientistsâ research interests: while âdiversityâ disrupts and expands research interests, âpowerâ is associated with more stable research agendas. Social capital plays a more crucial role in shifts between cognitively distant research areas. More generally, this work suggests new approaches for understanding, measuring and modeling collective adaptation using Optimal Transport.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.14040&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lucas Gautheron</name></author><category term="stat.AP" /><summary type="html">How do scientists navigate between the need to capitalize on their prior knowledge through specialization, and the urge to adapt to evolving research opportunities? Drawing from diverse perspectives on adaptation, including cultural evolution, this paper proposes an unsupervised Bayesian approach motivated by Optimal Transport of the evolution of scientistsâ research portfolios in response to transformations in their field. The model relies on $186,162$ scientific abstracts and authorship data to evaluate the influence of intellectual, social, and institutional resources on scientistsâ trajectories within a cohort of $2\,195$ high-energy physicists between 2000 and 2019. Using Inverse Optimal Transport, the reallocation of research efforts is shown to be shaped by learning costs, thus enhancing the utility of the scientific capital disseminated among scientists. Two dimensions of social capital, namely âdiversityâ and âpowerâ, have opposite associations with the magnitude of change in scientistsâ research interests: while âdiversityâ disrupts and expands research interests, âpowerâ is associated with more stable research agendas. Social capital plays a more crucial role in shifts between cognitively distant research areas. More generally, this work suggests new approaches for understanding, measuring and modeling collective adaptation using Optimal Transport.</summary></entry><entry><title type="html">Bayesian Optimization of Sample Entropy Hyperparameters for Short Time Series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/BayesianOptimizationofSampleEntropyHyperparametersforShortTimeSeries.html" rel="alternate" type="text/html" title="Bayesian Optimization of Sample Entropy Hyperparameters for Short Time Series" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/BayesianOptimizationofSampleEntropyHyperparametersforShortTimeSeries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/BayesianOptimizationofSampleEntropyHyperparametersforShortTimeSeries.html">&lt;p&gt;Quantifying the complexity and irregularity of time series data is a primary pursuit across various data-scientific disciplines. Sample entropy (SampEn) is a widely adopted metric for this purpose, but its reliability is sensitive to the choice of its hyperparameters, the embedding dimension $(m)$ and the similarity radius $(r)$, especially for short-duration signals. This paper presents a novel methodology that addresses this challenge. We introduce a Bayesian optimization framework, integrated with a bootstrap-based variance estimator tailored for short signals, to simultaneously and optimally select the values of $m$ and $r$ for reliable SampEn estimation. Through validation on synthetic signal experiments, our approach outperformed existing benchmarks. It achieved a 60 to 90% reduction in relative error for estimating SampEn variance and a 22 to 45% decrease in relative mean squared error for SampEn estimation itself ($p \leq 0.043$). Applying our method to publicly available short-signal benchmarks yielded promising results. Unlike existing competitors, our approach was the only one to successfully identify known entropy differences across all signal sets ($p \leq 0.042$). Additionally, we introduce âEristroPy,â an open-source Python package that implements our proposed optimization framework for SampEn hyperparameter selection. This work holds potential for applications where accurate estimation of entropy from short-duration signals is paramount.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06112&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zachary Blanks, Donald E. Brown</name></author><category term="stat.AP" /><summary type="html">Quantifying the complexity and irregularity of time series data is a primary pursuit across various data-scientific disciplines. Sample entropy (SampEn) is a widely adopted metric for this purpose, but its reliability is sensitive to the choice of its hyperparameters, the embedding dimension $(m)$ and the similarity radius $(r)$, especially for short-duration signals. This paper presents a novel methodology that addresses this challenge. We introduce a Bayesian optimization framework, integrated with a bootstrap-based variance estimator tailored for short signals, to simultaneously and optimally select the values of $m$ and $r$ for reliable SampEn estimation. Through validation on synthetic signal experiments, our approach outperformed existing benchmarks. It achieved a 60 to 90% reduction in relative error for estimating SampEn variance and a 22 to 45% decrease in relative mean squared error for SampEn estimation itself ($p \leq 0.043$). Applying our method to publicly available short-signal benchmarks yielded promising results. Unlike existing competitors, our approach was the only one to successfully identify known entropy differences across all signal sets ($p \leq 0.042$). Additionally, we introduce âEristroPy,â an open-source Python package that implements our proposed optimization framework for SampEn hyperparameter selection. This work holds potential for applications where accurate estimation of entropy from short-duration signals is paramount.</summary></entry><entry><title type="html">Bayesian factor zero-inflated Poisson model for multiple grouped count data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/BayesianfactorzeroinflatedPoissonmodelformultiplegroupedcountdata.html" rel="alternate" type="text/html" title="Bayesian factor zero-inflated Poisson model for multiple grouped count data" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/BayesianfactorzeroinflatedPoissonmodelformultiplegroupedcountdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/BayesianfactorzeroinflatedPoissonmodelformultiplegroupedcountdata.html">&lt;p&gt;This paper proposes a computationally efficient Bayesian factor model for multiple grouped count data. Adopting the link function approach, the proposed model can capture the association within and between the at-risk probabilities and Poisson counts over multiple dimensions. The likelihood function for the grouped count data consists of the differences of the cumulative distribution functions evaluated at the endpoints of the groups, defining the probabilities of each data point falling in the groups. The combination of the data augmentation of underlying counts, the P&apos;{o}lya-Gamma augmentation to approximate the Poisson distribution, and parameter expansion for the factor components is used to facilitate posterior computing. The efficacy of the proposed factor model is demonstrated using the simulated data and real data on the involvement of youths in the nineteen illegal activities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06335&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Genya Kobayashi, Yuta Yamauchi</name></author><category term="stat.ME" /><summary type="html">This paper proposes a computationally efficient Bayesian factor model for multiple grouped count data. Adopting the link function approach, the proposed model can capture the association within and between the at-risk probabilities and Poisson counts over multiple dimensions. The likelihood function for the grouped count data consists of the differences of the cumulative distribution functions evaluated at the endpoints of the groups, defining the probabilities of each data point falling in the groups. The combination of the data augmentation of underlying counts, the P&apos;{o}lya-Gamma augmentation to approximate the Poisson distribution, and parameter expansion for the factor components is used to facilitate posterior computing. The efficacy of the proposed factor model is demonstrated using the simulated data and real data on the involvement of youths in the nineteen illegal activities.</summary></entry><entry><title type="html">Deep Learning-Based Residual Useful Lifetime Prediction for Assets with Uncertain Failure Modes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/DeepLearningBasedResidualUsefulLifetimePredictionforAssetswithUncertainFailureModes.html" rel="alternate" type="text/html" title="Deep Learning-Based Residual Useful Lifetime Prediction for Assets with Uncertain Failure Modes" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/DeepLearningBasedResidualUsefulLifetimePredictionforAssetswithUncertainFailureModes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/DeepLearningBasedResidualUsefulLifetimePredictionforAssetswithUncertainFailureModes.html">&lt;p&gt;Industrial prognostics focuses on utilizing degradation signals to forecast and continually update the residual useful life of complex engineering systems. However, existing prognostic models for systems with multiple failure modes face several challenges in real-world applications, including overlapping degradation signals from multiple components, the presence of unlabeled historical data, and the similarity of signals across different failure modes. To tackle these issues, this research introduces two prognostic models that integrate the mixture (log)-location-scale distribution with deep learning. This integration facilitates the modeling of overlapping degradation signals, eliminates the need for explicit failure mode identification, and utilizes deep learning to capture complex nonlinear relationships between degradation signals and residual useful lifetimes. Numerical studies validate the superior performance of these proposed models compared to existing methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06068&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuqi Su, Xiaolei Fang</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Industrial prognostics focuses on utilizing degradation signals to forecast and continually update the residual useful life of complex engineering systems. However, existing prognostic models for systems with multiple failure modes face several challenges in real-world applications, including overlapping degradation signals from multiple components, the presence of unlabeled historical data, and the similarity of signals across different failure modes. To tackle these issues, this research introduces two prognostic models that integrate the mixture (log)-location-scale distribution with deep learning. This integration facilitates the modeling of overlapping degradation signals, eliminates the need for explicit failure mode identification, and utilizes deep learning to capture complex nonlinear relationships between degradation signals and residual useful lifetimes. Numerical studies validate the superior performance of these proposed models compared to existing methods.</summary></entry><entry><title type="html">Distributionally robust halfspace depth</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Distributionallyrobusthalfspacedepth.html" rel="alternate" type="text/html" title="Distributionally robust halfspace depth" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Distributionallyrobusthalfspacedepth</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Distributionallyrobusthalfspacedepth.html">&lt;p&gt;Tukeyâs halfspace depth can be seen as a stochastic program and as such it is not guarded against optimizerâs curse, so that a limited training sample may easily result in a poor out-of-sample performance. We propose a generalized halfspace depth concept relying on the recent advances in distributionally robust optimization, where every halfspace is examined using the respective worst-case distribution in the Wasserstein ball of radius $\delta\geq 0$ centered at the empirical law. This new depth can be seen as a smoothed and regularized classical halfspace depth which is retrieved as $\delta\downarrow 0$. It inherits most of the main properties of the latter and, additionally, enjoys various new attractive features such as continuity and strict positivity beyond the convex hull of the support. We provide numerical illustrations of the new depth and its advantages, and develop some fundamental theory. In particular, we study the upper level sets and the median region including their breakdown properties.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2101.00726&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jevgenijs Ivanovs, Pavlo Mozharovskyi</name></author><category term="stat.ME" /><summary type="html">Tukeyâs halfspace depth can be seen as a stochastic program and as such it is not guarded against optimizerâs curse, so that a limited training sample may easily result in a poor out-of-sample performance. We propose a generalized halfspace depth concept relying on the recent advances in distributionally robust optimization, where every halfspace is examined using the respective worst-case distribution in the Wasserstein ball of radius $\delta\geq 0$ centered at the empirical law. This new depth can be seen as a smoothed and regularized classical halfspace depth which is retrieved as $\delta\downarrow 0$. It inherits most of the main properties of the latter and, additionally, enjoys various new attractive features such as continuity and strict positivity beyond the convex hull of the support. We provide numerical illustrations of the new depth and its advantages, and develop some fundamental theory. In particular, we study the upper level sets and the median region including their breakdown properties.</summary></entry><entry><title type="html">Do NHL goalies get hot in the playoffs? A multilevel logistic regression analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/DoNHLgoaliesgethotintheplayoffsAmultilevellogisticregressionanalysis.html" rel="alternate" type="text/html" title="Do NHL goalies get hot in the playoffs? A multilevel logistic regression analysis" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/DoNHLgoaliesgethotintheplayoffsAmultilevellogisticregressionanalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/DoNHLgoaliesgethotintheplayoffsAmultilevellogisticregressionanalysis.html">&lt;p&gt;The hot-hand theory posits that an athlete who has performed well in the recent past performs better in the present. We use multilevel logistic regression to test this theory for National Hockey League playoff goaltenders, controlling for a variety of shot-related and game-related characteristics. Our data consists of 48,431 shots for 93 goaltenders in the 2008-2016 playoffs. Using a wide range of shot-based windows to quantify recent save performance, we find no evidence for the hot-hand theory, and some evidence that good recent save performance negatively impacts the next-shot save probability. We use a permutation test to rule out a regression to the mean explanation for our findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2102.09689&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Likang Ding, Ivor Cribben, Armann Ingolfsson, Monica Tran</name></author><category term="stat.AP" /><summary type="html">The hot-hand theory posits that an athlete who has performed well in the recent past performs better in the present. We use multilevel logistic regression to test this theory for National Hockey League playoff goaltenders, controlling for a variety of shot-related and game-related characteristics. Our data consists of 48,431 shots for 93 goaltenders in the 2008-2016 playoffs. Using a wide range of shot-based windows to quantify recent save performance, we find no evidence for the hot-hand theory, and some evidence that good recent save performance negatively impacts the next-shot save probability. We use a permutation test to rule out a regression to the mean explanation for our findings.</summary></entry><entry><title type="html">Enhancing Model Fit Evaluation in SEM: Practical Tips for Optimizing Chi-Square Tests</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/EnhancingModelFitEvaluationinSEMPracticalTipsforOptimizingChiSquareTests.html" rel="alternate" type="text/html" title="Enhancing Model Fit Evaluation in SEM: Practical Tips for Optimizing Chi-Square Tests" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/EnhancingModelFitEvaluationinSEMPracticalTipsforOptimizingChiSquareTests</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/EnhancingModelFitEvaluationinSEMPracticalTipsforOptimizingChiSquareTests.html">&lt;p&gt;This paper underscores the vital role of the chi-square test within political science research utilizing structural equation modeling (SEM). The ongoing debate regarding the inclusion of chi-square test statistics alongside fit indices in result presentations has sparked controversy. Despite the recognized limitations of relying solely on the chi-square test, its judicious application can enhance its effectiveness in evaluating model fit and specification. To exemplify this, we present three common scenarios pertinent to political science research where fit indices may inadequately address goodness-of-fit concerns, while the chi-square statistic can be effectively harnessed. Through Monte Carlo simulations, we examine strategies for enhancing chi-square tests within these scenarios, showcasing the potential of appropriately employed chi-square tests to provide a comprehensive model fit assessment. Our recommendation is to report both the chi-square test and fit indices, with a priority on precise model specification to ensure the trustworthiness of model fit indicators.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.13939&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bang Quan Zheng, Peter M. Bentler</name></author><category term="stat.AP" /><summary type="html">This paper underscores the vital role of the chi-square test within political science research utilizing structural equation modeling (SEM). The ongoing debate regarding the inclusion of chi-square test statistics alongside fit indices in result presentations has sparked controversy. Despite the recognized limitations of relying solely on the chi-square test, its judicious application can enhance its effectiveness in evaluating model fit and specification. To exemplify this, we present three common scenarios pertinent to political science research where fit indices may inadequately address goodness-of-fit concerns, while the chi-square statistic can be effectively harnessed. Through Monte Carlo simulations, we examine strategies for enhancing chi-square tests within these scenarios, showcasing the potential of appropriately employed chi-square tests to provide a comprehensive model fit assessment. Our recommendation is to report both the chi-square test and fit indices, with a priority on precise model specification to ensure the trustworthiness of model fit indicators.</summary></entry><entry><title type="html">Flexible and efficient spatial extremes emulation via variational autoencoders</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Flexibleandefficientspatialextremesemulationviavariationalautoencoders.html" rel="alternate" type="text/html" title="Flexible and efficient spatial extremes emulation via variational autoencoders" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Flexibleandefficientspatialextremesemulationviavariationalautoencoders</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Flexibleandefficientspatialextremesemulationviavariationalautoencoders.html">&lt;p&gt;Many real-world processes have complex tail dependence structures that cannot be characterized using classical Gaussian processes. More flexible spatial extremes models exhibit appealing extremal dependence properties but are often exceedingly prohibitive to fit and simulate from in high dimensions. In this paper, we aim to push the boundaries on computation and modeling of high-dimensional spatial extremes via integrating a new spatial extremes model that has flexible and non-stationary dependence properties in the encoding-decoding structure of a variational autoencoder called the XVAE. The XVAE can emulate spatial observations and produce outputs that have the same statistical properties as the inputs, especially in the tail. Our approach also provides a novel way of making fast inference with complex extreme-value processes. Through extensive simulation studies, we show that our XVAE is substantially more time-efficient than traditional Bayesian inference while outperforming many spatial extremes models with a stationary dependence structure. Lastly, we analyze a high-resolution satellite-derived dataset of sea surface temperature in the Red Sea, which includes 30 years of daily measurements at 16703 grid cells. We demonstrate how to use XVAE to identify regions susceptible to marine heatwaves under climate change and examine the spatial and temporal variability of the extremal dependence structure.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.08079&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Likun Zhang, Xiaoyu Ma, Christopher K. Wikle, RaphaÃ«l Huser</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Many real-world processes have complex tail dependence structures that cannot be characterized using classical Gaussian processes. More flexible spatial extremes models exhibit appealing extremal dependence properties but are often exceedingly prohibitive to fit and simulate from in high dimensions. In this paper, we aim to push the boundaries on computation and modeling of high-dimensional spatial extremes via integrating a new spatial extremes model that has flexible and non-stationary dependence properties in the encoding-decoding structure of a variational autoencoder called the XVAE. The XVAE can emulate spatial observations and produce outputs that have the same statistical properties as the inputs, especially in the tail. Our approach also provides a novel way of making fast inference with complex extreme-value processes. Through extensive simulation studies, we show that our XVAE is substantially more time-efficient than traditional Bayesian inference while outperforming many spatial extremes models with a stationary dependence structure. Lastly, we analyze a high-resolution satellite-derived dataset of sea surface temperature in the Red Sea, which includes 30 years of daily measurements at 16703 grid cells. We demonstrate how to use XVAE to identify regions susceptible to marine heatwaves under climate change and examine the spatial and temporal variability of the extremal dependence structure.</summary></entry><entry><title type="html">Forecasting high-dimensional functional time series with dual-factor structures</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Forecastinghighdimensionalfunctionaltimeserieswithdualfactorstructures.html" rel="alternate" type="text/html" title="Forecasting high-dimensional functional time series with dual-factor structures" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Forecastinghighdimensionalfunctionaltimeserieswithdualfactorstructures</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Forecastinghighdimensionalfunctionaltimeserieswithdualfactorstructures.html">&lt;p&gt;We propose a dual-factor model for high-dimensional functional time series (HDFTS) that considers multiple populations. The HDFTS is first decomposed into a collection of functional time series (FTS) in a lower dimension and a group of population-specific basis functions. The system of basis functions describes cross-sectional heterogeneity, while the reduced-dimension FTS retains most of the information common to multiple populations. The low-dimensional FTS is further decomposed into a product of common functional loadings and a matrix-valued time series that contains the most temporal dynamics embedded in the original HDFTS. The proposed general-form dual-factor structure is connected to several commonly used functional factor models. We demonstrate the finite-sample performances of the proposed method in recovering cross-sectional basis functions and extracting common features using simulated HDFTS. An empirical study shows that the proposed model produces more accurate point and interval forecasts for subnational age-specific mortality rates in Japan. The financial benefits associated with the improved mortality forecasts are translated into a life annuity pricing scheme.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2109.04146&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chen Tang, Han Lin Shang, Yanrong Yang, Yang Yang</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">We propose a dual-factor model for high-dimensional functional time series (HDFTS) that considers multiple populations. The HDFTS is first decomposed into a collection of functional time series (FTS) in a lower dimension and a group of population-specific basis functions. The system of basis functions describes cross-sectional heterogeneity, while the reduced-dimension FTS retains most of the information common to multiple populations. The low-dimensional FTS is further decomposed into a product of common functional loadings and a matrix-valued time series that contains the most temporal dynamics embedded in the original HDFTS. The proposed general-form dual-factor structure is connected to several commonly used functional factor models. We demonstrate the finite-sample performances of the proposed method in recovering cross-sectional basis functions and extracting common features using simulated HDFTS. An empirical study shows that the proposed model produces more accurate point and interval forecasts for subnational age-specific mortality rates in Japan. The financial benefits associated with the improved mortality forecasts are translated into a life annuity pricing scheme.</summary></entry><entry><title type="html">Inferring Skin-Brain-Skin Connections from Infodemiology Data using Dynamic Bayesian Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/InferringSkinBrainSkinConnectionsfromInfodemiologyDatausingDynamicBayesianNetworks.html" rel="alternate" type="text/html" title="Inferring Skin-Brain-Skin Connections from Infodemiology Data using Dynamic Bayesian Networks" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/InferringSkinBrainSkinConnectionsfromInfodemiologyDatausingDynamicBayesianNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/InferringSkinBrainSkinConnectionsfromInfodemiologyDatausingDynamicBayesianNetworks.html">&lt;p&gt;The relationship between skin diseases and mental illnesses has been extensively studied using cross-sectional epidemiological data. Typically, such data can only measure association (rather than causation) and include only a subset of the diseases we may be interested in. In this paper, we complement the evidence from such analyses by learning an overarching causal network model over twelve health conditions from the Google Search Trends Symptoms public data set.
  We learned the causal network model using a dynamic Bayesian network, which can represent both cyclic and acyclic causal relationships, is easy to interpret and accounts for the spatio-temporal trends in the data in a probabilistically rigorous way. The causal network confirms a large number of cyclic relationships between the selected health conditions and the interplay between skin and mental diseases. For acne, we observe a cyclic relationship with anxiety and attention deficit hyperactivity disorder (ADHD) and an indirect relationship with depression through sleep disorders. For dermatitis, we observe directed links to anxiety, depression and sleep disorders and a cyclic relationship with ADHD. We also observe a link between dermatitis and ADHD and a cyclic relationship between acne and ADHD. Furthermore, the network includes several direct connections between sleep disorders and other health conditions, highlighting the impact of the former on the overall health and well-being of the patient.
  Mapping disease interplay, indirect relationships, and the key role of mediators, such as sleep disorders, will allow healthcare professionals to address disease management holistically and more effectively. Even if we consider all skin and mental diseases jointly, each disease subnetwork is unique, allowing for more targeted interventions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06405&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marco Scutari, Delphine Kerob, Samir Salah</name></author><category term="stat.AP" /><summary type="html">The relationship between skin diseases and mental illnesses has been extensively studied using cross-sectional epidemiological data. Typically, such data can only measure association (rather than causation) and include only a subset of the diseases we may be interested in. In this paper, we complement the evidence from such analyses by learning an overarching causal network model over twelve health conditions from the Google Search Trends Symptoms public data set. We learned the causal network model using a dynamic Bayesian network, which can represent both cyclic and acyclic causal relationships, is easy to interpret and accounts for the spatio-temporal trends in the data in a probabilistically rigorous way. The causal network confirms a large number of cyclic relationships between the selected health conditions and the interplay between skin and mental diseases. For acne, we observe a cyclic relationship with anxiety and attention deficit hyperactivity disorder (ADHD) and an indirect relationship with depression through sleep disorders. For dermatitis, we observe directed links to anxiety, depression and sleep disorders and a cyclic relationship with ADHD. We also observe a link between dermatitis and ADHD and a cyclic relationship between acne and ADHD. Furthermore, the network includes several direct connections between sleep disorders and other health conditions, highlighting the impact of the former on the overall health and well-being of the patient. Mapping disease interplay, indirect relationships, and the key role of mediators, such as sleep disorders, will allow healthcare professionals to address disease management holistically and more effectively. Even if we consider all skin and mental diseases jointly, each disease subnetwork is unique, allowing for more targeted interventions.</summary></entry><entry><title type="html">Informativeness of Weighted Conformal Prediction</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/InformativenessofWeightedConformalPrediction.html" rel="alternate" type="text/html" title="Informativeness of Weighted Conformal Prediction" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/InformativenessofWeightedConformalPrediction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/InformativenessofWeightedConformalPrediction.html">&lt;p&gt;Weighted conformal prediction (WCP), a recently proposed framework, provides uncertainty quantification with the flexibility to accommodate different covariate distributions between training and test data. However, it is pointed out in this paper that the effectiveness of WCP heavily relies on the overlap between covariate distributions; insufficient overlap can lead to uninformative prediction intervals. To enhance the informativeness of WCP, we propose two methods for scenarios involving multiple sources with varied covariate distributions. We establish theoretical guarantees for our proposed methods and demonstrate their efficacy through simulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06479&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mufang Ying, Wenge Guo, Koulik Khamaru, Ying Hung</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Weighted conformal prediction (WCP), a recently proposed framework, provides uncertainty quantification with the flexibility to accommodate different covariate distributions between training and test data. However, it is pointed out in this paper that the effectiveness of WCP heavily relies on the overlap between covariate distributions; insufficient overlap can lead to uninformative prediction intervals. To enhance the informativeness of WCP, we propose two methods for scenarios involving multiple sources with varied covariate distributions. We establish theoretical guarantees for our proposed methods and demonstrate their efficacy through simulations.</summary></entry><entry><title type="html">Intrinsic Bayesian CramÃ©r-Rao Bound with an Application to Covariance Matrix Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/IntrinsicBayesianCram%C3%A9rRaoBoundwithanApplicationtoCovarianceMatrixEstimation.html" rel="alternate" type="text/html" title="Intrinsic Bayesian CramÃ©r-Rao Bound with an Application to Covariance Matrix Estimation" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/IntrinsicBayesianCram%C3%A9rRaoBoundwithanApplicationtoCovarianceMatrixEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/IntrinsicBayesianCram%C3%A9rRaoBoundwithanApplicationtoCovarianceMatrixEstimation.html">&lt;p&gt;This paper presents a new performance bound for estimation problems where the parameter to estimate lies in a Riemannian manifold (a smooth manifold endowed with a Riemannian metric) and follows a given prior distribution. In this setup, the chosen Riemannian metric induces a geometry for the parameter manifold, as well as an intrinsic notion of the estimation error measure. Performance bound for such error measure were previously obtained in the non-Bayesian case (when the unknown parameter is assumed to deterministic), and referred to as \textit{intrinsic} Cram&apos;er-Rao bound. The presented result then appears either as: \textit{a}) an extension of the intrinsic Cram&apos;er-Rao bound to the Bayesian estimation framework; \textit{b}) a generalization of the Van-Trees inequality (Bayesian Cram&apos;er-Rao bound) that accounts for the aforementioned geometric structures. In a second part, we leverage this formalism to study the problem of covariance matrix estimation when the data follow a Gaussian distribution, and whose covariance matrix is drawn from an inverse Wishart distribution. Performance bounds for this problem are obtained for both the mean squared error (Euclidean metric) and the natural Riemannian distance for Hermitian positive definite matrices (affine invariant metric). Numerical simulation illustrate that assessing the error with the affine invariant metric is revealing of interesting properties of the maximum a posteriori and minimum mean square error estimator, which are not observed when using the Euclidean metric.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.04748&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Florent Bouchard, Alexandre Renaux, Guillaume Ginolhac, Arnaud Breloy</name></author><category term="stat.AP," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">This paper presents a new performance bound for estimation problems where the parameter to estimate lies in a Riemannian manifold (a smooth manifold endowed with a Riemannian metric) and follows a given prior distribution. In this setup, the chosen Riemannian metric induces a geometry for the parameter manifold, as well as an intrinsic notion of the estimation error measure. Performance bound for such error measure were previously obtained in the non-Bayesian case (when the unknown parameter is assumed to deterministic), and referred to as \textit{intrinsic} Cram&apos;er-Rao bound. The presented result then appears either as: \textit{a}) an extension of the intrinsic Cram&apos;er-Rao bound to the Bayesian estimation framework; \textit{b}) a generalization of the Van-Trees inequality (Bayesian Cram&apos;er-Rao bound) that accounts for the aforementioned geometric structures. In a second part, we leverage this formalism to study the problem of covariance matrix estimation when the data follow a Gaussian distribution, and whose covariance matrix is drawn from an inverse Wishart distribution. Performance bounds for this problem are obtained for both the mean squared error (Euclidean metric) and the natural Riemannian distance for Hermitian positive definite matrices (affine invariant metric). Numerical simulation illustrate that assessing the error with the affine invariant metric is revealing of interesting properties of the maximum a posteriori and minimum mean square error estimator, which are not observed when using the Euclidean metric.</summary></entry><entry><title type="html">Local Longitudinal Modified Treatment Policies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/LocalLongitudinalModifiedTreatmentPolicies.html" rel="alternate" type="text/html" title="Local Longitudinal Modified Treatment Policies" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/LocalLongitudinalModifiedTreatmentPolicies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/LocalLongitudinalModifiedTreatmentPolicies.html">&lt;p&gt;Longitudinal Modified Treatment Policies (LMTPs) provide a framework for defining a broad class of causal target parameters for continuous and categorical exposures. We propose Local LMTPs, a generalization of LMTPs to settings where the target parameter is conditional on subsets of units defined by the treatment or exposure. Such parameters have wide scientific relevance, with well-known parameters such as the Average Treatment Effect on the Treated (ATT) falling within the class. We provide a formal causal identification result that expresses the Local LMTP parameter in terms of sequential regressions, and derive the efficient influence function of the parameter which defines its semi-parametric and local asymptotic minimax efficiency bound. Efficient semi-parametric inference of Local LMTP parameters requires estimating the ratios of functions of complex conditional probabilities (or densities). We propose an estimator for Local LMTP parameters that directly estimates these required ratios via empirical loss minimization, drawing on the theory of Riesz representers. The estimator is implemented using a combination of ensemble machine learning algorithms and deep neural networks, and evaluated via simulation studies. We illustrate in simulation that estimation of the density ratios using Riesz representation might provide more stable estimators in finite samples in the presence of empirical violations of the overlap/positivity assumption.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06135&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Herbert Susmann, IvÃ¡n DÃ­az</name></author><category term="stat.ME" /><summary type="html">Longitudinal Modified Treatment Policies (LMTPs) provide a framework for defining a broad class of causal target parameters for continuous and categorical exposures. We propose Local LMTPs, a generalization of LMTPs to settings where the target parameter is conditional on subsets of units defined by the treatment or exposure. Such parameters have wide scientific relevance, with well-known parameters such as the Average Treatment Effect on the Treated (ATT) falling within the class. We provide a formal causal identification result that expresses the Local LMTP parameter in terms of sequential regressions, and derive the efficient influence function of the parameter which defines its semi-parametric and local asymptotic minimax efficiency bound. Efficient semi-parametric inference of Local LMTP parameters requires estimating the ratios of functions of complex conditional probabilities (or densities). We propose an estimator for Local LMTP parameters that directly estimates these required ratios via empirical loss minimization, drawing on the theory of Riesz representers. The estimator is implemented using a combination of ensemble machine learning algorithms and deep neural networks, and evaluated via simulation studies. We illustrate in simulation that estimation of the density ratios using Riesz representation might provide more stable estimators in finite samples in the presence of empirical violations of the overlap/positivity assumption.</summary></entry><entry><title type="html">Logistic-beta processes for dependent random probabilities with beta marginals</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Logisticbetaprocessesfordependentrandomprobabilitieswithbetamarginals.html" rel="alternate" type="text/html" title="Logistic-beta processes for dependent random probabilities with beta marginals" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Logisticbetaprocessesfordependentrandomprobabilitieswithbetamarginals</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Logisticbetaprocessesfordependentrandomprobabilitieswithbetamarginals.html">&lt;p&gt;The beta distribution serves as a canonical tool for modelling probabilities in statistics and machine learning. However, there is limited work on flexible and computationally convenient stochastic process extensions for modelling dependent random probabilities. We propose a novel stochastic process called the logistic-beta process, whose logistic transformation yields a stochastic process with common beta marginals. Logistic-beta processes can model dependence on both discrete and continuous domains, such as space or time, and have a flexible dependence structure through correlation kernels. Moreover, its normal variance-mean mixture representation leads to effective posterior inference algorithms. We illustrate the benefits through nonparametric binary regression and conditional density estimation examples, both in simulation studies and in a pregnancy outcome application.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.07048&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Changwoo J. Lee, Alessandro Zito, Huiyan Sang, David B. Dunson</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">The beta distribution serves as a canonical tool for modelling probabilities in statistics and machine learning. However, there is limited work on flexible and computationally convenient stochastic process extensions for modelling dependent random probabilities. We propose a novel stochastic process called the logistic-beta process, whose logistic transformation yields a stochastic process with common beta marginals. Logistic-beta processes can model dependence on both discrete and continuous domains, such as space or time, and have a flexible dependence structure through correlation kernels. Moreover, its normal variance-mean mixture representation leads to effective posterior inference algorithms. We illustrate the benefits through nonparametric binary regression and conditional density estimation examples, both in simulation studies and in a pregnancy outcome application.</summary></entry><entry><title type="html">Multivariate Interval-Valued Models in Frequentist and Bayesian Schemes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/MultivariateIntervalValuedModelsinFrequentistandBayesianSchemes.html" rel="alternate" type="text/html" title="Multivariate Interval-Valued Models in Frequentist and Bayesian Schemes" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/MultivariateIntervalValuedModelsinFrequentistandBayesianSchemes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/MultivariateIntervalValuedModelsinFrequentistandBayesianSchemes.html">&lt;p&gt;In recent years, addressing the challenges posed by massive datasets has led researchers to explore aggregated data, particularly leveraging interval-valued data, akin to traditional symbolic data analysis. While much recent research, with the exception of Samdai et al. (2023) who focused on the bivariate case, has primarily concentrated on parameter estimation in single-variable scenarios, this paper extends such investigations to the multivariate domain for the first time. We derive maximum likelihood (ML) estimators for the parameters and establish their asymptotic distributions. Additionally, we pioneer a theoretical Bayesian framework, previously confined to the univariate setting, for multivariate data. We provide a detailed exposition of the proposed estimators and conduct comparative performance analyses. Finally, we validate the effectiveness of our estimators through simulations and real-world data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06635&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ali Sadeghkhani, Abdolnasser Sadeghkhani</name></author><category term="stat.ME" /><summary type="html">In recent years, addressing the challenges posed by massive datasets has led researchers to explore aggregated data, particularly leveraging interval-valued data, akin to traditional symbolic data analysis. While much recent research, with the exception of Samdai et al. (2023) who focused on the bivariate case, has primarily concentrated on parameter estimation in single-variable scenarios, this paper extends such investigations to the multivariate domain for the first time. We derive maximum likelihood (ML) estimators for the parameters and establish their asymptotic distributions. Additionally, we pioneer a theoretical Bayesian framework, previously confined to the univariate setting, for multivariate data. We provide a detailed exposition of the proposed estimators and conduct comparative performance analyses. Finally, we validate the effectiveness of our estimators through simulations and real-world data analysis.</summary></entry><entry><title type="html">Next generation clinical trials: Seamless designs and master protocols</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/NextgenerationclinicaltrialsSeamlessdesignsandmasterprotocols.html" rel="alternate" type="text/html" title="Next generation clinical trials: Seamless designs and master protocols" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/NextgenerationclinicaltrialsSeamlessdesignsandmasterprotocols</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/NextgenerationclinicaltrialsSeamlessdesignsandmasterprotocols.html">&lt;p&gt;Background: Drug development is often inefficient, costly and lengthy, yet it is essential for evaluating the safety and efficacy of new interventions. Compared with other disease areas, this is particularly true for Phase II / III cancer clinical trials where high attrition rates and reduced regulatory approvals are being seen. In response to these challenges, seamless clinical trials and master protocols have emerged to streamline the drug development process. Methods: Seamless clinical trials, characterized by their ability to transition seamlessly from one phase to another, can lead to accelerating the development of promising therapies while Master protocols provide a framework for investigating multiple treatment options and patient subgroups within a single trial. Results: We discuss the advantages of these methods through real trial examples and the principals that lead to their success while also acknowledging the associated regulatory considerations and challenges. Conclusion: Seamless designs and Master protocols have the potential to improve confirmatory clinical trials. In the disease area of cancer, this ultimately means that patients can receive life-saving treatments sooner.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06353&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Abigail Burdon, Thomas Jaki, Xijin Chen, Pavel Mozgunov, Haiyan Zheng, Richard Baird</name></author><category term="stat.ME" /><summary type="html">Background: Drug development is often inefficient, costly and lengthy, yet it is essential for evaluating the safety and efficacy of new interventions. Compared with other disease areas, this is particularly true for Phase II / III cancer clinical trials where high attrition rates and reduced regulatory approvals are being seen. In response to these challenges, seamless clinical trials and master protocols have emerged to streamline the drug development process. Methods: Seamless clinical trials, characterized by their ability to transition seamlessly from one phase to another, can lead to accelerating the development of promising therapies while Master protocols provide a framework for investigating multiple treatment options and patient subgroups within a single trial. Results: We discuss the advantages of these methods through real trial examples and the principals that lead to their success while also acknowledging the associated regulatory considerations and challenges. Conclusion: Seamless designs and Master protocols have the potential to improve confirmatory clinical trials. In the disease area of cancer, this ultimately means that patients can receive life-saving treatments sooner.</summary></entry><entry><title type="html">On foundation of generative statistics with F-entropy: a gradient-based approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach.html" rel="alternate" type="text/html" title="On foundation of generative statistics with F-entropy: a gradient-based approach" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach.html">&lt;p&gt;This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can {\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g. missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisherâs divergence. (The F-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for Possible future projects include discrete data and Bayesian variational inference.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05389&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bing Cheng, Howell Tong</name></author><category term="stat.ME" /><summary type="html">This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can {\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g. missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisherâs divergence. (The F-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for Possible future projects include discrete data and Bayesian variational inference.</summary></entry><entry><title type="html">Random matrix theory improved FrÃ©chet mean of symmetric positive definite matrices</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/RandommatrixtheoryimprovedFr%C3%A9chetmeanofsymmetricpositivedefinitematrices.html" rel="alternate" type="text/html" title="Random matrix theory improved FrÃ©chet mean of symmetric positive definite matrices" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/RandommatrixtheoryimprovedFr%C3%A9chetmeanofsymmetricpositivedefinitematrices</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/RandommatrixtheoryimprovedFr%C3%A9chetmeanofsymmetricpositivedefinitematrices.html">&lt;p&gt;In this study, we consider the realm of covariance matrices in machine learning, particularly focusing on computing Fr&apos;echet means on the manifold of symmetric positive definite matrices, commonly referred to as Karcher or geometric means. Such means are leveraged in numerous machine-learning tasks. Relying on advanced statistical tools, we introduce a random matrix theory-based method that estimates Fr&apos;echet means, which is particularly beneficial when dealing with low sample support and a high number of matrices to average. Our experimental evaluation, involving both synthetic and real-world EEG and hyperspectral datasets, shows that we largely outperform state-of-the-art methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06558&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Florent Bouchard, Ammar Mian, Malik Tiomoko, Guillaume Ginolhac, FrÃ©dÃ©ric Pascal</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">In this study, we consider the realm of covariance matrices in machine learning, particularly focusing on computing Fr&apos;echet means on the manifold of symmetric positive definite matrices, commonly referred to as Karcher or geometric means. Such means are leveraged in numerous machine-learning tasks. Relying on advanced statistical tools, we introduce a random matrix theory-based method that estimates Fr&apos;echet means, which is particularly beneficial when dealing with low sample support and a high number of matrices to average. Our experimental evaluation, involving both synthetic and real-world EEG and hyperspectral datasets, shows that we largely outperform state-of-the-art methods.</summary></entry><entry><title type="html">Sampling the Swadesh List to Identify Similar Languages with Tree Spaces</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SamplingtheSwadeshListtoIdentifySimilarLanguageswithTreeSpaces.html" rel="alternate" type="text/html" title="Sampling the Swadesh List to Identify Similar Languages with Tree Spaces" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SamplingtheSwadeshListtoIdentifySimilarLanguageswithTreeSpaces</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SamplingtheSwadeshListtoIdentifySimilarLanguageswithTreeSpaces.html">&lt;p&gt;Communication plays a vital role in human interaction. Studying language is a worthwhile task and more recently has become quantitative in nature with developments of fields like quantitative comparative linguistics and lexicostatistics. With respect to the authors own native languages, the ancestry of the English language and the Latin alphabet are of the primary interest. The Indo-European Tree traces many modern languages back to the Proto-Indo-European root. Swadeshâs cognates played a large role in developing that historical perspective where some of the primary branches are Germanic, Celtic, Italic, and Balto-Slavic. This paper will use data analysis on open books where the simplest singular space is the 3-spider - a union T3 of three rays with their endpoints glued at a point 0 - which can represent these tree spaces for language clustering. These trees are built using a single linkage method for clustering based on distances between samples from languages which use the Latin Script. Taking three languages at a time, the barycenter is determined. Some initial results have found both non-sticky and sticky sample means. If the mean exhibits non-sticky properties, then one language may come from a different ancestor than the other two. If the mean is considered sticky, then the languages may share a common ancestor or all languages may have different ancestry.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06549&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Garett Ordway, Vic Patrangenaru</name></author><category term="stat.AP" /><summary type="html">Communication plays a vital role in human interaction. Studying language is a worthwhile task and more recently has become quantitative in nature with developments of fields like quantitative comparative linguistics and lexicostatistics. With respect to the authors own native languages, the ancestry of the English language and the Latin alphabet are of the primary interest. The Indo-European Tree traces many modern languages back to the Proto-Indo-European root. Swadeshâs cognates played a large role in developing that historical perspective where some of the primary branches are Germanic, Celtic, Italic, and Balto-Slavic. This paper will use data analysis on open books where the simplest singular space is the 3-spider - a union T3 of three rays with their endpoints glued at a point 0 - which can represent these tree spaces for language clustering. These trees are built using a single linkage method for clustering based on distances between samples from languages which use the Latin Script. Taking three languages at a time, the barycenter is determined. Some initial results have found both non-sticky and sticky sample means. If the mean exhibits non-sticky properties, then one language may come from a different ancestor than the other two. If the mean is considered sticky, then the languages may share a common ancestor or all languages may have different ancestry.</summary></entry><entry><title type="html">Separating States in Astronomical Sources Using Hidden Markov Models: With a Case Study of Flaring and Quiescence on EV Lac</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SeparatingStatesinAstronomicalSourcesUsingHiddenMarkovModelsWithaCaseStudyofFlaringandQuiescenceonEVLac.html" rel="alternate" type="text/html" title="Separating States in Astronomical Sources Using Hidden Markov Models: With a Case Study of Flaring and Quiescence on EV Lac" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SeparatingStatesinAstronomicalSourcesUsingHiddenMarkovModelsWithaCaseStudyofFlaringandQuiescenceonEVLac</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SeparatingStatesinAstronomicalSourcesUsingHiddenMarkovModelsWithaCaseStudyofFlaringandQuiescenceonEVLac.html">&lt;p&gt;We present a new method to distinguish between different states (e.g., high and low, quiescent and flaring) in astronomical sources with count data. The method models the underlying physical process as latent variables following a continuous-space Markov chain that determines the expected Poisson counts in observed light curves in multiple passbands. For the underlying state process, we consider several autoregressive processes, yielding continuous-space hidden Markov models of varying complexity. Under these models, we can infer the state that the object is in at any given time. The state predictions from these models are then dichotomized with the help of a finite-mixture model to produce state classifications. We apply these techniques to X-ray data from the active dMe flare star EV Lac, splitting the data into quiescent and flaring states. We find that a first-order vector autoregressive process efficiently separates flaring from quiescence: flaring occurs over 30-40% of the observation durations, a well-defined persistent quiescent state can be identified, and the flaring state is characterized by higher temperatures and emission measures.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06540&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Robert Zimmerman, David A. van Dyk, Vinay L. Kashyap, Aneta Siemiginowska</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">We present a new method to distinguish between different states (e.g., high and low, quiescent and flaring) in astronomical sources with count data. The method models the underlying physical process as latent variables following a continuous-space Markov chain that determines the expected Poisson counts in observed light curves in multiple passbands. For the underlying state process, we consider several autoregressive processes, yielding continuous-space hidden Markov models of varying complexity. Under these models, we can infer the state that the object is in at any given time. The state predictions from these models are then dichotomized with the help of a finite-mixture model to produce state classifications. We apply these techniques to X-ray data from the active dMe flare star EV Lac, splitting the data into quiescent and flaring states. We find that a first-order vector autoregressive process efficiently separates flaring from quiescence: flaring occurs over 30-40% of the observation durations, a well-defined persistent quiescent state can be identified, and the flaring state is characterized by higher temperatures and emission measures.</summary></entry><entry><title type="html">Simultaneously detecting spatiotemporal changes with penalized Poisson regression models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SimultaneouslydetectingspatiotemporalchangeswithpenalizedPoissonregressionmodels.html" rel="alternate" type="text/html" title="Simultaneously detecting spatiotemporal changes with penalized Poisson regression models" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SimultaneouslydetectingspatiotemporalchangeswithpenalizedPoissonregressionmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SimultaneouslydetectingspatiotemporalchangeswithpenalizedPoissonregressionmodels.html">&lt;p&gt;In the realm of large-scale spatiotemporal data, abrupt changes are commonly occurring across both spatial and temporal domains. This study aims to address the concurrent challenges of detecting change points and identifying spatial clusters within spatiotemporal count data. We introduce an innovative method based on the Poisson regression model, employing doubly fused penalization to unveil the underlying spatiotemporal change patterns. To efficiently estimate the model, we present an iterative shrinkage and threshold based algorithm to minimize the doubly penalized likelihood function. We establish the statistical consistency properties of the proposed estimator, confirming its reliability and accuracy. Furthermore, we conduct extensive numerical experiments to validate our theoretical findings, thereby highlighting the superior performance of our method when compared to existing competitive approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06613&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zerui Zhang, Xin Wang, Xin Zhang, Jing Zhang</name></author><category term="stat.ME" /><summary type="html">In the realm of large-scale spatiotemporal data, abrupt changes are commonly occurring across both spatial and temporal domains. This study aims to address the concurrent challenges of detecting change points and identifying spatial clusters within spatiotemporal count data. We introduce an innovative method based on the Poisson regression model, employing doubly fused penalization to unveil the underlying spatiotemporal change patterns. To efficiently estimate the model, we present an iterative shrinkage and threshold based algorithm to minimize the doubly penalized likelihood function. We establish the statistical consistency properties of the proposed estimator, confirming its reliability and accuracy. Furthermore, we conduct extensive numerical experiments to validate our theoretical findings, thereby highlighting the superior performance of our method when compared to existing competitive approaches.</summary></entry><entry><title type="html">Single-seed generation of Brownian paths and integrals for adaptive and high order SDE solvers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SingleseedgenerationofBrownianpathsandintegralsforadaptiveandhighorderSDEsolvers.html" rel="alternate" type="text/html" title="Single-seed generation of Brownian paths and integrals for adaptive and high order SDE solvers" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SingleseedgenerationofBrownianpathsandintegralsforadaptiveandhighorderSDEsolvers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SingleseedgenerationofBrownianpathsandintegralsforadaptiveandhighorderSDEsolvers.html">&lt;p&gt;Despite the success of adaptive time-stepping in ODE simulation, it has so far seen few applications for Stochastic Differential Equations (SDEs). To simulate SDEs adaptively, methods such as the Virtual Brownian Tree (VBT) have been developed, which can generate Brownian motion (BM) non-chronologically. However, in most applications, knowing only the values of Brownian motion is not enough to achieve a high order of convergence; for that, we must compute time-integrals of BM such as $\int_s^t W_r \, dr$. With the aim of using high order SDE solvers adaptively, we extend the VBT to generate these integrals of BM in addition to the Brownian increments. A JAX-based implementation of our construction is included in the popular Diffrax library (https://github.com/patrick-kidger/diffrax).
  Since the entire Brownian path produced by VBT is uniquely determined by a single PRNG seed, previously generated samples need not be stored, which results in a constant memory footprint and enables experiment repeatability and strong error estimation. Based on binary search, the VBTâs time complexity is logarithmic in the tolerance parameter $\varepsilon$. Unlike the original VBT algorithm, which was only precise at some dyadic times, we prove that our construction exactly matches the joint distribution of the Brownian motion and its time integrals at any query times, provided they are at least $\varepsilon$ apart.
  We present two applications of adaptive high order solvers enabled by our new VBT. Using adaptive solvers to simulate a high-volatility CIR model, we achieve more than twice the convergence order of constant stepping. We apply an adaptive third order underdamped or kinetic Langevin solver to an MCMC problem, where our approach outperforms the No U-Turn Sampler, while using only a tenth of its function evaluations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06464&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>AndraÅ¾ JelinÄiÄ, James Foster, Patrick Kidger</name></author><category term="stat.CO" /><summary type="html">Despite the success of adaptive time-stepping in ODE simulation, it has so far seen few applications for Stochastic Differential Equations (SDEs). To simulate SDEs adaptively, methods such as the Virtual Brownian Tree (VBT) have been developed, which can generate Brownian motion (BM) non-chronologically. However, in most applications, knowing only the values of Brownian motion is not enough to achieve a high order of convergence; for that, we must compute time-integrals of BM such as $\int_s^t W_r \, dr$. With the aim of using high order SDE solvers adaptively, we extend the VBT to generate these integrals of BM in addition to the Brownian increments. A JAX-based implementation of our construction is included in the popular Diffrax library (https://github.com/patrick-kidger/diffrax). Since the entire Brownian path produced by VBT is uniquely determined by a single PRNG seed, previously generated samples need not be stored, which results in a constant memory footprint and enables experiment repeatability and strong error estimation. Based on binary search, the VBTâs time complexity is logarithmic in the tolerance parameter $\varepsilon$. Unlike the original VBT algorithm, which was only precise at some dyadic times, we prove that our construction exactly matches the joint distribution of the Brownian motion and its time integrals at any query times, provided they are at least $\varepsilon$ apart. We present two applications of adaptive high order solvers enabled by our new VBT. Using adaptive solvers to simulate a high-volatility CIR model, we achieve more than twice the convergence order of constant stepping. We apply an adaptive third order underdamped or kinetic Langevin solver to an MCMC problem, where our approach outperforms the No U-Turn Sampler, while using only a tenth of its function evaluations.</summary></entry><entry><title type="html">Skewness of a randomized quasi-Monte Carlo estimate</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SkewnessofarandomizedquasiMonteCarloestimate.html" rel="alternate" type="text/html" title="Skewness of a randomized quasi-Monte Carlo estimate" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SkewnessofarandomizedquasiMonteCarloestimate</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/SkewnessofarandomizedquasiMonteCarloestimate.html">&lt;p&gt;Some recent work on confidence intervals for randomized quasi-Monte Carlo (RQMC) sampling found a surprising result: ordinary Student $t$ 95\% confidence intervals based on a modest number of replicates were seen to be very effective and even more reliable than some bootstrap $t$ intervals that were expected to be best. One potential explanation is that those RQMC estimates have small skewness. In this paper we give conditions under which the skewness is $O(n^\epsilon)$ for any $\epsilon&amp;gt;0$, so `almost $O(1)$â. Under a random generator matrix model, we can improve this rate to $O(n^{-1/2+\epsilon})$ with very high probability. We also improve some probabilistic bounds on the distribution of the quality parameter $t$ for a digital net in a prime base under random sampling of generator matrices.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06136&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zexin Pan, Art B. Owen</name></author><category term="stat.CO" /><summary type="html">Some recent work on confidence intervals for randomized quasi-Monte Carlo (RQMC) sampling found a surprising result: ordinary Student $t$ 95\% confidence intervals based on a modest number of replicates were seen to be very effective and even more reliable than some bootstrap $t$ intervals that were expected to be best. One potential explanation is that those RQMC estimates have small skewness. In this paper we give conditions under which the skewness is $O(n^\epsilon)$ for any $\epsilon&amp;gt;0$, so `almost $O(1)$â. Under a random generator matrix model, we can improve this rate to $O(n^{-1/2+\epsilon})$ with very high probability. We also improve some probabilistic bounds on the distribution of the quality parameter $t$ for a digital net in a prime base under random sampling of generator matrices.</summary></entry><entry><title type="html">The landscapemetrics and motif packages for measuring landscape patterns and processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Thelandscapemetricsandmotifpackagesformeasuringlandscapepatternsandprocesses.html" rel="alternate" type="text/html" title="The landscapemetrics and motif packages for measuring landscape patterns and processes" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Thelandscapemetricsandmotifpackagesformeasuringlandscapepatternsandprocesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/Thelandscapemetricsandmotifpackagesformeasuringlandscapepatternsandprocesses.html">&lt;p&gt;This book chapter emphasizes the significance of categorical raster data in ecological studies, specifically land use or land cover (LULC) data, and highlights the pivotal role of landscape metrics and pattern-based spatial analysis in comprehending environmental patterns and their dynamics. It explores the usage of R packages, particularly landscapemetrics and motif, for quantifying and analyzing landscape patterns using LULC data from three distinct European regions. It showcases the computation, visualization, and comparison of landscape metrics, while also addressing additional features such as patch value extraction, sub-region sampling, and moving window computation. Furthermore, the chapter delves into the intricacies of pattern-based spatial analysis, explaining how spatial signatures are computed and how the motif package facilitates comparisons and clustering of landscape patterns. The chapter concludes by discussing the potential of customization and expansion of the presented tools.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06559&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jakub Nowosad, Maximilian H. K. Hesselbarth</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">This book chapter emphasizes the significance of categorical raster data in ecological studies, specifically land use or land cover (LULC) data, and highlights the pivotal role of landscape metrics and pattern-based spatial analysis in comprehending environmental patterns and their dynamics. It explores the usage of R packages, particularly landscapemetrics and motif, for quantifying and analyzing landscape patterns using LULC data from three distinct European regions. It showcases the computation, visualization, and comparison of landscape metrics, while also addressing additional features such as patch value extraction, sub-region sampling, and moving window computation. Furthermore, the chapter delves into the intricacies of pattern-based spatial analysis, explaining how spatial signatures are computed and how the motif package facilitates comparisons and clustering of landscape patterns. The chapter concludes by discussing the potential of customization and expansion of the presented tools.</summary></entry><entry><title type="html">Variational Inference for Acceleration of SN Ia Photometric Distance Estimation with BayeSN</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/VariationalInferenceforAccelerationofSNIaPhotometricDistanceEstimationwithBayeSN.html" rel="alternate" type="text/html" title="Variational Inference for Acceleration of SN Ia Photometric Distance Estimation with BayeSN" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/VariationalInferenceforAccelerationofSNIaPhotometricDistanceEstimationwithBayeSN</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/VariationalInferenceforAccelerationofSNIaPhotometricDistanceEstimationwithBayeSN.html">&lt;p&gt;Type Ia supernovae (SNe Ia) are standarizable candles whose observed light curves can be used to infer their distances, which can in turn be used in cosmological analyses. As the quantity of observed SNe Ia grows with current and upcoming surveys, increasingly scalable analyses are necessary to take full advantage of these new datasets for precise estimation of cosmological parameters. Bayesian inference methods enable fitting SN Ia light curves with robust uncertainty quantification, but traditional posterior sampling using Markov Chain Monte Carlo (MCMC) is computationally expensive. We present an implementation of variational inference (VI) to accelerate the fitting of SN Ia light curves using the BayeSN hierarchical Bayesian model for time-varying SN Ia spectral energy distributions (SEDs). We demonstrate and evaluate its performance on both simulated light curves and data from the Foundation Supernova Survey with two different forms of surrogate posterior â a multivariate normal and a custom multivariate zero-lower-truncated normal distribution â and compare them with the Laplace Approximation and full MCMC analysis. To validate of our variational approximation, we calculate the pareto-smoothed importance sampling (PSIS) diagnostic, and perform variational simulation-based calibration (VSBC). The VI approximation achieves similar results to MCMC but with an order-of-magnitude speedup for the inference of the photometric distance moduli. Overall, we show that VI is a promising method for scalable parameter inference that enables analysis of larger datasets for precision cosmology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06013&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ana SofÃ­a M. Uzsoy, Stephen Thorp, Matthew Grayling, Kaisey S. Mandel</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Type Ia supernovae (SNe Ia) are standarizable candles whose observed light curves can be used to infer their distances, which can in turn be used in cosmological analyses. As the quantity of observed SNe Ia grows with current and upcoming surveys, increasingly scalable analyses are necessary to take full advantage of these new datasets for precise estimation of cosmological parameters. Bayesian inference methods enable fitting SN Ia light curves with robust uncertainty quantification, but traditional posterior sampling using Markov Chain Monte Carlo (MCMC) is computationally expensive. We present an implementation of variational inference (VI) to accelerate the fitting of SN Ia light curves using the BayeSN hierarchical Bayesian model for time-varying SN Ia spectral energy distributions (SEDs). We demonstrate and evaluate its performance on both simulated light curves and data from the Foundation Supernova Survey with two different forms of surrogate posterior â a multivariate normal and a custom multivariate zero-lower-truncated normal distribution â and compare them with the Laplace Approximation and full MCMC analysis. To validate of our variational approximation, we calculate the pareto-smoothed importance sampling (PSIS) diagnostic, and perform variational simulation-based calibration (VSBC). The VI approximation achieves similar results to MCMC but with an order-of-magnitude speedup for the inference of the photometric distance moduli. Overall, we show that VI is a promising method for scalable parameter inference that enables analysis of larger datasets for precision cosmology.</summary></entry><entry><title type="html">When Respondents Donât Care Anymore: Identifying the Onset of Careless Responding</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/WhenRespondentsDontCareAnymoreIdentifyingtheOnsetofCarelessResponding.html" rel="alternate" type="text/html" title="When Respondents Donât Care Anymore: Identifying the Onset of Careless Responding" /><published>2024-05-13T00:00:00+00:00</published><updated>2024-05-13T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/WhenRespondentsDontCareAnymoreIdentifyingtheOnsetofCarelessResponding</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/13/WhenRespondentsDontCareAnymoreIdentifyingtheOnsetofCarelessResponding.html">&lt;p&gt;Questionnaires in the behavioral and organizational sciences tend to be lengthy: survey measures comprising hundreds of items are the norm rather than the exception. However, literature suggests that the longer a questionnaire takes, the higher the probability that participants lose interest and start responding carelessly. Consequently, in long surveys a large number of participants may engage in careless responding, posing a major threat to internal validity. We propose a novel method for identifying the onset of careless responding (or an absence thereof) for each participant. It is based on combined measurements of multiple dimensions in which carelessness may manifest, such as inconsistency and invariability. Since a structural break in either dimension is potentially indicative of carelessness, the proposed method searches for evidence for changepoints along the combined measurements. It is highly flexible, based on machine learning, and provides statistical guarantees on its performance. An empirical application on data from a seminal study on the incidence of careless responding reveals that the reported incidence has likely been substantially underestimated due to the presence of respondents that were careless for only parts of the questionnaire. In simulation experiments, we find that the proposed method achieves high reliability in correctly identifying carelessness onset, discriminates well between careless and attentive respondents, and captures a variety of careless response types, even when a large number of careless respondents are present. Furthermore, we provide freely available open source software to enhance accessibility and facilitate adoption by empirical researchers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.07167&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Max Welz, Andreas Alfons</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Questionnaires in the behavioral and organizational sciences tend to be lengthy: survey measures comprising hundreds of items are the norm rather than the exception. However, literature suggests that the longer a questionnaire takes, the higher the probability that participants lose interest and start responding carelessly. Consequently, in long surveys a large number of participants may engage in careless responding, posing a major threat to internal validity. We propose a novel method for identifying the onset of careless responding (or an absence thereof) for each participant. It is based on combined measurements of multiple dimensions in which carelessness may manifest, such as inconsistency and invariability. Since a structural break in either dimension is potentially indicative of carelessness, the proposed method searches for evidence for changepoints along the combined measurements. It is highly flexible, based on machine learning, and provides statistical guarantees on its performance. An empirical application on data from a seminal study on the incidence of careless responding reveals that the reported incidence has likely been substantially underestimated due to the presence of respondents that were careless for only parts of the questionnaire. In simulation experiments, we find that the proposed method achieves high reliability in correctly identifying carelessness onset, discriminates well between careless and attentive respondents, and captures a variety of careless response types, even when a large number of careless respondents are present. Furthermore, we provide freely available open source software to enhance accessibility and facilitate adoption by empirical researchers.</summary></entry></feed>