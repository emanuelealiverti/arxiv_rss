<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-06T07:15:12+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Bayesian shrinkage estimator for transfer learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/ABayesianshrinkageestimatorfortransferlearning.html" rel="alternate" type="text/html" title="A Bayesian shrinkage estimator for transfer learning" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/ABayesianshrinkageestimatorfortransferlearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/ABayesianshrinkageestimatorfortransferlearning.html">&lt;p&gt;Transfer learning (TL) has emerged as a powerful tool to supplement data collected for a target task with data collected for a related source task. The Bayesian framework is natural for TL because information from the source data can be incorporated in the prior distribution for the target data analysis. In this paper, we propose and study Bayesian TL methods for the normal-means problem and multiple linear regression. We propose two classes of prior distributions. The first class assumes the difference in the parameters for the source and target tasks is sparse, i.e., many parameters are shared across tasks. The second assumes that none of the parameters are shared across tasks, but the differences are bounded in $\ell_2$-norm. For the sparse case, we propose a Bayes shrinkage estimator with theoretical guarantees under mild assumptions. The proposed methodology is tested on synthetic data and outperforms state-of-the-art TL methods. We then use this method to fine-tune the last layer of a neural network model to predict the molecular gap property in a material science application. We report improved performance compared to classical fine tuning and methods using only the target data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.17321&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mohamed A. Abba, Jonathan P. Williams, Brian J. Reich</name></author><category term="stat.ME" /><summary type="html">Transfer learning (TL) has emerged as a powerful tool to supplement data collected for a target task with data collected for a related source task. The Bayesian framework is natural for TL because information from the source data can be incorporated in the prior distribution for the target data analysis. In this paper, we propose and study Bayesian TL methods for the normal-means problem and multiple linear regression. We propose two classes of prior distributions. The first class assumes the difference in the parameters for the source and target tasks is sparse, i.e., many parameters are shared across tasks. The second assumes that none of the parameters are shared across tasks, but the differences are bounded in $\ell_2$-norm. For the sparse case, we propose a Bayes shrinkage estimator with theoretical guarantees under mild assumptions. The proposed methodology is tested on synthetic data and outperforms state-of-the-art TL methods. We then use this method to fine-tune the last layer of a neural network model to predict the molecular gap property in a material science application. We report improved performance compared to classical fine tuning and methods using only the target data.</summary></entry><entry><title type="html">ARK: Robust Knockoffs Inference with Coupling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/ARKRobustKnockoffsInferencewithCoupling.html" rel="alternate" type="text/html" title="ARK: Robust Knockoffs Inference with Coupling" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/ARKRobustKnockoffsInferencewithCoupling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/ARKRobustKnockoffsInferencewithCoupling.html">&lt;p&gt;We investigate the robustness of the model-X knockoffs framework with respect to the misspecified or estimated feature distribution. We achieve such a goal by theoretically studying the feature selection performance of a practically implemented knockoffs algorithm, which we name as the approximate knockoffs (ARK) procedure, under the measures of the false discovery rate (FDR) and $k$-familywise error rate ($k$-FWER). The approximate knockoffs procedure differs from the model-X knockoffs procedure only in that the former uses the misspecified or estimated feature distribution. A key technique in our theoretical analyses is to couple the approximate knockoffs procedure with the model-X knockoffs procedure so that random variables in these two procedures can be close in realizations. We prove that if such coupled model-X knockoffs procedure exists, the approximate knockoffs procedure can achieve the asymptotic FDR or $k$-FWER control at the target level. We showcase three specific constructions of such coupled model-X knockoff variables, verifying their existence and justifying the robustness of the model-X knockoffs framework. Additionally, we formally connect our concept of knockoff variable coupling to a type of Wasserstein distance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.04400&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yingying Fan, Lan Gao, Jinchi Lv</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">We investigate the robustness of the model-X knockoffs framework with respect to the misspecified or estimated feature distribution. We achieve such a goal by theoretically studying the feature selection performance of a practically implemented knockoffs algorithm, which we name as the approximate knockoffs (ARK) procedure, under the measures of the false discovery rate (FDR) and $k$-familywise error rate ($k$-FWER). The approximate knockoffs procedure differs from the model-X knockoffs procedure only in that the former uses the misspecified or estimated feature distribution. A key technique in our theoretical analyses is to couple the approximate knockoffs procedure with the model-X knockoffs procedure so that random variables in these two procedures can be close in realizations. We prove that if such coupled model-X knockoffs procedure exists, the approximate knockoffs procedure can achieve the asymptotic FDR or $k$-FWER control at the target level. We showcase three specific constructions of such coupled model-X knockoff variables, verifying their existence and justifying the robustness of the model-X knockoffs framework. Additionally, we formally connect our concept of knockoff variable coupling to a type of Wasserstein distance.</summary></entry><entry><title type="html">Accounting for multiplicity in machine learning benchmark performance</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Accountingformultiplicityinmachinelearningbenchmarkperformance.html" rel="alternate" type="text/html" title="Accounting for multiplicity in machine learning benchmark performance" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Accountingformultiplicityinmachinelearningbenchmarkperformance</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Accountingformultiplicityinmachinelearningbenchmarkperformance.html">&lt;p&gt;Machine learning methods are commonly evaluated and compared by their performance on data sets from public repositories. This allows for multiple methods, oftentimes several thousands, to be evaluated under identical conditions and across time. The highest ranked performance on a problem is referred to as state-of-the-art (SOTA) performance, and is used, among other things, as a reference point for publication of new methods. Using the highest-ranked performance as an estimate for SOTA is a biased estimator, giving overly optimistic results. The mechanisms at play are those of multiplicity, a topic that is well-studied in the context of multiple comparisons and multiple testing, but has, as far as the authors are aware of, been nearly absent from the discussion regarding SOTA estimates. The optimistic state-of-the-art estimate is used as a standard for evaluating new methods, and methods with substantial inferior results are easily overlooked. In this article, we provide a probability distribution for the case of multiple classifiers so that known analyses methods can be engaged and a better SOTA estimate can be provided. We demonstrate the impact of multiplicity through a simulated example with independent classifiers. We show how classifier dependency impacts the variance, but also that the impact is limited when the accuracy is high. Finally, we discuss three real-world examples; Kaggle competitions that demonstrate various aspects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.07272&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kajsa M{\o}llersen, Einar Holsb{\o}</name></author><category term="stat.ME" /><summary type="html">Machine learning methods are commonly evaluated and compared by their performance on data sets from public repositories. This allows for multiple methods, oftentimes several thousands, to be evaluated under identical conditions and across time. The highest ranked performance on a problem is referred to as state-of-the-art (SOTA) performance, and is used, among other things, as a reference point for publication of new methods. Using the highest-ranked performance as an estimate for SOTA is a biased estimator, giving overly optimistic results. The mechanisms at play are those of multiplicity, a topic that is well-studied in the context of multiple comparisons and multiple testing, but has, as far as the authors are aware of, been nearly absent from the discussion regarding SOTA estimates. The optimistic state-of-the-art estimate is used as a standard for evaluating new methods, and methods with substantial inferior results are easily overlooked. In this article, we provide a probability distribution for the case of multiple classifiers so that known analyses methods can be engaged and a better SOTA estimate can be provided. We demonstrate the impact of multiplicity through a simulated example with independent classifiers. We show how classifier dependency impacts the variance, but also that the impact is limited when the accuracy is high. Finally, we discuss three real-world examples; Kaggle competitions that demonstrate various aspects.</summary></entry><entry><title type="html">Asymptotic inference with flexible covariate adjustment under rerandomization and stratified rerandomization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Asymptoticinferencewithflexiblecovariateadjustmentunderrerandomizationandstratifiedrerandomization.html" rel="alternate" type="text/html" title="Asymptotic inference with flexible covariate adjustment under rerandomization and stratified rerandomization" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Asymptoticinferencewithflexiblecovariateadjustmentunderrerandomizationandstratifiedrerandomization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Asymptoticinferencewithflexiblecovariateadjustmentunderrerandomizationandstratifiedrerandomization.html">&lt;p&gt;Rerandomization is an effective treatment allocation procedure to control for baseline covariate imbalance. For estimating the average treatment effect, rerandomization has been previously shown to improve the precision of the unadjusted and the linearly-adjusted estimators over simple randomization without compromising consistency. However, it remains unclear whether such results apply more generally to the class of M-estimators, including the g-computation formula with generalized linear regression and doubly-robust methods, and more broadly, to efficient estimators with data-adaptive machine learners. In this paper, using a super-population framework, we develop the asymptotic theory for a more general class of covariate-adjusted estimators under rerandomization and its stratified extension. We prove that the asymptotic linearity and the influence function remain identical for any M-estimator under simple randomization and rerandomization, but rerandomization may lead to a non-Gaussian asymptotic distribution. We further explain, drawing examples from several common M-estimators, that asymptotic normality can be achieved if rerandomization variables are appropriately adjusted for in the final estimator. These results are extended to stratified rerandomization. Finally, we study the asymptotic theory for efficient estimators based on data-adaptive machine learners, and prove their efficiency optimality under rerandomization and stratified rerandomization. Our results are demonstrated via simulations and re-analyses of a cluster-randomized experiment that used stratified rerandomization.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02834&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bingkai Wang, Fan Li</name></author><category term="stat.ME" /><summary type="html">Rerandomization is an effective treatment allocation procedure to control for baseline covariate imbalance. For estimating the average treatment effect, rerandomization has been previously shown to improve the precision of the unadjusted and the linearly-adjusted estimators over simple randomization without compromising consistency. However, it remains unclear whether such results apply more generally to the class of M-estimators, including the g-computation formula with generalized linear regression and doubly-robust methods, and more broadly, to efficient estimators with data-adaptive machine learners. In this paper, using a super-population framework, we develop the asymptotic theory for a more general class of covariate-adjusted estimators under rerandomization and its stratified extension. We prove that the asymptotic linearity and the influence function remain identical for any M-estimator under simple randomization and rerandomization, but rerandomization may lead to a non-Gaussian asymptotic distribution. We further explain, drawing examples from several common M-estimators, that asymptotic normality can be achieved if rerandomization variables are appropriately adjusted for in the final estimator. These results are extended to stratified rerandomization. Finally, we study the asymptotic theory for efficient estimators based on data-adaptive machine learners, and prove their efficiency optimality under rerandomization and stratified rerandomization. Our results are demonstrated via simulations and re-analyses of a cluster-randomized experiment that used stratified rerandomization.</summary></entry><entry><title type="html">Bayesian Adaptive Trials for Social Policy</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/BayesianAdaptiveTrialsforSocialPolicy.html" rel="alternate" type="text/html" title="Bayesian Adaptive Trials for Social Policy" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/BayesianAdaptiveTrialsforSocialPolicy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/BayesianAdaptiveTrialsforSocialPolicy.html">&lt;p&gt;This paper proposes Bayesian Adaptive Trials (BAT) as both an efficient method to conduct trials and a unifying framework for evaluation social policy interventions, addressing limitations inherent in traditional methods such as Randomized Controlled Trials (RCT). Recognizing the crucial need for evidence-based approaches in public policy, the proposal aims to lower barriers to the adoption of evidence-based methods and align evaluation processes more closely with the dynamic nature of policy cycles. BATs, grounded in decision theory, offer a dynamic, ``learning as we go’’ approach, enabling the integration of diverse information types and facilitating a continuous, iterative process of policy evaluation. BATs’ adaptive nature is particularly advantageous in policy settings, allowing for more timely and context-sensitive decisions. Moreover, BATs’ ability to value potential future information sources positions it as an optimal strategy for sequential data acquisition during policy implementation. While acknowledging the assumptions and models intrinsic to BATs, such as prior distributions and likelihood functions, the paper argues that these are advantageous for decision-makers in social policy, effectively merging the best features of various methodologies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02868&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sally Cripps, Anna Lopatnikova, Hadi Mohasel Afshar, Ben Gales, Roman Marchant, Gilad Francis, Catarina Moreira, Alex Fischer</name></author><category term="stat.AP" /><summary type="html">This paper proposes Bayesian Adaptive Trials (BAT) as both an efficient method to conduct trials and a unifying framework for evaluation social policy interventions, addressing limitations inherent in traditional methods such as Randomized Controlled Trials (RCT). Recognizing the crucial need for evidence-based approaches in public policy, the proposal aims to lower barriers to the adoption of evidence-based methods and align evaluation processes more closely with the dynamic nature of policy cycles. BATs, grounded in decision theory, offer a dynamic, ``learning as we go’’ approach, enabling the integration of diverse information types and facilitating a continuous, iterative process of policy evaluation. BATs’ adaptive nature is particularly advantageous in policy settings, allowing for more timely and context-sensitive decisions. Moreover, BATs’ ability to value potential future information sources positions it as an optimal strategy for sequential data acquisition during policy implementation. While acknowledging the assumptions and models intrinsic to BATs, such as prior distributions and likelihood functions, the paper argues that these are advantageous for decision-makers in social policy, effectively merging the best features of various methodologies.</summary></entry><entry><title type="html">Bayesian Quantile Estimation and Regression with Martingale Posteriors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/BayesianQuantileEstimationandRegressionwithMartingalePosteriors.html" rel="alternate" type="text/html" title="Bayesian Quantile Estimation and Regression with Martingale Posteriors" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/BayesianQuantileEstimationandRegressionwithMartingalePosteriors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/BayesianQuantileEstimationandRegressionwithMartingalePosteriors.html">&lt;p&gt;Quantile estimation and regression within the Bayesian framework is challenging as the choice of likelihood and prior is not obvious. In this paper, we introduce a novel Bayesian nonparametric method for quantile estimation and regression based on the recently introduced martingale posterior (MP) framework. The core idea of the MP is that posterior sampling is equivalent to predictive imputation, which allows us to break free of the stringent likelihood-prior specification. We demonstrate that a recursive estimate of a smooth quantile function, subject to a martingale condition, is entirely sufficient for full nonparametric Bayesian inference. We term the resulting posterior distribution as the quantile martingale posterior (QMP), which arises from an implicit generative predictive distribution. Associated with the QMP is an expedient, MCMC-free and parallelizable posterior computation scheme, which can be further accelerated with an asymptotic approximation based on a Gaussian process. Furthermore, the well-known issue of monotonicity in quantile estimation is naturally alleviated through increasing rearrangement due to the connections to the Bayesian bootstrap. Finally, the QMP has a particularly tractable form that allows for comprehensive theoretical study, which forms a main focus of the work. We demonstrate the ease of posterior computation in simulations and real data experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03358&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Edwin Fong, Andrew Yiu</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Quantile estimation and regression within the Bayesian framework is challenging as the choice of likelihood and prior is not obvious. In this paper, we introduce a novel Bayesian nonparametric method for quantile estimation and regression based on the recently introduced martingale posterior (MP) framework. The core idea of the MP is that posterior sampling is equivalent to predictive imputation, which allows us to break free of the stringent likelihood-prior specification. We demonstrate that a recursive estimate of a smooth quantile function, subject to a martingale condition, is entirely sufficient for full nonparametric Bayesian inference. We term the resulting posterior distribution as the quantile martingale posterior (QMP), which arises from an implicit generative predictive distribution. Associated with the QMP is an expedient, MCMC-free and parallelizable posterior computation scheme, which can be further accelerated with an asymptotic approximation based on a Gaussian process. Furthermore, the well-known issue of monotonicity in quantile estimation is naturally alleviated through increasing rearrangement due to the connections to the Bayesian bootstrap. Finally, the QMP has a particularly tractable form that allows for comprehensive theoretical study, which forms a main focus of the work. We demonstrate the ease of posterior computation in simulations and real data experiments.</summary></entry><entry><title type="html">Bayesian Statistics: A Review and a Reminder for the Practicing Reliability Engineer</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/BayesianStatisticsAReviewandaReminderforthePracticingReliabilityEngineer.html" rel="alternate" type="text/html" title="Bayesian Statistics: A Review and a Reminder for the Practicing Reliability Engineer" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/BayesianStatisticsAReviewandaReminderforthePracticingReliabilityEngineer</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/BayesianStatisticsAReviewandaReminderforthePracticingReliabilityEngineer.html">&lt;p&gt;This paper introduces and reviews some of the principles and methods used in Bayesian reliability. It specifically discusses methods used in the analysis of success/no-success data and then reminds the reader of a simple (yet infrequently applied) Monte Carlo algorithm that can be used to calculate the posterior distribution of a system’s reliability.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02751&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carsten H. Botts</name></author><category term="stat.ME" /><summary type="html">This paper introduces and reviews some of the principles and methods used in Bayesian reliability. It specifically discusses methods used in the analysis of success/no-success data and then reminds the reader of a simple (yet infrequently applied) Monte Carlo algorithm that can be used to calculate the posterior distribution of a system’s reliability.</summary></entry><entry><title type="html">Bayesian inference for scale mixtures of skew-normal linear models under the centered parameterization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Bayesianinferenceforscalemixturesofskewnormallinearmodelsunderthecenteredparameterization.html" rel="alternate" type="text/html" title="Bayesian inference for scale mixtures of skew-normal linear models under the centered parameterization" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Bayesianinferenceforscalemixturesofskewnormallinearmodelsunderthecenteredparameterization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Bayesianinferenceforscalemixturesofskewnormallinearmodelsunderthecenteredparameterization.html">&lt;p&gt;In many situations we are interested in modeling real data where the response distribution, even conditionally on the covariates, presents asymmetry and/or heavy/light tails. In these situations, it is more suitable to consider models based on the skewed and/or heavy/light tailed distributions, such as the class of scale mixtures of skew-normal distributions. The classical parameterization of this distributions may not be good due to the some inferential issues when the skewness parameter is in a neighborhood of 0, then, the centered parameterization becomes more appropriate. In this paper, we developed a class of scale mixtures of skew-normal distributions under the centered parameterization, also a linear regression model based on them was proposed. We explore a hierarchical representation and set up a MCMC scheme for parameter estimation. Furthermore, we developed residuals and influence analysis tools. A Monte Carlo experiment is conducted to evaluate the performance of the MCMC algorithm and the behavior of the residual distribution. The methodology is illustrated with the analysis of a real data set.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03432&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>João Victor B. de Freitas, Caio L. N. Azevedo</name></author><category term="stat.ME" /><summary type="html">In many situations we are interested in modeling real data where the response distribution, even conditionally on the covariates, presents asymmetry and/or heavy/light tails. In these situations, it is more suitable to consider models based on the skewed and/or heavy/light tailed distributions, such as the class of scale mixtures of skew-normal distributions. The classical parameterization of this distributions may not be good due to the some inferential issues when the skewness parameter is in a neighborhood of 0, then, the centered parameterization becomes more appropriate. In this paper, we developed a class of scale mixtures of skew-normal distributions under the centered parameterization, also a linear regression model based on them was proposed. We explore a hierarchical representation and set up a MCMC scheme for parameter estimation. Furthermore, we developed residuals and influence analysis tools. A Monte Carlo experiment is conducted to evaluate the performance of the MCMC algorithm and the behavior of the residual distribution. The methodology is illustrated with the analysis of a real data set.</summary></entry><entry><title type="html">COVID-19 incidence in the Republic of Ireland: A case study for network-based time series models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/COVID19incidenceintheRepublicofIrelandAcasestudyfornetworkbasedtimeseriesmodels.html" rel="alternate" type="text/html" title="COVID-19 incidence in the Republic of Ireland: A case study for network-based time series models" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/COVID19incidenceintheRepublicofIrelandAcasestudyfornetworkbasedtimeseriesmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/COVID19incidenceintheRepublicofIrelandAcasestudyfornetworkbasedtimeseriesmodels.html">&lt;p&gt;The generalised network autoregressive (GNAR) model conceptualises time series on the vertices of a network; it has an autoregressive component for temporal dependence and a spatial autoregressive component for dependence between neighbouring vertices in the network. Consequently, the choice of underlying network is essential. This paper assesses the performance of GNAR models on different networks in predicting COVID-19 cases for the 26 counties in the Republic of Ireland, over two distinct pandemic phases (restricted and unrestricted), characterised by inter-county movement restrictions. Ten static networks are constructed, in which vertices represent counties, and edges are built upon neighbourhood relations, such as railway lines. We find that a GNAR model based on the fairly sparse Economic hub network explains the data best for the restricted pandemic phase while the fairly dense 21-nearest neighbour network performs best for the unrestricted phase. Across phases, GNAR models have higher predictive accuracy than standard ARIMA models which ignore the network structure. For county-specific predictions, in pandemic phases with more lenient or no COVID-19 regulation, the network effect is not quite as pronounced. The results indicate some robustness to the precise network architecture as long as the densities of the networks are similar. An analysis of the residuals justifies the model assumptions for the restricted phase but raises questions regarding their validity for the unrestricted phase. While generally performing better than ARIMA models which ignore network effects, there is scope for further development of the GNAR model to better model complex infectious diseases, including COVID-19.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.06199&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Stephanie Armbruster, Gesine Reinert</name></author><category term="stat.AP" /><summary type="html">The generalised network autoregressive (GNAR) model conceptualises time series on the vertices of a network; it has an autoregressive component for temporal dependence and a spatial autoregressive component for dependence between neighbouring vertices in the network. Consequently, the choice of underlying network is essential. This paper assesses the performance of GNAR models on different networks in predicting COVID-19 cases for the 26 counties in the Republic of Ireland, over two distinct pandemic phases (restricted and unrestricted), characterised by inter-county movement restrictions. Ten static networks are constructed, in which vertices represent counties, and edges are built upon neighbourhood relations, such as railway lines. We find that a GNAR model based on the fairly sparse Economic hub network explains the data best for the restricted pandemic phase while the fairly dense 21-nearest neighbour network performs best for the unrestricted phase. Across phases, GNAR models have higher predictive accuracy than standard ARIMA models which ignore the network structure. For county-specific predictions, in pandemic phases with more lenient or no COVID-19 regulation, the network effect is not quite as pronounced. The results indicate some robustness to the precise network architecture as long as the densities of the networks are similar. An analysis of the residuals justifies the model assumptions for the restricted phase but raises questions regarding their validity for the unrestricted phase. While generally performing better than ARIMA models which ignore network effects, there is scope for further development of the GNAR model to better model complex infectious diseases, including COVID-19.</summary></entry><entry><title type="html">Calibrated and Conformal Propensity Scores for Causal Effect Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/CalibratedandConformalPropensityScoresforCausalEffectEstimation.html" rel="alternate" type="text/html" title="Calibrated and Conformal Propensity Scores for Causal Effect Estimation" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/CalibratedandConformalPropensityScoresforCausalEffectEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/CalibratedandConformalPropensityScoresforCausalEffectEstimation.html">&lt;p&gt;Propensity scores are commonly used to estimate treatment effects from observational data. We argue that the probabilistic output of a learned propensity score model should be calibrated – i.e., a predictive treatment probability of 90% should correspond to 90% of individuals being assigned the treatment group – and we propose simple recalibration techniques to ensure this property. We prove that calibration is a necessary condition for unbiased treatment effect estimation when using popular inverse propensity weighted and doubly robust estimators. We derive error bounds on causal effect estimates that directly relate to the quality of uncertainties provided by the probabilistic propensity score model and show that calibration strictly improves this error bound while also avoiding extreme propensity weights. We demonstrate improved causal effect estimation with calibrated propensity scores in several tasks including high-dimensional image covariates and genome-wide association studies (GWASs). Calibrated propensity scores improve the speed of GWAS analysis by more than two-fold by enabling the use of simpler models that are faster to train.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.00382&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shachi Deshpande, Volodymyr Kuleshov</name></author><category term="stat.ME" /><summary type="html">Propensity scores are commonly used to estimate treatment effects from observational data. We argue that the probabilistic output of a learned propensity score model should be calibrated – i.e., a predictive treatment probability of 90% should correspond to 90% of individuals being assigned the treatment group – and we propose simple recalibration techniques to ensure this property. We prove that calibration is a necessary condition for unbiased treatment effect estimation when using popular inverse propensity weighted and doubly robust estimators. We derive error bounds on causal effect estimates that directly relate to the quality of uncertainties provided by the probabilistic propensity score model and show that calibration strictly improves this error bound while also avoiding extreme propensity weights. We demonstrate improved causal effect estimation with calibrated propensity scores in several tasks including high-dimensional image covariates and genome-wide association studies (GWASs). Calibrated propensity scores improve the speed of GWAS analysis by more than two-fold by enabling the use of simpler models that are faster to train.</summary></entry><entry><title type="html">Combining an experimental study with external data: study designs and identification strategies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Combininganexperimentalstudywithexternaldatastudydesignsandidentificationstrategies.html" rel="alternate" type="text/html" title="Combining an experimental study with external data: study designs and identification strategies" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Combininganexperimentalstudywithexternaldatastudydesignsandidentificationstrategies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Combininganexperimentalstudywithexternaldatastudydesignsandidentificationstrategies.html">&lt;p&gt;There is increasing interest in combining information from experimental studies, including randomized and single-group trials, with information from external experimental or observational data sources. Such efforts are usually motivated by the desire to compare treatments evaluated in different studies – for instance, through the introduction of external treatment groups – or to estimate treatment effects with greater precision. Proposals to combine experimental studies with external data were made at least as early as the 1970s, but in recent years have come under increasing consideration by regulatory agencies involved in drug and device evaluation, particularly with the increasing availability of rich observational data. In this paper, we describe basic templates of study designs and data structures for combining information from experimental studies with external data, and use the potential (counterfactual) outcomes framework to elaborate identification strategies for potential outcome means and average treatment effects in these designs. In formalizing designs and identification strategies for combining information from experimental studies with external data, we hope to provide a conceptual foundation to support the systematic use and evaluation of such efforts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03302&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lawson Ung, Guanbo Wang, Sebastien Haneuse, Miguel A. Hernan, Issa J. Dahabreh</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">There is increasing interest in combining information from experimental studies, including randomized and single-group trials, with information from external experimental or observational data sources. Such efforts are usually motivated by the desire to compare treatments evaluated in different studies – for instance, through the introduction of external treatment groups – or to estimate treatment effects with greater precision. Proposals to combine experimental studies with external data were made at least as early as the 1970s, but in recent years have come under increasing consideration by regulatory agencies involved in drug and device evaluation, particularly with the increasing availability of rich observational data. In this paper, we describe basic templates of study designs and data structures for combining information from experimental studies with external data, and use the potential (counterfactual) outcomes framework to elaborate identification strategies for potential outcome means and average treatment effects in these designs. In formalizing designs and identification strategies for combining information from experimental studies with external data, we hope to provide a conceptual foundation to support the systematic use and evaluation of such efforts.</summary></entry><entry><title type="html">Continuous-time modeling and bootstrap for chain ladder reserving</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Continuoustimemodelingandbootstrapforchainladderreserving.html" rel="alternate" type="text/html" title="Continuous-time modeling and bootstrap for chain ladder reserving" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Continuoustimemodelingandbootstrapforchainladderreserving</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Continuoustimemodelingandbootstrapforchainladderreserving.html">&lt;p&gt;We revisit the famous Mack’s model which gives an estimate for the mean square error of prediction of the chain ladder claims reserves. We introduce a stochastic differential equation driven by a Brownian motion to model accumulated total claims amount for the chain ladder method. Within this continuous-time framework, we propose a bootstrap technique for estimating the distribution of claims reserves. It turns out that our approach leads to inherently capturing asymmetry and non-negativity, eliminating the necessity for additional assumptions. We conclude with a case study and comparative analysis against alternative methodologies based on Mack’s model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03252&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nicolas Baradel</name></author><category term="stat.ME" /><summary type="html">We revisit the famous Mack’s model which gives an estimate for the mean square error of prediction of the chain ladder claims reserves. We introduce a stochastic differential equation driven by a Brownian motion to model accumulated total claims amount for the chain ladder method. Within this continuous-time framework, we propose a bootstrap technique for estimating the distribution of claims reserves. It turns out that our approach leads to inherently capturing asymmetry and non-negativity, eliminating the necessity for additional assumptions. We conclude with a case study and comparative analysis against alternative methodologies based on Mack’s model.</summary></entry><entry><title type="html">Cooperative learning of Pl@ntNet’s Artificial Intelligence algorithm: how does it work and how can we improve it?</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/CooperativelearningofPlntNetsArtificialIntelligencealgorithmhowdoesitworkandhowcanweimproveit.html" rel="alternate" type="text/html" title="Cooperative learning of Pl@ntNet’s Artificial Intelligence algorithm: how does it work and how can we improve it?" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/CooperativelearningofPlntNetsArtificialIntelligencealgorithmhowdoesitworkandhowcanweimproveit</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/CooperativelearningofPlntNetsArtificialIntelligencealgorithmhowdoesitworkandhowcanweimproveit.html">&lt;p&gt;Deep learning models for plant species identification rely on large annotated datasets. The PlantNet system enables global data collection by allowing users to upload and annotate plant observations, leading to noisy labels due to diverse user skills. Achieving consensus is crucial for training, but the vast scale of collected data makes traditional label aggregation strategies challenging. Existing methods either retain all observations, resulting in noisy training data or selectively keep those with sufficient votes, discarding valuable information. Additionally, as many species are rarely observed, user expertise can not be evaluated as an inter-user agreement: otherwise, botanical experts would have a lower weight in the AI training step than the average user. Our proposed label aggregation strategy aims to cooperatively train plant identification AI models. This strategy estimates user expertise as a trust score per user based on their ability to identify plant species from crowdsourced data. The trust score is recursively estimated from correctly identified species given the current estimated labels. This interpretable score exploits botanical experts’ knowledge and the heterogeneity of users. Subsequently, our strategy removes unreliable observations but retains those with limited trusted annotations, unlike other approaches. We evaluate PlantNet’s strategy on a released large subset of the PlantNet database focused on European flora, comprising over 6M observations and 800K users. We demonstrate that estimating users’ skills based on the diversity of their expertise enhances labeling performance. Our findings emphasize the synergy of human annotation and data filtering in improving AI performance for a refined dataset. We explore incorporating AI-based votes alongside human input. This can further enhance human-AI interactions to detect unreliable observations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03356&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tanguy Lefort, Antoine Affouard, Benjamin Charlier, Jean-Christophe Lombardo, Mathias Chouet, Hervé Goëau, Joseph Salmon, Pierre Bonnet, Alexis Joly</name></author><category term="stat.AP" /><summary type="html">Deep learning models for plant species identification rely on large annotated datasets. The PlantNet system enables global data collection by allowing users to upload and annotate plant observations, leading to noisy labels due to diverse user skills. Achieving consensus is crucial for training, but the vast scale of collected data makes traditional label aggregation strategies challenging. Existing methods either retain all observations, resulting in noisy training data or selectively keep those with sufficient votes, discarding valuable information. Additionally, as many species are rarely observed, user expertise can not be evaluated as an inter-user agreement: otherwise, botanical experts would have a lower weight in the AI training step than the average user. Our proposed label aggregation strategy aims to cooperatively train plant identification AI models. This strategy estimates user expertise as a trust score per user based on their ability to identify plant species from crowdsourced data. The trust score is recursively estimated from correctly identified species given the current estimated labels. This interpretable score exploits botanical experts’ knowledge and the heterogeneity of users. Subsequently, our strategy removes unreliable observations but retains those with limited trusted annotations, unlike other approaches. We evaluate PlantNet’s strategy on a released large subset of the PlantNet database focused on European flora, comprising over 6M observations and 800K users. We demonstrate that estimating users’ skills based on the diversity of their expertise enhances labeling performance. Our findings emphasize the synergy of human annotation and data filtering in improving AI performance for a refined dataset. We explore incorporating AI-based votes alongside human input. This can further enhance human-AI interactions to detect unreliable observations.</summary></entry><entry><title type="html">Copula-based semiparametric nonnormal transformed linear model for survival data with dependent censoring</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Copulabasedsemiparametricnonnormaltransformedlinearmodelforsurvivaldatawithdependentcensoring.html" rel="alternate" type="text/html" title="Copula-based semiparametric nonnormal transformed linear model for survival data with dependent censoring" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Copulabasedsemiparametricnonnormaltransformedlinearmodelforsurvivaldatawithdependentcensoring</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Copulabasedsemiparametricnonnormaltransformedlinearmodelforsurvivaldatawithdependentcensoring.html">&lt;p&gt;Although the independent censoring assumption is commonly used in survival analysis, it can be violated when the censoring time is related to the survival time, which often happens in many practical applications. To address this issue, we propose a flexible semiparametric method for dependent censored data. Our approach involves fitting the survival time and the censoring time with a joint transformed linear model, where the transformed function is unspecified. This allows for a very general class of models that can account for possible covariate effects, while also accommodating administrative censoring. We assume that the transformed variables have a bivariate nonnormal distribution based on parametric copulas and parametric marginals, which further enhances the flexibility of our method. We demonstrate the identifiability of the proposed model and establish the consistency and asymptotic normality of the model parameters under appropriate regularity conditions and assumptions. Furthermore, we evaluate the performance of our method through extensive simulation studies, and provide a real data example for illustration.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02948&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Huazhen Yu, Lixin Zhang</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Although the independent censoring assumption is commonly used in survival analysis, it can be violated when the censoring time is related to the survival time, which often happens in many practical applications. To address this issue, we propose a flexible semiparametric method for dependent censored data. Our approach involves fitting the survival time and the censoring time with a joint transformed linear model, where the transformed function is unspecified. This allows for a very general class of models that can account for possible covariate effects, while also accommodating administrative censoring. We assume that the transformed variables have a bivariate nonnormal distribution based on parametric copulas and parametric marginals, which further enhances the flexibility of our method. We demonstrate the identifiability of the proposed model and establish the consistency and asymptotic normality of the model parameters under appropriate regularity conditions and assumptions. Furthermore, we evaluate the performance of our method through extensive simulation studies, and provide a real data example for illustration.</summary></entry><entry><title type="html">Decision synthesis in monetary policy</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Decisionsynthesisinmonetarypolicy.html" rel="alternate" type="text/html" title="Decision synthesis in monetary policy" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Decisionsynthesisinmonetarypolicy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Decisionsynthesisinmonetarypolicy.html">&lt;p&gt;The macroeconomy is a sophisticated dynamic system involving significant uncertainties that complicate modelling. In response, decision makers consider multiple models that provide different predictions and policy recommendations which are then synthesized into a policy decision. In this setting, we introduce and develop Bayesian predictive decision synthesis (BPDS) to formalize monetary policy decision processes. BPDS draws on recent developments in model combination and statistical decision theory that yield new opportunities in combining multiple models, emphasizing the integration of decision goals, expectations and outcomes into the model synthesis process. Our case study concerns central bank policy decisions about target interest rates with a focus on implications for multi-step macroeconomic forecasting.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03321&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tony Chernis, Gary Koop, Emily Tallman, Mike West</name></author><category term="stat.ME" /><summary type="html">The macroeconomy is a sophisticated dynamic system involving significant uncertainties that complicate modelling. In response, decision makers consider multiple models that provide different predictions and policy recommendations which are then synthesized into a policy decision. In this setting, we introduce and develop Bayesian predictive decision synthesis (BPDS) to formalize monetary policy decision processes. BPDS draws on recent developments in model combination and statistical decision theory that yield new opportunities in combining multiple models, emphasizing the integration of decision goals, expectations and outcomes into the model synthesis process. Our case study concerns central bank policy decisions about target interest rates with a focus on implications for multi-step macroeconomic forecasting.</summary></entry><entry><title type="html">Democratizing Propensity Score Matching Using Web Application</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/DemocratizingPropensityScoreMatchingUsingWebApplication.html" rel="alternate" type="text/html" title="Democratizing Propensity Score Matching Using Web Application" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/DemocratizingPropensityScoreMatchingUsingWebApplication</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/DemocratizingPropensityScoreMatchingUsingWebApplication.html">&lt;p&gt;Traditionally, data scientists use exploratory data analysis techniques such as correlation analysis, summary statistics, and regression analysis for identifying the most product enhancements and roadmap planning. However, these conventional approaches often yield biased conclusions and suboptimal solutions, leading to a waste of valuable time and missed opportunities for higher-value outcomes. In contrast, there are alternative techniques that involve the use of causal inference methods. However, these methods suffer from issues of limited accessibility, as they are not easily understandable or effectively utilized by inexperienced practitioners. Additionally, their implementation necessitates a substantial investment of time and effort. To this end, this paper tackles these challenges by democratizing one of the causal inference methods called Propensity Score Matching (PSM) and enhancing its accessibility for less technically inclined users through the automation of the entire workflow using a web application. Our approach not only fills this accessibility gap but also contributes to the existing literature by introducing a more rigorous model selection process and an enhanced sensitivity analysis. By overcoming the limitations of traditional exploratory data analysis methods, our web application has empowered data scientists at Booking.com to make better use of PSM, thereby improving the overall efficacy of their analyses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02743&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Adam Gajtkowski, Felipe Moraes</name></author><category term="stat.AP" /><summary type="html">Traditionally, data scientists use exploratory data analysis techniques such as correlation analysis, summary statistics, and regression analysis for identifying the most product enhancements and roadmap planning. However, these conventional approaches often yield biased conclusions and suboptimal solutions, leading to a waste of valuable time and missed opportunities for higher-value outcomes. In contrast, there are alternative techniques that involve the use of causal inference methods. However, these methods suffer from issues of limited accessibility, as they are not easily understandable or effectively utilized by inexperienced practitioners. Additionally, their implementation necessitates a substantial investment of time and effort. To this end, this paper tackles these challenges by democratizing one of the causal inference methods called Propensity Score Matching (PSM) and enhancing its accessibility for less technically inclined users through the automation of the entire workflow using a web application. Our approach not only fills this accessibility gap but also contributes to the existing literature by introducing a more rigorous model selection process and an enhanced sensitivity analysis. By overcoming the limitations of traditional exploratory data analysis methods, our web application has empowered data scientists at Booking.com to make better use of PSM, thereby improving the overall efficacy of their analyses.</summary></entry><entry><title type="html">Discrete Autoregressive Switching Processes in Sparse Graphical Modeling of Multivariate Time Series Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/DiscreteAutoregressiveSwitchingProcessesinSparseGraphicalModelingofMultivariateTimeSeriesData.html" rel="alternate" type="text/html" title="Discrete Autoregressive Switching Processes in Sparse Graphical Modeling of Multivariate Time Series Data" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/DiscreteAutoregressiveSwitchingProcessesinSparseGraphicalModelingofMultivariateTimeSeriesData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/DiscreteAutoregressiveSwitchingProcessesinSparseGraphicalModelingofMultivariateTimeSeriesData.html">&lt;p&gt;We propose a flexible Bayesian approach for sparse Gaussian graphical modeling of multivariate time series. We account for temporal correlation in the data by assuming that observations are characterized by an underlying and unobserved hidden discrete autoregressive process. We assume multivariate Gaussian emission distributions and capture spatial dependencies by modeling the state-specific precision matrices via graphical horseshoe priors. We characterize the mixing probabilities of the hidden process via a cumulative shrinkage prior that accommodates zero-inflated parameters for non-active components, and further incorporate a sparsity-inducing Dirichlet prior to estimate the effective number of states from the data. For posterior inference, we develop a sampling procedure that allows estimation of the number of discrete autoregressive lags and the number of states, and that cleverly avoids having to deal with the changing dimensions of the parameter space. We thoroughly investigate performance of our proposed methodology through several simulation studies. We further illustrate the use of our approach for the estimation of dynamic brain connectivity based on fMRI data collected on a subject performing a task-based experiment on latent learning&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03385&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Beniamino Hadj-Amar, Aaron M. Bornstein, Michele Guindani, Marina Vannucci</name></author><category term="stat.ME" /><summary type="html">We propose a flexible Bayesian approach for sparse Gaussian graphical modeling of multivariate time series. We account for temporal correlation in the data by assuming that observations are characterized by an underlying and unobserved hidden discrete autoregressive process. We assume multivariate Gaussian emission distributions and capture spatial dependencies by modeling the state-specific precision matrices via graphical horseshoe priors. We characterize the mixing probabilities of the hidden process via a cumulative shrinkage prior that accommodates zero-inflated parameters for non-active components, and further incorporate a sparsity-inducing Dirichlet prior to estimate the effective number of states from the data. For posterior inference, we develop a sampling procedure that allows estimation of the number of discrete autoregressive lags and the number of states, and that cleverly avoids having to deal with the changing dimensions of the parameter space. We thoroughly investigate performance of our proposed methodology through several simulation studies. We further illustrate the use of our approach for the estimation of dynamic brain connectivity based on fMRI data collected on a subject performing a task-based experiment on latent learning</summary></entry><entry><title type="html">Efficient Exploration for LLMs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/EfficientExplorationforLLMs.html" rel="alternate" type="text/html" title="Efficient Exploration for LLMs" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/EfficientExplorationforLLMs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/EfficientExplorationforLLMs.html">&lt;p&gt;We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.00396&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vikranth Dwaracherla, Seyed Mohammad Asghari, Botao Hao, Benjamin Van Roy</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We present evidence of substantial benefit from efficient exploration in gathering human feedback to improve large language models. In our experiments, an agent sequentially generates queries while fitting a reward model to the feedback received. Our best-performing agent generates queries using double Thompson sampling, with uncertainty represented by an epistemic neural network. Our results demonstrate that efficient exploration enables high levels of performance with far fewer queries. Further, both uncertainty estimation and the choice of exploration scheme play critical roles.</summary></entry><entry><title type="html">Estimating Disease-Free Life Expectancy based on Clinical Data from the French Hospital Discharge Database</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/EstimatingDiseaseFreeLifeExpectancybasedonClinicalDatafromtheFrenchHospitalDischargeDatabase.html" rel="alternate" type="text/html" title="Estimating Disease-Free Life Expectancy based on Clinical Data from the French Hospital Discharge Database" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/EstimatingDiseaseFreeLifeExpectancybasedonClinicalDatafromtheFrenchHospitalDischargeDatabase</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/EstimatingDiseaseFreeLifeExpectancybasedonClinicalDatafromtheFrenchHospitalDischargeDatabase.html">&lt;p&gt;The development of health indicators to measure healthy life expectancy (HLE) is an active field of research aimed at summarizing the health of a population. Although many health indicators have emerged in the literature as critical metrics in public health assessments, the methods and data to conduct this evaluation vary considerably in nature and quality. Traditionally, health data collection relies on population surveys. However, these studies, typically of limited size, encompass only a small yet representative segment of the population. This limitation can necessitate the separate estimation of incidence and mortality rates, significantly restricting the available analysis methods. In this article, we leverage an extract from the French National Hospital Discharge database to define health indicators. Our analysis focuses on the resulting Disease-Free Life Expectancy (Dis-FLE) indicator, which provides insights based on the hospital trajectory of each patient admitted to hospital in France during 2008-13. Through this research, we illustrate the advantages and disadvantages of employing large clinical datasets as the foundation for more robust health indicators. We shed light on the opportunities that such data offer for a more comprehensive understanding of the health status of a population. In particular, we estimate age-dependent hazard rates associated with sex, alcohol abuse, tobacco consumption, and obesity, as well as geographic location. Simultaneously, we delve into the challenges and limitations that arise when adopting such a data-driven approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02934&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Oleksandr Sorochynskyi, Quentin Guibert, Frédéric Planchet, Michaël Schwarzinger</name></author><category term="stat.AP" /><summary type="html">The development of health indicators to measure healthy life expectancy (HLE) is an active field of research aimed at summarizing the health of a population. Although many health indicators have emerged in the literature as critical metrics in public health assessments, the methods and data to conduct this evaluation vary considerably in nature and quality. Traditionally, health data collection relies on population surveys. However, these studies, typically of limited size, encompass only a small yet representative segment of the population. This limitation can necessitate the separate estimation of incidence and mortality rates, significantly restricting the available analysis methods. In this article, we leverage an extract from the French National Hospital Discharge database to define health indicators. Our analysis focuses on the resulting Disease-Free Life Expectancy (Dis-FLE) indicator, which provides insights based on the hospital trajectory of each patient admitted to hospital in France during 2008-13. Through this research, we illustrate the advantages and disadvantages of employing large clinical datasets as the foundation for more robust health indicators. We shed light on the opportunities that such data offer for a more comprehensive understanding of the health status of a population. In particular, we estimate age-dependent hazard rates associated with sex, alcohol abuse, tobacco consumption, and obesity, as well as geographic location. Simultaneously, we delve into the challenges and limitations that arise when adopting such a data-driven approach.</summary></entry><entry><title type="html">Exact Inference for Random Effects Meta-Analyses with Small, Sparse Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/ExactInferenceforRandomEffectsMetaAnalyseswithSmallSparseData.html" rel="alternate" type="text/html" title="Exact Inference for Random Effects Meta-Analyses with Small, Sparse Data" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/ExactInferenceforRandomEffectsMetaAnalyseswithSmallSparseData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/ExactInferenceforRandomEffectsMetaAnalyseswithSmallSparseData.html">&lt;p&gt;Meta-analysis aggregates information across related studies to provide more reliable statistical inference and has been a vital tool for assessing the safety and efficacy of many high profile pharmaceutical products. A key challenge in conducting a meta-analysis is that the number of related studies is typically small. Applying classical methods that are asymptotic in the number of studies can compromise the validity of inference, particularly when heterogeneity across studies is present. Moreover, serious adverse events are often rare and can result in one or more studies with no events in at least one study arm. Practitioners often apply arbitrary continuity corrections or remove zero-event studies to stabilize or define effect estimates in such settings, which can further invalidate subsequent inference. To address these significant practical issues, we introduce an exact inference method for comparing event rates in two treatment arms under a random effects framework, which we coin “XRRmeta”. In contrast to existing methods, the coverage of the confidence interval from XRRmeta is guaranteed to be at or above the nominal level (up to Monte Carlo error) when the event rates, number of studies, and/or the within-study sample sizes are small. Extensive numerical studies indicate that XRRmeta does not yield overly conservative inference and we apply our proposed method to two real-data examples using our open source R package.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.11907&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jessica Gronsbell, Zachary R McCaw, Timothy Regis, Lu Tian</name></author><category term="stat.ME" /><summary type="html">Meta-analysis aggregates information across related studies to provide more reliable statistical inference and has been a vital tool for assessing the safety and efficacy of many high profile pharmaceutical products. A key challenge in conducting a meta-analysis is that the number of related studies is typically small. Applying classical methods that are asymptotic in the number of studies can compromise the validity of inference, particularly when heterogeneity across studies is present. Moreover, serious adverse events are often rare and can result in one or more studies with no events in at least one study arm. Practitioners often apply arbitrary continuity corrections or remove zero-event studies to stabilize or define effect estimates in such settings, which can further invalidate subsequent inference. To address these significant practical issues, we introduce an exact inference method for comparing event rates in two treatment arms under a random effects framework, which we coin “XRRmeta”. In contrast to existing methods, the coverage of the confidence interval from XRRmeta is guaranteed to be at or above the nominal level (up to Monte Carlo error) when the event rates, number of studies, and/or the within-study sample sizes are small. Extensive numerical studies indicate that XRRmeta does not yield overly conservative inference and we apply our proposed method to two real-data examples using our open source R package.</summary></entry><entry><title type="html">Gaussian Copula Models for Nonignorable Missing Data Using Auxiliary Marginal Quantiles</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/GaussianCopulaModelsforNonignorableMissingDataUsingAuxiliaryMarginalQuantiles.html" rel="alternate" type="text/html" title="Gaussian Copula Models for Nonignorable Missing Data Using Auxiliary Marginal Quantiles" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/GaussianCopulaModelsforNonignorableMissingDataUsingAuxiliaryMarginalQuantiles</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/GaussianCopulaModelsforNonignorableMissingDataUsingAuxiliaryMarginalQuantiles.html">&lt;p&gt;We present an approach for modeling and imputation of nonignorable missing data under Gaussian copulas. The analyst posits a set of quantiles of the marginal distributions of the study variables, for example, reflecting information from external data sources or elicited expert opinion. When these quantiles are accurately specified, we prove it is possible to consistently estimate the copula correlation and perform multiple imputation in the presence of nonignorable missing data. We develop algorithms for estimation and imputation that are computationally efficient, which we evaluate in simulation studies of multiple imputation inferences. We apply the model to analyze associations between lead exposure levels and end-of-grade test scores for 170,000 students in North Carolina. These measurements are not missing at random, as children deemed at-risk for high lead exposure are more likely to be measured. We construct plausible marginal quantiles for lead exposure using national statistics provided by the Centers for Disease Control and Prevention. Complete cases and missing at random analyses appear to underestimate the relationships between certain variables and end-of-grade test scores, while multiple imputation inferences under our model support stronger adverse associations between lead exposure and educational outcomes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03463&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joseph Feldman, Jerome P. Reiter, Daniel R. Kowal</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.ML" /><summary type="html">We present an approach for modeling and imputation of nonignorable missing data under Gaussian copulas. The analyst posits a set of quantiles of the marginal distributions of the study variables, for example, reflecting information from external data sources or elicited expert opinion. When these quantiles are accurately specified, we prove it is possible to consistently estimate the copula correlation and perform multiple imputation in the presence of nonignorable missing data. We develop algorithms for estimation and imputation that are computationally efficient, which we evaluate in simulation studies of multiple imputation inferences. We apply the model to analyze associations between lead exposure levels and end-of-grade test scores for 170,000 students in North Carolina. These measurements are not missing at random, as children deemed at-risk for high lead exposure are more likely to be measured. We construct plausible marginal quantiles for lead exposure using national statistics provided by the Centers for Disease Control and Prevention. Complete cases and missing at random analyses appear to underestimate the relationships between certain variables and end-of-grade test scores, while multiple imputation inferences under our model support stronger adverse associations between lead exposure and educational outcomes.</summary></entry><entry><title type="html">Griddy-Gibbs sampling for Bayesian P-splines models with Poisson data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/GriddyGibbssamplingforBayesianPsplinesmodelswithPoissondata.html" rel="alternate" type="text/html" title="Griddy-Gibbs sampling for Bayesian P-splines models with Poisson data" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/GriddyGibbssamplingforBayesianPsplinesmodelswithPoissondata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/GriddyGibbssamplingforBayesianPsplinesmodelswithPoissondata.html">&lt;p&gt;P-splines are appealing for smoothing Poisson distributed counts. They provide a flexible setting for modeling nonlinear model components based on a discretized penalty structure with a relatively simple computational backbone. Under a Bayesian inferential process relying on Markov chain Monte Carlo, estimates of spline coefficients are typically obtained by means of Metropolis-type algorithms, which may suffer from convergence issues if the proposal distribution is not properly chosen. To avoid such a sensitive calibration choice, we extend the Griddy-Gibbs sampler to Bayesian P-splines models with a Poisson response variable. In this model class, conditional posterior distributions of spline components are shown to have attractive mathematical properties. Despite their non-conjugate nature, conditional posteriors of spline coefficients can be efficiently explored with a Gibbs sampling scheme by relying on grid-based approximations. The proposed Griddy-Gibbs sampler for Bayesian P-splines (GGSBPS) algorithm is an interesting calibration-free tool for density estimation and histogram smoothing that is made available in a compact and user-friendly routine. The performance of our approach is assessed in different simulation settings and the GGSBPS algorithm is illustrated on two real datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03336&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Oswaldo Gressani, Paul H. C. Eilers</name></author><category term="stat.ME" /><summary type="html">P-splines are appealing for smoothing Poisson distributed counts. They provide a flexible setting for modeling nonlinear model components based on a discretized penalty structure with a relatively simple computational backbone. Under a Bayesian inferential process relying on Markov chain Monte Carlo, estimates of spline coefficients are typically obtained by means of Metropolis-type algorithms, which may suffer from convergence issues if the proposal distribution is not properly chosen. To avoid such a sensitive calibration choice, we extend the Griddy-Gibbs sampler to Bayesian P-splines models with a Poisson response variable. In this model class, conditional posterior distributions of spline components are shown to have attractive mathematical properties. Despite their non-conjugate nature, conditional posteriors of spline coefficients can be efficiently explored with a Gibbs sampling scheme by relying on grid-based approximations. The proposed Griddy-Gibbs sampler for Bayesian P-splines (GGSBPS) algorithm is an interesting calibration-free tool for density estimation and histogram smoothing that is made available in a compact and user-friendly routine. The performance of our approach is assessed in different simulation settings and the GGSBPS algorithm is illustrated on two real datasets.</summary></entry><entry><title type="html">Inferring the Long-Term Causal Effects of Long-Term Treatments from Short-Term Experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/InferringtheLongTermCausalEffectsofLongTermTreatmentsfromShortTermExperiments.html" rel="alternate" type="text/html" title="Inferring the Long-Term Causal Effects of Long-Term Treatments from Short-Term Experiments" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/InferringtheLongTermCausalEffectsofLongTermTreatmentsfromShortTermExperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/InferringtheLongTermCausalEffectsofLongTermTreatmentsfromShortTermExperiments.html">&lt;p&gt;We study inference on the long-term causal effect of a continual exposure to a novel intervention, which we term a long-term treatment, based on an experiment involving only short-term observations. Key examples include the long-term health effects of regularly-taken medicine or of environmental hazards and the long-term effects on users of changes to an online platform. This stands in contrast to short-term treatments or “shocks,” whose long-term effect can reasonably be mediated by short-term observations, enabling the use of surrogate methods. Long-term treatments by definition have direct effects on long-term outcomes via continual exposure, so surrogacy conditions cannot reasonably hold. We connect the problem with offline reinforcement learning, leveraging doubly-robust estimators to estimate long-term causal effects for long-term treatments and construct confidence intervals.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.08527&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Allen Tran, Aurélien Bibaut, Nathan Kallus</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">We study inference on the long-term causal effect of a continual exposure to a novel intervention, which we term a long-term treatment, based on an experiment involving only short-term observations. Key examples include the long-term health effects of regularly-taken medicine or of environmental hazards and the long-term effects on users of changes to an online platform. This stands in contrast to short-term treatments or “shocks,” whose long-term effect can reasonably be mediated by short-term observations, enabling the use of surrogate methods. Long-term treatments by definition have direct effects on long-term outcomes via continual exposure, so surrogacy conditions cannot reasonably hold. We connect the problem with offline reinforcement learning, leveraging doubly-robust estimators to estimate long-term causal effects for long-term treatments and construct confidence intervals.</summary></entry><entry><title type="html">Is Knowledge All Large Language Models Needed for Causal Reasoning?</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/IsKnowledgeAllLargeLanguageModelsNeededforCausalReasoning.html" rel="alternate" type="text/html" title="Is Knowledge All Large Language Models Needed for Causal Reasoning?" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/IsKnowledgeAllLargeLanguageModelsNeededforCausalReasoning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/IsKnowledgeAllLargeLanguageModelsNeededforCausalReasoning.html">&lt;p&gt;This paper explores the causal reasoning of large language models (LLMs) to enhance their interpretability and reliability in advancing artificial intelligence. Despite the proficiency of LLMs in a range of tasks, their potential for understanding causality requires further exploration. We propose a novel causal attribution model that utilizes ``do-operators” for constructing counterfactual scenarios, allowing us to systematically quantify the influence of input numerical data and LLMs’ pre-existing knowledge on their causal reasoning processes. Our newly developed experimental setup assesses LLMs’ reliance on contextual information and inherent knowledge across various domains. Our evaluation reveals that LLMs’ causal reasoning ability mainly depends on the context and domain-specific knowledge provided. In the absence of such knowledge, LLMs can still maintain a degree of causal reasoning using the available numerical data, albeit with limitations in the calculations. This motivates the proposed fine-tuned LLM for pairwise causal discovery, effectively leveraging both knowledge and numerical information.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.00139&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hengrui Cai, Shengjie Liu, Rui Song</name></author><category term="stat.ME" /><summary type="html">This paper explores the causal reasoning of large language models (LLMs) to enhance their interpretability and reliability in advancing artificial intelligence. Despite the proficiency of LLMs in a range of tasks, their potential for understanding causality requires further exploration. We propose a novel causal attribution model that utilizes ``do-operators” for constructing counterfactual scenarios, allowing us to systematically quantify the influence of input numerical data and LLMs’ pre-existing knowledge on their causal reasoning processes. Our newly developed experimental setup assesses LLMs’ reliance on contextual information and inherent knowledge across various domains. Our evaluation reveals that LLMs’ causal reasoning ability mainly depends on the context and domain-specific knowledge provided. In the absence of such knowledge, LLMs can still maintain a degree of causal reasoning using the available numerical data, albeit with limitations in the calculations. This motivates the proposed fine-tuned LLM for pairwise causal discovery, effectively leveraging both knowledge and numerical information.</summary></entry><entry><title type="html">MPCR: Multi- and Mixed-Precision Computations Package in R</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/MPCRMultiandMixedPrecisionComputationsPackageinR.html" rel="alternate" type="text/html" title="MPCR: Multi- and Mixed-Precision Computations Package in R" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/MPCRMultiandMixedPrecisionComputationsPackageinR</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/MPCRMultiandMixedPrecisionComputationsPackageinR.html">&lt;p&gt;Computational statistics has traditionally utilized double-precision (64-bit) data structures and full-precision operations, resulting in higher-than-necessary accuracy for certain applications. Recently, there has been a growing interest in exploring low-precision options that could reduce computational complexity while still achieving the required level of accuracy. This trend has been amplified by new hardware such as NVIDIA’s Tensor Cores in their V100, A100, and H100 GPUs, which are optimized for mixed-precision computations, Intel CPUs with Deep Learning (DL) boost, Google Tensor Processing Units (TPUs), Field Programmable Gate Arrays (FPGAs), ARM CPUs, and others. However, using lower precision may introduce numerical instabilities and accuracy issues. Nevertheless, some applications have shown robustness to low-precision computations, leading to new multi- and mixed-precision algorithms that balance accuracy and computational cost. To address this need, we introduce MPCR, a novel R package that supports three different precision types (16-, 32-, and 64-bit) and their combinations, along with its usage in commonly-used Frequentist/Bayesian statistical examples. The MPCR package is written in C++ and integrated into R through the \pkg{Rcpp} package, enabling highly optimized operations in various precisions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02701&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mary Lai O. Salvana, Sameh Abdulah, Minwoo Kim, David Helmy, Ying Sun, Marc G. Genton</name></author><category term="stat.CO" /><summary type="html">Computational statistics has traditionally utilized double-precision (64-bit) data structures and full-precision operations, resulting in higher-than-necessary accuracy for certain applications. Recently, there has been a growing interest in exploring low-precision options that could reduce computational complexity while still achieving the required level of accuracy. This trend has been amplified by new hardware such as NVIDIA’s Tensor Cores in their V100, A100, and H100 GPUs, which are optimized for mixed-precision computations, Intel CPUs with Deep Learning (DL) boost, Google Tensor Processing Units (TPUs), Field Programmable Gate Arrays (FPGAs), ARM CPUs, and others. However, using lower precision may introduce numerical instabilities and accuracy issues. Nevertheless, some applications have shown robustness to low-precision computations, leading to new multi- and mixed-precision algorithms that balance accuracy and computational cost. To address this need, we introduce MPCR, a novel R package that supports three different precision types (16-, 32-, and 64-bit) and their combinations, along with its usage in commonly-used Frequentist/Bayesian statistical examples. The MPCR package is written in C++ and integrated into R through the \pkg{Rcpp} package, enabling highly optimized operations in various precisions.</summary></entry><entry><title type="html">Multi-relational Network Autoregression Model with Latent Group Structures</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/MultirelationalNetworkAutoregressionModelwithLatentGroupStructures.html" rel="alternate" type="text/html" title="Multi-relational Network Autoregression Model with Latent Group Structures" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/MultirelationalNetworkAutoregressionModelwithLatentGroupStructures</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/MultirelationalNetworkAutoregressionModelwithLatentGroupStructures.html">&lt;p&gt;Multi-relational networks among entities are frequently observed in the era of big data. Quantifying the effects of multiple networks have attracted significant research interest recently. In this work, we model multiple network effects through an autoregressive framework for tensor-valued time series. To characterize the potential heterogeneity of the networks and handle the high dimensionality of the time series data simultaneously, we assume a separate group structure for entities in each network and estimate all group memberships in a data-driven fashion. Specifically, we propose a group tensor network autoregression (GTNAR) model, which assumes that within each network, entities in the same group share the same set of model parameters, and the parameters differ across networks. An iterative algorithm is developed to estimate the model parameters and the latent group memberships simultaneously. Theoretically, we show that the group-wise parameters and group memberships can be consistently estimated when the group numbers are correctly- or possibly over-specified. An information criterion for group number estimation of each network is also provided to consistently select the group numbers. Lastly, we implement the method on a Yelp dataset to illustrate the usefulness of the method.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03296&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yimeng Ren, Xuening Zhu, Ganggang Xu, Yanyuan Ma</name></author><category term="stat.ME" /><summary type="html">Multi-relational networks among entities are frequently observed in the era of big data. Quantifying the effects of multiple networks have attracted significant research interest recently. In this work, we model multiple network effects through an autoregressive framework for tensor-valued time series. To characterize the potential heterogeneity of the networks and handle the high dimensionality of the time series data simultaneously, we assume a separate group structure for entities in each network and estimate all group memberships in a data-driven fashion. Specifically, we propose a group tensor network autoregression (GTNAR) model, which assumes that within each network, entities in the same group share the same set of model parameters, and the parameters differ across networks. An iterative algorithm is developed to estimate the model parameters and the latent group memberships simultaneously. Theoretically, we show that the group-wise parameters and group memberships can be consistently estimated when the group numbers are correctly- or possibly over-specified. An information criterion for group number estimation of each network is also provided to consistently select the group numbers. Lastly, we implement the method on a Yelp dataset to illustrate the usefulness of the method.</summary></entry><entry><title type="html">Non-stationary Spatio-Temporal Modeling Using the Stochastic Advection-Diffusion Equation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/NonstationarySpatioTemporalModelingUsingtheStochasticAdvectionDiffusionEquation.html" rel="alternate" type="text/html" title="Non-stationary Spatio-Temporal Modeling Using the Stochastic Advection-Diffusion Equation" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/NonstationarySpatioTemporalModelingUsingtheStochasticAdvectionDiffusionEquation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/NonstationarySpatioTemporalModelingUsingtheStochasticAdvectionDiffusionEquation.html">&lt;p&gt;We construct flexible spatio-temporal models through stochastic partial differential equations (SPDEs) where both diffusion and advection can be spatially varying. Computations are done through a Gaussian Markov random field approximation of the solution of the SPDE, which is constructed through a finite volume method. The new flexible non-separable model is compared to a flexible separable model both for reconstruction and forecasting and evaluated in terms of root mean square errors and continuous rank probability scores. A simulation study demonstrates that the non-separable model performs better when the data is simulated with non-separable effects such as diffusion and advection. Further, we estimate surrogate models for emulating the output of a ocean model in Trondheimsfjorden, Norway, and simulate observations of autonoumous underwater vehicles. The results show that the flexible non-separable model outperforms the flexible separable model for real-time prediction of unobserved locations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03400&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Martin Outzen Berild, Geir-Arne Fuglstad</name></author><category term="stat.ME" /><summary type="html">We construct flexible spatio-temporal models through stochastic partial differential equations (SPDEs) where both diffusion and advection can be spatially varying. Computations are done through a Gaussian Markov random field approximation of the solution of the SPDE, which is constructed through a finite volume method. The new flexible non-separable model is compared to a flexible separable model both for reconstruction and forecasting and evaluated in terms of root mean square errors and continuous rank probability scores. A simulation study demonstrates that the non-separable model performs better when the data is simulated with non-separable effects such as diffusion and advection. Further, we estimate surrogate models for emulating the output of a ocean model in Trondheimsfjorden, Norway, and simulate observations of autonoumous underwater vehicles. The results show that the flexible non-separable model outperforms the flexible separable model for real-time prediction of unobserved locations.</summary></entry><entry><title type="html">Ordinal Mixed-Effects Random Forest</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/OrdinalMixedEffectsRandomForest.html" rel="alternate" type="text/html" title="Ordinal Mixed-Effects Random Forest" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/OrdinalMixedEffectsRandomForest</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/OrdinalMixedEffectsRandomForest.html">&lt;p&gt;We propose an innovative statistical method, called Ordinal Mixed-Effect Random Forest (OMERF), that extends the use of random forest to the analysis of hierarchical data and ordinal responses. The model preserves the flexibility and ability of modeling complex patterns of both categorical and continuous variables, typical of tree-based ensemble methods, and, at the same time, takes into account the structure of hierarchical data, modeling the dependence structure induced by the grouping and allowing statistical inference at all data levels. A simulation study is conducted to validate the performance of the proposed method and to compare it to the one of other state-of-the art models. The application of OMERF is exemplified in a case study focusing on predicting students performances using data from the Programme for International Student Assessment (PISA) 2022. The model identifies discriminating student characteristics and estimates the school-effect.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03130&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Giulia Bergonzoli, Lidia Rossi, Chiara Masci</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We propose an innovative statistical method, called Ordinal Mixed-Effect Random Forest (OMERF), that extends the use of random forest to the analysis of hierarchical data and ordinal responses. The model preserves the flexibility and ability of modeling complex patterns of both categorical and continuous variables, typical of tree-based ensemble methods, and, at the same time, takes into account the structure of hierarchical data, modeling the dependence structure induced by the grouping and allowing statistical inference at all data levels. A simulation study is conducted to validate the performance of the proposed method and to compare it to the one of other state-of-the art models. The application of OMERF is exemplified in a case study focusing on predicting students performances using data from the Programme for International Student Assessment (PISA) 2022. The model identifies discriminating student characteristics and estimates the school-effect.</summary></entry><entry><title type="html">Planetary Causal Inference: Implications for the Geography of Poverty</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/PlanetaryCausalInferenceImplicationsfortheGeographyofPoverty.html" rel="alternate" type="text/html" title="Planetary Causal Inference: Implications for the Geography of Poverty" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/PlanetaryCausalInferenceImplicationsfortheGeographyofPoverty</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/PlanetaryCausalInferenceImplicationsfortheGeographyofPoverty.html">&lt;p&gt;Earth observation data such as satellite imagery can, when combined with machine learning, have profound impacts on our understanding of the geography of poverty through the prediction of living conditions, especially where government-derived economic indicators are either unavailable or potentially untrustworthy. Recent work has progressed in using EO data not only to predict spatial economic outcomes, but also to explore cause and effect, an understanding which is critical for downstream policy analysis. In this review, we first document the growth of interest in EO-ML analyses in the causal space. We then trace the relationship between spatial statistics and EO-ML methods before discussing the four ways in which EO data has been used in causal ML pipelines – (1.) poverty outcome imputation for downstream causal analysis, (2.) EO image deconfounding, (3.) EO-based treatment effect heterogeneity, and (4.) EO-based transportability analysis. We conclude by providing a workflow for how researchers can incorporate EO data in causal ML analysis going forward.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02584&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kazuki Sakamoto, Connor T. Jerzak, Adel Daoud</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Earth observation data such as satellite imagery can, when combined with machine learning, have profound impacts on our understanding of the geography of poverty through the prediction of living conditions, especially where government-derived economic indicators are either unavailable or potentially untrustworthy. Recent work has progressed in using EO data not only to predict spatial economic outcomes, but also to explore cause and effect, an understanding which is critical for downstream policy analysis. In this review, we first document the growth of interest in EO-ML analyses in the causal space. We then trace the relationship between spatial statistics and EO-ML methods before discussing the four ways in which EO data has been used in causal ML pipelines – (1.) poverty outcome imputation for downstream causal analysis, (2.) EO image deconfounding, (3.) EO-based treatment effect heterogeneity, and (4.) EO-based transportability analysis. We conclude by providing a workflow for how researchers can incorporate EO data in causal ML analysis going forward.</summary></entry><entry><title type="html">PriME: Privacy-aware Membership profile Estimation in networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/PriMEPrivacyawareMembershipprofileEstimationinnetworks.html" rel="alternate" type="text/html" title="PriME: Privacy-aware Membership profile Estimation in networks" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/PriMEPrivacyawareMembershipprofileEstimationinnetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/PriMEPrivacyawareMembershipprofileEstimationinnetworks.html">&lt;p&gt;This paper presents a novel approach to estimating community membership probabilities for network vertices generated by the Degree Corrected Mixed Membership Stochastic Block Model while preserving individual edge privacy. Operating within the $\varepsilon$-edge local differential privacy framework, we introduce an optimal private algorithm based on a symmetric edge flip mechanism and spectral clustering for accurate estimation of vertex community memberships. We conduct a comprehensive analysis of the estimation risk and establish the optimality of our procedure by providing matching lower bounds to the minimax risk under privacy constraints. To validate our approach, we demonstrate its performance through numerical simulations and its practical application to real-world data. This work represents a significant step forward in balancing accurate community membership estimation with stringent privacy preservation in network data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02794&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Abhinav Chakraborty, Sayak Chatterjee, Sagnik Nandy</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">This paper presents a novel approach to estimating community membership probabilities for network vertices generated by the Degree Corrected Mixed Membership Stochastic Block Model while preserving individual edge privacy. Operating within the $\varepsilon$-edge local differential privacy framework, we introduce an optimal private algorithm based on a symmetric edge flip mechanism and spectral clustering for accurate estimation of vertex community memberships. We conduct a comprehensive analysis of the estimation risk and establish the optimality of our procedure by providing matching lower bounds to the minimax risk under privacy constraints. To validate our approach, we demonstrate its performance through numerical simulations and its practical application to real-world data. This work represents a significant step forward in balancing accurate community membership estimation with stringent privacy preservation in network data analysis.</summary></entry><entry><title type="html">Privacy-Preserving Line Outage Detection in Distribution Grids: An Efficient Approach with Uncompromised Performance</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/PrivacyPreservingLineOutageDetectioninDistributionGridsAnEfficientApproachwithUncompromisedPerformance.html" rel="alternate" type="text/html" title="Privacy-Preserving Line Outage Detection in Distribution Grids: An Efficient Approach with Uncompromised Performance" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/PrivacyPreservingLineOutageDetectioninDistributionGridsAnEfficientApproachwithUncompromisedPerformance</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/PrivacyPreservingLineOutageDetectioninDistributionGridsAnEfficientApproachwithUncompromisedPerformance.html">&lt;p&gt;Recent advancements in research have shown the efficacy of employing sensor measurements, such as voltage and power data, in identifying line outages within distribution grids. However, these measurements inadvertently pose privacy risks to electricity customers by potentially revealing their sensitive information, such as household occupancy and economic status, to adversaries. To safeguard raw data from direct exposure to third-party adversaries, this paper proposes a novel decentralized data encryption scheme. The effectiveness of this encryption strategy is validated via demonstration of its differential privacy attributes by studying the Gaussian differential privacy. Recognizing that the encryption of raw data could affect the efficacy of outage detection, this paper analyzes the performance degradation by examining the Kullback-Leibler divergence between data distributions before and after the line outage. This analysis allows us to further alleviate the performance degradation by designing an innovative detection statistic that accurately approximates the optimal one. Manipulating the variance of this statistic, we demonstrate its ability to approach the optimal detection performance. The proposed privacy-aware detection procedure is evaluated using representative distribution grids and real load profiles, covering 17 distinct outage configurations. Our empirical results confirm the privacy-preserving nature of our approach and show that it achieves comparable detection performance to the optimal baseline.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.05140&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chenhan Xiao, Yizheng Liao, Yang Weng</name></author><category term="stat.AP" /><summary type="html">Recent advancements in research have shown the efficacy of employing sensor measurements, such as voltage and power data, in identifying line outages within distribution grids. However, these measurements inadvertently pose privacy risks to electricity customers by potentially revealing their sensitive information, such as household occupancy and economic status, to adversaries. To safeguard raw data from direct exposure to third-party adversaries, this paper proposes a novel decentralized data encryption scheme. The effectiveness of this encryption strategy is validated via demonstration of its differential privacy attributes by studying the Gaussian differential privacy. Recognizing that the encryption of raw data could affect the efficacy of outage detection, this paper analyzes the performance degradation by examining the Kullback-Leibler divergence between data distributions before and after the line outage. This analysis allows us to further alleviate the performance degradation by designing an innovative detection statistic that accurately approximates the optimal one. Manipulating the variance of this statistic, we demonstrate its ability to approach the optimal detection performance. The proposed privacy-aware detection procedure is evaluated using representative distribution grids and real load profiles, covering 17 distinct outage configurations. Our empirical results confirm the privacy-preserving nature of our approach and show that it achieves comparable detection performance to the optimal baseline.</summary></entry><entry><title type="html">Profiled Transfer Learning for High Dimensional Linear Model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/ProfiledTransferLearningforHighDimensionalLinearModel.html" rel="alternate" type="text/html" title="Profiled Transfer Learning for High Dimensional Linear Model" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/ProfiledTransferLearningforHighDimensionalLinearModel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/ProfiledTransferLearningforHighDimensionalLinearModel.html">&lt;p&gt;We develop here a novel transfer learning methodology called Profiled Transfer Learning (PTL). The method is based on the \textit{approximate-linear} assumption between the source and target parameters. Compared with the commonly assumed \textit{vanishing-difference} assumption and \textit{low-rank} assumption in the literature, the \textit{approximate-linear} assumption is more flexible and less stringent. Specifically, the PTL estimator is constructed by two major steps. Firstly, we regress the response on the transferred feature, leading to the profiled responses. Subsequently, we learn the regression relationship between profiled responses and the covariates on the target data. The final estimator is then assembled based on the \textit{approximate-linear} relationship. To theoretically support the PTL estimator, we derive the non-asymptotic upper bound and minimax lower bound. We find that the PTL estimator is minimax optimal under appropriate regularity conditions. Extensive simulation studies are presented to demonstrate the finite sample performance of the new method. A real data example about sentence prediction is also presented with very encouraging results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.00701&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ziqian Lin, Junlong Zhao, Fang Wang, Hansheng Wang</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We develop here a novel transfer learning methodology called Profiled Transfer Learning (PTL). The method is based on the \textit{approximate-linear} assumption between the source and target parameters. Compared with the commonly assumed \textit{vanishing-difference} assumption and \textit{low-rank} assumption in the literature, the \textit{approximate-linear} assumption is more flexible and less stringent. Specifically, the PTL estimator is constructed by two major steps. Firstly, we regress the response on the transferred feature, leading to the profiled responses. Subsequently, we learn the regression relationship between profiled responses and the covariates on the target data. The final estimator is then assembled based on the \textit{approximate-linear} relationship. To theoretically support the PTL estimator, we derive the non-asymptotic upper bound and minimax lower bound. We find that the PTL estimator is minimax optimal under appropriate regularity conditions. Extensive simulation studies are presented to demonstrate the finite sample performance of the new method. A real data example about sentence prediction is also presented with very encouraging results.</summary></entry><entry><title type="html">Random matrix theory improved Fréchet mean of symmetric positive definite matrices</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/RandommatrixtheoryimprovedFr%C3%A9chetmeanofsymmetricpositivedefinitematrices.html" rel="alternate" type="text/html" title="Random matrix theory improved Fréchet mean of symmetric positive definite matrices" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/RandommatrixtheoryimprovedFr%C3%A9chetmeanofsymmetricpositivedefinitematrices</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/RandommatrixtheoryimprovedFr%C3%A9chetmeanofsymmetricpositivedefinitematrices.html">&lt;p&gt;In this study, we consider the realm of covariance matrices in machine learning, particularly focusing on computing Fr&apos;echet means on the manifold of symmetric positive definite matrices, commonly referred to as Karcher or geometric means. Such means are leveraged in numerous machine-learning tasks. Relying on advanced statistical tools, we introduce a random matrix theory-based method that estimates Fr&apos;echet means, which is particularly beneficial when dealing with low sample support and a high number of matrices to average. Our experimental evaluation, involving both synthetic and real-world EEG and hyperspectral datasets, shows that we largely outperform state-of-the-art methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.06558&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Florent Bouchard, Ammar Mian, Malik Tiomoko, Guillaume Ginolhac, Frédéric Pascal</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">In this study, we consider the realm of covariance matrices in machine learning, particularly focusing on computing Fr&apos;echet means on the manifold of symmetric positive definite matrices, commonly referred to as Karcher or geometric means. Such means are leveraged in numerous machine-learning tasks. Relying on advanced statistical tools, we introduce a random matrix theory-based method that estimates Fr&apos;echet means, which is particularly beneficial when dealing with low sample support and a high number of matrices to average. Our experimental evaluation, involving both synthetic and real-world EEG and hyperspectral datasets, shows that we largely outperform state-of-the-art methods.</summary></entry><entry><title type="html">Regression-Based Proximal Causal Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/RegressionBasedProximalCausalInference.html" rel="alternate" type="text/html" title="Regression-Based Proximal Causal Inference" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/RegressionBasedProximalCausalInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/RegressionBasedProximalCausalInference.html">&lt;p&gt;Negative controls are increasingly used to evaluate the presence of potential unmeasured confounding in observational studies. Beyond the use of negative controls to detect the presence of residual confounding, proximal causal inference (PCI) was recently proposed to de-bias confounded causal effect estimates, by leveraging a pair of treatment and outcome negative control or confounding proxy variables. While formal methods for statistical inference have been developed for PCI, these methods can be challenging to implement as they involve solving complex integral equations that are typically ill-posed. We develop a regression-based PCI approach, employing two-stage generalized linear regression models (GLMs) to implement PCI, which obviates the need to solve difficult integral equations. The proposed approach has merit in that (i) it is applicable to continuous, count, and binary outcomes cases, making it relevant to a wide range of real-world applications, and (ii) it is easy to implement using off-the-shelf software for GLMs. We establish the statistical properties of regression-based PCI and illustrate their performance in both synthetic and real-world empirical applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.00335&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiewen Liu, Chan Park, Kendrick Li, Eric J. Tchetgen Tchetgen</name></author><category term="stat.ME" /><summary type="html">Negative controls are increasingly used to evaluate the presence of potential unmeasured confounding in observational studies. Beyond the use of negative controls to detect the presence of residual confounding, proximal causal inference (PCI) was recently proposed to de-bias confounded causal effect estimates, by leveraging a pair of treatment and outcome negative control or confounding proxy variables. While formal methods for statistical inference have been developed for PCI, these methods can be challenging to implement as they involve solving complex integral equations that are typically ill-posed. We develop a regression-based PCI approach, employing two-stage generalized linear regression models (GLMs) to implement PCI, which obviates the need to solve difficult integral equations. The proposed approach has merit in that (i) it is applicable to continuous, count, and binary outcomes cases, making it relevant to a wide range of real-world applications, and (ii) it is easy to implement using off-the-shelf software for GLMs. We establish the statistical properties of regression-based PCI and illustrate their performance in both synthetic and real-world empirical applications.</summary></entry><entry><title type="html">Relational hyperevent models for the coevolution of coauthoring and citation networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Relationalhypereventmodelsforthecoevolutionofcoauthoringandcitationnetworks.html" rel="alternate" type="text/html" title="Relational hyperevent models for the coevolution of coauthoring and citation networks" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Relationalhypereventmodelsforthecoevolutionofcoauthoringandcitationnetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/Relationalhypereventmodelsforthecoevolutionofcoauthoringandcitationnetworks.html">&lt;p&gt;The development of suitable statistical models for the analysis of bibliographic networks has trailed behind the empirical ambitions expressed by recent studies of science of science. Extant research typically restricts the analytical focus to either paper citation networks, or author collaboration networks. These networks involve not only direct relationships between papers or authors, but also a broader system of dependencies between the references of papers connected through multiple simultaneous citation links. In this work, we extend recently developed relational hyperevent models (RHEM) to analyze scientific networks - systems of scientific publications connected by citations and authorship. We introduce new covariates that represent theoretically relevant and empirically meaningful sub-network configurations. The new model specification supports testing of hypotheses that align with the polyadic nature of scientific publication events and the multiple interdependencies between authors and references of current and prior papers. We implement the model using open-source software to analyze a large, publicly available scientific network dataset. A significant finding of the study is the tendency for subsets of papers to be repeatedly cited together across publications. This result is crucial as it suggests that the papers’ impact may be partly due to endogenous network processes. More broadly, the study shows that models accounting for both the hyperedge structure of publication events and the interconnections between authors and references significantly enhance our understanding of the network mechanisms that drive scientific production, productivity, and impact.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.01722&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jürgen Lerner, Marian-Gabriel Hâncean, Alessandro Lomi</name></author><category term="stat.AP" /><summary type="html">The development of suitable statistical models for the analysis of bibliographic networks has trailed behind the empirical ambitions expressed by recent studies of science of science. Extant research typically restricts the analytical focus to either paper citation networks, or author collaboration networks. These networks involve not only direct relationships between papers or authors, but also a broader system of dependencies between the references of papers connected through multiple simultaneous citation links. In this work, we extend recently developed relational hyperevent models (RHEM) to analyze scientific networks - systems of scientific publications connected by citations and authorship. We introduce new covariates that represent theoretically relevant and empirically meaningful sub-network configurations. The new model specification supports testing of hypotheses that align with the polyadic nature of scientific publication events and the multiple interdependencies between authors and references of current and prior papers. We implement the model using open-source software to analyze a large, publicly available scientific network dataset. A significant finding of the study is the tendency for subsets of papers to be repeatedly cited together across publications. This result is crucial as it suggests that the papers’ impact may be partly due to endogenous network processes. More broadly, the study shows that models accounting for both the hyperedge structure of publication events and the interconnections between authors and references significantly enhance our understanding of the network mechanisms that drive scientific production, productivity, and impact.</summary></entry><entry><title type="html">Sampling From Multiscale Densities With Delayed Rejection Generalized Hamiltonian Monte Carlo</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SamplingFromMultiscaleDensitiesWithDelayedRejectionGeneralizedHamiltonianMonteCarlo.html" rel="alternate" type="text/html" title="Sampling From Multiscale Densities With Delayed Rejection Generalized Hamiltonian Monte Carlo" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SamplingFromMultiscaleDensitiesWithDelayedRejectionGeneralizedHamiltonianMonteCarlo</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SamplingFromMultiscaleDensitiesWithDelayedRejectionGeneralizedHamiltonianMonteCarlo.html">&lt;p&gt;With the increasing prevalence of probabilistic programming languages, Hamiltonian Monte Carlo (HMC) has become the mainstay of applied Bayesian inference. However HMC still struggles to sample from densities with multiscale geometry: a large step size is needed to efficiently explore low curvature regions while a small step size is needed to accurately explore high curvature regions. We introduce the delayed rejection generalized HMC (DR-G-HMC) sampler that overcomes this challenge by employing dynamic step size selection, inspired by differential equation solvers. In a single sampling iteration, DR-G-HMC sequentially makes proposals with geometrically decreasing step sizes if necessary. This simulates Hamiltonian dynamics with increasing fidelity that, in high curvature regions, generates proposals with a higher chance of acceptance. DR-G-HMC also makes generalized HMC competitive by decreasing the number of rejections which otherwise cause inefficient backtracking and prevents directed movement. We present experiments to demonstrate that DR-G-HMC (1) correctly samples from multiscale densities, (2) makes generalized HMC methods competitive with the state of the art No-U-Turn sampler, and (3) is robust to tuning parameters.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02741&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Gilad Turok, Chirag Modi, Bob Carpenter</name></author><category term="stat.CO" /><summary type="html">With the increasing prevalence of probabilistic programming languages, Hamiltonian Monte Carlo (HMC) has become the mainstay of applied Bayesian inference. However HMC still struggles to sample from densities with multiscale geometry: a large step size is needed to efficiently explore low curvature regions while a small step size is needed to accurately explore high curvature regions. We introduce the delayed rejection generalized HMC (DR-G-HMC) sampler that overcomes this challenge by employing dynamic step size selection, inspired by differential equation solvers. In a single sampling iteration, DR-G-HMC sequentially makes proposals with geometrically decreasing step sizes if necessary. This simulates Hamiltonian dynamics with increasing fidelity that, in high curvature regions, generates proposals with a higher chance of acceptance. DR-G-HMC also makes generalized HMC competitive by decreasing the number of rejections which otherwise cause inefficient backtracking and prevents directed movement. We present experiments to demonstrate that DR-G-HMC (1) correctly samples from multiscale densities, (2) makes generalized HMC methods competitive with the state of the art No-U-Turn sampler, and (3) is robust to tuning parameters.</summary></entry><entry><title type="html">Sampling in Unit Time with Kernel Fisher-Rao Flow</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SamplinginUnitTimewithKernelFisherRaoFlow.html" rel="alternate" type="text/html" title="Sampling in Unit Time with Kernel Fisher-Rao Flow" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SamplinginUnitTimewithKernelFisherRaoFlow</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SamplinginUnitTimewithKernelFisherRaoFlow.html">&lt;p&gt;We introduce a new mean-field ODE and corresponding interacting particle systems (IPS) for sampling from an unnormalized target density. The IPS are gradient-free, available in closed form, and only require the ability to sample from a reference density and compute the (unnormalized) target-to-reference density ratio. The mean-field ODE is obtained by solving a Poisson equation for a velocity field that transports samples along the geometric mixture of the two densities, which is the path of a particular Fisher-Rao gradient flow. We employ a RKHS ansatz for the velocity field, which makes the Poisson equation tractable and enables discretization of the resulting mean-field ODE over finite samples. The mean-field ODE can be additionally be derived from a discrete-time perspective as the limit of successive linearizations of the Monge-Amp`ere equations within a framework known as sample-driven optimal transport. We introduce a stochastic variant of our approach and demonstrate empirically that our IPS can produce high-quality samples from varied target distributions, outperforming comparable gradient-free particle systems and competitive with gradient-based alternatives.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.03892&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aimee Maurais, Youssef Marzouk</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">We introduce a new mean-field ODE and corresponding interacting particle systems (IPS) for sampling from an unnormalized target density. The IPS are gradient-free, available in closed form, and only require the ability to sample from a reference density and compute the (unnormalized) target-to-reference density ratio. The mean-field ODE is obtained by solving a Poisson equation for a velocity field that transports samples along the geometric mixture of the two densities, which is the path of a particular Fisher-Rao gradient flow. We employ a RKHS ansatz for the velocity field, which makes the Poisson equation tractable and enables discretization of the resulting mean-field ODE over finite samples. The mean-field ODE can be additionally be derived from a discrete-time perspective as the limit of successive linearizations of the Monge-Amp`ere equations within a framework known as sample-driven optimal transport. We introduce a stochastic variant of our approach and demonstrate empirically that our IPS can produce high-quality samples from varied target distributions, outperforming comparable gradient-free particle systems and competitive with gradient-based alternatives.</summary></entry><entry><title type="html">Second-order differential operators, stochastic differential equations and Brownian motions on embedded manifolds</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SecondorderdifferentialoperatorsstochasticdifferentialequationsandBrownianmotionsonembeddedmanifolds.html" rel="alternate" type="text/html" title="Second-order differential operators, stochastic differential equations and Brownian motions on embedded manifolds" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SecondorderdifferentialoperatorsstochasticdifferentialequationsandBrownianmotionsonembeddedmanifolds</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SecondorderdifferentialoperatorsstochasticdifferentialequationsandBrownianmotionsonembeddedmanifolds.html">&lt;p&gt;We specify the conditions when a manifold M embedded in an inner product space E is an invariant manifold of a stochastic differential equation (SDE) on E, linking it with the notion of second-order differential operators on M. When M is given a Riemannian metric, we derive a simple formula for the Laplace-Beltrami operator in terms of the gradient and Hessian on E and construct the Riemannian Brownian motions on M as solutions of conservative Stratonovich and Ito SDEs on E. We derive explicitly the SDE for Brownian motions on several important manifolds in applications, including left-invariant matrix Lie groups using embedded coordinates. Numerically, we propose three simulation schemes to solve SDEs on manifolds. In addition to the stochastic projection method, to simulate Riemannian Brownian motions, we construct a second-order tangent retraction of the Levi-Civita connection using a given E-tubular retraction. We also propose the retractive Euler-Maruyama method to solve a SDE, taking into account the second-order term of a tangent retraction. We provide software to implement the methods in the paper, including Brownian motions of the manifolds discussed. We verify numerically that on several compact Riemannian manifolds, the long-term limit of Brownian simulation converges to the uniform distributions, suggesting a method to sample Riemannian uniform distributions&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02879&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Du Nguyen, Stefan Sommer</name></author><category term="stat.CO" /><summary type="html">We specify the conditions when a manifold M embedded in an inner product space E is an invariant manifold of a stochastic differential equation (SDE) on E, linking it with the notion of second-order differential operators on M. When M is given a Riemannian metric, we derive a simple formula for the Laplace-Beltrami operator in terms of the gradient and Hessian on E and construct the Riemannian Brownian motions on M as solutions of conservative Stratonovich and Ito SDEs on E. We derive explicitly the SDE for Brownian motions on several important manifolds in applications, including left-invariant matrix Lie groups using embedded coordinates. Numerically, we propose three simulation schemes to solve SDEs on manifolds. In addition to the stochastic projection method, to simulate Riemannian Brownian motions, we construct a second-order tangent retraction of the Levi-Civita connection using a given E-tubular retraction. We also propose the retractive Euler-Maruyama method to solve a SDE, taking into account the second-order term of a tangent retraction. We provide software to implement the methods in the paper, including Brownian motions of the manifolds discussed. We verify numerically that on several compact Riemannian manifolds, the long-term limit of Brownian simulation converges to the uniform distributions, suggesting a method to sample Riemannian uniform distributions</summary></entry><entry><title type="html">Sparse two-stage Bayesian meta-analysis for individualized treatments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SparsetwostageBayesianmetaanalysisforindividualizedtreatments.html" rel="alternate" type="text/html" title="Sparse two-stage Bayesian meta-analysis for individualized treatments" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SparsetwostageBayesianmetaanalysisforindividualizedtreatments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SparsetwostageBayesianmetaanalysisforindividualizedtreatments.html">&lt;p&gt;Individualized treatment rules tailor treatments to patients based on clinical, demographic, and other characteristics. Estimation of individualized treatment rules requires the identification of individuals who benefit most from the particular treatments and thus the detection of variability in treatment effects. To develop an effective individualized treatment rule, data from multisite studies may be required due to the low power provided by smaller datasets for detecting the often small treatment-covariate interactions. However, sharing of individual-level data is sometimes constrained. Furthermore, sparsity may arise in two senses: different data sites may recruit from different populations, making it infeasible to estimate identical models or all parameters of interest at all sites, and the number of non-zero parameters in the model for the treatment rule may be small. To address these issues, we adopt a two-stage Bayesian meta-analysis approach to estimate individualized treatment rules which optimize expected patient outcomes using multisite data without disclosing individual-level data beyond the sites. Simulation results demonstrate that our approach can provide consistent estimates of the parameters which fully characterize the optimal individualized treatment rule. We estimate the optimal Warfarin dose strategy using data from the International Warfarin Pharmacogenetics Consortium, where data sparsity and small treatment-covariate interaction effects pose additional statistical challenges.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03056&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Junwei Shen, Erica E. M. Moodie, Shirin Golchi</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Individualized treatment rules tailor treatments to patients based on clinical, demographic, and other characteristics. Estimation of individualized treatment rules requires the identification of individuals who benefit most from the particular treatments and thus the detection of variability in treatment effects. To develop an effective individualized treatment rule, data from multisite studies may be required due to the low power provided by smaller datasets for detecting the often small treatment-covariate interactions. However, sharing of individual-level data is sometimes constrained. Furthermore, sparsity may arise in two senses: different data sites may recruit from different populations, making it infeasible to estimate identical models or all parameters of interest at all sites, and the number of non-zero parameters in the model for the treatment rule may be small. To address these issues, we adopt a two-stage Bayesian meta-analysis approach to estimate individualized treatment rules which optimize expected patient outcomes using multisite data without disclosing individual-level data beyond the sites. Simulation results demonstrate that our approach can provide consistent estimates of the parameters which fully characterize the optimal individualized treatment rule. We estimate the optimal Warfarin dose strategy using data from the International Warfarin Pharmacogenetics Consortium, where data sparsity and small treatment-covariate interaction effects pose additional statistical challenges.</summary></entry><entry><title type="html">Spatial data fusion adjusting for preferential sampling using INLA and SPDE</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SpatialdatafusionadjustingforpreferentialsamplingusingINLAandSPDE.html" rel="alternate" type="text/html" title="Spatial data fusion adjusting for preferential sampling using INLA and SPDE" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SpatialdatafusionadjustingforpreferentialsamplingusingINLAandSPDE</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/SpatialdatafusionadjustingforpreferentialsamplingusingINLAandSPDE.html">&lt;p&gt;Spatially misaligned data can be fused by using a Bayesian melding model that assumes that underlying all observations there is a spatially continuous Gaussian random field process. This model can be used, for example, to predict air pollution levels by combining point data from monitoring stations and areal data from satellite imagery.
  However, if the data presents preferential sampling, that is, if the observed point locations are not independent of the underlying spatial process, the inference obtained from models that ignore such a dependence structure might not be valid.
  In this paper, we present a Bayesian spatial model for the fusion of point and areal data that takes into account preferential sampling. The model combines the Bayesian melding specification and a model for the stochastically dependent sampling and underlying spatial processes.
  Fast Bayesian inference is performed using the integrated nested Laplace approximation (INLA) and the stochastic partial differential equation (SPDE) approaches. The performance of the model is assessed using simulated data in a range of scenarios and sampling strategies that can appear in real settings. The model is also applied to predict air pollution in the USA.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.03316&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ruiman Zhong, André Victor Ribeiro Amaral, Paula Moraga</name></author><category term="stat.ME" /><summary type="html">Spatially misaligned data can be fused by using a Bayesian melding model that assumes that underlying all observations there is a spatially continuous Gaussian random field process. This model can be used, for example, to predict air pollution levels by combining point data from monitoring stations and areal data from satellite imagery. However, if the data presents preferential sampling, that is, if the observed point locations are not independent of the underlying spatial process, the inference obtained from models that ignore such a dependence structure might not be valid. In this paper, we present a Bayesian spatial model for the fusion of point and areal data that takes into account preferential sampling. The model combines the Bayesian melding specification and a model for the stochastically dependent sampling and underlying spatial processes. Fast Bayesian inference is performed using the integrated nested Laplace approximation (INLA) and the stochastic partial differential equation (SPDE) approaches. The performance of the model is assessed using simulated data in a range of scenarios and sampling strategies that can appear in real settings. The model is also applied to predict air pollution in the USA.</summary></entry><entry><title type="html">Statistical inference of convex order by Wasserstein projection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/StatisticalinferenceofconvexorderbyWassersteinprojection.html" rel="alternate" type="text/html" title="Statistical inference of convex order by Wasserstein projection" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/StatisticalinferenceofconvexorderbyWassersteinprojection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/StatisticalinferenceofconvexorderbyWassersteinprojection.html">&lt;p&gt;Ranking distributions according to a stochastic order has wide applications in diverse areas. Although stochastic dominance has received much attention,convex order, particularly in general dimensions, has yet to be investigated from a statistical point of view. This article addresses this gap by introducing a simple statistical test for convex order based on the Wasserstein projection distance. This projection distance not only encodes whether two distributions are indeed in convex order, but also quantifies the deviation from the desired convex order and produces an optimal convex order approximation. Lipschitz stability of the backward and forward Wasserstein projection distance is proved, which leads to elegant consistency results of the estimator we employ as our test statistic. Combining these with state of the art results regarding the convergence rate of empirical distributions, we also derive upper bounds for the $p$-value and type I error our test statistic, as well as upper bounds on the type II error for an appropriate class of strict alternatives. Lastly, we provide an efficient numerical scheme for our test statistic, by way of an entropic Frank-Wolfe algorithm. Some experiments based on synthetic data sets illuminates the success of our approach empirically.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02840&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jakwang Kim, Young-Heon Kim, Yuanlong Ruan, Andrew Warren</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Ranking distributions according to a stochastic order has wide applications in diverse areas. Although stochastic dominance has received much attention,convex order, particularly in general dimensions, has yet to be investigated from a statistical point of view. This article addresses this gap by introducing a simple statistical test for convex order based on the Wasserstein projection distance. This projection distance not only encodes whether two distributions are indeed in convex order, but also quantifies the deviation from the desired convex order and produces an optimal convex order approximation. Lipschitz stability of the backward and forward Wasserstein projection distance is proved, which leads to elegant consistency results of the estimator we employ as our test statistic. Combining these with state of the art results regarding the convergence rate of empirical distributions, we also derive upper bounds for the $p$-value and type I error our test statistic, as well as upper bounds on the type II error for an appropriate class of strict alternatives. Lastly, we provide an efficient numerical scheme for our test statistic, by way of an entropic Frank-Wolfe algorithm. Some experiments based on synthetic data sets illuminates the success of our approach empirically.</summary></entry><entry><title type="html">Tackling GenAI Copyright Issues: Originality Estimation and Genericization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/TacklingGenAICopyrightIssuesOriginalityEstimationandGenericization.html" rel="alternate" type="text/html" title="Tackling GenAI Copyright Issues: Originality Estimation and Genericization" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/TacklingGenAICopyrightIssuesOriginalityEstimationandGenericization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/TacklingGenAICopyrightIssuesOriginalityEstimationandGenericization.html">&lt;p&gt;The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. While some studies explore methods to mitigate copyright risks by steering the outputs of generative models away from those resembling copyrighted data, little attention has been paid to the question of how much of a resemblance is undesirable; more original or unique data are afforded stronger protection, and the threshold level of resemblance for constituting infringement correspondingly lower. Here, leveraging this principle, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright. To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework. This metric can be practically estimated by drawing samples from a generative model, which is then used for the genericization process. Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03341&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hiroaki Chiba-Okabe, Weijie J. Su</name></author><category term="stat.ME" /><summary type="html">The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. While some studies explore methods to mitigate copyright risks by steering the outputs of generative models away from those resembling copyrighted data, little attention has been paid to the question of how much of a resemblance is undesirable; more original or unique data are afforded stronger protection, and the threshold level of resemblance for constituting infringement correspondingly lower. Here, leveraging this principle, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright. To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework. This metric can be practically estimated by drawing samples from a generative model, which is then used for the genericization process. Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images.</summary></entry><entry><title type="html">Test and Measure for Partial Mean Dependence Based on Machine Learning Methods</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/TestandMeasureforPartialMeanDependenceBasedonMachineLearningMethods.html" rel="alternate" type="text/html" title="Test and Measure for Partial Mean Dependence Based on Machine Learning Methods" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/TestandMeasureforPartialMeanDependenceBasedonMachineLearningMethods</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/TestandMeasureforPartialMeanDependenceBasedonMachineLearningMethods.html">&lt;p&gt;It is of importance to investigate the significance of a subset of covariates $W$ for the response $Y$ given covariates $Z$ in regression modeling. To this end, we propose a significance test for the partial mean independence problem based on machine learning methods and data splitting. The test statistic converges to the standard chi-squared distribution under the null hypothesis while it converges to a normal distribution under the fixed alternative hypothesis. Power enhancement and algorithm stability are also discussed. If the null hypothesis is rejected, we propose a partial Generalized Measure of Correlation (pGMC) to measure the partial mean dependence of $Y$ given $W$ after controlling for the nonlinear effect of $Z$. We present the appealing theoretical properties of the pGMC and establish the asymptotic normality of its estimator with the optimal root-$N$ convergence rate. Furthermore, the valid confidence interval for the pGMC is also derived. As an important special case when there are no conditional covariates $Z$, we introduce a new test of overall significance of covariates for the response in a model-free setting. Numerical studies and real data analysis are also conducted to compare with existing approaches and to demonstrate the validity and flexibility of our proposed procedures.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.12874&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Leheng Cai, Xu Guo, Wei Zhong</name></author><category term="stat.ME" /><summary type="html">It is of importance to investigate the significance of a subset of covariates $W$ for the response $Y$ given covariates $Z$ in regression modeling. To this end, we propose a significance test for the partial mean independence problem based on machine learning methods and data splitting. The test statistic converges to the standard chi-squared distribution under the null hypothesis while it converges to a normal distribution under the fixed alternative hypothesis. Power enhancement and algorithm stability are also discussed. If the null hypothesis is rejected, we propose a partial Generalized Measure of Correlation (pGMC) to measure the partial mean dependence of $Y$ given $W$ after controlling for the nonlinear effect of $Z$. We present the appealing theoretical properties of the pGMC and establish the asymptotic normality of its estimator with the optimal root-$N$ convergence rate. Furthermore, the valid confidence interval for the pGMC is also derived. As an important special case when there are no conditional covariates $Z$, we introduce a new test of overall significance of covariates for the response in a model-free setting. Numerical studies and real data analysis are also conducted to compare with existing approaches and to demonstrate the validity and flexibility of our proposed procedures.</summary></entry><entry><title type="html">The Impact of Stocks on Correlations between Crop Yields and Prices and on Revenue Insurance Premiums using Semiparametric Quantile Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/TheImpactofStocksonCorrelationsbetweenCropYieldsandPricesandonRevenueInsurancePremiumsusingSemiparametricQuantileRegression.html" rel="alternate" type="text/html" title="The Impact of Stocks on Correlations between Crop Yields and Prices and on Revenue Insurance Premiums using Semiparametric Quantile Regression" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/TheImpactofStocksonCorrelationsbetweenCropYieldsandPricesandonRevenueInsurancePremiumsusingSemiparametricQuantileRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/TheImpactofStocksonCorrelationsbetweenCropYieldsandPricesandonRevenueInsurancePremiumsusingSemiparametricQuantileRegression.html">&lt;p&gt;Crop yields and harvest prices are often considered to be negatively correlated, thus acting as a natural risk management hedge through stabilizing revenues. Storage theory gives reason to believe that the correlation is an increasing function of stocks carried over from previous years. Stock-conditioned second moments have implications for price movements during shortages and for hedging needs, while spatially varying yield-price correlation structures have implications for who benefits from commodity support policies. In this paper, we propose to use semi-parametric quantile regression (SQR) with penalized B-splines to estimate a stock-conditioned joint distribution of yield and price. The proposed method, validated through a comprehensive simulation study, enables sampling from the true joint distribution using SQR. Then it is applied to approximate stock-conditioned correlation and revenue insurance premium for both corn and soybeans in the United States. For both crops, Cornbelt core regions have more negative correlations than do peripheral regions. We find strong evidence that correlation becomes less negative as stocks increase. We also show that conditioning on stocks is important when calculating actuarially fair revenue insurance premiums. In particular, revenue insurance premiums in the Cornbelt core will be biased upward if the model for calculating premiums does not allow correlation to vary with stocks available. The stock-dependent correlation can be viewed as a form of tail dependence that, if unacknowledged, leads to mispricing of revenue insurance products.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.11805&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matthew Stuart, Cindy Yu, David A. Hennessy</name></author><category term="stat.ME" /><summary type="html">Crop yields and harvest prices are often considered to be negatively correlated, thus acting as a natural risk management hedge through stabilizing revenues. Storage theory gives reason to believe that the correlation is an increasing function of stocks carried over from previous years. Stock-conditioned second moments have implications for price movements during shortages and for hedging needs, while spatially varying yield-price correlation structures have implications for who benefits from commodity support policies. In this paper, we propose to use semi-parametric quantile regression (SQR) with penalized B-splines to estimate a stock-conditioned joint distribution of yield and price. The proposed method, validated through a comprehensive simulation study, enables sampling from the true joint distribution using SQR. Then it is applied to approximate stock-conditioned correlation and revenue insurance premium for both corn and soybeans in the United States. For both crops, Cornbelt core regions have more negative correlations than do peripheral regions. We find strong evidence that correlation becomes less negative as stocks increase. We also show that conditioning on stocks is important when calculating actuarially fair revenue insurance premiums. In particular, revenue insurance premiums in the Cornbelt core will be biased upward if the model for calculating premiums does not allow correlation to vary with stocks available. The stock-dependent correlation can be viewed as a form of tail dependence that, if unacknowledged, leads to mispricing of revenue insurance products.</summary></entry><entry><title type="html">The Impossibility of Fair LLMs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/TheImpossibilityofFairLLMs.html" rel="alternate" type="text/html" title="The Impossibility of Fair LLMs" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/TheImpossibilityofFairLLMs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/TheImpossibilityofFairLLMs.html">&lt;p&gt;The need for fair AI is increasingly clear in the era of general-purpose systems such as ChatGPT, Gemini, and other large language models (LLMs). However, the increasing complexity of human-AI interaction and its social impacts have raised questions of how fairness standards could be applied. Here, we review the technical frameworks that machine learning researchers have used to evaluate fairness, such as group fairness and fair representations, and find that their application to LLMs faces inherent limitations. We show that each framework either does not logically extend to LLMs or presents a notion of fairness that is intractable for LLMs, primarily due to the multitudes of populations affected, sensitive attributes, and use cases. To address these challenges, we develop guidelines for the more realistic goal of achieving fairness in particular use cases: the criticality of context, the responsibility of LLM developers, and the need for stakeholder participation in an iterative process of design and evaluation. Moreover, it may eventually be possible and even necessary to use the general-purpose capabilities of AI systems to address fairness challenges as a form of scalable AI-assisted alignment.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03198&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jacy Anthis, Kristian Lum, Michael Ekstrand, Avi Feller, Alexander D&apos;Amour, Chenhao Tan</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">The need for fair AI is increasingly clear in the era of general-purpose systems such as ChatGPT, Gemini, and other large language models (LLMs). However, the increasing complexity of human-AI interaction and its social impacts have raised questions of how fairness standards could be applied. Here, we review the technical frameworks that machine learning researchers have used to evaluate fairness, such as group fairness and fair representations, and find that their application to LLMs faces inherent limitations. We show that each framework either does not logically extend to LLMs or presents a notion of fairness that is intractable for LLMs, primarily due to the multitudes of populations affected, sensitive attributes, and use cases. To address these challenges, we develop guidelines for the more realistic goal of achieving fairness in particular use cases: the criticality of context, the responsibility of LLM developers, and the need for stakeholder participation in an iterative process of design and evaluation. Moreover, it may eventually be possible and even necessary to use the general-purpose capabilities of AI systems to address fairness challenges as a form of scalable AI-assisted alignment.</summary></entry><entry><title type="html">Variational Pseudo Marginal Methods for Jet Reconstruction in Particle Physics</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/VariationalPseudoMarginalMethodsforJetReconstructioninParticlePhysics.html" rel="alternate" type="text/html" title="Variational Pseudo Marginal Methods for Jet Reconstruction in Particle Physics" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/VariationalPseudoMarginalMethodsforJetReconstructioninParticlePhysics</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/VariationalPseudoMarginalMethodsforJetReconstructioninParticlePhysics.html">&lt;p&gt;Reconstructing jets, which provide vital insights into the properties and histories of subatomic particles produced in high-energy collisions, is a main problem in data analyses in collider physics. This intricate task deals with estimating the latent structure of a jet (binary tree) and involves parameters such as particle energy, momentum, and types. While Bayesian methods offer a natural approach for handling uncertainty and leveraging prior knowledge, they face significant challenges due to the super-exponential growth of potential jet topologies as the number of observed particles increases. To address this, we introduce a Combinatorial Sequential Monte Carlo approach for inferring jet latent structures. As a second contribution, we leverage the resulting estimator to develop a variational inference algorithm for parameter learning. Building on this, we introduce a variational family using a pseudo-marginal framework for a fully Bayesian treatment of all variables, unifying the generative model with the inference process. We illustrate our method’s effectiveness through experiments using data generated with a collider physics generative model, highlighting superior speed and accuracy across a range of tasks.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.03242&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hanming Yang, Antonio Khalil Moretti, Sebastian Macaluso, Philippe Chlenski, Christian A. Naesseth, Itsik Pe&apos;er</name></author><category term="stat.CO" /><summary type="html">Reconstructing jets, which provide vital insights into the properties and histories of subatomic particles produced in high-energy collisions, is a main problem in data analyses in collider physics. This intricate task deals with estimating the latent structure of a jet (binary tree) and involves parameters such as particle energy, momentum, and types. While Bayesian methods offer a natural approach for handling uncertainty and leveraging prior knowledge, they face significant challenges due to the super-exponential growth of potential jet topologies as the number of observed particles increases. To address this, we introduce a Combinatorial Sequential Monte Carlo approach for inferring jet latent structures. As a second contribution, we leverage the resulting estimator to develop a variational inference algorithm for parameter learning. Building on this, we introduce a variational family using a pseudo-marginal framework for a fully Bayesian treatment of all variables, unifying the generative model with the inference process. We illustrate our method’s effectiveness through experiments using data generated with a collider physics generative model, highlighting superior speed and accuracy across a range of tasks.</summary></entry><entry><title type="html">You Only Accept Samples Once: Fast, Self-Correcting Stochastic Variational Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/YouOnlyAcceptSamplesOnceFastSelfCorrectingStochasticVariationalInference.html" rel="alternate" type="text/html" title="You Only Accept Samples Once: Fast, Self-Correcting Stochastic Variational Inference" /><published>2024-06-06T00:00:00+00:00</published><updated>2024-06-06T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/YouOnlyAcceptSamplesOnceFastSelfCorrectingStochasticVariationalInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/06/YouOnlyAcceptSamplesOnceFastSelfCorrectingStochasticVariationalInference.html">&lt;p&gt;We introduce YOASOVI, an algorithm for performing fast, self-correcting stochastic optimization for Variational Inference (VI) on large Bayesian heirarchical models. To accomplish this, we take advantage of available information on the objective function used for stochastic VI at each iteration and replace regular Monte Carlo sampling with acceptance sampling. Rather than spend computational resources drawing and evaluating over a large sample for the gradient, we draw only one sample and accept it with probability proportional to the expected improvement in the objective. The following paper develops two versions of the algorithm: the first one based on a naive intuition, and another building up the algorithm as a Metropolis-type scheme. Empirical results based on simulations and benchmark datasets for multivariate Gaussian mixture models show that YOASOVI consistently converges faster (in clock time) and within better optimal neighborhoods than both regularized Monte Carlo and Quasi-Monte Carlo VI algorithms.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.02838&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Dominic B. Dayta</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">We introduce YOASOVI, an algorithm for performing fast, self-correcting stochastic optimization for Variational Inference (VI) on large Bayesian heirarchical models. To accomplish this, we take advantage of available information on the objective function used for stochastic VI at each iteration and replace regular Monte Carlo sampling with acceptance sampling. Rather than spend computational resources drawing and evaluating over a large sample for the gradient, we draw only one sample and accept it with probability proportional to the expected improvement in the objective. The following paper develops two versions of the algorithm: the first one based on a naive intuition, and another building up the algorithm as a Metropolis-type scheme. Empirical results based on simulations and benchmark datasets for multivariate Gaussian mixture models show that YOASOVI consistently converges faster (in clock time) and within better optimal neighborhoods than both regularized Monte Carlo and Quasi-Monte Carlo VI algorithms.</summary></entry></feed>