<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-03T07:13:22+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Fast and Accurate Numerical Method for the Left Tail of Sums of Independent Random Variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/AFastandAccurateNumericalMethodfortheLeftTailofSumsofIndependentRandomVariables.html" rel="alternate" type="text/html" title="A Fast and Accurate Numerical Method for the Left Tail of Sums of Independent Random Variables" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/AFastandAccurateNumericalMethodfortheLeftTailofSumsofIndependentRandomVariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/AFastandAccurateNumericalMethodfortheLeftTailofSumsofIndependentRandomVariables.html">&lt;p&gt;We present a flexible, deterministic numerical method for computing left-tail rare events of sums of non-negative, independent random variables. The method is based on iterative numerical integration of linear convolutions by means of Newtons-Cotes rules. The periodicity properties of convoluted densities combined with the Trapezoidal rule are exploited to produce a robust and efficient method, and the method is flexible in the sense that it can be applied to all kinds of non-negative continuous RVs. We present an error analysis and study the benefits of utilizing Newton-Cotes rules versus the fast Fourier transform (FFT) for numerical integration, showing that although there can be efficiency-benefits to using FFT, Newton-Cotes rules tend to preserve the relative error better, and indeed do so at an acceptable computational cost. Numerical studies on problems with both known and unknown rare-event probabilities showcase the method’s performance and support our theoretical findings.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01465&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nadhir Ben Rached, H{\aa}kon Hoel, Johannes Vincent Meo</name></author><category term="stat.CO" /><summary type="html">We present a flexible, deterministic numerical method for computing left-tail rare events of sums of non-negative, independent random variables. The method is based on iterative numerical integration of linear convolutions by means of Newtons-Cotes rules. The periodicity properties of convoluted densities combined with the Trapezoidal rule are exploited to produce a robust and efficient method, and the method is flexible in the sense that it can be applied to all kinds of non-negative continuous RVs. We present an error analysis and study the benefits of utilizing Newton-Cotes rules versus the fast Fourier transform (FFT) for numerical integration, showing that although there can be efficiency-benefits to using FFT, Newton-Cotes rules tend to preserve the relative error better, and indeed do so at an acceptable computational cost. Numerical studies on problems with both known and unknown rare-event probabilities showcase the method’s performance and support our theoretical findings.</summary></entry><entry><title type="html">A Model-Based Approach to Shot Charts Estimation in Basketball</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/AModelBasedApproachtoShotChartsEstimationinBasketball.html" rel="alternate" type="text/html" title="A Model-Based Approach to Shot Charts Estimation in Basketball" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/AModelBasedApproachtoShotChartsEstimationinBasketball</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/AModelBasedApproachtoShotChartsEstimationinBasketball.html">&lt;p&gt;Shot charts in basketball analytics provide an indispensable tool for evaluating players’ shooting performance by visually representing the distribution of field goal attempts across different court locations. However, conventional methods often overlook the bounded nature of the basketball court, leading to inaccurate representations, particularly along the boundaries and corners. In this paper, we propose a novel model-based approach to shot chart estimation and visualization that explicitly considers the physical boundaries of the basketball court. By employing Gaussian mixtures for bounded data, our methodology allows to obtain more accurate estimation of shot density distributions for both made and missed shots. Bayes’ rule is then applied to derive estimates for the probability of successful shooting from any given locations, and to identify the regions with the highest expected scores. To illustrate the efficacy of our proposal, we apply it to data from the 2022-23 NBA regular season, showing its usefulness through detailed analyses of shot patterns for two prominent players.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01182&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luca Scrucca, Dimitris Karlis</name></author><category term="stat.ME" /><summary type="html">Shot charts in basketball analytics provide an indispensable tool for evaluating players’ shooting performance by visually representing the distribution of field goal attempts across different court locations. However, conventional methods often overlook the bounded nature of the basketball court, leading to inaccurate representations, particularly along the boundaries and corners. In this paper, we propose a novel model-based approach to shot chart estimation and visualization that explicitly considers the physical boundaries of the basketball court. By employing Gaussian mixtures for bounded data, our methodology allows to obtain more accurate estimation of shot density distributions for both made and missed shots. Bayes’ rule is then applied to derive estimates for the probability of successful shooting from any given locations, and to identify the regions with the highest expected scores. To illustrate the efficacy of our proposal, we apply it to data from the 2022-23 NBA regular season, showing its usefulness through detailed analyses of shot patterns for two prominent players.</summary></entry><entry><title type="html">A mixed effects cosinor modelling framework for circadian gene expression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Amixedeffectscosinormodellingframeworkforcircadiangeneexpression.html" rel="alternate" type="text/html" title="A mixed effects cosinor modelling framework for circadian gene expression" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Amixedeffectscosinormodellingframeworkforcircadiangeneexpression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Amixedeffectscosinormodellingframeworkforcircadiangeneexpression.html">&lt;p&gt;The cosinor model is frequently used to represent gene expression given the 24 hour day-night cycle time at which a corresponding tissue sample is collected. However, the timing of many biological processes are based on individual-specific internal timing systems that are offset relative to day-night cycle time. When these offsets are unknown, they pose a challenge in performing statistical analyses with a cosinor model. To clarify, when sample collection times are mis-recorded, cosinor regression can yield attenuated parameter estimates, which would also attenuate test statistics. This attenuation bias would inflate type II error rates in identifying genes with oscillatory behavior. This paper proposes a heuristic method to account for unknown offsets when tissue samples are collected in a longitudinal design. Specifically, this method involves first estimating individual-specific cosinor models for each gene. The times of sample collection for that individual are then translated based on the estimated phase-shifts across every gene. Simulation studies confirm that this method mitigates bias in estimation and inference. Illustrations with real data from three circadian biology studies highlight that this method produces parameter estimates and inferences akin to those obtained when each individual’s offset is known.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01450&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael T. Gorczyca</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">The cosinor model is frequently used to represent gene expression given the 24 hour day-night cycle time at which a corresponding tissue sample is collected. However, the timing of many biological processes are based on individual-specific internal timing systems that are offset relative to day-night cycle time. When these offsets are unknown, they pose a challenge in performing statistical analyses with a cosinor model. To clarify, when sample collection times are mis-recorded, cosinor regression can yield attenuated parameter estimates, which would also attenuate test statistics. This attenuation bias would inflate type II error rates in identifying genes with oscillatory behavior. This paper proposes a heuristic method to account for unknown offsets when tissue samples are collected in a longitudinal design. Specifically, this method involves first estimating individual-specific cosinor models for each gene. The times of sample collection for that individual are then translated based on the estimated phase-shifts across every gene. Simulation studies confirm that this method mitigates bias in estimation and inference. Illustrations with real data from three circadian biology studies highlight that this method produces parameter estimates and inferences akin to those obtained when each individual’s offset is known.</summary></entry><entry><title type="html">Automating the Discovery of Partial Differential Equations in Dynamical Systems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/AutomatingtheDiscoveryofPartialDifferentialEquationsinDynamicalSystems.html" rel="alternate" type="text/html" title="Automating the Discovery of Partial Differential Equations in Dynamical Systems" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/AutomatingtheDiscoveryofPartialDifferentialEquationsinDynamicalSystems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/AutomatingtheDiscoveryofPartialDifferentialEquationsinDynamicalSystems.html">&lt;p&gt;Identifying partial differential equations (PDEs) from data is crucial for understanding the governing mechanisms of natural phenomena, yet it remains a challenging task. We present an extension to the ARGOS framework, ARGOS-RAL, which leverages sparse regression with the recurrent adaptive lasso to identify PDEs from limited prior knowledge automatically. Our method automates calculating partial derivatives, constructing a candidate library, and estimating a sparse model. We rigorously evaluate the performance of ARGOS-RAL in identifying canonical PDEs under various noise levels and sample sizes, demonstrating its robustness in handling noisy and non-uniformly distributed data. We also test the algorithm’s performance on datasets consisting solely of random noise to simulate scenarios with severely compromised data quality. Our results show that ARGOS-RAL effectively and reliably identifies the underlying PDEs from data, outperforming the sequential threshold ridge regression method in most cases. We highlight the potential of combining statistical methods, machine learning, and dynamical systems theory to automatically discover governing equations from collected data, streamlining the scientific modeling process.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.16444&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Weizhen Li, Rui Carvalho</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Identifying partial differential equations (PDEs) from data is crucial for understanding the governing mechanisms of natural phenomena, yet it remains a challenging task. We present an extension to the ARGOS framework, ARGOS-RAL, which leverages sparse regression with the recurrent adaptive lasso to identify PDEs from limited prior knowledge automatically. Our method automates calculating partial derivatives, constructing a candidate library, and estimating a sparse model. We rigorously evaluate the performance of ARGOS-RAL in identifying canonical PDEs under various noise levels and sample sizes, demonstrating its robustness in handling noisy and non-uniformly distributed data. We also test the algorithm’s performance on datasets consisting solely of random noise to simulate scenarios with severely compromised data quality. Our results show that ARGOS-RAL effectively and reliably identifies the underlying PDEs from data, outperforming the sequential threshold ridge regression method in most cases. We highlight the potential of combining statistical methods, machine learning, and dynamical systems theory to automatically discover governing equations from collected data, streamlining the scientific modeling process.</summary></entry><entry><title type="html">Causal Discovery via Conditional Independence Testing with Proxy Variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/CausalDiscoveryviaConditionalIndependenceTestingwithProxyVariables.html" rel="alternate" type="text/html" title="Causal Discovery via Conditional Independence Testing with Proxy Variables" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/CausalDiscoveryviaConditionalIndependenceTestingwithProxyVariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/CausalDiscoveryviaConditionalIndependenceTestingwithProxyVariables.html">&lt;p&gt;Distinguishing causal connections from correlations is important in many scenarios. However, the presence of unobserved variables, such as the latent confounder, can introduce bias in conditional independence testing commonly employed in constraint-based causal discovery for identifying causal relations. To address this issue, existing methods introduced proxy variables to adjust for the bias caused by unobserveness. However, these methods were either limited to categorical variables or relied on strong parametric assumptions for identification. In this paper, we propose a novel hypothesis-testing procedure that can effectively examine the existence of the causal relationship over continuous variables, without any parametric constraint. Our procedure is based on discretization, which under completeness conditions, is able to asymptotically establish a linear equation whose coefficient vector is identifiable under the causal null hypothesis. Based on this, we introduce our test statistic and demonstrate its asymptotic level and power. We validate the effectiveness of our procedure using both synthetic and real-world data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.05281&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mingzhou Liu, Xinwei Sun, Yu Qiao, Yizhou Wang</name></author><category term="stat.ME" /><summary type="html">Distinguishing causal connections from correlations is important in many scenarios. However, the presence of unobserved variables, such as the latent confounder, can introduce bias in conditional independence testing commonly employed in constraint-based causal discovery for identifying causal relations. To address this issue, existing methods introduced proxy variables to adjust for the bias caused by unobserveness. However, these methods were either limited to categorical variables or relied on strong parametric assumptions for identification. In this paper, we propose a novel hypothesis-testing procedure that can effectively examine the existence of the causal relationship over continuous variables, without any parametric constraint. Our procedure is based on discretization, which under completeness conditions, is able to asymptotically establish a linear equation whose coefficient vector is identifiable under the causal null hypothesis. Based on this, we introduce our test statistic and demonstrate its asymptotic level and power. We validate the effectiveness of our procedure using both synthetic and real-world data.</summary></entry><entry><title type="html">Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/ConformalDecisionTheorySafeAutonomousDecisionsfromImperfectPredictions.html" rel="alternate" type="text/html" title="Conformal Decision Theory: Safe Autonomous Decisions from Imperfect Predictions" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/ConformalDecisionTheorySafeAutonomousDecisionsfromImperfectPredictions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/ConformalDecisionTheorySafeAutonomousDecisionsfromImperfectPredictions.html">&lt;p&gt;We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to exhibit high throughput and low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans, automated stock trading, and robot manufacturing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.05921&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jordan Lekeufack, Anastasios N. Angelopoulos, Andrea Bajcsy, Michael I. Jordan, Jitendra Malik</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to exhibit high throughput and low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans, automated stock trading, and robot manufacturing.</summary></entry><entry><title type="html">Demistifying Inference after Adaptive Experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/DemistifyingInferenceafterAdaptiveExperiments.html" rel="alternate" type="text/html" title="Demistifying Inference after Adaptive Experiments" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/DemistifyingInferenceafterAdaptiveExperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/DemistifyingInferenceafterAdaptiveExperiments.html">&lt;p&gt;Adaptive experiments such as multi-arm bandits adapt the treatment-allocation policy and/or the decision to stop the experiment to the data observed so far. This has the potential to improve outcomes for study participants within the experiment, to improve the chance of identifying best treatments after the experiment, and to avoid wasting data. Seen as an experiment (rather than just a continually optimizing system) it is still desirable to draw statistical inferences with frequentist guarantees. The concentration inequalities and union bounds that generally underlie adaptive experimentation algorithms can yield overly conservative inferences, but at the same time the asymptotic normality we would usually appeal to in non-adaptive settings can be imperiled by adaptivity. In this article we aim to explain why, how, and when adaptivity is in fact an issue for inference and, when it is, understand the various ways to fix it: reweighting to stabilize variances and recover asymptotic normality, always-valid inference based on joint normality of an asymptotic limiting sequence, and characterizing and inverting the non-normal distributions induced by adaptivity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01281&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aurélien Bibaut, Nathan Kallus</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">Adaptive experiments such as multi-arm bandits adapt the treatment-allocation policy and/or the decision to stop the experiment to the data observed so far. This has the potential to improve outcomes for study participants within the experiment, to improve the chance of identifying best treatments after the experiment, and to avoid wasting data. Seen as an experiment (rather than just a continually optimizing system) it is still desirable to draw statistical inferences with frequentist guarantees. The concentration inequalities and union bounds that generally underlie adaptive experimentation algorithms can yield overly conservative inferences, but at the same time the asymptotic normality we would usually appeal to in non-adaptive settings can be imperiled by adaptivity. In this article we aim to explain why, how, and when adaptivity is in fact an issue for inference and, when it is, understand the various ways to fix it: reweighting to stabilize variances and recover asymptotic normality, always-valid inference based on joint normality of an asymptotic limiting sequence, and characterizing and inverting the non-normal distributions induced by adaptivity.</summary></entry><entry><title type="html">Diffusive Gibbs Sampling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/DiffusiveGibbsSampling.html" rel="alternate" type="text/html" title="Diffusive Gibbs Sampling" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/DiffusiveGibbsSampling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/DiffusiveGibbsSampling.html">&lt;p&gt;The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. A novel Metropolis-within-Gibbs scheme is proposed to enhance mixing in the denoising sampling step. DiGS exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering, attaining substantially improved performance across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.03008&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wenlin Chen, Mingtian Zhang, Brooks Paige, José Miguel Hernández-Lobato, David Barber</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. A novel Metropolis-within-Gibbs scheme is proposed to enhance mixing in the denoising sampling step. DiGS exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering, attaining substantially improved performance across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.</summary></entry><entry><title type="html">Dynamic Local Average Treatment Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/DynamicLocalAverageTreatmentEffects.html" rel="alternate" type="text/html" title="Dynamic Local Average Treatment Effects" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/DynamicLocalAverageTreatmentEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/DynamicLocalAverageTreatmentEffects.html">&lt;p&gt;We consider Dynamic Treatment Regimes (DTRs) with one sided non-compliance that arise in applications such as digital recommendations and adaptive medical trials. These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes. Importantly, individuals may choose to (not) comply with a treatment recommendation, whenever it is made available to them, based on unobserved confounding factors. We provide non-parametric identification, estimation, and inference for Dynamic Local Average Treatment Effects, which are expected values of multi-period treatment contrasts among appropriately defined complier subpopulations. Under standard assumptions in the Instrumental Variable and DTR literature, we show that one can identify local average effects of contrasts that correspond to offering treatment at any single time step. Under an additional cross-period effect-compliance independence assumption, which is satisfied in Staggered Adoption settings and a generalization of them, which we define as Staggered Compliance settings, we identify local average treatment effects of treating in multiple time periods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01463&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ravi B. Sojitra, Vasilis Syrgkanis</name></author><category term="stat.ME" /><summary type="html">We consider Dynamic Treatment Regimes (DTRs) with one sided non-compliance that arise in applications such as digital recommendations and adaptive medical trials. These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes. Importantly, individuals may choose to (not) comply with a treatment recommendation, whenever it is made available to them, based on unobserved confounding factors. We provide non-parametric identification, estimation, and inference for Dynamic Local Average Treatment Effects, which are expected values of multi-period treatment contrasts among appropriately defined complier subpopulations. Under standard assumptions in the Instrumental Variable and DTR literature, we show that one can identify local average effects of contrasts that correspond to offering treatment at any single time step. Under an additional cross-period effect-compliance independence assumption, which is satisfied in Staggered Adoption settings and a generalization of them, which we define as Staggered Compliance settings, we identify local average treatment effects of treating in multiple time periods.</summary></entry><entry><title type="html">Generalised envelope spectrum-based signal-to-noise objectives: Formulation, optimisation and application for gear fault detection under time-varying speed conditions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/GeneralisedenvelopespectrumbasedsignaltonoiseobjectivesFormulationoptimisationandapplicationforgearfaultdetectionundertimevaryingspeedconditions.html" rel="alternate" type="text/html" title="Generalised envelope spectrum-based signal-to-noise objectives: Formulation, optimisation and application for gear fault detection under time-varying speed conditions" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/GeneralisedenvelopespectrumbasedsignaltonoiseobjectivesFormulationoptimisationandapplicationforgearfaultdetectionundertimevaryingspeedconditions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/GeneralisedenvelopespectrumbasedsignaltonoiseobjectivesFormulationoptimisationandapplicationforgearfaultdetectionundertimevaryingspeedconditions.html">&lt;p&gt;In vibration-based condition monitoring, optimal filter design improves fault detection by enhancing weak fault signatures within vibration signals. This process involves optimising a derived objective function from a defined objective. The objectives are often based on proxy health indicators to determine the filter’s parameters. However, these indicators can be compromised by irrelevant extraneous signal components and fluctuating operational conditions, affecting the filter’s efficacy. Fault detection primarily uses the fault component’s prominence in the squared envelope spectrum, quantified by a squared envelope spectrum-based signal-to-noise ratio. New optimal filter objective functions are derived from the proposed generalised envelope spectrum-based signal-to-noise objective for machines operating under variable speed conditions. Instead of optimising proxy health indicators, the optimal filter coefficients of the formulation directly maximise the squared envelope spectrum-based signal-to-noise ratio over targeted frequency bands using standard gradient-based optimisers. Four derived objective functions from the proposed objective effectively outperform five prominent methods in tests on three experimental datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00727&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Stephan Schmidt, Daniel N. Wilke, Konstantinos C. Gryllias</name></author><category term="stat.ME" /><summary type="html">In vibration-based condition monitoring, optimal filter design improves fault detection by enhancing weak fault signatures within vibration signals. This process involves optimising a derived objective function from a defined objective. The objectives are often based on proxy health indicators to determine the filter’s parameters. However, these indicators can be compromised by irrelevant extraneous signal components and fluctuating operational conditions, affecting the filter’s efficacy. Fault detection primarily uses the fault component’s prominence in the squared envelope spectrum, quantified by a squared envelope spectrum-based signal-to-noise ratio. New optimal filter objective functions are derived from the proposed generalised envelope spectrum-based signal-to-noise objective for machines operating under variable speed conditions. Instead of optimising proxy health indicators, the optimal filter coefficients of the formulation directly maximise the squared envelope spectrum-based signal-to-noise ratio over targeted frequency bands using standard gradient-based optimisers. Four derived objective functions from the proposed objective effectively outperform five prominent methods in tests on three experimental datasets.</summary></entry><entry><title type="html">Implementing Bayesian inference on a stochastic CO2-based grey-box model for assessing indoor air quality in Canadian primary schools</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/ImplementingBayesianinferenceonastochasticCO2basedgreyboxmodelforassessingindoorairqualityinCanadianprimaryschools.html" rel="alternate" type="text/html" title="Implementing Bayesian inference on a stochastic CO2-based grey-box model for assessing indoor air quality in Canadian primary schools" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/ImplementingBayesianinferenceonastochasticCO2basedgreyboxmodelforassessingindoorairqualityinCanadianprimaryschools</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/ImplementingBayesianinferenceonastochasticCO2basedgreyboxmodelforassessingindoorairqualityinCanadianprimaryschools.html">&lt;p&gt;The COVID-19 pandemic brought global attention to indoor air quality (IAQ), which is intrinsically linked to clean air change rates. Estimating the air change rate in indoor environments, however, remains challenging. It is primarily due to the uncertainties associated with the air change rate estimation, such as pollutant generation rates, dynamics including weather and occupancies, and the limitations of deterministic approaches to accommodate these factors. In this study, Bayesian inference was implemented on a stochastic CO2-based grey-box model to infer modeled parameters and quantify uncertainties. The accuracy and robustness of the ventilation rate and CO2 emission rate estimated by the model were confirmed with CO2 tracer gas experiments conducted in an airtight chamber. Both prior and posterior predictive checks (PPC) were performed to demonstrate the advantage of this approach. In addition, uncertainties in real-life contexts were quantified with an incremental variance {\sigma} for the Wiener process. This approach was later applied to evaluate the ventilation conditions within two primary school classrooms in Montreal. The Equivalent Clean Airflow Rate (ECAi) was calculated following ASHRAE 241, and an insufficient clean air supply within both classrooms was identified. A supplement of 800 cfm clear air delivery rate (CADR) from air-cleaning devices is recommended for a sufficient ECAi. Finally, steady-state CO2 thresholds (Climit, Ctarget, and Cideal) were carried out to indicate when ECAi requirements could be achieved under various mitigation strategies, such as portable air cleaners and in-room ultraviolet light, with CADR values ranging from 200 to 1000 cfm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00582&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shujie Yan , Jiwei Zou , Chang Shu , Justin Berquist , Vincent Brochu , Marc Veillette , Danlin Hou , Caroline Duchaine ,  Liang ,  Zhou ,  Zhiqiang ,  Zhai ,  Liangzhu ,  Wang</name></author><category term="stat.AP" /><summary type="html">The COVID-19 pandemic brought global attention to indoor air quality (IAQ), which is intrinsically linked to clean air change rates. Estimating the air change rate in indoor environments, however, remains challenging. It is primarily due to the uncertainties associated with the air change rate estimation, such as pollutant generation rates, dynamics including weather and occupancies, and the limitations of deterministic approaches to accommodate these factors. In this study, Bayesian inference was implemented on a stochastic CO2-based grey-box model to infer modeled parameters and quantify uncertainties. The accuracy and robustness of the ventilation rate and CO2 emission rate estimated by the model were confirmed with CO2 tracer gas experiments conducted in an airtight chamber. Both prior and posterior predictive checks (PPC) were performed to demonstrate the advantage of this approach. In addition, uncertainties in real-life contexts were quantified with an incremental variance {\sigma} for the Wiener process. This approach was later applied to evaluate the ventilation conditions within two primary school classrooms in Montreal. The Equivalent Clean Airflow Rate (ECAi) was calculated following ASHRAE 241, and an insufficient clean air supply within both classrooms was identified. A supplement of 800 cfm clear air delivery rate (CADR) from air-cleaning devices is recommended for a sufficient ECAi. Finally, steady-state CO2 thresholds (Climit, Ctarget, and Cideal) were carried out to indicate when ECAi requirements could be achieved under various mitigation strategies, such as portable air cleaners and in-room ultraviolet light, with CADR values ranging from 200 to 1000 cfm.</summary></entry><entry><title type="html">Individual-level models of disease transmission incorporating piecewise spatial risk functions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Individuallevelmodelsofdiseasetransmissionincorporatingpiecewisespatialriskfunctions.html" rel="alternate" type="text/html" title="Individual-level models of disease transmission incorporating piecewise spatial risk functions" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Individuallevelmodelsofdiseasetransmissionincorporatingpiecewisespatialriskfunctions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Individuallevelmodelsofdiseasetransmissionincorporatingpiecewisespatialriskfunctions.html">&lt;p&gt;Modelling epidemics is crucial for understanding the emergence, transmission, impact and control of diseases. Spatial individual-level models (ILMs) that account for population heterogeneity are a useful tool, accounting for factors such as location, vaccination status and genetic information. Parametric forms for spatial risk functions, or kernels, are often used, but rely on strong assumptions about underlying transmission mechanisms. Here, we propose a class of non-parametric spatial disease transmission model, fitted within a Bayesian Markov chain Monte Carlo (MCMC) framework, allowing for more flexible assumptions when estimating the effect on spatial distance and infection risk. We focus upon two specific forms of non-parametric spatial infection kernel: piecewise constant and piecewise linear. Although these are relatively simple forms, we find them effective. The performance of these models is examined using simulated data, including under circumstances of model misspecification, and then applied to data from the UK 2001 foot-and-mouth disease.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00835&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chinmoy Roy Rahul, Rob Deardon</name></author><category term="stat.CO" /><summary type="html">Modelling epidemics is crucial for understanding the emergence, transmission, impact and control of diseases. Spatial individual-level models (ILMs) that account for population heterogeneity are a useful tool, accounting for factors such as location, vaccination status and genetic information. Parametric forms for spatial risk functions, or kernels, are often used, but rely on strong assumptions about underlying transmission mechanisms. Here, we propose a class of non-parametric spatial disease transmission model, fitted within a Bayesian Markov chain Monte Carlo (MCMC) framework, allowing for more flexible assumptions when estimating the effect on spatial distance and infection risk. We focus upon two specific forms of non-parametric spatial infection kernel: piecewise constant and piecewise linear. Although these are relatively simple forms, we find them effective. The performance of these models is examined using simulated data, including under circumstances of model misspecification, and then applied to data from the UK 2001 foot-and-mouth disease.</summary></entry><entry><title type="html">Integrating socioeconomic and geographic data to enhance infectious disease prediction in Brazilian cities</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/IntegratingsocioeconomicandgeographicdatatoenhanceinfectiousdiseasepredictioninBraziliancities.html" rel="alternate" type="text/html" title="Integrating socioeconomic and geographic data to enhance infectious disease prediction in Brazilian cities" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/IntegratingsocioeconomicandgeographicdatatoenhanceinfectiousdiseasepredictioninBraziliancities</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/IntegratingsocioeconomicandgeographicdatatoenhanceinfectiousdiseasepredictioninBraziliancities.html">&lt;p&gt;Supervised machine learning models and public surveillance data has been employed for infectious disease forecasting in many settings. These models leverage various data sources capturing drivers of disease spread, such as climate conditions or human behavior. However, few models have incorporated the organizational structure of different geographic locations for forecasting. Traveling waves of seasonal outbreaks have been reported for dengue, influenza, and other infectious diseases, and many of the drivers of infectious disease dynamics may be shared across different cities, either due to their geographic or socioeconomic proximity. In this study, we developed a machine learning model to predict case counts of four infectious diseases across Brazilian cities one week ahead by incorporating information from related cities. We compared selecting related cities using both geographic distance and GDP per capita. Incorporating information from geographically proximate cities improved predictive performance for two of the four diseases, specifically COVID-19 and Zika. We also discuss the impact on forecasts in the presence of anomalous contagion patterns and the limitations of the proposed methodology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01422&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luiza Lober, Kirstin O. Roster, Francisco A. Rodrigues</name></author><category term="stat.AP" /><summary type="html">Supervised machine learning models and public surveillance data has been employed for infectious disease forecasting in many settings. These models leverage various data sources capturing drivers of disease spread, such as climate conditions or human behavior. However, few models have incorporated the organizational structure of different geographic locations for forecasting. Traveling waves of seasonal outbreaks have been reported for dengue, influenza, and other infectious diseases, and many of the drivers of infectious disease dynamics may be shared across different cities, either due to their geographic or socioeconomic proximity. In this study, we developed a machine learning model to predict case counts of four infectious diseases across Brazilian cities one week ahead by incorporating information from related cities. We compared selecting related cities using both geographic distance and GDP per capita. Incorporating information from geographically proximate cities improved predictive performance for two of the four diseases, specifically COVID-19 and Zika. We also discuss the impact on forecasts in the presence of anomalous contagion patterns and the limitations of the proposed methodology.</summary></entry><entry><title type="html">Investigating the causal effects of multiple treatments using longitudinal data: a simulation study</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Investigatingthecausaleffectsofmultipletreatmentsusinglongitudinaldataasimulationstudy.html" rel="alternate" type="text/html" title="Investigating the causal effects of multiple treatments using longitudinal data: a simulation study" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Investigatingthecausaleffectsofmultipletreatmentsusinglongitudinaldataasimulationstudy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Investigatingthecausaleffectsofmultipletreatmentsusinglongitudinaldataasimulationstudy.html">&lt;p&gt;Many clinical questions involve estimating the effects of multiple treatments using observational data. When using longitudinal data, the interest is often in the effect of treatment strategies that involve sustaining treatment over time. This requires causal inference methods appropriate for handling multiple treatments and time-dependent confounding. Robins Generalised methods (g-methods) are a family of methods which can deal with time-dependent confounding and some of these have been extended to situations with multiple treatments, although there are currently no studies comparing different methods in this setting. We show how five g-methods (inverse-probability-of-treatment weighted estimation of marginal structural models, g-formula, g-estimation, censoring and weighting, and a sequential trials approach) can be extended to situations with multiple treatments, compare their performances in a simulation study, and demonstrate their application with an example using data from the UK CF Registry.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01110&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Emily Granger, Gwyneth Davies, Ruth H. Keogh</name></author><category term="stat.ME" /><summary type="html">Many clinical questions involve estimating the effects of multiple treatments using observational data. When using longitudinal data, the interest is often in the effect of treatment strategies that involve sustaining treatment over time. This requires causal inference methods appropriate for handling multiple treatments and time-dependent confounding. Robins Generalised methods (g-methods) are a family of methods which can deal with time-dependent confounding and some of these have been extended to situations with multiple treatments, although there are currently no studies comparing different methods in this setting. We show how five g-methods (inverse-probability-of-treatment weighted estimation of marginal structural models, g-formula, g-estimation, censoring and weighting, and a sequential trials approach) can be extended to situations with multiple treatments, compare their performances in a simulation study, and demonstrate their application with an example using data from the UK CF Registry.</summary></entry><entry><title type="html">Overcoming model uncertainty – how equivalence tests can benefit from model averaging</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Overcomingmodeluncertaintyhowequivalencetestscanbenefitfrommodelaveraging.html" rel="alternate" type="text/html" title="Overcoming model uncertainty – how equivalence tests can benefit from model averaging" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Overcomingmodeluncertaintyhowequivalencetestscanbenefitfrommodelaveraging</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Overcomingmodeluncertaintyhowequivalencetestscanbenefitfrommodelaveraging.html">&lt;p&gt;A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them. In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures. In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00827&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Niklas Hagemann, Kathrin Möllenhoff</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">A common problem in numerous research areas, particularly in clinical trials, is to test whether the effect of an explanatory variable on an outcome variable is equivalent across different groups. In practice, these tests are frequently used to compare the effect between patient groups, e.g. based on gender, age or treatments. Equivalence is usually assessed by testing whether the difference between the groups does not exceed a pre-specified equivalence threshold. Classical approaches are based on testing the equivalence of single quantities, e.g. the mean, the area under the curve (AUC) or other values of interest. However, when differences depending on a particular covariate are observed, these approaches can turn out to be not very accurate. Instead, whole regression curves over the entire covariate range, describing for instance the time window or a dose range, are considered and tests are based on a suitable distance measure of two such curves, as, for example, the maximum absolute distance between them. In this regard, a key assumption is that the true underlying regression models are known, which is rarely the case in practice. However, misspecification can lead to severe problems as inflated type I errors or, on the other hand, conservative test procedures. In this paper, we propose a solution to this problem by introducing a flexible extension of such an equivalence test using model averaging in order to overcome this assumption and making the test applicable under model uncertainty. Precisely, we introduce model averaging based on smooth AIC weights and we propose a testing procedure which makes use of the duality between confidence intervals and hypothesis testing. We demonstrate the validity of our approach by means of a simulation study and demonstrate the practical relevance of the approach considering a time-response case study with toxicological gene expression data.</summary></entry><entry><title type="html">Pricing Catastrophe Bonds – A Probabilistic Machine Learning Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/PricingCatastropheBondsAProbabilisticMachineLearningApproach.html" rel="alternate" type="text/html" title="Pricing Catastrophe Bonds – A Probabilistic Machine Learning Approach" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/PricingCatastropheBondsAProbabilisticMachineLearningApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/PricingCatastropheBondsAProbabilisticMachineLearningApproach.html">&lt;p&gt;This paper proposes a probabilistic machine learning method to price catastrophe (CAT) bonds in the primary market. The proposed method combines machine-learning-based predictive models with Conformal Prediction, an innovative algorithm that generates distribution-free probabilistic forecasts for CAT bond prices. Using primary market CAT bond transaction records between January 1999 and March 2021, the proposed method is found to be more robust and yields more accurate predictions of the bond spreads than traditional regression-based methods. Furthermore, the proposed method generates more informative prediction intervals than linear regression and identifies important nonlinear relationships between various risk factors and bond spreads, suggesting that linear regressions could misestimate the bond spreads. Overall, this paper demonstrates the potential of machine learning methods in improving the pricing of CAT bonds.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00697&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xiaowei Chen, Hong Li, Yufan Lu, Rui Zhou</name></author><category term="stat.AP" /><summary type="html">This paper proposes a probabilistic machine learning method to price catastrophe (CAT) bonds in the primary market. The proposed method combines machine-learning-based predictive models with Conformal Prediction, an innovative algorithm that generates distribution-free probabilistic forecasts for CAT bond prices. Using primary market CAT bond transaction records between January 1999 and March 2021, the proposed method is found to be more robust and yields more accurate predictions of the bond spreads than traditional regression-based methods. Furthermore, the proposed method generates more informative prediction intervals than linear regression and identifies important nonlinear relationships between various risk factors and bond spreads, suggesting that linear regressions could misestimate the bond spreads. Overall, this paper demonstrates the potential of machine learning methods in improving the pricing of CAT bonds.</summary></entry><entry><title type="html">Quantification of vaccine waning as a challenge effect</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Quantificationofvaccinewaningasachallengeeffect.html" rel="alternate" type="text/html" title="Quantification of vaccine waning as a challenge effect" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Quantificationofvaccinewaningasachallengeeffect</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Quantificationofvaccinewaningasachallengeeffect.html">&lt;p&gt;Knowing whether vaccine protection wanes over time is important for health policy and drug development. However, quantifying waning effects is difficult. A simple contrast of vaccine efficacy at two different times compares different populations of individuals: those who were uninfected at the first time versus those who remain uninfected until the second time. Thus, the contrast of vaccine efficacy at early and late times can not be interpreted as a causal effect. We propose to quantify vaccine waning using the challenge effect, which is a contrast of outcomes under controlled exposures to the infectious agent following vaccination. We identify sharp bounds on the challenge effect under non-parametric assumptions that are broadly applicable in vaccine trials using routinely collected data. We demonstrate that the challenge effect can differ substantially from the conventional vaccine efficacy due to depletion of susceptible individuals from the risk set over time. Finally, we apply the methods to derive bounds on the waning of the BNT162b2 COVID-19 vaccine using data from a placebo-controlled randomized trial. Our estimates of the challenge effect suggest waning protection after 2 months beyond administration of the second vaccine dose.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01336&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matias Janvin, Mats J. Stensrud</name></author><category term="stat.ME" /><summary type="html">Knowing whether vaccine protection wanes over time is important for health policy and drug development. However, quantifying waning effects is difficult. A simple contrast of vaccine efficacy at two different times compares different populations of individuals: those who were uninfected at the first time versus those who remain uninfected until the second time. Thus, the contrast of vaccine efficacy at early and late times can not be interpreted as a causal effect. We propose to quantify vaccine waning using the challenge effect, which is a contrast of outcomes under controlled exposures to the infectious agent following vaccination. We identify sharp bounds on the challenge effect under non-parametric assumptions that are broadly applicable in vaccine trials using routinely collected data. We demonstrate that the challenge effect can differ substantially from the conventional vaccine efficacy due to depletion of susceptible individuals from the risk set over time. Finally, we apply the methods to derive bounds on the waning of the BNT162b2 COVID-19 vaccine using data from a placebo-controlled randomized trial. Our estimates of the challenge effect suggest waning protection after 2 months beyond administration of the second vaccine dose.</summary></entry><entry><title type="html">Random Pareto front surfaces</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/RandomParetofrontsurfaces.html" rel="alternate" type="text/html" title="Random Pareto front surfaces" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/RandomParetofrontsurfaces</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/RandomParetofrontsurfaces.html">&lt;p&gt;The Pareto front of a set of vectors is the subset which is comprised solely of all of the best trade-off points. By interpolating this subset, we obtain the optimal trade-off surface. In this work, we prove a very useful result which states that all Pareto front surfaces can be explicitly parametrised using polar coordinates. In particular, our polar parametrisation result tells us that we can fully characterise any Pareto front surface using the length function, which is a scalar-valued function that returns the projected length along any positive radial direction. Consequently, by exploiting this representation, we show how it is possible to generalise many useful concepts from linear algebra, probability and statistics, and decision theory to function over the space of Pareto front surfaces. Notably, we focus our attention on the stochastic setting where the Pareto front surface itself is a stochastic process. Among other things, we showcase how it is possible to define and estimate many statistical quantities of interest such as the expectation, covariance and quantile of any Pareto front surface distribution. As a motivating example, we investigate how these statistics can be used within a design of experiments setting, where the goal is to both infer and use the Pareto front surface distribution in order to make effective decisions. Besides this, we also illustrate how these Pareto front ideas can be used within the context of extreme value theory. Finally, as a numerical example, we applied some of our new methodology on a real-world air pollution data set.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01404&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ben Tu, Nikolas Kantas, Robert M. Lee, Behrang Shafei</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">The Pareto front of a set of vectors is the subset which is comprised solely of all of the best trade-off points. By interpolating this subset, we obtain the optimal trade-off surface. In this work, we prove a very useful result which states that all Pareto front surfaces can be explicitly parametrised using polar coordinates. In particular, our polar parametrisation result tells us that we can fully characterise any Pareto front surface using the length function, which is a scalar-valued function that returns the projected length along any positive radial direction. Consequently, by exploiting this representation, we show how it is possible to generalise many useful concepts from linear algebra, probability and statistics, and decision theory to function over the space of Pareto front surfaces. Notably, we focus our attention on the stochastic setting where the Pareto front surface itself is a stochastic process. Among other things, we showcase how it is possible to define and estimate many statistical quantities of interest such as the expectation, covariance and quantile of any Pareto front surface distribution. As a motivating example, we investigate how these statistics can be used within a design of experiments setting, where the goal is to both infer and use the Pareto front surface distribution in order to make effective decisions. Besides this, we also illustrate how these Pareto front ideas can be used within the context of extreme value theory. Finally, as a numerical example, we applied some of our new methodology on a real-world air pollution data set.</summary></entry><entry><title type="html">Robust inference of causality in high-dimensional dynamical processes from the Information Imbalance of distance ranks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/RobustinferenceofcausalityinhighdimensionaldynamicalprocessesfromtheInformationImbalanceofdistanceranks.html" rel="alternate" type="text/html" title="Robust inference of causality in high-dimensional dynamical processes from the Information Imbalance of distance ranks" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/RobustinferenceofcausalityinhighdimensionaldynamicalprocessesfromtheInformationImbalanceofdistanceranks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/RobustinferenceofcausalityinhighdimensionaldynamicalprocessesfromtheInformationImbalanceofdistanceranks.html">&lt;p&gt;We introduce an approach which allows detecting causal relationships between variables for which the time evolution is available. Causality is assessed by a variational scheme based on the Information Imbalance of distance ranks, a statistical test capable of inferring the relative information content of different distance measures. We test whether the predictability of a putative driven system Y can be improved by incorporating information from a potential driver system X, without explicitly modeling the underlying dynamics and without the need to compute probability densities of the dynamic variables. This framework makes causality detection possible even between high-dimensional systems where only few of the variables are known or measured. Benchmark tests on coupled chaotic dynamical systems demonstrate that our approach outperforms other model-free causality detection methods, successfully handling both unidirectional and bidirectional couplings. We also show that the method can be used to robustly detect causality in human electroencephalography data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.10817&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Vittorio Del Tatto, Gianfranco Fortunato, Domenica Bueti, Alessandro Laio</name></author><category term="stat.ME" /><summary type="html">We introduce an approach which allows detecting causal relationships between variables for which the time evolution is available. Causality is assessed by a variational scheme based on the Information Imbalance of distance ranks, a statistical test capable of inferring the relative information content of different distance measures. We test whether the predictability of a putative driven system Y can be improved by incorporating information from a potential driver system X, without explicitly modeling the underlying dynamics and without the need to compute probability densities of the dynamic variables. This framework makes causality detection possible even between high-dimensional systems where only few of the variables are known or measured. Benchmark tests on coupled chaotic dynamical systems demonstrate that our approach outperforms other model-free causality detection methods, successfully handling both unidirectional and bidirectional couplings. We also show that the method can be used to robustly detect causality in human electroencephalography data.</summary></entry><entry><title type="html">Scalable network reconstruction in subquadratic time</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Scalablenetworkreconstructioninsubquadratictime.html" rel="alternate" type="text/html" title="Scalable network reconstruction in subquadratic time" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Scalablenetworkreconstructioninsubquadratictime</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/Scalablenetworkreconstructioninsubquadratictime.html">&lt;p&gt;Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings – typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $\Omega(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that significantly outperforms this quadratic baseline. Our algorithm relies on a stochastic second neighbor search (Dong et al., 2011) that produces the best edge candidates with high probability, thus bypassing an exhaustive quadratic search. If we rely on the conjecture that the second-neighbor search finishes in log-linear time (Baron &amp;amp; Darling, 2020; 2022), we demonstrate theoretically that our algorithm finishes in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. In practice, we show that our algorithm achieves a performance that is many orders of magnitude faster than the quadratic baseline – in a manner consistent with our theoretical analysis – allows for easy parallelization, and thus enables the reconstruction of networks with hundreds of thousands and even millions of nodes and edges.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.01404&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tiago P. Peixoto</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">Network reconstruction consists in determining the unobserved pairwise couplings between $N$ nodes given only observational data on the resulting behavior that is conditioned on those couplings – typically a time-series or independent samples from a graphical model. A major obstacle to the scalability of algorithms proposed for this problem is a seemingly unavoidable quadratic complexity of $\Omega(N^2)$, corresponding to the requirement of each possible pairwise coupling being contemplated at least once, despite the fact that most networks of interest are sparse, with a number of non-zero couplings that is only $O(N)$. Here we present a general algorithm applicable to a broad range of reconstruction problems that significantly outperforms this quadratic baseline. Our algorithm relies on a stochastic second neighbor search (Dong et al., 2011) that produces the best edge candidates with high probability, thus bypassing an exhaustive quadratic search. If we rely on the conjecture that the second-neighbor search finishes in log-linear time (Baron &amp;amp; Darling, 2020; 2022), we demonstrate theoretically that our algorithm finishes in subquadratic time, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\log N)$, but with a more typical log-linear complexity of $O(N\log^2N)$. In practice, we show that our algorithm achieves a performance that is many orders of magnitude faster than the quadratic baseline – in a manner consistent with our theoretical analysis – allows for easy parallelization, and thus enables the reconstruction of networks with hundreds of thousands and even millions of nodes and edges.</summary></entry><entry><title type="html">Semiparametric mean and variance joint models with clipped-Laplace link functions for bounded integer-valued time series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/SemiparametricmeanandvariancejointmodelswithclippedLaplacelinkfunctionsforboundedintegervaluedtimeseries.html" rel="alternate" type="text/html" title="Semiparametric mean and variance joint models with clipped-Laplace link functions for bounded integer-valued time series" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/SemiparametricmeanandvariancejointmodelswithclippedLaplacelinkfunctionsforboundedintegervaluedtimeseries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/SemiparametricmeanandvariancejointmodelswithclippedLaplacelinkfunctionsforboundedintegervaluedtimeseries.html">&lt;p&gt;We present a novel approach for modeling bounded count time series data, by deriving accurate upper and lower bounds for the variance of a bounded count random variable while maintaining a fixed mean. Leveraging these bounds, we propose semiparametric mean and variance joint (MVJ) models utilizing a clipped-Laplace link function. These models offer a flexible and feasible structure for both mean and variance, accommodating various scenarios of under-dispersion, equi-dispersion, or over-dispersion in bounded time series. The proposed MVJ models feature a linear mean structure with positive regression coefficients summing to one and allow for negative regression cefficients and autocorrelations. We demonstrate that the autocorrelation structure of MVJ models mirrors that of an autoregressive moving-average (ARMA) process, provided the proposed clipped-Laplace link functions with nonnegative regression coefficients summing to one are utilized. We establish conditions ensuring the stationarity and ergodicity properties of the MVJ process, along with demonstrating the consistency and asymptotic normality of the conditional least squares estimators. To aid model selection and diagnostics, we introduce two model selection criteria and apply two model diagnostics statistics. Finally, we conduct simulations and real data analyses to investigate the finite-sample properties of the proposed MVJ models, providing insights into their efficacy and applicability in practical scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00917&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tianqing Liu, Xiaohui Yuan</name></author><category term="stat.ME" /><summary type="html">We present a novel approach for modeling bounded count time series data, by deriving accurate upper and lower bounds for the variance of a bounded count random variable while maintaining a fixed mean. Leveraging these bounds, we propose semiparametric mean and variance joint (MVJ) models utilizing a clipped-Laplace link function. These models offer a flexible and feasible structure for both mean and variance, accommodating various scenarios of under-dispersion, equi-dispersion, or over-dispersion in bounded time series. The proposed MVJ models feature a linear mean structure with positive regression coefficients summing to one and allow for negative regression cefficients and autocorrelations. We demonstrate that the autocorrelation structure of MVJ models mirrors that of an autoregressive moving-average (ARMA) process, provided the proposed clipped-Laplace link functions with nonnegative regression coefficients summing to one are utilized. We establish conditions ensuring the stationarity and ergodicity properties of the MVJ process, along with demonstrating the consistency and asymptotic normality of the conditional least squares estimators. To aid model selection and diagnostics, we introduce two model selection criteria and apply two model diagnostics statistics. Finally, we conduct simulations and real data analyses to investigate the finite-sample properties of the proposed MVJ models, providing insights into their efficacy and applicability in practical scenarios.</summary></entry><entry><title type="html">Skeleton Regression: A Graph-Based Approach to Estimation with Manifold Structure</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/SkeletonRegressionAGraphBasedApproachtoEstimationwithManifoldStructure.html" rel="alternate" type="text/html" title="Skeleton Regression: A Graph-Based Approach to Estimation with Manifold Structure" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/SkeletonRegressionAGraphBasedApproachtoEstimationwithManifoldStructure</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/SkeletonRegressionAGraphBasedApproachtoEstimationwithManifoldStructure.html">&lt;p&gt;We introduce a new regression framework designed to deal with large-scale, complex data that lies around a low-dimensional manifold with noises. Our approach first constructs a graph representation, referred to as the skeleton, to capture the underlying geometric structure. We then define metrics on the skeleton graph and apply nonparametric regression techniques, along with feature transformations based on the graph, to estimate the regression function. We also discuss the limitations of some nonparametric regressors with respect to the general metric space such as the skeleton graph. The proposed regression framework suggests a novel way to deal with data with underlying geometric structures and provides additional advantages in handling the union of multiple manifolds, additive noises, and noisy observations. We provide statistical guarantees for the proposed method and demonstrate its effectiveness through simulations and real data examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.11786&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zeyu Wei, Yen-Chi Chen</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We introduce a new regression framework designed to deal with large-scale, complex data that lies around a low-dimensional manifold with noises. Our approach first constructs a graph representation, referred to as the skeleton, to capture the underlying geometric structure. We then define metrics on the skeleton graph and apply nonparametric regression techniques, along with feature transformations based on the graph, to estimate the regression function. We also discuss the limitations of some nonparametric regressors with respect to the general metric space such as the skeleton graph. The proposed regression framework suggests a novel way to deal with data with underlying geometric structures and provides additional advantages in handling the union of multiple manifolds, additive noises, and noisy observations. We provide statistical guarantees for the proposed method and demonstrate its effectiveness through simulations and real data examples.</summary></entry><entry><title type="html">Spatial Joint Species N-Mixture Models for Multi-Source Observational Data with Application to Wild Deer Population Abundance</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/SpatialJointSpeciesNMixtureModelsforMultiSourceObservationalDatawithApplicationtoWildDeerPopulationAbundance.html" rel="alternate" type="text/html" title="Spatial Joint Species N-Mixture Models for Multi-Source Observational Data with Application to Wild Deer Population Abundance" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/SpatialJointSpeciesNMixtureModelsforMultiSourceObservationalDatawithApplicationtoWildDeerPopulationAbundance</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/SpatialJointSpeciesNMixtureModelsforMultiSourceObservationalDatawithApplicationtoWildDeerPopulationAbundance.html">&lt;p&gt;Accurate predictions of the populations and spatial distributions of wild animal species is critical from a species management and conservation perspective. Culling is a measure taken for various reasons, including when overpopulation of a species is observed or suspected. Thus accurate estimates of population numbers are essential for specifying, monitoring, and evaluating the impact of such programmes. Population data for wild animals is generally collated from various sources and at differing spatial resolutions. Citizen science projects typically provide point referenced data, whereas site surveys, hunter reports, and official government data may be aggregated and released at a small area or regional level. Jointly modelling these data resources involves overcoming challenges of spatial misalignment.
  In this article, we develop an N mixture modelling methodology for joint modelling of species populations in the presence of spatially misaligned data, motivated by the three main species of wild deer in the Republic of Ireland; fallow, red and sika. Previous studies of deer populations investigated the distribution and abundance on a species by species basis, failing to account for possible correlation between individual species and the impact of ecological covariates on their distributions.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.19993&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aoife K. Hurley, Ruth F. Carden, Sally Cook, Irish Deer Commission, Ferdia Marnell, Pieter A. J. Brama, Daniel J. Buckley, James Sweeney</name></author><category term="stat.AP" /><summary type="html">Accurate predictions of the populations and spatial distributions of wild animal species is critical from a species management and conservation perspective. Culling is a measure taken for various reasons, including when overpopulation of a species is observed or suspected. Thus accurate estimates of population numbers are essential for specifying, monitoring, and evaluating the impact of such programmes. Population data for wild animals is generally collated from various sources and at differing spatial resolutions. Citizen science projects typically provide point referenced data, whereas site surveys, hunter reports, and official government data may be aggregated and released at a small area or regional level. Jointly modelling these data resources involves overcoming challenges of spatial misalignment. In this article, we develop an N mixture modelling methodology for joint modelling of species populations in the presence of spatially misaligned data, motivated by the three main species of wild deer in the Republic of Ireland; fallow, red and sika. Previous studies of deer populations investigated the distribution and abundance on a species by species basis, failing to account for possible correlation between individual species and the impact of ecological covariates on their distributions.</summary></entry><entry><title type="html">Statistical Inference on the Cumulative Distribution Function using Judgment Post Stratification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/StatisticalInferenceontheCumulativeDistributionFunctionusingJudgmentPostStratification.html" rel="alternate" type="text/html" title="Statistical Inference on the Cumulative Distribution Function using Judgment Post Stratification" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/StatisticalInferenceontheCumulativeDistributionFunctionusingJudgmentPostStratification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/StatisticalInferenceontheCumulativeDistributionFunctionusingJudgmentPostStratification.html">&lt;p&gt;In this work, we discuss a general class of the estimators for the cumulative distribution function (CDF) based on judgment post stratification (JPS) sampling scheme which includes both empirical and kernel distribution functions. Specifically, we obtain the expectation of the estimators in this class and show that they are asymptotically more efficient than their competitors in simple random sampling (SRS), as long as the rankings are better than random guessing. We find a mild condition that is necessary and sufficient for them to be asymptotically unbiased. We also prove that given the same condition, the estimators in this class are strongly uniformly consistent estimators of the true CDF, and converge in distribution to a normal distribution when the sample size goes to infinity.
  We then focus on the kernel distribution function (KDF) in the JPS design and obtain the optimal bandwidth. We next carry out a comprehensive Monte Carlo simulation to compare the performance of the KDF in the JPS design for different choices of sample size, set size, ranking quality, parent distribution, kernel function as well as both perfect and imperfect rankings set-ups with its counterpart in SRS design. It is found that the JPS estimator dramatically improves the efficiency of the KDF as compared to its SRS competitor for a wide range of the settings. Finally, we apply the described procedure on a real dataset from medical context to show their usefulness and applicability in practice.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01072&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mina Azizi Kouhanestani, Ehsan Zamanzade, Sareh Goli</name></author><category term="stat.ME" /><summary type="html">In this work, we discuss a general class of the estimators for the cumulative distribution function (CDF) based on judgment post stratification (JPS) sampling scheme which includes both empirical and kernel distribution functions. Specifically, we obtain the expectation of the estimators in this class and show that they are asymptotically more efficient than their competitors in simple random sampling (SRS), as long as the rankings are better than random guessing. We find a mild condition that is necessary and sufficient for them to be asymptotically unbiased. We also prove that given the same condition, the estimators in this class are strongly uniformly consistent estimators of the true CDF, and converge in distribution to a normal distribution when the sample size goes to infinity. We then focus on the kernel distribution function (KDF) in the JPS design and obtain the optimal bandwidth. We next carry out a comprehensive Monte Carlo simulation to compare the performance of the KDF in the JPS design for different choices of sample size, set size, ranking quality, parent distribution, kernel function as well as both perfect and imperfect rankings set-ups with its counterpart in SRS design. It is found that the JPS estimator dramatically improves the efficiency of the KDF as compared to its SRS competitor for a wide range of the settings. Finally, we apply the described procedure on a real dataset from medical context to show their usefulness and applicability in practice.</summary></entry><entry><title type="html">Statistical algorithms for low-frequency diffusion data: A PDE approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/StatisticalalgorithmsforlowfrequencydiffusiondataAPDEapproach.html" rel="alternate" type="text/html" title="Statistical algorithms for low-frequency diffusion data: A PDE approach" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/StatisticalalgorithmsforlowfrequencydiffusiondataAPDEapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/StatisticalalgorithmsforlowfrequencydiffusiondataAPDEapproach.html">&lt;p&gt;We consider the problem of making nonparametric inference in multi-dimensional diffusion models from low-frequency data. Statistical analysis in this setting is notoriously challenging due to the intractability of the likelihood and its gradient, and computational methods have thus far largely resorted to expensive simulation-based techniques. In this article, we propose a new computational approach which is motivated by PDE theory and is built around the characterisation of the transition densities as solutions of the associated heat (Fokker-Planck) equation. Employing optimal regularity results from the theory of parabolic PDEs, we prove a novel characterisation for the gradient of the likelihood. Using these developments, for the nonlinear inverse problem of recovering the diffusivity (in divergence form models), we then show that the numerical evaluation of the likelihood and its gradient can be reduced to standard elliptic eigenvalue problems, solvable by powerful finite element methods. This enables the efficient implementation of a large class of statistical algorithms, including (i) preconditioned Crank-Nicolson and Langevin-type methods for posterior sampling, and (ii) gradient-based descent optimisation schemes to compute maximum likelihood and maximum-a-posteriori estimates. We showcase the effectiveness of these methods via extensive simulation studies in a nonparametric Bayesian model with Gaussian process priors. Interestingly, the optimisation schemes provided satisfactory numerical recovery while exhibiting rapid convergence towards stationary points despite the problem nonlinearity; thus our approach may lead to significant computational speed-ups. The reproducible code is available online at https://github.com/MattGiord/LF-Diffusion.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01372&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matteo Giordano, Sven Wang</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">We consider the problem of making nonparametric inference in multi-dimensional diffusion models from low-frequency data. Statistical analysis in this setting is notoriously challenging due to the intractability of the likelihood and its gradient, and computational methods have thus far largely resorted to expensive simulation-based techniques. In this article, we propose a new computational approach which is motivated by PDE theory and is built around the characterisation of the transition densities as solutions of the associated heat (Fokker-Planck) equation. Employing optimal regularity results from the theory of parabolic PDEs, we prove a novel characterisation for the gradient of the likelihood. Using these developments, for the nonlinear inverse problem of recovering the diffusivity (in divergence form models), we then show that the numerical evaluation of the likelihood and its gradient can be reduced to standard elliptic eigenvalue problems, solvable by powerful finite element methods. This enables the efficient implementation of a large class of statistical algorithms, including (i) preconditioned Crank-Nicolson and Langevin-type methods for posterior sampling, and (ii) gradient-based descent optimisation schemes to compute maximum likelihood and maximum-a-posteriori estimates. We showcase the effectiveness of these methods via extensive simulation studies in a nonparametric Bayesian model with Gaussian process priors. Interestingly, the optimisation schemes provided satisfactory numerical recovery while exhibiting rapid convergence towards stationary points despite the problem nonlinearity; thus our approach may lead to significant computational speed-ups. The reproducible code is available online at https://github.com/MattGiord/LF-Diffusion.</summary></entry><entry><title type="html">Stochastic Geometry Analysis of EMF Exposure of Idle Users and Network Performance with Dynamic Beamforming</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/StochasticGeometryAnalysisofEMFExposureofIdleUsersandNetworkPerformancewithDynamicBeamforming.html" rel="alternate" type="text/html" title="Stochastic Geometry Analysis of EMF Exposure of Idle Users and Network Performance with Dynamic Beamforming" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/StochasticGeometryAnalysisofEMFExposureofIdleUsersandNetworkPerformancewithDynamicBeamforming</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/StochasticGeometryAnalysisofEMFExposureofIdleUsersandNetworkPerformancewithDynamicBeamforming.html">&lt;p&gt;This paper presents a novel mathematical framework based on stochastic geometry to investigate the electromagnetic field exposure of idle and active users in cellular networks implementing dynamic beamforming. Accurate modeling of antenna gain becomes crucial in this context, encompassing both the main and the side lobes. The marginal distribution of EMF exposure for each type of users is initially derived. Subsequently, network performance is scrutinized by introducing a new metric aimed at ensuring minimal downlink coverage while simultaneously maintaining EMF exposure below distinct thresholds for both idle and active users. The metrics exhibit a high dependency on various parameters, such as the distance between active and idle users and the number of antenna elements.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01190&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Quentin Gontier, Charles Wiame, Joe Wiart, François Horlin, Christo Tsigros, Claude Oestges, Philippe De Doncker</name></author><category term="stat.AP" /><summary type="html">This paper presents a novel mathematical framework based on stochastic geometry to investigate the electromagnetic field exposure of idle and active users in cellular networks implementing dynamic beamforming. Accurate modeling of antenna gain becomes crucial in this context, encompassing both the main and the side lobes. The marginal distribution of EMF exposure for each type of users is initially derived. Subsequently, network performance is scrutinized by introducing a new metric aimed at ensuring minimal downlink coverage while simultaneously maintaining EMF exposure below distinct thresholds for both idle and active users. The metrics exhibit a high dependency on various parameters, such as the distance between active and idle users and the number of antenna elements.</summary></entry><entry><title type="html">Strategies for Rare Population Detection and Sampling: A Methodological Approach in Liguria</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/StrategiesforRarePopulationDetectionandSamplingAMethodologicalApproachinLiguria.html" rel="alternate" type="text/html" title="Strategies for Rare Population Detection and Sampling: A Methodological Approach in Liguria" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/StrategiesforRarePopulationDetectionandSamplingAMethodologicalApproachinLiguria</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/StrategiesforRarePopulationDetectionandSamplingAMethodologicalApproachinLiguria.html">&lt;p&gt;Economic policy sciences are constantly investigating the quality of well-being of broad sections of the population in order to describe the current interdependence between unequal living conditions, low levels of education and a lack of integration into society. Such studies are often carried out in the form of surveys, e.g. as part of the EU-SILC program. If the survey is designed at national or international level, the results of the study are often used as a reference by a broad range of public institutions. However, the sampling strategy per se may not capture enough information to provide an accurate representation of all population strata. Problems might arise from rare, or hard-to-sample, populations and the conclusion of the study may be compromised or unrealistic. We propose here a two-phase methodology to identify rare, poorly sampled populations and then resample the hard-to-sample strata. We focused our attention on the 2019 EU-SILC section concerning the Italian region of Liguria. Methods based on dispersion indices or deep learning were used to detect rare populations. A multi-frame survey was proposed as the sampling design. The results showed that factors such as citizenship, material deprivation and large families are still fundamental characteristics that are difficult to capture.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01342&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>G. Lancia, E. Riccomagno</name></author><category term="stat.AP," /><category term="stat.CO," /><category term="stat.OT" /><summary type="html">Economic policy sciences are constantly investigating the quality of well-being of broad sections of the population in order to describe the current interdependence between unequal living conditions, low levels of education and a lack of integration into society. Such studies are often carried out in the form of surveys, e.g. as part of the EU-SILC program. If the survey is designed at national or international level, the results of the study are often used as a reference by a broad range of public institutions. However, the sampling strategy per se may not capture enough information to provide an accurate representation of all population strata. Problems might arise from rare, or hard-to-sample, populations and the conclusion of the study may be compromised or unrealistic. We propose here a two-phase methodology to identify rare, poorly sampled populations and then resample the hard-to-sample strata. We focused our attention on the 2019 EU-SILC section concerning the Italian region of Liguria. Methods based on dispersion indices or deep learning were used to detect rare populations. A multi-frame survey was proposed as the sampling design. The results showed that factors such as citizenship, material deprivation and large families are still fundamental characteristics that are difficult to capture.</summary></entry><entry><title type="html">Tracking and classifying objects with DAS data along railway</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/TrackingandclassifyingobjectswithDASdataalongrailway.html" rel="alternate" type="text/html" title="Tracking and classifying objects with DAS data along railway" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/TrackingandclassifyingobjectswithDASdataalongrailway</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/TrackingandclassifyingobjectswithDASdataalongrailway.html">&lt;p&gt;Distributed acoustic sensing through fiber-optical cables can contribute to traffic monitoring systems. Using data from a day of field testing on a 50 km long fiber-optic cable along a railroad track in Norway, we detect and track cars and trains along a segment of the fiber-optic cable where the road runs parallel to the railroad tracks. We develop a method for automatic detection of events and then use these in a Kalman filter variant known as joint probabilistic data association for object tracking and classification. Model parameters are specified using in-situ log data along with the fiber-optic signals. Running the algorithm over an entire day, we highlight results of counting cars and trains over time and their estimated velocities.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01140&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Simon L. B. Fredriksen, The Tien Mai, Kevin Growe, Jo Eidsvik</name></author><category term="stat.AP" /><summary type="html">Distributed acoustic sensing through fiber-optical cables can contribute to traffic monitoring systems. Using data from a day of field testing on a 50 km long fiber-optic cable along a railroad track in Norway, we detect and track cars and trains along a segment of the fiber-optic cable where the road runs parallel to the railroad tracks. We develop a method for automatic detection of events and then use these in a Kalman filter variant known as joint probabilistic data association for object tracking and classification. Model parameters are specified using in-situ log data along with the fiber-optic signals. Running the algorithm over an entire day, we highlight results of counting cars and trains over time and their estimated velocities.</summary></entry><entry><title type="html">Variable Selection in Ultra-high Dimensional Feature Space for the Cox Model with Interval-Censored Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/VariableSelectioninUltrahighDimensionalFeatureSpacefortheCoxModelwithIntervalCensoredData.html" rel="alternate" type="text/html" title="Variable Selection in Ultra-high Dimensional Feature Space for the Cox Model with Interval-Censored Data" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/VariableSelectioninUltrahighDimensionalFeatureSpacefortheCoxModelwithIntervalCensoredData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/VariableSelectioninUltrahighDimensionalFeatureSpacefortheCoxModelwithIntervalCensoredData.html">&lt;p&gt;We develop a set of variable selection methods for the Cox model under interval censoring, in the ultra-high dimensional setting where the dimensionality can grow exponentially with the sample size. The methods select covariates via a penalized nonparametric maximum likelihood estimation with some popular penalty functions, including lasso, adaptive lasso, SCAD, and MCP. We prove that our penalized variable selection methods with folded concave penalties or adaptive lasso penalty enjoy the oracle property. Extensive numerical experiments show that the proposed methods have satisfactory empirical performance under various scenarios. The utility of the methods is illustrated through an application to a genome-wide association study of age to early childhood caries.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01275&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Daewoo Pak, Jianrui Zhang, Di Wu, Haolei Weng, Chenxi Li</name></author><category term="stat.ME" /><summary type="html">We develop a set of variable selection methods for the Cox model under interval censoring, in the ultra-high dimensional setting where the dimensionality can grow exponentially with the sample size. The methods select covariates via a penalized nonparametric maximum likelihood estimation with some popular penalty functions, including lasso, adaptive lasso, SCAD, and MCP. We prove that our penalized variable selection methods with folded concave penalties or adaptive lasso penalty enjoy the oracle property. Extensive numerical experiments show that the proposed methods have satisfactory empirical performance under various scenarios. The utility of the methods is illustrated through an application to a genome-wide association study of age to early childhood caries.</summary></entry><entry><title type="html">WATCH: A Workflow to Assess Treatment Effect Heterogeneity in Drug Development for Clinical Trial Sponsors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/WATCHAWorkflowtoAssessTreatmentEffectHeterogeneityinDrugDevelopmentforClinicalTrialSponsors.html" rel="alternate" type="text/html" title="WATCH: A Workflow to Assess Treatment Effect Heterogeneity in Drug Development for Clinical Trial Sponsors" /><published>2024-05-03T00:00:00+00:00</published><updated>2024-05-03T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/WATCHAWorkflowtoAssessTreatmentEffectHeterogeneityinDrugDevelopmentforClinicalTrialSponsors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/03/WATCHAWorkflowtoAssessTreatmentEffectHeterogeneityinDrugDevelopmentforClinicalTrialSponsors.html">&lt;p&gt;This paper proposes a Workflow for Assessing Treatment effeCt Heterogeneity (WATCH) in clinical drug development targeted at clinical trial sponsors. The workflow is designed to address the challenges of investigating treatment effect heterogeneity (TEH) in randomized clinical trials, where sample size and multiplicity limit the reliability of findings. The proposed workflow includes four steps: Analysis Planning, Initial Data Analysis and Analysis Dataset Creation, TEH Exploration, and Multidisciplinary Assessment. The workflow aims to provide a systematic approach to explore treatment effect heterogeneity in the exploratory setting, taking into account external evidence and best scientific understanding.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.00859&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Konstantinos Sechidis, Sophie Sun, Yao Chen, Jiarui Lu, Cong Zang, Mark Baillie, David Ohlssen, Marc Vandemeulebroecke, Rob Hemmings, Stephen Ruberg, Björn Bornkamp</name></author><category term="stat.AP" /><summary type="html">This paper proposes a Workflow for Assessing Treatment effeCt Heterogeneity (WATCH) in clinical drug development targeted at clinical trial sponsors. The workflow is designed to address the challenges of investigating treatment effect heterogeneity (TEH) in randomized clinical trials, where sample size and multiplicity limit the reliability of findings. The proposed workflow includes four steps: Analysis Planning, Initial Data Analysis and Analysis Dataset Creation, TEH Exploration, and Multidisciplinary Assessment. The workflow aims to provide a systematic approach to explore treatment effect heterogeneity in the exploratory setting, taking into account external evidence and best scientific understanding.</summary></entry></feed>