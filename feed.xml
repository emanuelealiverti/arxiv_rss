<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-15T07:14:15+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Deep Learning Approach for Overall Survival Prediction in Lung Cancer with Missing Values</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/ADeepLearningApproachforOverallSurvivalPredictioninLungCancerwithMissingValues.html" rel="alternate" type="text/html" title="A Deep Learning Approach for Overall Survival Prediction in Lung Cancer with Missing Values" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/ADeepLearningApproachforOverallSurvivalPredictioninLungCancerwithMissingValues</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/ADeepLearningApproachforOverallSurvivalPredictioninLungCancerwithMissingValues.html">&lt;p&gt;In the field of lung cancer research, particularly in the analysis of overall survival (OS), artificial intelligence (AI) serves crucial roles with specific aims. Given the prevalent issue of missing data in the medical domain, our primary objective is to develop an AI model capable of dynamically handling this missing data. Additionally, we aim to leverage all accessible data, effectively analyzing both uncensored patients who have experienced the event of interest and censored patients who have not, by embedding a specialized technique within our AI model, not commonly utilized in other AI tasks. Through the realization of these objectives, our model aims to provide precise OS predictions for non-small cell lung cancer (NSCLC) patients, thus overcoming these significant challenges. We present a novel approach to survival analysis with missing values in the context of NSCLC, which exploits the strengths of the transformer architecture to account only for available features without requiring any imputation strategy. More specifically, this model tailors the transformer architecture to tabular data by adapting its feature embedding and masked self-attention to mask missing data and fully exploit the available ones. By making use of ad-hoc designed losses for OS, it is able to account for both censored and uncensored patients, as well as changes in risks over time. We compared our method with state-of-the-art models for survival analysis coupled with different imputation strategies. We evaluated the results obtained over a period of 6 years using different time granularities obtaining a Ct-index, a time-dependent variant of the C-index, of 71.97, 77.58 and 80.72 for time units of 1 month, 1 year and 2 years, respectively, outperforming all state-of-the-art methods regardless of the imputation method used.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.11465&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Camillo Maria Caruso, Valerio Guarrasi, Sara Ramella, Paolo Soda</name></author><category term="stat.AP" /><summary type="html">In the field of lung cancer research, particularly in the analysis of overall survival (OS), artificial intelligence (AI) serves crucial roles with specific aims. Given the prevalent issue of missing data in the medical domain, our primary objective is to develop an AI model capable of dynamically handling this missing data. Additionally, we aim to leverage all accessible data, effectively analyzing both uncensored patients who have experienced the event of interest and censored patients who have not, by embedding a specialized technique within our AI model, not commonly utilized in other AI tasks. Through the realization of these objectives, our model aims to provide precise OS predictions for non-small cell lung cancer (NSCLC) patients, thus overcoming these significant challenges. We present a novel approach to survival analysis with missing values in the context of NSCLC, which exploits the strengths of the transformer architecture to account only for available features without requiring any imputation strategy. More specifically, this model tailors the transformer architecture to tabular data by adapting its feature embedding and masked self-attention to mask missing data and fully exploit the available ones. By making use of ad-hoc designed losses for OS, it is able to account for both censored and uncensored patients, as well as changes in risks over time. We compared our method with state-of-the-art models for survival analysis coupled with different imputation strategies. We evaluated the results obtained over a period of 6 years using different time granularities obtaining a Ct-index, a time-dependent variant of the C-index, of 71.97, 77.58 and 80.72 for time units of 1 month, 1 year and 2 years, respectively, outperforming all state-of-the-art methods regardless of the imputation method used.</summary></entry><entry><title type="html">A Fast and Scalable Pathwise-Solver for Group Lasso and Elastic Net Penalized Regression via Block-Coordinate Descent</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AFastandScalablePathwiseSolverforGroupLassoandElasticNetPenalizedRegressionviaBlockCoordinateDescent.html" rel="alternate" type="text/html" title="A Fast and Scalable Pathwise-Solver for Group Lasso and Elastic Net Penalized Regression via Block-Coordinate Descent" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AFastandScalablePathwiseSolverforGroupLassoandElasticNetPenalizedRegressionviaBlockCoordinateDescent</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AFastandScalablePathwiseSolverforGroupLassoandElasticNetPenalizedRegressionviaBlockCoordinateDescent.html">&lt;p&gt;We develop fast and scalable algorithms based on block-coordinate descent to solve the group lasso and the group elastic net for generalized linear models along a regularization path. Special attention is given when the loss is the usual least squares loss (Gaussian loss). We show that each block-coordinate update can be solved efficiently using Newton’s method and further improved using an adaptive bisection method, solving these updates with a quadratic convergence rate. Our benchmarks show that our package adelie performs 3 to 10 times faster than the next fastest package on a wide array of both simulated and real datasets. Moreover, we demonstrate that our package is a competitive lasso solver as well, matching the performance of the popular lasso package glmnet.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08631&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>James Yang, Trevor Hastie</name></author><category term="stat.CO" /><summary type="html">We develop fast and scalable algorithms based on block-coordinate descent to solve the group lasso and the group elastic net for generalized linear models along a regularization path. Special attention is given when the loss is the usual least squares loss (Gaussian loss). We show that each block-coordinate update can be solved efficiently using Newton’s method and further improved using an adaptive bisection method, solving these updates with a quadratic convergence rate. Our benchmarks show that our package adelie performs 3 to 10 times faster than the next fastest package on a wide array of both simulated and real datasets. Moreover, we demonstrate that our package is a competitive lasso solver as well, matching the performance of the popular lasso package glmnet.</summary></entry><entry><title type="html">A Generalized Difference-in-Differences Estimator for Unbiased Estimation of Desired Estimands from Staggered Adoption and Stepped-Wedge Settings</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AGeneralizedDifferenceinDifferencesEstimatorforUnbiasedEstimationofDesiredEstimandsfromStaggeredAdoptionandSteppedWedgeSettings.html" rel="alternate" type="text/html" title="A Generalized Difference-in-Differences Estimator for Unbiased Estimation of Desired Estimands from Staggered Adoption and Stepped-Wedge Settings" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AGeneralizedDifferenceinDifferencesEstimatorforUnbiasedEstimationofDesiredEstimandsfromStaggeredAdoptionandSteppedWedgeSettings</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AGeneralizedDifferenceinDifferencesEstimatorforUnbiasedEstimationofDesiredEstimandsfromStaggeredAdoptionandSteppedWedgeSettings.html">&lt;p&gt;Staggered treatment adoption arises in the evaluation of policy impact and implementation in a variety of settings. This occurs in both randomized stepped-wedge trials and non-randomized quasi-experimental designs using causal inference methods based on difference-in-differences analysis. In both settings, it is crucial to carefully consider the target estimand and possible treatment effect heterogeneities in order to estimate the effect without bias and in an interpretable fashion. This paper proposes a novel non-parametric approach to this estimation for either setting. By constructing an estimator using two-by-two difference-in-difference comparisons as building blocks with arbitrary weights, the investigator can select weights to target the desired estimand in an unbiased manner under assumed treatment effect homogeneity, and minimize the variance under an assumed working covariance structure. This provides desirable bias properties with a relatively small sacrifice in variance and power by using the comparisons efficiently. The method is demonstrated on toy examples to show the process, as well as in the re-analysis of a stepped wedge trial on the impact of novel tuberculosis diagnostic tools. A full algorithm with R code is provided to implement this method. The proposed method allows for high flexibility and clear targeting of desired effects, providing one solution to the bias-variance-generalizability tradeoff.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08730&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lee Kennedy-Shaffer</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Staggered treatment adoption arises in the evaluation of policy impact and implementation in a variety of settings. This occurs in both randomized stepped-wedge trials and non-randomized quasi-experimental designs using causal inference methods based on difference-in-differences analysis. In both settings, it is crucial to carefully consider the target estimand and possible treatment effect heterogeneities in order to estimate the effect without bias and in an interpretable fashion. This paper proposes a novel non-parametric approach to this estimation for either setting. By constructing an estimator using two-by-two difference-in-difference comparisons as building blocks with arbitrary weights, the investigator can select weights to target the desired estimand in an unbiased manner under assumed treatment effect homogeneity, and minimize the variance under an assumed working covariance structure. This provides desirable bias properties with a relatively small sacrifice in variance and power by using the comparisons efficiently. The method is demonstrated on toy examples to show the process, as well as in the re-analysis of a stepped wedge trial on the impact of novel tuberculosis diagnostic tools. A full algorithm with R code is provided to implement this method. The proposed method allows for high flexibility and clear targeting of desired effects, providing one solution to the bias-variance-generalizability tradeoff.</summary></entry><entry><title type="html">A Generalized Logrank-type Test for Comparison of Treatment Regimes in Sequential Multiple Assignment Randomized Trials</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AGeneralizedLogranktypeTestforComparisonofTreatmentRegimesinSequentialMultipleAssignmentRandomizedTrials.html" rel="alternate" type="text/html" title="A Generalized Logrank-type Test for Comparison of Treatment Regimes in Sequential Multiple Assignment Randomized Trials" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AGeneralizedLogranktypeTestforComparisonofTreatmentRegimesinSequentialMultipleAssignmentRandomizedTrials</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AGeneralizedLogranktypeTestforComparisonofTreatmentRegimesinSequentialMultipleAssignmentRandomizedTrials.html">&lt;p&gt;The sequential multiple assignment randomized trial (SMART) is the ideal study design for the evaluation of multistage treatment regimes, which comprise sequential decision rules that recommend treatments for a patient at each of a series of decision points based on their evolving characteristics. A common goal is to compare the set of so-called embedded regimes represented in the design on the basis of a primary outcome of interest. In the study of chronic diseases and disorders, this outcome is often a time to an event, and a goal is to compare the distributions of the time-to-event outcome associated with each regime in the set. We present a general statistical framework in which we develop a logrank-type test for comparison of the survival distributions associated with regimes within a specified set based on the data from a SMART with an arbitrary number of stages that allows incorporation of covariate information to enhance efficiency and can also be used with data from an observational study. The framework provides clarification of the assumptions required to yield a principled test procedure, and the proposed test subsumes or offers an improved alternative to existing methods. We demonstrate performance of the methods in a suite of simulation studies. The methods are applied to a SMART in patients with acute promyelocytic leukemia.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.16813&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Anastasios A. Tsiatis, Marie Davidian</name></author><category term="stat.ME" /><summary type="html">The sequential multiple assignment randomized trial (SMART) is the ideal study design for the evaluation of multistage treatment regimes, which comprise sequential decision rules that recommend treatments for a patient at each of a series of decision points based on their evolving characteristics. A common goal is to compare the set of so-called embedded regimes represented in the design on the basis of a primary outcome of interest. In the study of chronic diseases and disorders, this outcome is often a time to an event, and a goal is to compare the distributions of the time-to-event outcome associated with each regime in the set. We present a general statistical framework in which we develop a logrank-type test for comparison of the survival distributions associated with regimes within a specified set based on the data from a SMART with an arbitrary number of stages that allows incorporation of covariate information to enhance efficiency and can also be used with data from an observational study. The framework provides clarification of the assumptions required to yield a principled test procedure, and the proposed test subsumes or offers an improved alternative to existing methods. We demonstrate performance of the methods in a suite of simulation studies. The methods are applied to a SMART in patients with acute promyelocytic leukemia.</summary></entry><entry><title type="html">A Unification of Exchangeability and Continuous Exposure and Confounder Measurement Errors: Probabilistic Exchangeability</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AUnificationofExchangeabilityandContinuousExposureandConfounderMeasurementErrorsProbabilisticExchangeability.html" rel="alternate" type="text/html" title="A Unification of Exchangeability and Continuous Exposure and Confounder Measurement Errors: Probabilistic Exchangeability" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AUnificationofExchangeabilityandContinuousExposureandConfounderMeasurementErrorsProbabilisticExchangeability</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AUnificationofExchangeabilityandContinuousExposureandConfounderMeasurementErrorsProbabilisticExchangeability.html">&lt;p&gt;Exchangeability concerning a continuous exposure, X, implies no confounding bias when identifying average exposure effects of X, AEE(X). When X is measured with error (Xep), two challenges arise in identifying AEE(X). Firstly, exchangeability regarding Xep does not equal exchangeability regarding X. Secondly, the necessity of the non-differential error assumption (NDEA), overly stringent in practice, remains uncertain. To address them, this article proposes unifying exchangeability and exposure and confounder measurement errors with three novel concepts. The first, Probabilistic Exchangeability (PE), states that the outcomes of those with Xep=e are probabilistically exchangeable with the outcomes of those truly exposed to X=eT. The relationship between AEE(Xep) and AEE(X) in risk difference and ratio scales is mathematically expressed as a probabilistic certainty, termed exchangeability probability (Pe). Squared Pe (Pe.sq) quantifies the extent to which AEE(Xep) differs from AEE(X) due to exposure measurement error through mechanisms not akin to confounding mechanisms. The coefficient of determination (R.sq) in the regression of X against Xep may sometimes be sufficient to measure Pe.sq. The second concept, Emergent Pseudo Confounding (EPC), describes the bias introduced by exposure measurement error through mechanisms akin to confounding mechanisms. PE can hold when EPC is controlled for, which is weaker than NDEA. The third, Emergent Confounding, describes when bias due to confounder measurement error arises. Adjustment for E(P)C can be performed like confounding adjustment to ensure PE. This paper provides formal justifications for using AEE(Xep) and maximum insight into potential divergence of AEE(Xep) from AEE(X) and how to measure it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07910&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Honghyok Kim</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Exchangeability concerning a continuous exposure, X, implies no confounding bias when identifying average exposure effects of X, AEE(X). When X is measured with error (Xep), two challenges arise in identifying AEE(X). Firstly, exchangeability regarding Xep does not equal exchangeability regarding X. Secondly, the necessity of the non-differential error assumption (NDEA), overly stringent in practice, remains uncertain. To address them, this article proposes unifying exchangeability and exposure and confounder measurement errors with three novel concepts. The first, Probabilistic Exchangeability (PE), states that the outcomes of those with Xep=e are probabilistically exchangeable with the outcomes of those truly exposed to X=eT. The relationship between AEE(Xep) and AEE(X) in risk difference and ratio scales is mathematically expressed as a probabilistic certainty, termed exchangeability probability (Pe). Squared Pe (Pe.sq) quantifies the extent to which AEE(Xep) differs from AEE(X) due to exposure measurement error through mechanisms not akin to confounding mechanisms. The coefficient of determination (R.sq) in the regression of X against Xep may sometimes be sufficient to measure Pe.sq. The second concept, Emergent Pseudo Confounding (EPC), describes the bias introduced by exposure measurement error through mechanisms akin to confounding mechanisms. PE can hold when EPC is controlled for, which is weaker than NDEA. The third, Emergent Confounding, describes when bias due to confounder measurement error arises. Adjustment for E(P)C can be performed like confounding adjustment to ensure PE. This paper provides formal justifications for using AEE(Xep) and maximum insight into potential divergence of AEE(Xep) from AEE(X) and how to measure it.</summary></entry><entry><title type="html">Addressing Misspecification in Simulation-based Inference through Data-driven Calibration</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AddressingMisspecificationinSimulationbasedInferencethroughDatadrivenCalibration.html" rel="alternate" type="text/html" title="Addressing Misspecification in Simulation-based Inference through Data-driven Calibration" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AddressingMisspecificationinSimulationbasedInferencethroughDatadrivenCalibration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AddressingMisspecificationinSimulationbasedInferencethroughDatadrivenCalibration.html">&lt;p&gt;Driven by steady progress in generative modeling, simulation-based inference (SBI) has enabled inference over stochastic simulators. However, recent work has demonstrated that model misspecification can harm SBI’s reliability. This work introduces robust posterior estimation (ROPE), a framework that overcomes model misspecification with a small real-world calibration set of ground truth parameter measurements. We formalize the misspecification gap as the solution of an optimal transport problem between learned representations of real-world and simulated observations. Assuming the prior distribution over the parameters of interest is known and well-specified, our method offers a controllable balance between calibrated uncertainty and informative inference under all possible misspecifications of the simulator. Our empirical results on four synthetic tasks and two real-world problems demonstrate that ROPE outperforms baselines and consistently returns informative and calibrated credible intervals.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08719&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Antoine Wehenkel, Juan L. Gamella, Ozan Sener, Jens Behrmann, Guillermo Sapiro, Marco Cuturi, Jörn-Henrik Jacobsen</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Driven by steady progress in generative modeling, simulation-based inference (SBI) has enabled inference over stochastic simulators. However, recent work has demonstrated that model misspecification can harm SBI’s reliability. This work introduces robust posterior estimation (ROPE), a framework that overcomes model misspecification with a small real-world calibration set of ground truth parameter measurements. We formalize the misspecification gap as the solution of an optimal transport problem between learned representations of real-world and simulated observations. Assuming the prior distribution over the parameters of interest is known and well-specified, our method offers a controllable balance between calibrated uncertainty and informative inference under all possible misspecifications of the simulator. Our empirical results on four synthetic tasks and two real-world problems demonstrate that ROPE outperforms baselines and consistently returns informative and calibrated credible intervals.</summary></entry><entry><title type="html">A method for characterizing disease emergence curves from paired pathogen detection and serology data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Amethodforcharacterizingdiseaseemergencecurvesfrompairedpathogendetectionandserologydata.html" rel="alternate" type="text/html" title="A method for characterizing disease emergence curves from paired pathogen detection and serology data" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Amethodforcharacterizingdiseaseemergencecurvesfrompairedpathogendetectionandserologydata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Amethodforcharacterizingdiseaseemergencecurvesfrompairedpathogendetectionandserologydata.html">&lt;p&gt;Wildlife disease surveillance programs and research studies track infection and identify risk factors for wild populations, humans, and agriculture. Often, several types of samples are collected from individuals to provide more complete information about an animal’s infection history. Methods that jointly analyze multiple data streams to study disease emergence and drivers of infection via epidemiological process models remain underdeveloped. Joint-analysis methods can more thoroughly analyze all available data, more precisely quantifying epidemic processes, outbreak status, and risks. We contribute a paired data modeling approach that analyzes multiple samples from individuals. We use “characterization maps” to link paired data to epidemiological processes through a hierarchical statistical observation model. Our approach can provide both Bayesian and frequentist estimates of epidemiological parameters and state. We motivate our approach through the need to use paired pathogen and antibody detection tests to estimate parameters and infection trajectories for the widely applicable susceptible, infectious, recovered (SIR) model. We contribute general formulas to link characterization maps to arbitrary process models and datasets and an extended SIR model that better accommodates paired data. We find via simulation that paired data can more efficiently estimate SIR parameters than unpaired data, requiring samples from 5-10 times fewer individuals. We then study SARS-CoV-2 in wild White-tailed deer (Odocoileus virginianus) from three counties in the United States. Estimates for average infectious times corroborate captive animal studies. Our methods use general statistical theory to let applications extend beyond the SIR model we consider, and to more complicated examples of paired data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.10057&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joshua Hewitt, Grete Wilson-Henjum, Derek T. Collins, Jourdan M. Ringenberg, Christopher A. Quintanal, Robert Pleszewski, Jeffrey C. Chandler, Thomas J. DeLiberto, Kim M. Pepin</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Wildlife disease surveillance programs and research studies track infection and identify risk factors for wild populations, humans, and agriculture. Often, several types of samples are collected from individuals to provide more complete information about an animal’s infection history. Methods that jointly analyze multiple data streams to study disease emergence and drivers of infection via epidemiological process models remain underdeveloped. Joint-analysis methods can more thoroughly analyze all available data, more precisely quantifying epidemic processes, outbreak status, and risks. We contribute a paired data modeling approach that analyzes multiple samples from individuals. We use “characterization maps” to link paired data to epidemiological processes through a hierarchical statistical observation model. Our approach can provide both Bayesian and frequentist estimates of epidemiological parameters and state. We motivate our approach through the need to use paired pathogen and antibody detection tests to estimate parameters and infection trajectories for the widely applicable susceptible, infectious, recovered (SIR) model. We contribute general formulas to link characterization maps to arbitrary process models and datasets and an extended SIR model that better accommodates paired data. We find via simulation that paired data can more efficiently estimate SIR parameters than unpaired data, requiring samples from 5-10 times fewer individuals. We then study SARS-CoV-2 in wild White-tailed deer (Odocoileus virginianus) from three counties in the United States. Estimates for average infectious times corroborate captive animal studies. Our methods use general statistical theory to let applications extend beyond the SIR model we consider, and to more complicated examples of paired data.</summary></entry><entry><title type="html">An adaptive enrichment design using Bayesian model averaging for selection and threshold-identification of tailoring variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AnadaptiveenrichmentdesignusingBayesianmodelaveragingforselectionandthresholdidentificationoftailoringvariables.html" rel="alternate" type="text/html" title="An adaptive enrichment design using Bayesian model averaging for selection and threshold-identification of tailoring variables" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AnadaptiveenrichmentdesignusingBayesianmodelaveragingforselectionandthresholdidentificationoftailoringvariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AnadaptiveenrichmentdesignusingBayesianmodelaveragingforselectionandthresholdidentificationoftailoringvariables.html">&lt;p&gt;Precision medicine stands as a transformative approach in healthcare, offering tailored treatments that can enhance patient outcomes and reduce healthcare costs. As understanding of complex disease improves, clinical trials are being designed to detect subgroups of patients with enhanced treatment effects. Biomarker-driven adaptive enrichment designs, which enroll a general population initially and later restrict accrual to treatment-sensitive patients, are gaining popularity. Current practice often assumes either pre-trial knowledge of biomarkers defining treatment-sensitive subpopulations or a simple, linear relationship between continuous markers and treatment effectiveness. Motivated by a trial studying rheumatoid arthritis treatment, we propose a Bayesian adaptive enrichment design which identifies important tailoring variables out of a larger set of candidate biomarkers. Our proposed design is equipped with a flexible modelling framework where the effects of continuous biomarkers are introduced using free knot B-splines. The parameters of interest are then estimated by marginalizing over the space of all possible variable combinations using Bayesian model averaging. At interim analyses, we assess whether a biomarker-defined subgroup has enhanced or reduced treatment effects, allowing for early termination due to efficacy or futility and restricting future enrollment to treatment-sensitive patients. We consider pre-categorized and continuous biomarkers, the latter of which may have complex, nonlinear relationships to the outcome and treatment effect. Using simulations, we derive the operating characteristics of our design and compare its performance to two existing approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08180&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lara Maleyeff, Shirin Golchi, Erica E. M. Moodie, Marie Hudson</name></author><category term="stat.ME" /><summary type="html">Precision medicine stands as a transformative approach in healthcare, offering tailored treatments that can enhance patient outcomes and reduce healthcare costs. As understanding of complex disease improves, clinical trials are being designed to detect subgroups of patients with enhanced treatment effects. Biomarker-driven adaptive enrichment designs, which enroll a general population initially and later restrict accrual to treatment-sensitive patients, are gaining popularity. Current practice often assumes either pre-trial knowledge of biomarkers defining treatment-sensitive subpopulations or a simple, linear relationship between continuous markers and treatment effectiveness. Motivated by a trial studying rheumatoid arthritis treatment, we propose a Bayesian adaptive enrichment design which identifies important tailoring variables out of a larger set of candidate biomarkers. Our proposed design is equipped with a flexible modelling framework where the effects of continuous biomarkers are introduced using free knot B-splines. The parameters of interest are then estimated by marginalizing over the space of all possible variable combinations using Bayesian model averaging. At interim analyses, we assess whether a biomarker-defined subgroup has enhanced or reduced treatment effects, allowing for early termination due to efficacy or futility and restricting future enrollment to treatment-sensitive patients. We consider pre-categorized and continuous biomarkers, the latter of which may have complex, nonlinear relationships to the outcome and treatment effect. Using simulations, we derive the operating characteristics of our design and compare its performance to two existing approaches.</summary></entry><entry><title type="html">Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AnytimevalidttestsandconfidencesequencesforGaussianmeanswithunknownvariance.html" rel="alternate" type="text/html" title="Anytime-valid t-tests and confidence sequences for Gaussian means with unknown variance" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AnytimevalidttestsandconfidencesequencesforGaussianmeanswithunknownvariance</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/AnytimevalidttestsandconfidencesequencesforGaussianmeanswithunknownvariance.html">&lt;p&gt;In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$ of a Gaussian distribution with unknown variance $\sigma^2$. Curiously, he employed both an improper (right Haar) mixture over $\sigma$ and an improper (flat) mixture over $\mu$. Here, we elaborate carefully on the details of his construction, which use generalized nonintegrable martingales and an extended Ville’s inequality. While this does yield a sequential t-test, it does not yield an “e-process” (due to the nonintegrability of his martingale). In this paper, we develop two new e-processes and confidence sequences for the same setting: one is a test martingale in a reduced filtration, while the other is an e-process in the canonical data filtration. These are respectively obtained by swapping Lai’s flat mixture for a Gaussian mixture, and swapping the right Haar mixture over $\sigma$ with the maximum likelihood estimate under the null, as done in universal inference. We also analyze the width of resulting confidence sequences, which have a curious polynomial dependence on the error probability $\alpha$ that we prove to be not only unavoidable, but (for universal inference) even better than the classical fixed-sample t-test. Numerical experiments are provided along the way to compare and contrast the various approaches, including some recent suboptimal ones.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.03722&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hongjian Wang, Aaditya Ramdas</name></author><category term="stat.ME," /><category term="stat.ML," /><category term="stat.TH" /><summary type="html">In 1976, Lai constructed a nontrivial confidence sequence for the mean $\mu$ of a Gaussian distribution with unknown variance $\sigma^2$. Curiously, he employed both an improper (right Haar) mixture over $\sigma$ and an improper (flat) mixture over $\mu$. Here, we elaborate carefully on the details of his construction, which use generalized nonintegrable martingales and an extended Ville’s inequality. While this does yield a sequential t-test, it does not yield an “e-process” (due to the nonintegrability of his martingale). In this paper, we develop two new e-processes and confidence sequences for the same setting: one is a test martingale in a reduced filtration, while the other is an e-process in the canonical data filtration. These are respectively obtained by swapping Lai’s flat mixture for a Gaussian mixture, and swapping the right Haar mixture over $\sigma$ with the maximum likelihood estimate under the null, as done in universal inference. We also analyze the width of resulting confidence sequences, which have a curious polynomial dependence on the error probability $\alpha$ that we prove to be not only unavoidable, but (for universal inference) even better than the classical fixed-sample t-test. Numerical experiments are provided along the way to compare and contrast the various approaches, including some recent suboptimal ones.</summary></entry><entry><title type="html">A structured regression approach for evaluating model performance across intersectional subgroups</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Astructuredregressionapproachforevaluatingmodelperformanceacrossintersectionalsubgroups.html" rel="alternate" type="text/html" title="A structured regression approach for evaluating model performance across intersectional subgroups" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Astructuredregressionapproachforevaluatingmodelperformanceacrossintersectionalsubgroups</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Astructuredregressionapproachforevaluatingmodelperformanceacrossintersectionalsubgroups.html">&lt;p&gt;Disaggregated evaluation is a central task in AI fairness assessment, where the goal is to measure an AI system’s performance across different subgroups defined by combinations of demographic or other sensitive attributes. The standard approach is to stratify the evaluation data across subgroups and compute performance metrics separately for each group. However, even for moderately-sized evaluation datasets, sample sizes quickly get small once considering intersectional subgroups, which greatly limits the extent to which intersectional groups are included in analysis. In this work, we introduce a structured regression approach to disaggregated evaluation that we demonstrate can yield reliable system performance estimates even for very small subgroups. We provide corresponding inference strategies for constructing confidence intervals and explore how goodness-of-fit testing can yield insight into the structure of fairness-related harms experienced by intersectional groups. We evaluate our approach on two publicly available datasets, and several variants of semi-synthetic data. The results show that our method is considerably more accurate than the standard approach, especially for small subgroups, and demonstrate how goodness-of-fit testing helps identify the key factors that drive differences in performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.14893&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Christine Herlihy, Kimberly Truong, Alexandra Chouldechova, Miroslav Dudik</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Disaggregated evaluation is a central task in AI fairness assessment, where the goal is to measure an AI system’s performance across different subgroups defined by combinations of demographic or other sensitive attributes. The standard approach is to stratify the evaluation data across subgroups and compute performance metrics separately for each group. However, even for moderately-sized evaluation datasets, sample sizes quickly get small once considering intersectional subgroups, which greatly limits the extent to which intersectional groups are included in analysis. In this work, we introduce a structured regression approach to disaggregated evaluation that we demonstrate can yield reliable system performance estimates even for very small subgroups. We provide corresponding inference strategies for constructing confidence intervals and explore how goodness-of-fit testing can yield insight into the structure of fairness-related harms experienced by intersectional groups. We evaluate our approach on two publicly available datasets, and several variants of semi-synthetic data. The results show that our method is considerably more accurate than the standard approach, especially for small subgroups, and demonstrate how goodness-of-fit testing helps identify the key factors that drive differences in performance.</summary></entry><entry><title type="html">Calibrated sensitivity models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Calibratedsensitivitymodels.html" rel="alternate" type="text/html" title="Calibrated sensitivity models" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Calibratedsensitivitymodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Calibratedsensitivitymodels.html">&lt;p&gt;In causal inference, sensitivity models assess how unmeasured confounders could alter causal analyses. However, the sensitivity parameter in these models – which quantifies the degree of unmeasured confounding – is often difficult to interpret. For this reason, researchers will sometimes compare the magnitude of the sensitivity parameter to an estimate for measured confounding. This is known as calibration. We propose novel calibrated sensitivity models, which directly incorporate measured confounding, and bound the degree of unmeasured confounding by a multiple of measured confounding. We illustrate how to construct calibrated sensitivity models via several examples. We also demonstrate their advantages over standard sensitivity analyses and calibration; in particular, the calibrated sensitivity parameter is an intuitive unit-less ratio of unmeasured divided by measured confounding, unlike standard sensitivity parameters, and one can correctly incorporate uncertainty due to estimating measured confounding, which standard calibration methods fail to do. By incorporating uncertainty due to measured confounding, we observe that causal analyses can be less robust or more robust to unmeasured confounding than would have been shown with standard approaches. We develop efficient estimators and methods for inference for bounds on the average treatment effect with three calibrated sensitivity models, and establish that our estimators are doubly robust and attain parametric efficiency and asymptotic normality under nonparametric conditions on their nuisance function estimators. We illustrate our methods with data analyses on the effect of exposure to violence on attitudes towards peace in Darfur and the effect of mothers’ smoking on infant birthweight.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08738&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alec McClean, Zach Branson, Edward H. Kennedy</name></author><category term="stat.ME" /><summary type="html">In causal inference, sensitivity models assess how unmeasured confounders could alter causal analyses. However, the sensitivity parameter in these models – which quantifies the degree of unmeasured confounding – is often difficult to interpret. For this reason, researchers will sometimes compare the magnitude of the sensitivity parameter to an estimate for measured confounding. This is known as calibration. We propose novel calibrated sensitivity models, which directly incorporate measured confounding, and bound the degree of unmeasured confounding by a multiple of measured confounding. We illustrate how to construct calibrated sensitivity models via several examples. We also demonstrate their advantages over standard sensitivity analyses and calibration; in particular, the calibrated sensitivity parameter is an intuitive unit-less ratio of unmeasured divided by measured confounding, unlike standard sensitivity parameters, and one can correctly incorporate uncertainty due to estimating measured confounding, which standard calibration methods fail to do. By incorporating uncertainty due to measured confounding, we observe that causal analyses can be less robust or more robust to unmeasured confounding than would have been shown with standard approaches. We develop efficient estimators and methods for inference for bounds on the average treatment effect with three calibrated sensitivity models, and establish that our estimators are doubly robust and attain parametric efficiency and asymptotic normality under nonparametric conditions on their nuisance function estimators. We illustrate our methods with data analyses on the effect of exposure to violence on attitudes towards peace in Darfur and the effect of mothers’ smoking on infant birthweight.</summary></entry><entry><title type="html">Causal Quantile Treatment Effects with missing data by double-sampling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/CausalQuantileTreatmentEffectswithmissingdatabydoublesampling.html" rel="alternate" type="text/html" title="Causal Quantile Treatment Effects with missing data by double-sampling" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/CausalQuantileTreatmentEffectswithmissingdatabydoublesampling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/CausalQuantileTreatmentEffectswithmissingdatabydoublesampling.html">&lt;p&gt;Causal weighted quantile treatment effects (WQTE) are a useful complement to standard causal contrasts that focus on the mean when interest lies at the tails of the counterfactual distribution. To-date, however, methods for estimation and inference regarding causal WQTEs have assumed complete data on all relevant factors. In most practical settings, however, data will be missing or incomplete data, particularly when the data are not collected for research purposes, as is the case for electronic health records and disease registries. Furthermore, such data sources may be particularly susceptible to the outcome data being missing-not-at-random (MNAR). In this paper, we consider the use of double-sampling, through which the otherwise missing data are ascertained on a sub-sample of study units, as a strategy to mitigate bias due to MNAR data in the estimation of causal WQTEs. With the additional data in-hand, we present identifying conditions that do not require assumptions regarding missingness in the original data. We then propose a novel inverse-probability weighted estimator and derive its asymptotic properties, both pointwise at specific quantiles and uniformly across a range of quantiles over some compact subset of (0,1), allowing the propensity score and double-sampling probabilities to be estimated. For practical inference, we develop a bootstrap method that can be used for both pointwise and uniform inference. A simulation study is conducted to examine the finite sample performance of the proposed estimators. The proposed method is illustrated with data from an EHR-based study examining the relative effects of two bariatric surgery procedures on BMI loss at 3 years post-surgery.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.09239&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shuo ,  Sun, Sebastien Haneuse, Alexander W. Levis, Catherine Lee, David E Arterburn, Heidi Fischer, Susan Shortreed, Rajarshi Mukherjee</name></author><category term="stat.ME" /><summary type="html">Causal weighted quantile treatment effects (WQTE) are a useful complement to standard causal contrasts that focus on the mean when interest lies at the tails of the counterfactual distribution. To-date, however, methods for estimation and inference regarding causal WQTEs have assumed complete data on all relevant factors. In most practical settings, however, data will be missing or incomplete data, particularly when the data are not collected for research purposes, as is the case for electronic health records and disease registries. Furthermore, such data sources may be particularly susceptible to the outcome data being missing-not-at-random (MNAR). In this paper, we consider the use of double-sampling, through which the otherwise missing data are ascertained on a sub-sample of study units, as a strategy to mitigate bias due to MNAR data in the estimation of causal WQTEs. With the additional data in-hand, we present identifying conditions that do not require assumptions regarding missingness in the original data. We then propose a novel inverse-probability weighted estimator and derive its asymptotic properties, both pointwise at specific quantiles and uniformly across a range of quantiles over some compact subset of (0,1), allowing the propensity score and double-sampling probabilities to be estimated. For practical inference, we develop a bootstrap method that can be used for both pointwise and uniform inference. A simulation study is conducted to examine the finite sample performance of the proposed estimators. The proposed method is illustrated with data from an EHR-based study examining the relative effects of two bariatric surgery procedures on BMI loss at 3 years post-surgery.</summary></entry><entry><title type="html">Causal health impacts of power plant emission controls under modeled and uncertain physical process interference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Causalhealthimpactsofpowerplantemissioncontrolsundermodeledanduncertainphysicalprocessinterference.html" rel="alternate" type="text/html" title="Causal health impacts of power plant emission controls under modeled and uncertain physical process interference" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Causalhealthimpactsofpowerplantemissioncontrolsundermodeledanduncertainphysicalprocessinterference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Causalhealthimpactsofpowerplantemissioncontrolsundermodeledanduncertainphysicalprocessinterference.html">&lt;p&gt;Causal inference with spatial environmental data is often challenging due to the presence of interference: outcomes for observational units depend on some combination of local and non-local treatment. This is especially relevant when estimating the effect of power plant emissions controls on population health, as pollution exposure is dictated by (i) the location of point-source emissions, as well as (ii) the transport of pollutants across space via dynamic physical-chemical processes. In this work, we estimate the effectiveness of air quality interventions at coal-fired power plants in reducing two adverse health outcomes in Texas in 2016: pediatric asthma ED visits and Medicare all-cause mortality. We develop methods for causal inference with interference when the underlying network structure is not known with certainty and instead must be estimated from ancillary data. Notably, uncertainty in the interference structure is propagated to the resulting causal effect estimates. We offer a Bayesian, spatial mechanistic model for the interference mapping which we combine with a flexible non-parametric outcome model to marginalize estimates of causal effects over uncertainty in the structure of interference. Our analysis finds some evidence that emissions controls at upwind power plants reduce asthma ED visits and all-cause mortality, however accounting for uncertainty in the interference renders the results largely inconclusive.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.05665&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nathan B. Wikle, Corwin M. Zigler</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">Causal inference with spatial environmental data is often challenging due to the presence of interference: outcomes for observational units depend on some combination of local and non-local treatment. This is especially relevant when estimating the effect of power plant emissions controls on population health, as pollution exposure is dictated by (i) the location of point-source emissions, as well as (ii) the transport of pollutants across space via dynamic physical-chemical processes. In this work, we estimate the effectiveness of air quality interventions at coal-fired power plants in reducing two adverse health outcomes in Texas in 2016: pediatric asthma ED visits and Medicare all-cause mortality. We develop methods for causal inference with interference when the underlying network structure is not known with certainty and instead must be estimated from ancillary data. Notably, uncertainty in the interference structure is propagated to the resulting causal effect estimates. We offer a Bayesian, spatial mechanistic model for the interference mapping which we combine with a flexible non-parametric outcome model to marginalize estimates of causal effects over uncertainty in the structure of interference. Our analysis finds some evidence that emissions controls at upwind power plants reduce asthma ED visits and all-cause mortality, however accounting for uncertainty in the interference renders the results largely inconclusive.</summary></entry><entry><title type="html">Community detection in bipartite signed networks is highly dependent on parameter choice</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Communitydetectioninbipartitesignednetworksishighlydependentonparameterchoice.html" rel="alternate" type="text/html" title="Community detection in bipartite signed networks is highly dependent on parameter choice" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Communitydetectioninbipartitesignednetworksishighlydependentonparameterchoice</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Communitydetectioninbipartitesignednetworksishighlydependentonparameterchoice.html">&lt;p&gt;Decision-making processes often involve voting. Human interactions with exogenous entities such as legislations or products can be effectively modeled as two-mode (bipartite) signed networks-where people can either vote positively, negatively, or abstain from voting on the entities. Detecting communities in such networks could help us understand underlying properties: for example ideological camps or consumer preferences. While community detection is an established practice separately for bipartite and signed networks, it remains largely unexplored in the case of bipartite signed networks. In this paper, we systematically evaluate the efficacy of community detection methods on bipartite signed networks using a synthetic benchmark and real-world datasets. Our findings reveal that when no communities are present in the data, these methods often recover spurious communities. When communities are present, the algorithms exhibit promising performance, although their performance is highly susceptible to parameter choice. This indicates that researchers using community detection methods in the context of bipartite signed networks should not take the communities found at face value: it is essential to assess the robustness of parameter choices or perform domain-specific external validation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08203&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Elena Candellone, Erik-Jan van Kesteren, Sofia Chelmi, Javier Garcia-Bernardo</name></author><category term="stat.ME" /><summary type="html">Decision-making processes often involve voting. Human interactions with exogenous entities such as legislations or products can be effectively modeled as two-mode (bipartite) signed networks-where people can either vote positively, negatively, or abstain from voting on the entities. Detecting communities in such networks could help us understand underlying properties: for example ideological camps or consumer preferences. While community detection is an established practice separately for bipartite and signed networks, it remains largely unexplored in the case of bipartite signed networks. In this paper, we systematically evaluate the efficacy of community detection methods on bipartite signed networks using a synthetic benchmark and real-world datasets. Our findings reveal that when no communities are present in the data, these methods often recover spurious communities. When communities are present, the algorithms exhibit promising performance, although their performance is highly susceptible to parameter choice. This indicates that researchers using community detection methods in the context of bipartite signed networks should not take the communities found at face value: it is essential to assess the robustness of parameter choices or perform domain-specific external validation.</summary></entry><entry><title type="html">Conditional probability tensor decompositions for multivariate categorical response regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Conditionalprobabilitytensordecompositionsformultivariatecategoricalresponseregression.html" rel="alternate" type="text/html" title="Conditional probability tensor decompositions for multivariate categorical response regression" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Conditionalprobabilitytensordecompositionsformultivariatecategoricalresponseregression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Conditionalprobabilitytensordecompositionsformultivariatecategoricalresponseregression.html">&lt;p&gt;In many modern regression applications, the response consists of multiple categorical random variables whose probability mass is a function of a common set of predictors. In this article, we propose a new method for modeling such a probability mass function in settings where the number of response variables, the number of categories per response, and the dimension of the predictor are large. Our method relies on a functional probability tensor decomposition: a decomposition of a tensor-valued function such that its range is a restricted set of low-rank probability tensors. This decomposition is motivated by the connection between the conditional independence of responses, or lack thereof, and their probability tensor rank. We show that the model implied by such a low-rank functional probability tensor decomposition can be interpreted in terms of a mixture of regressions and can thus be fit using maximum likelihood. We derive an efficient and scalable penalized expectation maximization algorithm to fit this model and examine its statistical properties. We demonstrate the encouraging performance of our method through both simulation studies and an application to modeling the functional classes of genes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2206.10676&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aaron J. Molstad, Xin Zhang</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In many modern regression applications, the response consists of multiple categorical random variables whose probability mass is a function of a common set of predictors. In this article, we propose a new method for modeling such a probability mass function in settings where the number of response variables, the number of categories per response, and the dimension of the predictor are large. Our method relies on a functional probability tensor decomposition: a decomposition of a tensor-valued function such that its range is a restricted set of low-rank probability tensors. This decomposition is motivated by the connection between the conditional independence of responses, or lack thereof, and their probability tensor rank. We show that the model implied by such a low-rank functional probability tensor decomposition can be interpreted in terms of a mixture of regressions and can thus be fit using maximum likelihood. We derive an efficient and scalable penalized expectation maximization algorithm to fit this model and examine its statistical properties. We demonstrate the encouraging performance of our method through both simulation studies and an application to modeling the functional classes of genes.</summary></entry><entry><title type="html">Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation.html" rel="alternate" type="text/html" title="Differentiable Pareto-Smoothed Weighting for High-Dimensional Heterogeneous Treatment Effect Estimation" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/DifferentiableParetoSmoothedWeightingforHighDimensionalHeterogeneousTreatmentEffectEstimation.html">&lt;p&gt;There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using inverse probability weighting (IPW). However, due to their numerically unstable IPW weights, these methods suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator by weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Our experimental results show that by effectively correcting the weight values, our proposed method outperforms the existing ones, including traditional weighting schemes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17483&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yoichi Chikahara, Kansei Ushiyama</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">There is a growing interest in estimating heterogeneous treatment effects across individuals using their high-dimensional feature attributes. Achieving high performance in such high-dimensional heterogeneous treatment effect estimation is challenging because in this setup, it is usual that some features induce sample selection bias while others do not but are predictive of potential outcomes. To avoid losing such predictive feature information, existing methods learn separate feature representations using inverse probability weighting (IPW). However, due to their numerically unstable IPW weights, these methods suffer from estimation bias under a finite sample setup. To develop a numerically robust estimator by weighted representation learning, we propose a differentiable Pareto-smoothed weighting framework that replaces extreme weight values in an end-to-end fashion. Our experimental results show that by effectively correcting the weight values, our proposed method outperforms the existing ones, including traditional weighting schemes.</summary></entry><entry><title type="html">Do Bayesian imaging methods report trustworthy probabilities?</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/DoBayesianimagingmethodsreporttrustworthyprobabilities.html" rel="alternate" type="text/html" title="Do Bayesian imaging methods report trustworthy probabilities?" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/DoBayesianimagingmethodsreporttrustworthyprobabilities</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/DoBayesianimagingmethodsreporttrustworthyprobabilities.html">&lt;p&gt;Bayesian statistics is a cornerstone of imaging sciences, underpinning many and varied approaches from Markov random fields to score-based denoising diffusion models. In addition to powerful image estimation methods, the Bayesian paradigm also provides a framework for uncertainty quantification and for using image data as quantitative evidence. These probabilistic capabilities are important for the rigorous interpretation of experimental results and for robust interfacing of quantitative imaging pipelines with scientific and decision-making processes. However, are the probabilities delivered by existing Bayesian imaging methods meaningful under replication of an experiment, or are they only meaningful as subjective measures of belief? This paper presents a Monte Carlo method to explore this question. We then leverage the proposed Monte Carlo method and run a large experiment requiring 1,000 GPU-hours to probe the accuracy of five canonical Bayesian imaging methods that are representative of some of the main Bayesian imaging strategies from the past decades (a score-based denoising diffusion technique, a plug-and-play Langevin algorithm utilising a Lipschitz-regularised DnCNN denoiser, a Bayesian method with a dictionary-based prior trained subject to a log-concavity constraint, an empirical Bayesian method with a total-variation prior, and a hierarchical Bayesian Gibbs sampler based on a Gaussian Markov random field model). We find that, a few cases, the probabilities reported by modern Bayesian imaging techniques are in broad agreement with long-term averages as observed over a large number of replication of an experiment, but existing Bayesian imaging methods are generally not able to deliver reliable uncertainty quantification results.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08179&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David Y. W. Thong, Charlesquin Kemajou Mbakam, Marcelo Pereyra</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Bayesian statistics is a cornerstone of imaging sciences, underpinning many and varied approaches from Markov random fields to score-based denoising diffusion models. In addition to powerful image estimation methods, the Bayesian paradigm also provides a framework for uncertainty quantification and for using image data as quantitative evidence. These probabilistic capabilities are important for the rigorous interpretation of experimental results and for robust interfacing of quantitative imaging pipelines with scientific and decision-making processes. However, are the probabilities delivered by existing Bayesian imaging methods meaningful under replication of an experiment, or are they only meaningful as subjective measures of belief? This paper presents a Monte Carlo method to explore this question. We then leverage the proposed Monte Carlo method and run a large experiment requiring 1,000 GPU-hours to probe the accuracy of five canonical Bayesian imaging methods that are representative of some of the main Bayesian imaging strategies from the past decades (a score-based denoising diffusion technique, a plug-and-play Langevin algorithm utilising a Lipschitz-regularised DnCNN denoiser, a Bayesian method with a dictionary-based prior trained subject to a log-concavity constraint, an empirical Bayesian method with a total-variation prior, and a hierarchical Bayesian Gibbs sampler based on a Gaussian Markov random field model). We find that, a few cases, the probabilities reported by modern Bayesian imaging techniques are in broad agreement with long-term averages as observed over a large number of replication of an experiment, but existing Bayesian imaging methods are generally not able to deliver reliable uncertainty quantification results.</summary></entry><entry><title type="html">Doubly-robust inference and optimality in structure-agnostic models with smoothness</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Doublyrobustinferenceandoptimalityinstructureagnosticmodelswithsmoothness.html" rel="alternate" type="text/html" title="Doubly-robust inference and optimality in structure-agnostic models with smoothness" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Doublyrobustinferenceandoptimalityinstructureagnosticmodelswithsmoothness</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Doublyrobustinferenceandoptimalityinstructureagnosticmodelswithsmoothness.html">&lt;p&gt;We study the problem of constructing an estimator of the average treatment effect (ATE) that exhibits doubly-robust asymptotic linearity (DRAL). This is a stronger requirement than doubly-robust consistency. A DRAL estimator can yield asymptotically valid Wald-type confidence intervals even when the propensity score or the outcome model is inconsistently estimated. On the contrary, the celebrated doubly-robust, augmented-IPW (AIPW) estimator generally requires consistent estimation of both nuisance functions for standard root-n inference. We make three main contributions. First, we propose a new hybrid class of distributions that consists of the structure-agnostic class introduced in Balakrishnan et al (2023) with additional smoothness constraints. While DRAL is generally not possible in the pure structure-agnostic class, we show that it can be attained in the new hybrid one. Second, we calculate minimax lower bounds for estimating the ATE in the new class, as well as in the pure structure-agnostic one. Third, building upon the literature on doubly-robust inference (van der Laan, 2014, Benkeser et al, 2017, Dukes et al 2021), we propose a new estimator of the ATE that enjoys DRAL. Under certain conditions, we show that its rate of convergence in the new class can be much faster than that achieved by the AIPW estimator and, in particular, matches the minimax lower bound rate, thereby establishing its optimality. Finally, we clarify the connection between DRAL estimators and those based on higher-order influence functions (Robins et al, 2017) and complement our theoretical findings with simulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08525&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matteo Bonvini, Edward H. Kennedy, Oliver Dukes, Sivaraman Balakrishnan</name></author><category term="stat.ME" /><summary type="html">We study the problem of constructing an estimator of the average treatment effect (ATE) that exhibits doubly-robust asymptotic linearity (DRAL). This is a stronger requirement than doubly-robust consistency. A DRAL estimator can yield asymptotically valid Wald-type confidence intervals even when the propensity score or the outcome model is inconsistently estimated. On the contrary, the celebrated doubly-robust, augmented-IPW (AIPW) estimator generally requires consistent estimation of both nuisance functions for standard root-n inference. We make three main contributions. First, we propose a new hybrid class of distributions that consists of the structure-agnostic class introduced in Balakrishnan et al (2023) with additional smoothness constraints. While DRAL is generally not possible in the pure structure-agnostic class, we show that it can be attained in the new hybrid one. Second, we calculate minimax lower bounds for estimating the ATE in the new class, as well as in the pure structure-agnostic one. Third, building upon the literature on doubly-robust inference (van der Laan, 2014, Benkeser et al, 2017, Dukes et al 2021), we propose a new estimator of the ATE that enjoys DRAL. Under certain conditions, we show that its rate of convergence in the new class can be much faster than that achieved by the AIPW estimator and, in particular, matches the minimax lower bound rate, thereby establishing its optimality. Finally, we clarify the connection between DRAL estimators and those based on higher-order influence functions (Robins et al, 2017) and complement our theoretical findings with simulations.</summary></entry><entry><title type="html">Dynamic Local Average Treatment Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/DynamicLocalAverageTreatmentEffects.html" rel="alternate" type="text/html" title="Dynamic Local Average Treatment Effects" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/DynamicLocalAverageTreatmentEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/DynamicLocalAverageTreatmentEffects.html">&lt;p&gt;We consider Dynamic Treatment Regimes (DTRs) with One Sided Noncompliance that arise in applications such as digital recommendations and adaptive medical trials. These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes. Importantly, individuals may not comply with encouragements based on unobserved confounders. For settings with binary treatments and encouragements, we provide nonparametric identification, estimation, and inference for Dynamic Local Average Treatment Effects (LATEs), which are expected values of multiple time period treatment contrasts for the respective complier subpopulations. Under standard assumptions in the Instrumental Variable and DTR literature, we show that one can identify Dynamic LATEs that correspond to treating at single time steps. Under an additional cross-period effect-compliance independence assumption, which is satisfied in Staggered Adoption settings and a generalization of them, which we define as Staggered Compliance settings, we identify Dynamic LATEs for treating in multiple time periods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.01463&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ravi B. Sojitra, Vasilis Syrgkanis</name></author><category term="stat.ME" /><summary type="html">We consider Dynamic Treatment Regimes (DTRs) with One Sided Noncompliance that arise in applications such as digital recommendations and adaptive medical trials. These are settings where decision makers encourage individuals to take treatments over time, but adapt encouragements based on previous encouragements, treatments, states, and outcomes. Importantly, individuals may not comply with encouragements based on unobserved confounders. For settings with binary treatments and encouragements, we provide nonparametric identification, estimation, and inference for Dynamic Local Average Treatment Effects (LATEs), which are expected values of multiple time period treatment contrasts for the respective complier subpopulations. Under standard assumptions in the Instrumental Variable and DTR literature, we show that one can identify Dynamic LATEs that correspond to treating at single time steps. Under an additional cross-period effect-compliance independence assumption, which is satisfied in Staggered Adoption settings and a generalization of them, which we define as Staggered Compliance settings, we identify Dynamic LATEs for treating in multiple time periods.</summary></entry><entry><title type="html">Filtered Partial Differential Equations: a robust surrogate constraint in physics-informed deep learning framework</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/FilteredPartialDifferentialEquationsarobustsurrogateconstraintinphysicsinformeddeeplearningframework.html" rel="alternate" type="text/html" title="Filtered Partial Differential Equations: a robust surrogate constraint in physics-informed deep learning framework" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/FilteredPartialDifferentialEquationsarobustsurrogateconstraintinphysicsinformeddeeplearningframework</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/FilteredPartialDifferentialEquationsarobustsurrogateconstraintinphysicsinformeddeeplearningframework.html">&lt;p&gt;Embedding physical knowledge into neural network (NN) training has been a hot topic. However, when facing the complex real-world, most of the existing methods still strongly rely on the quantity and quality of observation data. Furthermore, the neural networks often struggle to converge when the solution to the real equation is very complex. Inspired by large eddy simulation in computational fluid dynamics, we propose an improved method based on filtering. We analyzed the causes of the difficulties in physics informed machine learning, and proposed a surrogate constraint (filtered PDE, FPDE in short) of the original physical equations to reduce the influence of noisy and sparse observation data. In the noise and sparsity experiment, the proposed FPDE models (which are optimized by FPDE constraints) have better robustness than the conventional PDE models. Experiments demonstrate that the FPDE model can obtain the same quality solution with 100% higher noise and 12% quantity of observation data of the baseline. Besides, two groups of real measurement data are used to show the FPDE improvements in real cases. The final results show that FPDE still gives more physically reasonable solutions when facing the incomplete equation problem and the extremely sparse and high-noise conditions. For combining real-world experiment data into physics-informed training, the proposed FPDE constraint is useful and performs well in two real-world experiments: modeling the blood velocity in vessels and cell migration in scratches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.03776&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Dashan Zhang, Yuntian Chen, Shiyi Chen</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Embedding physical knowledge into neural network (NN) training has been a hot topic. However, when facing the complex real-world, most of the existing methods still strongly rely on the quantity and quality of observation data. Furthermore, the neural networks often struggle to converge when the solution to the real equation is very complex. Inspired by large eddy simulation in computational fluid dynamics, we propose an improved method based on filtering. We analyzed the causes of the difficulties in physics informed machine learning, and proposed a surrogate constraint (filtered PDE, FPDE in short) of the original physical equations to reduce the influence of noisy and sparse observation data. In the noise and sparsity experiment, the proposed FPDE models (which are optimized by FPDE constraints) have better robustness than the conventional PDE models. Experiments demonstrate that the FPDE model can obtain the same quality solution with 100% higher noise and 12% quantity of observation data of the baseline. Besides, two groups of real measurement data are used to show the FPDE improvements in real cases. The final results show that FPDE still gives more physically reasonable solutions when facing the incomplete equation problem and the extremely sparse and high-noise conditions. For combining real-world experiment data into physics-informed training, the proposed FPDE constraint is useful and performs well in two real-world experiments: modeling the blood velocity in vessels and cell migration in scratches.</summary></entry><entry><title type="html">Finding Groups of Cross-Correlated Features in Bi-View Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/FindingGroupsofCrossCorrelatedFeaturesinBiViewData.html" rel="alternate" type="text/html" title="Finding Groups of Cross-Correlated Features in Bi-View Data" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/FindingGroupsofCrossCorrelatedFeaturesinBiViewData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/FindingGroupsofCrossCorrelatedFeaturesinBiViewData.html">&lt;p&gt;Datasets in which measurements of two (or more) types are obtained from a common set of samples arise in many scientific applications. A common problem in the exploratory analysis of such data is to identify groups of features of different data types that are strongly associated. A bimodule is a pair (A,B) of feature sets from two data types such that the aggregate cross-correlation between the features in A and those in B is large. A bimodule (A,B) is stable if A coincides with the set of features that have significant aggregate correlation with the features in B, and vice-versa. This paper proposes an iterative-testing based bimodule search procedure (BSP) to identify stable bimodules.
  Compared to existing methods for detecting cross-correlated features, BSP was the best at recovering true bimodules with sufficient signal, while limiting the false discoveries. In addition, we applied BSP to the problem of expression quantitative trait loci (eQTL) analysis using data from the GTEx consortium. BSP identified several thousand SNP-gene bimodules. While many of the individual SNP-gene pairs appearing in the discovered bimodules were identified by standard eQTL methods, the discovered bimodules revealed genomic subnetworks that appeared to be biologically meaningful and worthy of further scientific investigation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2009.05079&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Miheer Dewaskar, John Palowitch, Mark He, Michael I. Love, Andrew B. Nobel</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Datasets in which measurements of two (or more) types are obtained from a common set of samples arise in many scientific applications. A common problem in the exploratory analysis of such data is to identify groups of features of different data types that are strongly associated. A bimodule is a pair (A,B) of feature sets from two data types such that the aggregate cross-correlation between the features in A and those in B is large. A bimodule (A,B) is stable if A coincides with the set of features that have significant aggregate correlation with the features in B, and vice-versa. This paper proposes an iterative-testing based bimodule search procedure (BSP) to identify stable bimodules. Compared to existing methods for detecting cross-correlated features, BSP was the best at recovering true bimodules with sufficient signal, while limiting the false discoveries. In addition, we applied BSP to the problem of expression quantitative trait loci (eQTL) analysis using data from the GTEx consortium. BSP identified several thousand SNP-gene bimodules. While many of the individual SNP-gene pairs appearing in the discovered bimodules were identified by standard eQTL methods, the discovered bimodules revealed genomic subnetworks that appeared to be biologically meaningful and worthy of further scientific investigation.</summary></entry><entry><title type="html">How Inverse Conditional Flows Can Serve as a Substitute for Distributional Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/HowInverseConditionalFlowsCanServeasaSubstituteforDistributionalRegression.html" rel="alternate" type="text/html" title="How Inverse Conditional Flows Can Serve as a Substitute for Distributional Regression" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/HowInverseConditionalFlowsCanServeasaSubstituteforDistributionalRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/HowInverseConditionalFlowsCanServeasaSubstituteforDistributionalRegression.html">&lt;p&gt;Neural network representations of simple models, such as linear regression, are being studied increasingly to better understand the underlying principles of deep learning algorithms. However, neural representations of distributional regression models, such as the Cox model, have received little attention so far. We close this gap by proposing a framework for distributional regression using inverse flow transformations (DRIFT), which includes neural representations of the aforementioned models. We empirically demonstrate that the neural representations of models in DRIFT can serve as a substitute for their classical statistical counterparts in several applications involving continuous, ordered, time-series, and survival outcomes. We confirm that models in DRIFT empirically match the performance of several statistical methods in terms of estimation of partial effects, prediction, and aleatoric uncertainty quantification. DRIFT covers both interpretable statistical models and flexible neural networks opening up new avenues in both statistical modeling and deep learning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05429&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lucas Kook, Chris Kolb, Philipp Schiele, Daniel Dold, Marcel Arpogaus, Cornelius Fritz, Philipp F. Baumann, Philipp Kopper, Tobias Pielok, Emilio Dorigatti, David Rügamer</name></author><category term="stat.CO," /><category term="stat.ML" /><summary type="html">Neural network representations of simple models, such as linear regression, are being studied increasingly to better understand the underlying principles of deep learning algorithms. However, neural representations of distributional regression models, such as the Cox model, have received little attention so far. We close this gap by proposing a framework for distributional regression using inverse flow transformations (DRIFT), which includes neural representations of the aforementioned models. We empirically demonstrate that the neural representations of models in DRIFT can serve as a substitute for their classical statistical counterparts in several applications involving continuous, ordered, time-series, and survival outcomes. We confirm that models in DRIFT empirically match the performance of several statistical methods in terms of estimation of partial effects, prediction, and aleatoric uncertainty quantification. DRIFT covers both interpretable statistical models and flexible neural networks opening up new avenues in both statistical modeling and deep learning.</summary></entry><entry><title type="html">Intervention effects based on potential benefit</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Interventioneffectsbasedonpotentialbenefit.html" rel="alternate" type="text/html" title="Intervention effects based on potential benefit" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Interventioneffectsbasedonpotentialbenefit</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Interventioneffectsbasedonpotentialbenefit.html">&lt;p&gt;Optimal treatment rules are mappings from individual patient characteristics to tailored treatment assignments that maximize mean outcomes. In this work, we introduce a conditional potential benefit (CPB) metric that measures the expected improvement under an optimally chosen treatment compared to the status quo, within covariate strata. The potential benefit combines (i) the magnitude of the treatment effect, and (ii) the propensity for subjects to naturally select a suboptimal treatment. As a consequence, heterogeneity in the CPB can provide key insights into the mechanism by which a treatment acts and/or highlight potential barriers to treatment access or adverse effects. Moreover, we demonstrate that CPB is the natural prioritization score for individualized treatment policies when intervention capacity is constrained. That is, in the resource-limited setting where treatment options are freely accessible, but the ability to intervene on a portion of the target population is constrained (e.g., if the population is large, and follow-up and encouragement of treatment uptake is labor-intensive), targeting subjects with highest CPB maximizes the mean outcome. Focusing on this resource-limited setting, we derive formulas for optimal constrained treatment rules, and for any given budget, quantify the loss compared to the optimal unconstrained rule. We describe sufficient identification assumptions, and propose nonparametric, robust, and efficient estimators of the proposed quantities emerging from our framework.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08727&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexander W. Levis, Eli Ben-Michael, Edward H. Kennedy</name></author><category term="stat.ME" /><summary type="html">Optimal treatment rules are mappings from individual patient characteristics to tailored treatment assignments that maximize mean outcomes. In this work, we introduce a conditional potential benefit (CPB) metric that measures the expected improvement under an optimally chosen treatment compared to the status quo, within covariate strata. The potential benefit combines (i) the magnitude of the treatment effect, and (ii) the propensity for subjects to naturally select a suboptimal treatment. As a consequence, heterogeneity in the CPB can provide key insights into the mechanism by which a treatment acts and/or highlight potential barriers to treatment access or adverse effects. Moreover, we demonstrate that CPB is the natural prioritization score for individualized treatment policies when intervention capacity is constrained. That is, in the resource-limited setting where treatment options are freely accessible, but the ability to intervene on a portion of the target population is constrained (e.g., if the population is large, and follow-up and encouragement of treatment uptake is labor-intensive), targeting subjects with highest CPB maximizes the mean outcome. Focusing on this resource-limited setting, we derive formulas for optimal constrained treatment rules, and for any given budget, quantify the loss compared to the optimal unconstrained rule. We describe sufficient identification assumptions, and propose nonparametric, robust, and efficient estimators of the proposed quantities emerging from our framework.</summary></entry><entry><title type="html">Learning Linear Polytree Structural Equation Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/LearningLinearPolytreeStructuralEquationModels.html" rel="alternate" type="text/html" title="Learning Linear Polytree Structural Equation Models" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/LearningLinearPolytreeStructuralEquationModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/LearningLinearPolytreeStructuralEquationModels.html">&lt;p&gt;We are interested in the problem of learning the directed acyclic graph (DAG) when data are generated from a linear structural equation model (SEM) and the causal structure can be characterized by a polytree. Under the Gaussian polytree models, we study sufficient conditions on the sample sizes for the well-known Chow-Liu algorithm to exactly recover both the skeleton and the equivalence class of the polytree, which is uniquely represented by a CPDAG. On the other hand, necessary conditions on the required sample sizes for both skeleton and CPDAG recovery are also derived in terms of information-theoretic lower bounds, which match the respective sufficient conditions and thereby give a sharp characterization of the difficulty of these tasks. We also consider the problem of inverse correlation matrix estimation under the linear polytree models, and establish the estimation error bound in terms of the dimension and the total number of v-structures. We also consider an extension of group linear polytree models, in which each node represents a group of variables. Our theoretical findings are illustrated by comprehensive numerical simulations, and experiments on benchmark data also demonstrate the robustness of polytree learning when the true graphical structures can only be approximated by polytrees.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2107.10955&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xingmei Lou, Yu Hu, Xiaodong Li</name></author><category term="stat.ML," /><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We are interested in the problem of learning the directed acyclic graph (DAG) when data are generated from a linear structural equation model (SEM) and the causal structure can be characterized by a polytree. Under the Gaussian polytree models, we study sufficient conditions on the sample sizes for the well-known Chow-Liu algorithm to exactly recover both the skeleton and the equivalence class of the polytree, which is uniquely represented by a CPDAG. On the other hand, necessary conditions on the required sample sizes for both skeleton and CPDAG recovery are also derived in terms of information-theoretic lower bounds, which match the respective sufficient conditions and thereby give a sharp characterization of the difficulty of these tasks. We also consider the problem of inverse correlation matrix estimation under the linear polytree models, and establish the estimation error bound in terms of the dimension and the total number of v-structures. We also consider an extension of group linear polytree models, in which each node represents a group of variables. Our theoretical findings are illustrated by comprehensive numerical simulations, and experiments on benchmark data also demonstrate the robustness of polytree learning when the true graphical structures can only be approximated by polytrees.</summary></entry><entry><title type="html">Long-term Causal Inference Under Persistent Confounding via Data Combination</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/LongtermCausalInferenceUnderPersistentConfoundingviaDataCombination.html" rel="alternate" type="text/html" title="Long-term Causal Inference Under Persistent Confounding via Data Combination" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/LongtermCausalInferenceUnderPersistentConfoundingviaDataCombination</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/LongtermCausalInferenceUnderPersistentConfoundingviaDataCombination.html">&lt;p&gt;We study the identification and estimation of long-term treatment effects when both experimental and observational data are available. Since the long-term outcome is observed only after a long delay, it is not measured in the experimental data, but only recorded in the observational data. However, both types of data include observations of some short-term outcomes. In this paper, we uniquely tackle the challenge of persistent unmeasured confounders, i.e., some unmeasured confounders that can simultaneously affect the treatment, short-term outcomes and the long-term outcome, noting that they invalidate identification strategies in previous literature. To address this challenge, we exploit the sequential structure of multiple short-term outcomes, and develop three novel identification strategies for the average long-term treatment effect. We further propose three corresponding estimators and prove their asymptotic consistency and asymptotic normality. We finally apply our methods to estimate the effect of a job training program on long-term employment using semi-synthetic data. We numerically show that our proposals outperform existing methods that fail to handle persistent confounders.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2202.07234&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Guido Imbens, Nathan Kallus, Xiaojie Mao, Yuhao Wang</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">We study the identification and estimation of long-term treatment effects when both experimental and observational data are available. Since the long-term outcome is observed only after a long delay, it is not measured in the experimental data, but only recorded in the observational data. However, both types of data include observations of some short-term outcomes. In this paper, we uniquely tackle the challenge of persistent unmeasured confounders, i.e., some unmeasured confounders that can simultaneously affect the treatment, short-term outcomes and the long-term outcome, noting that they invalidate identification strategies in previous literature. To address this challenge, we exploit the sequential structure of multiple short-term outcomes, and develop three novel identification strategies for the average long-term treatment effect. We further propose three corresponding estimators and prove their asymptotic consistency and asymptotic normality. We finally apply our methods to estimate the effect of a job training program on long-term employment using semi-synthetic data. We numerically show that our proposals outperform existing methods that fail to handle persistent confounders.</summary></entry><entry><title type="html">Low-order outcomes and clustered designs: combining design and analysis for causal inference under network interference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Loworderoutcomesandclustereddesignscombiningdesignandanalysisforcausalinferenceundernetworkinterference.html" rel="alternate" type="text/html" title="Low-order outcomes and clustered designs: combining design and analysis for causal inference under network interference" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Loworderoutcomesandclustereddesignscombiningdesignandanalysisforcausalinferenceundernetworkinterference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Loworderoutcomesandclustereddesignscombiningdesignandanalysisforcausalinferenceundernetworkinterference.html">&lt;p&gt;Variance reduction for causal inference in the presence of network interference is often achieved through either outcome modeling, which is typically analyzed under unit-randomized Bernoulli designs, or clustered experimental designs, which are typically analyzed without strong parametric assumptions. In this work, we study the intersection of these two approaches and consider the problem of estimation in low-order outcome models using data from a general experimental design. Our contributions are threefold. First, we present an estimator of the total treatment effect (also called the global average treatment effect) in a low-degree outcome model when the data are collected under general experimental designs, generalizing previous results for Bernoulli designs. We refer to this estimator as the pseudoinverse estimator and give bounds on its bias and variance in terms of properties of the experimental design. Second, we evaluate these bounds for the case of cluster randomized designs with both Bernoulli and complete randomization. For clustered Bernoulli randomization, we find that our estimator is always unbiased and that its variance scales like the smaller of the variance obtained from a low-order assumption and the variance obtained from cluster randomization, showing that combining these variance reduction strategies is preferable to using either individually. For clustered complete randomization, we find a notable bias-variance trade-off mediated by specific features of the clustering. Third, when choosing a clustered experimental design, our bounds can be used to select a clustering from a set of candidate clusterings. Across a range of graphs and clustering algorithms, we show that our method consistently selects clusterings that perform well on a range of response models, suggesting that our bounds are useful to practitioners.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.07979&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matthew Eichhorn, Samir Khan, Johan Ugander, Christina Lee Yu</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Variance reduction for causal inference in the presence of network interference is often achieved through either outcome modeling, which is typically analyzed under unit-randomized Bernoulli designs, or clustered experimental designs, which are typically analyzed without strong parametric assumptions. In this work, we study the intersection of these two approaches and consider the problem of estimation in low-order outcome models using data from a general experimental design. Our contributions are threefold. First, we present an estimator of the total treatment effect (also called the global average treatment effect) in a low-degree outcome model when the data are collected under general experimental designs, generalizing previous results for Bernoulli designs. We refer to this estimator as the pseudoinverse estimator and give bounds on its bias and variance in terms of properties of the experimental design. Second, we evaluate these bounds for the case of cluster randomized designs with both Bernoulli and complete randomization. For clustered Bernoulli randomization, we find that our estimator is always unbiased and that its variance scales like the smaller of the variance obtained from a low-order assumption and the variance obtained from cluster randomization, showing that combining these variance reduction strategies is preferable to using either individually. For clustered complete randomization, we find a notable bias-variance trade-off mediated by specific features of the clustering. Third, when choosing a clustered experimental design, our bounds can be used to select a clustering from a set of candidate clusterings. Across a range of graphs and clustering algorithms, we show that our method consistently selects clusterings that perform well on a range of response models, suggesting that our bounds are useful to practitioners.</summary></entry><entry><title type="html">MCMC using $\textit{bouncy}$ Hamiltonian dynamics: A unifying framework for Hamiltonian Monte Carlo and piecewise deterministic Markov process samplers</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/MCMCusingtextitbouncyHamiltoniandynamicsAunifyingframeworkforHamiltonianMonteCarloandpiecewisedeterministicMarkovprocesssamplers.html" rel="alternate" type="text/html" title="MCMC using $\textit{bouncy}$ Hamiltonian dynamics: A unifying framework for Hamiltonian Monte Carlo and piecewise deterministic Markov process samplers" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/MCMCusingtextitbouncyHamiltoniandynamicsAunifyingframeworkforHamiltonianMonteCarloandpiecewisedeterministicMarkovprocesssamplers</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/MCMCusingtextitbouncyHamiltoniandynamicsAunifyingframeworkforHamiltonianMonteCarloandpiecewisedeterministicMarkovprocesssamplers.html">&lt;p&gt;Piecewise-deterministic Markov process (PDMP) samplers constitute a state of the art Markov chain Monte Carlo (MCMC) paradigm in Bayesian computation, with examples including the zig-zag and bouncy particle sampler (BPS). Recent work on the zig-zag has indicated its connection to Hamiltonian Monte Carlo, a version of the Metropolis algorithm that exploits Hamiltonian dynamics. Here we establish that, in fact, the connection between the paradigms extends far beyond the specific instance. The key lies in (1) the fact that any time-reversible deterministic dynamics provides a valid Metropolis proposal and (2) how PDMPs’ characteristic velocity changes constitute an alternative to the usual acceptance-rejection. We turn this observation into a rigorous framework for constructing rejection-free Metropolis proposals based on bouncy Hamiltonian dynamics which simultaneously possess Hamiltonian-like properties and generate discontinuous trajectories similar in appearance to PDMPs. When combined with periodic refreshment of the inertia, the dynamics converge strongly to PDMP equivalents in the limit of increasingly frequent refreshment. We demonstrate the practical implications of this new paradigm, with a sampler based on a bouncy Hamiltonian dynamics closely related to the BPS. The resulting sampler exhibits competitive performance on challenging real-data posteriors involving tens of thousands of parameters.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08290&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrew Chin, Akihiko Nishimura</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Piecewise-deterministic Markov process (PDMP) samplers constitute a state of the art Markov chain Monte Carlo (MCMC) paradigm in Bayesian computation, with examples including the zig-zag and bouncy particle sampler (BPS). Recent work on the zig-zag has indicated its connection to Hamiltonian Monte Carlo, a version of the Metropolis algorithm that exploits Hamiltonian dynamics. Here we establish that, in fact, the connection between the paradigms extends far beyond the specific instance. The key lies in (1) the fact that any time-reversible deterministic dynamics provides a valid Metropolis proposal and (2) how PDMPs’ characteristic velocity changes constitute an alternative to the usual acceptance-rejection. We turn this observation into a rigorous framework for constructing rejection-free Metropolis proposals based on bouncy Hamiltonian dynamics which simultaneously possess Hamiltonian-like properties and generate discontinuous trajectories similar in appearance to PDMPs. When combined with periodic refreshment of the inertia, the dynamics converge strongly to PDMP equivalents in the limit of increasingly frequent refreshment. We demonstrate the practical implications of this new paradigm, with a sampler based on a bouncy Hamiltonian dynamics closely related to the BPS. The resulting sampler exhibits competitive performance on challenging real-data posteriors involving tens of thousands of parameters.</summary></entry><entry><title type="html">One-shot Generative Data Augmentation with Bounded Divergence for UAV Identification in Limited RF Environments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/OneshotGenerativeDataAugmentationwithBoundedDivergenceforUAVIdentificationinLimitedRFEnvironments.html" rel="alternate" type="text/html" title="One-shot Generative Data Augmentation with Bounded Divergence for UAV Identification in Limited RF Environments" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/OneshotGenerativeDataAugmentationwithBoundedDivergenceforUAVIdentificationinLimitedRFEnvironments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/OneshotGenerativeDataAugmentationwithBoundedDivergenceforUAVIdentificationinLimitedRFEnvironments.html">&lt;p&gt;This work addresses the pressing need for cybersecurity in Unmanned Aerial Vehicles (UAVs), particularly focusing on the challenges of identifying UAVs using radiofrequency (RF) fingerprinting in constrained environments. The complexity and variability of RF signals, influenced by environmental interference and hardware imperfections, often render traditional RF-based identification methods ineffective. To address these complications, the study introduces the rigorous use of one-shot generative methods for augmenting transformed RF signals, offering a significant improvement in UAV identification. This approach shows promise in low-data regimes, outperforming deep generative methods like conditional generative adversarial networks (GANs) and variational autoencoders (VAEs). The paper provides a theoretical guarantee for the effectiveness of one-shot generative models in augmenting limited data, setting a precedent for their application in limited RF environments. This research not only contributes to the cybersecurity of UAVs but also rigorously broadens the scope of machine learning techniques in data-constrained scenarios, which may include atypical complex sequences beyond images and videos.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.08403&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Amir Kazemi, Salar Basiri, Volodymyr Kindratenko, Srinivasa Salapaka</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">This work addresses the pressing need for cybersecurity in Unmanned Aerial Vehicles (UAVs), particularly focusing on the challenges of identifying UAVs using radiofrequency (RF) fingerprinting in constrained environments. The complexity and variability of RF signals, influenced by environmental interference and hardware imperfections, often render traditional RF-based identification methods ineffective. To address these complications, the study introduces the rigorous use of one-shot generative methods for augmenting transformed RF signals, offering a significant improvement in UAV identification. This approach shows promise in low-data regimes, outperforming deep generative methods like conditional generative adversarial networks (GANs) and variational autoencoders (VAEs). The paper provides a theoretical guarantee for the effectiveness of one-shot generative models in augmenting limited data, setting a precedent for their application in limited RF environments. This research not only contributes to the cybersecurity of UAVs but also rigorously broadens the scope of machine learning techniques in data-constrained scenarios, which may include atypical complex sequences beyond images and videos.</summary></entry><entry><title type="html">On foundation of generative statistics with F-entropy: a gradient-based approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach.html" rel="alternate" type="text/html" title="On foundation of generative statistics with F-entropy: a gradient-based approach" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/OnfoundationofgenerativestatisticswithFentropyagradientbasedapproach.html">&lt;p&gt;This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can {\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g. missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisher’s divergence. (The F-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for Possible future projects include discrete data and Bayesian variational inference.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.05389&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bing Cheng, Howell Tong</name></author><category term="stat.ME" /><summary type="html">This paper explores the interplay between statistics and generative artificial intelligence. Generative statistics, an integral part of the latter, aims to construct models that can {\it generate} efficiently and meaningfully new data across the whole of the (usually high dimensional) sample space, e.g. a new photo. Within it, the gradient-based approach is a current favourite that exploits effectively, for the above purpose, the information contained in the observed sample, e.g. an old photo. However, often there are missing data in the observed sample, e.g. missing bits in the old photo. To handle this situation, we have proposed a gradient-based algorithm for generative modelling. More importantly, our paper underpins rigorously this powerful approach by introducing a new F-entropy that is related to Fisher’s divergence. (The F-entropy is also of independent interest.) The underpinning has enabled the gradient-based approach to expand its scope. For example, it can now provide a tool for Possible future projects include discrete data and Bayesian variational inference.</summary></entry><entry><title type="html">On the role of surrogates in the efficient estimation of treatment effects with limited outcome data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Ontheroleofsurrogatesintheefficientestimationoftreatmenteffectswithlimitedoutcomedata.html" rel="alternate" type="text/html" title="On the role of surrogates in the efficient estimation of treatment effects with limited outcome data" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Ontheroleofsurrogatesintheefficientestimationoftreatmenteffectswithlimitedoutcomedata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Ontheroleofsurrogatesintheefficientestimationoftreatmenteffectswithlimitedoutcomedata.html">&lt;p&gt;In many experiments and observational studies, the outcome of interest is often difficult or expensive to observe, reducing effective sample sizes for estimating average treatment effects (ATEs) even when identifiable. We study how incorporating data on units for which only surrogate outcomes not of primary interest are observed can increase the precision of ATE estimation. We refrain from imposing stringent surrogacy conditions, which permit surrogates as perfect replacements for the target outcome. Instead, we supplement the available, albeit limited, observations of the target outcome (which by themselves identify the ATE) with abundant observations of surrogate outcomes, without any assumptions beyond random assignment and missingness and corresponding overlap conditions. To quantify the potential gains, we derive the difference in efficiency bounds on ATE estimation with and without surrogates, both when an overwhelming or comparable number of units have missing outcomes. We develop robust ATE estimation and inference methods that realize these efficiency gains. We empirically demonstrate the gains by studying the long-term-earning effects of job training.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2003.12408&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nathan Kallus, Xiaojie Mao</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">In many experiments and observational studies, the outcome of interest is often difficult or expensive to observe, reducing effective sample sizes for estimating average treatment effects (ATEs) even when identifiable. We study how incorporating data on units for which only surrogate outcomes not of primary interest are observed can increase the precision of ATE estimation. We refrain from imposing stringent surrogacy conditions, which permit surrogates as perfect replacements for the target outcome. Instead, we supplement the available, albeit limited, observations of the target outcome (which by themselves identify the ATE) with abundant observations of surrogate outcomes, without any assumptions beyond random assignment and missingness and corresponding overlap conditions. To quantify the potential gains, we derive the difference in efficiency bounds on ATE estimation with and without surrogates, both when an overwhelming or comparable number of units have missing outcomes. We develop robust ATE estimation and inference methods that realize these efficiency gains. We empirically demonstrate the gains by studying the long-term-earning effects of job training.</summary></entry><entry><title type="html">Optimal Sequential Procedure for Early Detection of Multiple Side Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/OptimalSequentialProcedureforEarlyDetectionofMultipleSideEffects.html" rel="alternate" type="text/html" title="Optimal Sequential Procedure for Early Detection of Multiple Side Effects" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/OptimalSequentialProcedureforEarlyDetectionofMultipleSideEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/OptimalSequentialProcedureforEarlyDetectionofMultipleSideEffects.html">&lt;p&gt;In this paper, we propose an optimal sequential procedure for the early detection of potential side effects resulting from the administration of some treatment (e.g. a vaccine, say). The results presented here extend previous results obtained in Wang and Boukai (2024) who study the single side effect case to the case of two (or more) side effects. While the sequential procedure we employ, simultaneously monitors several of the treatment’s side effects, the $(\alpha, \beta)$-optimal test we propose does not require any information about the inter-correlation between these potential side effects. However, in all of the subsequent analyses, including the derivations of the exact expressions of the Average Sample Number (ASN), the Power function, and the properties of the post-test (or post-detection) estimators, we accounted specifically, for the correlation between the potential side effects. In the real-life application (such as post-marketing surveillance), the number of available observations is large enough to justify asymptotic analyses of the sequential procedure (testing and post-detection estimation) properties. Accordingly, we also derive the consistency and asymptotic normality of our post-test estimators; results which enable us to also provide (asymptotic, post-detection) confidence intervals for the probabilities of various side-effects. Moreover, to compare two specific side effects, their relative risk plays an important role. We derive the distribution of the estimated relative risk in the asymptotic framework to provide appropriate inference. To illustrate the theoretical results presented, we provide two detailed examples based on the data of side effects on COVID-19 vaccine collected in Nigeria (see Nigeria (see Ilori et al. (2022)).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08759&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiayue Wang, Ben Boukai</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In this paper, we propose an optimal sequential procedure for the early detection of potential side effects resulting from the administration of some treatment (e.g. a vaccine, say). The results presented here extend previous results obtained in Wang and Boukai (2024) who study the single side effect case to the case of two (or more) side effects. While the sequential procedure we employ, simultaneously monitors several of the treatment’s side effects, the $(\alpha, \beta)$-optimal test we propose does not require any information about the inter-correlation between these potential side effects. However, in all of the subsequent analyses, including the derivations of the exact expressions of the Average Sample Number (ASN), the Power function, and the properties of the post-test (or post-detection) estimators, we accounted specifically, for the correlation between the potential side effects. In the real-life application (such as post-marketing surveillance), the number of available observations is large enough to justify asymptotic analyses of the sequential procedure (testing and post-detection estimation) properties. Accordingly, we also derive the consistency and asymptotic normality of our post-test estimators; results which enable us to also provide (asymptotic, post-detection) confidence intervals for the probabilities of various side-effects. Moreover, to compare two specific side effects, their relative risk plays an important role. We derive the distribution of the estimated relative risk in the asymptotic framework to provide appropriate inference. To illustrate the theoretical results presented, we provide two detailed examples based on the data of side effects on COVID-19 vaccine collected in Nigeria (see Nigeria (see Ilori et al. (2022)).</summary></entry><entry><title type="html">Parameter identifiability, parameter estimation and model prediction for differential equation models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Parameteridentifiabilityparameterestimationandmodelpredictionfordifferentialequationmodels.html" rel="alternate" type="text/html" title="Parameter identifiability, parameter estimation and model prediction for differential equation models" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Parameteridentifiabilityparameterestimationandmodelpredictionfordifferentialequationmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Parameteridentifiabilityparameterestimationandmodelpredictionfordifferentialequationmodels.html">&lt;p&gt;Interpreting data with mathematical models is an important aspect of real-world applied mathematical modeling. Very often we are interested to understand the extent to which a particular data set informs and constrains model parameters. This question is closely related to the concept of parameter identifiability, and in this article we present a series of computational exercises to introduce tools that can be used to assess parameter identifiability, estimate parameters and generate model predictions. Taking a likelihood-based approach, we show that very similar ideas and algorithms can be used to deal with a range of different mathematical modelling frameworks. The exercises and results presented in this article are supported by a suite of open access codes that can be accessed on GitHub.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08177&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matthew J Simpson, Ruth E Baker</name></author><category term="stat.ME" /><summary type="html">Interpreting data with mathematical models is an important aspect of real-world applied mathematical modeling. Very often we are interested to understand the extent to which a particular data set informs and constrains model parameters. This question is closely related to the concept of parameter identifiability, and in this article we present a series of computational exercises to introduce tools that can be used to assess parameter identifiability, estimate parameters and generate model predictions. Taking a likelihood-based approach, we show that very similar ideas and algorithms can be used to deal with a range of different mathematical modelling frameworks. The exercises and results presented in this article are supported by a suite of open access codes that can be accessed on GitHub.</summary></entry><entry><title type="html">Predicting NVIDIA’s Next-Day Stock Price: A Comparative Analysis of LSTM, MLP, ARIMA, and ARIMA-GARCH Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/PredictingNVIDIAsNextDayStockPriceAComparativeAnalysisofLSTMMLPARIMAandARIMAGARCHModels.html" rel="alternate" type="text/html" title="Predicting NVIDIA’s Next-Day Stock Price: A Comparative Analysis of LSTM, MLP, ARIMA, and ARIMA-GARCH Models" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/PredictingNVIDIAsNextDayStockPriceAComparativeAnalysisofLSTMMLPARIMAandARIMAGARCHModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/PredictingNVIDIAsNextDayStockPriceAComparativeAnalysisofLSTMMLPARIMAandARIMAGARCHModels.html">&lt;p&gt;Forecasting stock prices remains a considerable challenge in financial markets, bearing significant implications for investors, traders, and financial institutions. Amid the ongoing AI revolution, NVIDIA has emerged as a key player driving innovation across various sectors. Given its prominence, we chose NVIDIA as the subject of our study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08284&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yiluan Xing, Chao Yan, Cathy Chang Xie</name></author><category term="stat.AP" /><summary type="html">Forecasting stock prices remains a considerable challenge in financial markets, bearing significant implications for investors, traders, and financial institutions. Amid the ongoing AI revolution, NVIDIA has emerged as a key player driving innovation across various sectors. Given its prominence, we chose NVIDIA as the subject of our study.</summary></entry><entry><title type="html">Predicting Short Response Ratings with Non-Content Related Features: A Hierarchical Modeling Approach</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/PredictingShortResponseRatingswithNonContentRelatedFeaturesAHierarchicalModelingApproach.html" rel="alternate" type="text/html" title="Predicting Short Response Ratings with Non-Content Related Features: A Hierarchical Modeling Approach" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/PredictingShortResponseRatingswithNonContentRelatedFeaturesAHierarchicalModelingApproach</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/PredictingShortResponseRatingswithNonContentRelatedFeaturesAHierarchicalModelingApproach.html">&lt;p&gt;We explore whether the human ratings of open ended responses can be explained with non-content related features, and if such effects vary across different mathematics-related items. When scoring is rigorously defined and rooted in a measurement framework, educators intend that the features of a response which are indicative of the respondent’s level of ability are contributing to scores. However, we find that features such as response length, a grammar score of the response, and a metric relating to key phrase frequency are significant predictors for response ratings. Although our findings are not causally conclusive, they may propel us to be more critical of he way in which we assess open ended responses, especially in high stakes scenarios. Educators take great care to provide unbiased, consistent ratings, but it may be that extraneous features unrelated to those which were intended to be rated are being evaluated.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08574&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aubrey Condor</name></author><category term="stat.AP," /><category term="stat.OT" /><summary type="html">We explore whether the human ratings of open ended responses can be explained with non-content related features, and if such effects vary across different mathematics-related items. When scoring is rigorously defined and rooted in a measurement framework, educators intend that the features of a response which are indicative of the respondent’s level of ability are contributing to scores. However, we find that features such as response length, a grammar score of the response, and a metric relating to key phrase frequency are significant predictors for response ratings. Although our findings are not causally conclusive, they may propel us to be more critical of he way in which we assess open ended responses, especially in high stakes scenarios. Educators take great care to provide unbiased, consistent ratings, but it may be that extraneous features unrelated to those which were intended to be rated are being evaluated.</summary></entry><entry><title type="html">Promoting AI Equity in Science: Generalized Domain Prompt Learning for Accessible VLM Research</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/PromotingAIEquityinScienceGeneralizedDomainPromptLearningforAccessibleVLMResearch.html" rel="alternate" type="text/html" title="Promoting AI Equity in Science: Generalized Domain Prompt Learning for Accessible VLM Research" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/PromotingAIEquityinScienceGeneralizedDomainPromptLearningforAccessibleVLMResearch</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/PromotingAIEquityinScienceGeneralizedDomainPromptLearningforAccessibleVLMResearch.html">&lt;p&gt;Large-scale Vision-Language Models (VLMs) have demonstrated exceptional performance in natural vision tasks, motivating researchers across domains to explore domain-specific VLMs. However, the construction of powerful domain-specific VLMs demands vast amounts of annotated data, substantial electrical energy, and computing resources, primarily accessible to industry, yet hindering VLM research in academia. To address this challenge and foster sustainable and equitable VLM research, we present the Generalized Domain Prompt Learning (GDPL) framework. GDPL facilitates the transfer of VLMs’ robust recognition capabilities from natural vision to specialized domains, without the need for extensive data or resources. By leveraging small-scale domain-specific foundation models and minimal prompt samples, GDPL empowers the language branch with domain knowledge through quaternion networks, uncovering cross-modal relationships between domain-specific vision features and natural vision-based contextual embeddings. Simultaneously, GDPL guides the vision branch into specific domains through hierarchical propagation of generated vision prompt features, grounded in well-matched vision-language relations. Furthermore, to fully harness the domain adaptation potential of VLMs, we introduce a novel low-rank adaptation approach. Extensive experiments across diverse domains like remote sensing, medical imaging, geology, Synthetic Aperture Radar, and fluid dynamics, validate the efficacy of GDPL, demonstrating its ability to achieve state-of-the-art domain recognition performance in a prompt learning paradigm. Our framework paves the way for sustainable and inclusive VLM research, transcending the barriers between academia and industry.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08668&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Qinglong Cao, Yuntian Chen, Lu Lu, Hao Sun, Zhenzhong Zeng, Xiaokang Yang, Dongxiao Zhang</name></author><category term="stat.AP" /><summary type="html">Large-scale Vision-Language Models (VLMs) have demonstrated exceptional performance in natural vision tasks, motivating researchers across domains to explore domain-specific VLMs. However, the construction of powerful domain-specific VLMs demands vast amounts of annotated data, substantial electrical energy, and computing resources, primarily accessible to industry, yet hindering VLM research in academia. To address this challenge and foster sustainable and equitable VLM research, we present the Generalized Domain Prompt Learning (GDPL) framework. GDPL facilitates the transfer of VLMs’ robust recognition capabilities from natural vision to specialized domains, without the need for extensive data or resources. By leveraging small-scale domain-specific foundation models and minimal prompt samples, GDPL empowers the language branch with domain knowledge through quaternion networks, uncovering cross-modal relationships between domain-specific vision features and natural vision-based contextual embeddings. Simultaneously, GDPL guides the vision branch into specific domains through hierarchical propagation of generated vision prompt features, grounded in well-matched vision-language relations. Furthermore, to fully harness the domain adaptation potential of VLMs, we introduce a novel low-rank adaptation approach. Extensive experiments across diverse domains like remote sensing, medical imaging, geology, Synthetic Aperture Radar, and fluid dynamics, validate the efficacy of GDPL, demonstrating its ability to achieve state-of-the-art domain recognition performance in a prompt learning paradigm. Our framework paves the way for sustainable and inclusive VLM research, transcending the barriers between academia and industry.</summary></entry><entry><title type="html">Quest for an efficient mathematical and computational method to explore optimal extreme weather modification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Questforanefficientmathematicalandcomputationalmethodtoexploreoptimalextremeweathermodification.html" rel="alternate" type="text/html" title="Quest for an efficient mathematical and computational method to explore optimal extreme weather modification" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Questforanefficientmathematicalandcomputationalmethodtoexploreoptimalextremeweathermodification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Questforanefficientmathematicalandcomputationalmethodtoexploreoptimalextremeweathermodification.html">&lt;p&gt;It is a grand challenge to find a feasible weather modification method to mitigate the impact of extreme weather events such as tropical cyclones. Previous works have proposed potentially effective actuators and assessed their capabilities to achieve weather modification objectives through numerical simulations. However, few studies have explored efficient mathematical and computational methods to inversely determine optimal actuators from specific modification goals. Here I demonstrate the utility of the ensemble Kalman filter (EnKF)-based control method, referred to as ensemble Kalman control (EnKC). The series of numerical experiments with the Lorenz 96 model indicates that EnKC efficiently identifies local, small, and intermittent control perturbations that can mitigate extreme events. The existing techniques of EnKF, such as background error covariance localization and observation error covariance inflation, can improve the sparsity and efficiency of the control. This work paves the way toward the real-world applications of EnKC to explore the controllability of extreme atmospheric events.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08387&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yohei Sawada</name></author><category term="stat.AP" /><summary type="html">It is a grand challenge to find a feasible weather modification method to mitigate the impact of extreme weather events such as tropical cyclones. Previous works have proposed potentially effective actuators and assessed their capabilities to achieve weather modification objectives through numerical simulations. However, few studies have explored efficient mathematical and computational methods to inversely determine optimal actuators from specific modification goals. Here I demonstrate the utility of the ensemble Kalman filter (EnKF)-based control method, referred to as ensemble Kalman control (EnKC). The series of numerical experiments with the Lorenz 96 model indicates that EnKC efficiently identifies local, small, and intermittent control perturbations that can mitigate extreme events. The existing techniques of EnKF, such as background error covariance localization and observation error covariance inflation, can improve the sparsity and efficiency of the control. This work paves the way toward the real-world applications of EnKC to explore the controllability of extreme atmospheric events.</summary></entry><entry><title type="html">Sample Observed Effects: Enumeration, Randomization and Generalization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SampleObservedEffectsEnumerationRandomizationandGeneralization.html" rel="alternate" type="text/html" title="Sample Observed Effects: Enumeration, Randomization and Generalization" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SampleObservedEffectsEnumerationRandomizationandGeneralization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SampleObservedEffectsEnumerationRandomizationandGeneralization.html">&lt;p&gt;The widely used ‘Counterfactual’ definition of Causal Effects was derived for unbiasedness and accuracy - and not generalizability. We propose a Combinatorial definition for the External Validity (EV) of intervention effects. We first define the concept of an effect observation ‘background’. We then formulate conditions for effect generalization based on their sets of (observed and unobserved) backgrounds. This reveals two limits for effect generalization: (1) when effects are observed under all their enumerable backgrounds, or, (2) when backgrounds have become sufficiently randomized. We use the resulting combinatorial framework to re-examine several issues in the original counterfactual formulation: out-of-sample validity, concurrent estimation of multiple effects, bias-variance tradeoffs, statistical power, and connections to current predictive and explaining techniques.
  Methodologically, the definitions also allow us to replace the parametric estimation problems that followed the counterfactual definition by combinatorial enumeration and randomization problems in non-experimental samples. We use this non-parametric framework to demonstrate (External Validity, Unconfoundness and Precision) tradeoffs in the performance of popular supervised, explaining, and causal-effect estimators. We also illustrate how the approach allows for the use of supervised and explaining methods in non-i.i.d. samples. The COVID19 pandemic highlighted the need for learning solutions to provide predictions in severally incomplete samples. We demonstrate applications in this pressing problem.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.04376&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Andre F. Ribeiro</name></author><category term="stat.ME" /><summary type="html">The widely used ‘Counterfactual’ definition of Causal Effects was derived for unbiasedness and accuracy - and not generalizability. We propose a Combinatorial definition for the External Validity (EV) of intervention effects. We first define the concept of an effect observation ‘background’. We then formulate conditions for effect generalization based on their sets of (observed and unobserved) backgrounds. This reveals two limits for effect generalization: (1) when effects are observed under all their enumerable backgrounds, or, (2) when backgrounds have become sufficiently randomized. We use the resulting combinatorial framework to re-examine several issues in the original counterfactual formulation: out-of-sample validity, concurrent estimation of multiple effects, bias-variance tradeoffs, statistical power, and connections to current predictive and explaining techniques. Methodologically, the definitions also allow us to replace the parametric estimation problems that followed the counterfactual definition by combinatorial enumeration and randomization problems in non-experimental samples. We use this non-parametric framework to demonstrate (External Validity, Unconfoundness and Precision) tradeoffs in the performance of popular supervised, explaining, and causal-effect estimators. We also illustrate how the approach allows for the use of supervised and explaining methods in non-i.i.d. samples. The COVID19 pandemic highlighted the need for learning solutions to provide predictions in severally incomplete samples. We demonstrate applications in this pressing problem.</summary></entry><entry><title type="html">Scalable Subsampling Inference for Deep Neural Networks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/ScalableSubsamplingInferenceforDeepNeuralNetworks.html" rel="alternate" type="text/html" title="Scalable Subsampling Inference for Deep Neural Networks" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/ScalableSubsamplingInferenceforDeepNeuralNetworks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/ScalableSubsamplingInferenceforDeepNeuralNetworks.html">&lt;p&gt;Deep neural networks (DNN) has received increasing attention in machine learning applications in the last several years. Recently, a non-asymptotic error bound has been developed to measure the performance of the fully connected DNN estimator with ReLU activation functions for estimating regression models. The paper at hand gives a small improvement on the current error bound based on the latest results on the approximation ability of DNN. More importantly, however, a non-random subsampling technique–scalable subsampling–is applied to construct a `subagged’ DNN estimator. Under regularity conditions, it is shown that the subagged DNN estimator is computationally efficient without sacrificing accuracy for either estimation or prediction tasks. Beyond point estimation/prediction, we propose different approaches to build confidence and prediction intervals based on the subagged DNN estimator. In addition to being asymptotically valid, the proposed confidence/prediction intervals appear to work well in finite samples. All in all, the scalable subsampling DNN estimator offers the complete package in terms of statistical inference, i.e., (a) computational efficiency; (b) point estimation/prediction accuracy; and (c) allowing for the construction of practically useful confidence and prediction intervals.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08276&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kejin Wu, Dimitris N. Politis</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Deep neural networks (DNN) has received increasing attention in machine learning applications in the last several years. Recently, a non-asymptotic error bound has been developed to measure the performance of the fully connected DNN estimator with ReLU activation functions for estimating regression models. The paper at hand gives a small improvement on the current error bound based on the latest results on the approximation ability of DNN. More importantly, however, a non-random subsampling technique–scalable subsampling–is applied to construct a `subagged’ DNN estimator. Under regularity conditions, it is shown that the subagged DNN estimator is computationally efficient without sacrificing accuracy for either estimation or prediction tasks. Beyond point estimation/prediction, we propose different approaches to build confidence and prediction intervals based on the subagged DNN estimator. In addition to being asymptotically valid, the proposed confidence/prediction intervals appear to work well in finite samples. All in all, the scalable subsampling DNN estimator offers the complete package in terms of statistical inference, i.e., (a) computational efficiency; (b) point estimation/prediction accuracy; and (c) allowing for the construction of practically useful confidence and prediction intervals.</summary></entry><entry><title type="html">Sequential Maximal Updated Density Parameter Estimation for Dynamical Systems with Parameter Drift</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SequentialMaximalUpdatedDensityParameterEstimationforDynamicalSystemswithParameterDrift.html" rel="alternate" type="text/html" title="Sequential Maximal Updated Density Parameter Estimation for Dynamical Systems with Parameter Drift" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SequentialMaximalUpdatedDensityParameterEstimationforDynamicalSystemswithParameterDrift</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SequentialMaximalUpdatedDensityParameterEstimationforDynamicalSystemswithParameterDrift.html">&lt;p&gt;We present a novel method for generating sequential parameter estimates and quantifying epistemic uncertainty in dynamical systems within a data-consistent (DC) framework. The DC framework differs from traditional Bayesian approaches due to the incorporation of the push-forward of an initial density, which performs selective regularization in parameter directions not informed by the data in the resulting updated density. This extends a previous study that included the linear Gaussian theory within the DC framework and introduced the maximal updated density (MUD) estimate as an alternative to both least squares and maximum a posterior (MAP) estimates. In this work, we introduce algorithms for operational settings of MUD estimation in real or near-real time where spatio-temporal datasets arrive in packets to provide updated estimates of parameters and identify potential parameter drift. Computational diagnostics within the DC framework prove critical for evaluating (1) the quality of the DC update and MUD estimate and (2) the detection of parameter value drift. The algorithms are applied to estimate (1) wind drag parameters in a high-fidelity storm surge model, (2) thermal diffusivity field for a heat conductivity problem, and (3) changing infection and incubation rates of an epidemiological model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08307&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carlos del-Castillo-Negrete, Rylan Spence, Troy Butler, Clint Dawson</name></author><category term="stat.ME," /><category term="stat.OT" /><summary type="html">We present a novel method for generating sequential parameter estimates and quantifying epistemic uncertainty in dynamical systems within a data-consistent (DC) framework. The DC framework differs from traditional Bayesian approaches due to the incorporation of the push-forward of an initial density, which performs selective regularization in parameter directions not informed by the data in the resulting updated density. This extends a previous study that included the linear Gaussian theory within the DC framework and introduced the maximal updated density (MUD) estimate as an alternative to both least squares and maximum a posterior (MAP) estimates. In this work, we introduce algorithms for operational settings of MUD estimation in real or near-real time where spatio-temporal datasets arrive in packets to provide updated estimates of parameters and identify potential parameter drift. Computational diagnostics within the DC framework prove critical for evaluating (1) the quality of the DC update and MUD estimate and (2) the detection of parameter value drift. The algorithms are applied to estimate (1) wind drag parameters in a high-fidelity storm surge model, (2) thermal diffusivity field for a heat conductivity problem, and (3) changing infection and incubation rates of an epidemiological model.</summary></entry><entry><title type="html">Simple binning algorithm and SimDec visualization for comprehensive sensitivity analysis of complex computational models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SimplebinningalgorithmandSimDecvisualizationforcomprehensivesensitivityanalysisofcomplexcomputationalmodels.html" rel="alternate" type="text/html" title="Simple binning algorithm and SimDec visualization for comprehensive sensitivity analysis of complex computational models" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SimplebinningalgorithmandSimDecvisualizationforcomprehensivesensitivityanalysisofcomplexcomputationalmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SimplebinningalgorithmandSimDecvisualizationforcomprehensivesensitivityanalysisofcomplexcomputationalmodels.html">&lt;p&gt;Models of complex technological systems inherently contain interactions and dependencies among their input variables that affect their joint influence on the output. Such models are often computationally expensive and few sensitivity analysis methods can effectively process such complexities. Moreover, the sensitivity analysis field as a whole pays limited attention to the nature of interaction effects, whose understanding can prove to be critical for the design of safe and reliable systems. In this paper, we introduce and extensively test a simple binning approach for computing sensitivity indices and demonstrate how complementing it with the smart visualization method, simulation decomposition (SimDec), can permit important insights into the behavior of complex engineering models. The simple binning approach computes first-, second-order effects, and a combined sensitivity index, and is considerably more computationally efficient than the mainstream measure for Sobol indices introduced by Saltelli et al. The totality of the sensitivity analysis framework provides an efficient and intuitive way to analyze the behavior of complex systems containing interactions and dependencies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.13446&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mariia Kozlova, Antti Ahola, Pamphile T. Roy, Julian Scott Yeomans</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Models of complex technological systems inherently contain interactions and dependencies among their input variables that affect their joint influence on the output. Such models are often computationally expensive and few sensitivity analysis methods can effectively process such complexities. Moreover, the sensitivity analysis field as a whole pays limited attention to the nature of interaction effects, whose understanding can prove to be critical for the design of safe and reliable systems. In this paper, we introduce and extensively test a simple binning approach for computing sensitivity indices and demonstrate how complementing it with the smart visualization method, simulation decomposition (SimDec), can permit important insights into the behavior of complex engineering models. The simple binning approach computes first-, second-order effects, and a combined sensitivity index, and is considerably more computationally efficient than the mainstream measure for Sobol indices introduced by Saltelli et al. The totality of the sensitivity analysis framework provides an efficient and intuitive way to analyze the behavior of complex systems containing interactions and dependencies.</summary></entry><entry><title type="html">Simplifying Debiased Inference via Automatic Differentiation and Probabilistic Programming</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SimplifyingDebiasedInferenceviaAutomaticDifferentiationandProbabilisticProgramming.html" rel="alternate" type="text/html" title="Simplifying Debiased Inference via Automatic Differentiation and Probabilistic Programming" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SimplifyingDebiasedInferenceviaAutomaticDifferentiationandProbabilisticProgramming</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SimplifyingDebiasedInferenceviaAutomaticDifferentiationandProbabilisticProgramming.html">&lt;p&gt;We introduce an algorithm that simplifies the construction of efficient estimators, making them accessible to a broader audience. ‘Dimple’ takes as input computer code representing a parameter of interest and outputs an efficient estimator. Unlike standard approaches, it does not require users to derive a functional derivative known as the efficient influence function. Dimple avoids this task by applying automatic differentiation to the statistical functional of interest. Doing so requires expressing this functional as a composition of primitives satisfying a novel differentiability condition. Dimple also uses this composition to determine the nuisances it must estimate. In software, primitives can be implemented independently of one another and reused across different estimation problems. We provide a proof-of-concept Python implementation and showcase through examples how it allows users to go from parameter specification to efficient estimation with just a few lines of code.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08675&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alex Luedtke</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">We introduce an algorithm that simplifies the construction of efficient estimators, making them accessible to a broader audience. ‘Dimple’ takes as input computer code representing a parameter of interest and outputs an efficient estimator. Unlike standard approaches, it does not require users to derive a functional derivative known as the efficient influence function. Dimple avoids this task by applying automatic differentiation to the statistical functional of interest. Doing so requires expressing this functional as a composition of primitives satisfying a novel differentiability condition. Dimple also uses this composition to determine the nuisances it must estimate. In software, primitives can be implemented independently of one another and reused across different estimation problems. We provide a proof-of-concept Python implementation and showcase through examples how it allows users to go from parameter specification to efficient estimation with just a few lines of code.</summary></entry><entry><title type="html">Spline Based Methods for Functional Data on Multivariate Domains</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SplineBasedMethodsforFunctionalDataonMultivariateDomains.html" rel="alternate" type="text/html" title="Spline Based Methods for Functional Data on Multivariate Domains" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SplineBasedMethodsforFunctionalDataonMultivariateDomains</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/SplineBasedMethodsforFunctionalDataonMultivariateDomains.html">&lt;p&gt;Functional data analysis is typically performed in two steps: first, functionally representing discrete observations, and then applying functional methods to the so-represented data. The initial choice of a functional representation may have a significant impact on the second phase of the analysis, as shown in recent research, where data-driven spline bases outperformed the predefined rigid choice of functional representation. The method chooses an initial functional basis by an efficient placement of the knots using a simple machine-learning algorithm. The approach does not apply directly when the data are defined on domains of a higher dimension than one such as, for example, images. The reason is that in higher dimensions the convenient and numerically efficient spline bases are obtained as tensor bases from 1D spline bases that require knots that are located on a lattice. This does not allow for a flexible knot placement that was fundamental for the 1D approach. The goal of this research is to propose two modified approaches that circumvent the problem by coding the irregular knot selection into their densities and utilizing these densities through the topology of the spaces of splines. This allows for regular grids for the knots and thus facilitates using the spline tensor bases. It is tested on 1D data showing that its performance is comparable to or better than the previous methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.16402&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rani Basna, Hiba Nassar, Krzysztof Podgórski</name></author><category term="stat.AP" /><summary type="html">Functional data analysis is typically performed in two steps: first, functionally representing discrete observations, and then applying functional methods to the so-represented data. The initial choice of a functional representation may have a significant impact on the second phase of the analysis, as shown in recent research, where data-driven spline bases outperformed the predefined rigid choice of functional representation. The method chooses an initial functional basis by an efficient placement of the knots using a simple machine-learning algorithm. The approach does not apply directly when the data are defined on domains of a higher dimension than one such as, for example, images. The reason is that in higher dimensions the convenient and numerically efficient spline bases are obtained as tensor bases from 1D spline bases that require knots that are located on a lattice. This does not allow for a flexible knot placement that was fundamental for the 1D approach. The goal of this research is to propose two modified approaches that circumvent the problem by coding the irregular knot selection into their densities and utilizing these densities through the topology of the spaces of splines. This allows for regular grids for the knots and thus facilitates using the spline tensor bases. It is tested on 1D data showing that its performance is comparable to or better than the previous methods.</summary></entry><entry><title type="html">Studying continuous, time-varying, and/or complex exposures using longitudinal modified treatment policies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Studyingcontinuoustimevaryingandorcomplexexposuresusinglongitudinalmodifiedtreatmentpolicies.html" rel="alternate" type="text/html" title="Studying continuous, time-varying, and/or complex exposures using longitudinal modified treatment policies" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Studyingcontinuoustimevaryingandorcomplexexposuresusinglongitudinalmodifiedtreatmentpolicies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/Studyingcontinuoustimevaryingandorcomplexexposuresusinglongitudinalmodifiedtreatmentpolicies.html">&lt;p&gt;This tutorial discusses methodology for causal inference using longitudinal modified treatment policies. This method facilitates the mathematical formalization, identification, and estimation of many novel parameters, and mathematically generalizes many commonly used parameters, such as the average treatment effect. Longitudinal modified treatment policies apply to a wide variety of exposures, including binary, multivariate, and continuous, and can accommodate time-varying treatments and confounders, competing risks, loss-to-follow-up, as well as survival, binary, or continuous outcomes. Longitudinal modified treatment policies can be seen as an extension of static and dynamic interventions to involve the natural value of treatment, and, like dynamic interventions, can be used to define alternative estimands with a positivity assumption that is more likely to be satisfied than estimands corresponding to static interventions. This tutorial aims to illustrate several practical uses of the longitudinal modified treatment policy methodology, including describing different estimation strategies and their corresponding advantages and disadvantages. We provide numerous examples of types of research questions which can be answered using longitudinal modified treatment policies. We go into more depth with one of these examples–specifically, estimating the effect of delaying intubation on critically ill COVID-19 patients’ mortality. We demonstrate the use of the open-source R package lmtp to estimate the effects, and we provide code on https://github.com/kathoffman/lmtp-tutorial.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.09460&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Katherine L. Hoffman, Diego Salazar-Barreto, Nicholas Williams, Kara E. Rudolph, Ivan Diaz</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.OT" /><summary type="html">This tutorial discusses methodology for causal inference using longitudinal modified treatment policies. This method facilitates the mathematical formalization, identification, and estimation of many novel parameters, and mathematically generalizes many commonly used parameters, such as the average treatment effect. Longitudinal modified treatment policies apply to a wide variety of exposures, including binary, multivariate, and continuous, and can accommodate time-varying treatments and confounders, competing risks, loss-to-follow-up, as well as survival, binary, or continuous outcomes. Longitudinal modified treatment policies can be seen as an extension of static and dynamic interventions to involve the natural value of treatment, and, like dynamic interventions, can be used to define alternative estimands with a positivity assumption that is more likely to be satisfied than estimands corresponding to static interventions. This tutorial aims to illustrate several practical uses of the longitudinal modified treatment policy methodology, including describing different estimation strategies and their corresponding advantages and disadvantages. We provide numerous examples of types of research questions which can be answered using longitudinal modified treatment policies. We go into more depth with one of these examples–specifically, estimating the effect of delaying intubation on critically ill COVID-19 patients’ mortality. We demonstrate the use of the open-source R package lmtp to estimate the effects, and we provide code on https://github.com/kathoffman/lmtp-tutorial.</summary></entry><entry><title type="html">Zigzag path connects two Monte Carlo samplers: Hamiltonian counterpart to a piecewise deterministic Markov process</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/ZigzagpathconnectstwoMonteCarlosamplersHamiltoniancounterparttoapiecewisedeterministicMarkovprocess.html" rel="alternate" type="text/html" title="Zigzag path connects two Monte Carlo samplers: Hamiltonian counterpart to a piecewise deterministic Markov process" /><published>2024-05-15T00:00:00+00:00</published><updated>2024-05-15T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/ZigzagpathconnectstwoMonteCarlosamplersHamiltoniancounterparttoapiecewisedeterministicMarkovprocess</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/15/ZigzagpathconnectstwoMonteCarlosamplersHamiltoniancounterparttoapiecewisedeterministicMarkovprocess.html">&lt;p&gt;Zigzag and other piecewise deterministic Markov process samplers have attracted significant interest for their non-reversibility and other appealing properties for Bayesian posterior computation. Hamiltonian Monte Carlo is another state-of-the-art sampler, exploiting fictitious momentum to guide Markov chains through complex target distributions. We establish an important connection between the zigzag sampler and a variant of Hamiltonian Monte Carlo based on Laplace-distributed momentum. The position and velocity component of the corresponding Hamiltonian dynamics travels along a zigzag path paralleling the Markovian zigzag process; however, the dynamics is non-Markovian in this position-velocity space as the momentum component encodes non-immediate pasts. This information is partially lost during a momentum refreshment step, in which we preserve its direction but re-sample magnitude. In the limit of increasingly frequent momentum refreshments, we prove that Hamiltonian zigzag converges strongly to its Markovian counterpart. This theoretical insight suggests that, when retaining full momentum information, Hamiltonian zigzag can better explore target distributions with highly correlated parameters by suppressing the diffusive behavior of Markovian zigzag. We corroborate this intuition by comparing performance of the two zigzag cousins on high-dimensional truncated multivariate Gaussians, including a 11,235-dimensional target arising from a Bayesian phylogenetic multivariate probit modeling of HIV virus data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.07694&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Akihiko Nishimura, Zhenyu Zhang, Marc A. Suchard</name></author><category term="stat.CO" /><summary type="html">Zigzag and other piecewise deterministic Markov process samplers have attracted significant interest for their non-reversibility and other appealing properties for Bayesian posterior computation. Hamiltonian Monte Carlo is another state-of-the-art sampler, exploiting fictitious momentum to guide Markov chains through complex target distributions. We establish an important connection between the zigzag sampler and a variant of Hamiltonian Monte Carlo based on Laplace-distributed momentum. The position and velocity component of the corresponding Hamiltonian dynamics travels along a zigzag path paralleling the Markovian zigzag process; however, the dynamics is non-Markovian in this position-velocity space as the momentum component encodes non-immediate pasts. This information is partially lost during a momentum refreshment step, in which we preserve its direction but re-sample magnitude. In the limit of increasingly frequent momentum refreshments, we prove that Hamiltonian zigzag converges strongly to its Markovian counterpart. This theoretical insight suggests that, when retaining full momentum information, Hamiltonian zigzag can better explore target distributions with highly correlated parameters by suppressing the diffusive behavior of Markovian zigzag. We corroborate this intuition by comparing performance of the two zigzag cousins on high-dimensional truncated multivariate Gaussians, including a 11,235-dimensional target arising from a Bayesian phylogenetic multivariate probit modeling of HIV virus data.</summary></entry></feed>