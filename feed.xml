<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-06-26T07:13:58+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo is All you Need</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/AcceleratingLookaheadinBayesianOptimizationMultilevelMonteCarloisAllyouNeed.html" rel="alternate" type="text/html" title="Accelerating Look-ahead in Bayesian Optimization: Multilevel Monte Carlo is All you Need" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/AcceleratingLookaheadinBayesianOptimizationMultilevelMonteCarloisAllyouNeed</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/AcceleratingLookaheadinBayesianOptimizationMultilevelMonteCarloisAllyouNeed.html">&lt;p&gt;We leverage multilevel Monte Carlo (MLMC) to improve the performance of multi-step look-ahead Bayesian optimization (BO) methods that involve nested expectations and maximizations. Often these expectations must be computed by Monte Carlo (MC). The complexity rate of naive MC degrades for nested operations, whereas MLMC is capable of achieving the canonical MC convergence rate for this type of problem, independently of dimension and without any smoothness assumptions. Our theoretical study focuses on the approximation improvements for twoand three-step look-ahead acquisition functions, but, as we discuss, the approach is generalizable in various ways, including beyond the context of BO. Our findings are verified numerically and the benefits of MLMC for BO are illustrated on several benchmark examples. Code is available at https://github.com/Shangda-Yang/MLMCBO .&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.02111&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shangda Yang, Vitaly Zankin, Maximilian Balandat, Stefan Scherer, Kevin Carlberg, Neil Walton, Kody J. H. Law</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">We leverage multilevel Monte Carlo (MLMC) to improve the performance of multi-step look-ahead Bayesian optimization (BO) methods that involve nested expectations and maximizations. Often these expectations must be computed by Monte Carlo (MC). The complexity rate of naive MC degrades for nested operations, whereas MLMC is capable of achieving the canonical MC convergence rate for this type of problem, independently of dimension and without any smoothness assumptions. Our theoretical study focuses on the approximation improvements for twoand three-step look-ahead acquisition functions, but, as we discuss, the approach is generalizable in various ways, including beyond the context of BO. Our findings are verified numerically and the benefits of MLMC for BO are illustrated on several benchmark examples. Code is available at https://github.com/Shangda-Yang/MLMCBO .</summary></entry><entry><title type="html">A hybrid predictive and prescriptive modelling framework for long-term mental healthcare workforce planning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Ahybridpredictiveandprescriptivemodellingframeworkforlongtermmentalhealthcareworkforceplanning.html" rel="alternate" type="text/html" title="A hybrid predictive and prescriptive modelling framework for long-term mental healthcare workforce planning" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Ahybridpredictiveandprescriptivemodellingframeworkforlongtermmentalhealthcareworkforceplanning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Ahybridpredictiveandprescriptivemodellingframeworkforlongtermmentalhealthcareworkforceplanning.html">&lt;p&gt;Over the past decade, there has been a severe staffing shortage in mental healthcare, exacerbated by increased demand for mental health services due to COVID-19. This demand is projected to increase over the next decade or so, necessitating proactive workforce planning to ensure sufficient staffing for ongoing service delivery. Despite the subject’s critical significance, the present literature lacks thorough research dedicated to developing a model that addresses the long-term workforce needs required for efficient mental healthcare planning. Furthermore, our interactions with mental health practitioners within the United Kingdom’s National Health Service (NHS) revealed the practical need for such a model. To address this gap, we aim to develop a hybrid predictive and prescriptive modelling framework, which combines long-term probabilistic forecasting with an analytical stock-flow model, designed specifically for mental health workforce planning. Given the vital role of nurses, who account for one-third of the total mental health workforce, we focus on modelling the headcount of nurses, but the proposed model can be generalised to other types of workforce planning in the healthcare sector. Using statistical and machine learning approaches and real-world data from NHS, we first identify factors contributing to variations in workforce requirements, then develop a long-term forecasting model to estimate future workforce needs, and finally integrate it into an analytical stock-flow method to provide policy analysis. Our findings highlight the unsustainable nature of present staffing plans, showing a growing nursing shortage. Furthermore, the policy analysis demonstrates the ineffectiveness of blanket remedies, highlighting the need for regional-level policy developments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17463&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Harsha Chamara Hewage, Bahman Rostami-Tabar</name></author><category term="stat.AP" /><summary type="html">Over the past decade, there has been a severe staffing shortage in mental healthcare, exacerbated by increased demand for mental health services due to COVID-19. This demand is projected to increase over the next decade or so, necessitating proactive workforce planning to ensure sufficient staffing for ongoing service delivery. Despite the subject’s critical significance, the present literature lacks thorough research dedicated to developing a model that addresses the long-term workforce needs required for efficient mental healthcare planning. Furthermore, our interactions with mental health practitioners within the United Kingdom’s National Health Service (NHS) revealed the practical need for such a model. To address this gap, we aim to develop a hybrid predictive and prescriptive modelling framework, which combines long-term probabilistic forecasting with an analytical stock-flow model, designed specifically for mental health workforce planning. Given the vital role of nurses, who account for one-third of the total mental health workforce, we focus on modelling the headcount of nurses, but the proposed model can be generalised to other types of workforce planning in the healthcare sector. Using statistical and machine learning approaches and real-world data from NHS, we first identify factors contributing to variations in workforce requirements, then develop a long-term forecasting model to estimate future workforce needs, and finally integrate it into an analytical stock-flow method to provide policy analysis. Our findings highlight the unsustainable nature of present staffing plans, showing a growing nursing shortage. Furthermore, the policy analysis demonstrates the ineffectiveness of blanket remedies, highlighting the need for regional-level policy developments.</summary></entry><entry><title type="html">An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/AnEmbeddedDiachronicSenseChangeModelwithaCaseStudyfromAncientGreek.html" rel="alternate" type="text/html" title="An Embedded Diachronic Sense Change Model with a Case Study from Ancient Greek" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/AnEmbeddedDiachronicSenseChangeModelwithaCaseStudyfromAncientGreek</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/AnEmbeddedDiachronicSenseChangeModelwithaCaseStudyfromAncientGreek.html">&lt;p&gt;Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small and sparse, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC (Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as “kosmos” (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal changes in these representations. This paper introduces EDiSC, an Embedded DiSC model, which combines word embeddings with DiSC to provide superior model performance. It is shown empirically that EDiSC offers improved predictive accuracy, ground-truth recovery and uncertainty quantification, as well as better sampling efficiency and scalability properties with MCMC methods. The challenges of fitting these models are also discussed.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.00541&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Schyan Zafar, Geoff K. Nicholls</name></author><category term="stat.ME" /><summary type="html">Word meanings change over time, and word senses evolve, emerge or die out in the process. For ancient languages, where the corpora are often small and sparse, modelling such changes accurately proves challenging, and quantifying uncertainty in sense-change estimates consequently becomes important. GASC (Genre-Aware Semantic Change) and DiSC (Diachronic Sense Change) are existing generative models that have been used to analyse sense change for target words from an ancient Greek text corpus, using unsupervised learning without the help of any pre-training. These models represent the senses of a given target word such as “kosmos” (meaning decoration, order or world) as distributions over context words, and sense prevalence as a distribution over senses. The models are fitted using Markov Chain Monte Carlo (MCMC) methods to measure temporal changes in these representations. This paper introduces EDiSC, an Embedded DiSC model, which combines word embeddings with DiSC to provide superior model performance. It is shown empirically that EDiSC offers improved predictive accuracy, ground-truth recovery and uncertainty quantification, as well as better sampling efficiency and scalability properties with MCMC methods. The challenges of fitting these models are also discussed.</summary></entry><entry><title type="html">An information-geometric approach for network decomposition using the q-state Potts model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/AninformationgeometricapproachfornetworkdecompositionusingtheqstatePottsmodel.html" rel="alternate" type="text/html" title="An information-geometric approach for network decomposition using the q-state Potts model" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/AninformationgeometricapproachfornetworkdecompositionusingtheqstatePottsmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/AninformationgeometricapproachfornetworkdecompositionusingtheqstatePottsmodel.html">&lt;p&gt;Complex networks are critical in many scientific, technological, and societal contexts due to their ability to represent and analyze intricate systems with interdependent components. Often, after labeling the nodes of a network with a community detection algorithm, its modular organization emerges, allowing a better understanding of the underlying structure by uncovering hidden relationships. In this paper, we introduce a novel information-geometric framework for the filtering and decomposition of networks whose nodes have been labeled. Our approach considers the labeled network as the outcome of a Markov random field modeled by a q-state Potts model. According to information geometry, the first and second order Fisher information matrices are related to the metric and curvature tensor of the parametric space of a statistical model. By computing an approximation to the local shape operator, the proposed methodology is able to identify low and high information nodes, allowing the decomposition of the labeled network in two complementary subgraphs. Hence, we call this method as the LO-HI decomposition. Experimental results with several kinds of networks show that the high information subgraph is often related to edges and boundaries, while the low information subgraph is a smoother version of the network, in the sense that the modular structure is improved.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17144&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexandre L. M. Levada</name></author><category term="stat.AP" /><summary type="html">Complex networks are critical in many scientific, technological, and societal contexts due to their ability to represent and analyze intricate systems with interdependent components. Often, after labeling the nodes of a network with a community detection algorithm, its modular organization emerges, allowing a better understanding of the underlying structure by uncovering hidden relationships. In this paper, we introduce a novel information-geometric framework for the filtering and decomposition of networks whose nodes have been labeled. Our approach considers the labeled network as the outcome of a Markov random field modeled by a q-state Potts model. According to information geometry, the first and second order Fisher information matrices are related to the metric and curvature tensor of the parametric space of a statistical model. By computing an approximation to the local shape operator, the proposed methodology is able to identify low and high information nodes, allowing the decomposition of the labeled network in two complementary subgraphs. Hence, we call this method as the LO-HI decomposition. Experimental results with several kinds of networks show that the high information subgraph is often related to edges and boundaries, while the low information subgraph is a smoother version of the network, in the sense that the modular structure is improved.</summary></entry><entry><title type="html">Bayesian Deep ICE</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/BayesianDeepICE.html" rel="alternate" type="text/html" title="Bayesian Deep ICE" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/BayesianDeepICE</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/BayesianDeepICE.html">&lt;p&gt;Deep Independent Component Estimation (DICE) has many applications in modern day machine learning as a feature engineering extraction method. We provide a novel latent variable representation of independent component analysis that enables both point estimates via expectation-maximization (EM) and full posterior sampling via Markov Chain Monte Carlo (MCMC) algorithms. Our methodology also applies to flow-based methods for nonlinear feature extraction. We discuss how to implement conditional posteriors and envelope-based methods for optimization. Through this representation hierarchy, we unify a number of hitherto disjoint estimation procedures. We illustrate our methodology and algorithms on a numerical example. Finally, we conclude with directions for future research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17058&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jyotishka Datta, Nicholas G. Polson</name></author><category term="stat.ME" /><summary type="html">Deep Independent Component Estimation (DICE) has many applications in modern day machine learning as a feature engineering extraction method. We provide a novel latent variable representation of independent component analysis that enables both point estimates via expectation-maximization (EM) and full posterior sampling via Markov Chain Monte Carlo (MCMC) algorithms. Our methodology also applies to flow-based methods for nonlinear feature extraction. We discuss how to implement conditional posteriors and envelope-based methods for optimization. Through this representation hierarchy, we unify a number of hitherto disjoint estimation procedures. We illustrate our methodology and algorithms on a numerical example. Finally, we conclude with directions for future research.</summary></entry><entry><title type="html">Bayesian Partial Reduced-Rank Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/BayesianPartialReducedRankRegression.html" rel="alternate" type="text/html" title="Bayesian Partial Reduced-Rank Regression" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/BayesianPartialReducedRankRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/BayesianPartialReducedRankRegression.html">&lt;p&gt;Reduced-rank (RR) regression may be interpreted as a dimensionality reduction technique able to reveal complex relationships among the data parsimoniously. However, RR regression models typically overlook any potential group structure among the responses by assuming a low-rank structure on the coefficient matrix. To address this limitation, a Bayesian Partial RR (BPRR) regression is exploited, where the response vector and the coefficient matrix are partitioned into low- and full-rank sub-groups. As opposed to the literature, which assumes known group structure and rank, a novel strategy is introduced that treats them as unknown parameters to be estimated. The main contribution is two-fold: an approach to infer the low- and full-rank group memberships from the data is proposed, and then, conditionally on this allocation, the corresponding (reduced) rank is estimated. Both steps are carried out in a Bayesian approach, allowing for full uncertainty quantification and based on a partially collapsed Gibbs sampler. It relies on a Laplace approximation of the marginal likelihood and the Metropolized Shotgun Stochastic Search to estimate the group allocation efficiently. Applications to synthetic and real-world data reveal the potential of the proposed method to reveal hidden structures in the data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17444&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maria F. Pintado, Matteo Iacopini, Luca Rossini, Alexander Y. Shestopaloff</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Reduced-rank (RR) regression may be interpreted as a dimensionality reduction technique able to reveal complex relationships among the data parsimoniously. However, RR regression models typically overlook any potential group structure among the responses by assuming a low-rank structure on the coefficient matrix. To address this limitation, a Bayesian Partial RR (BPRR) regression is exploited, where the response vector and the coefficient matrix are partitioned into low- and full-rank sub-groups. As opposed to the literature, which assumes known group structure and rank, a novel strategy is introduced that treats them as unknown parameters to be estimated. The main contribution is two-fold: an approach to infer the low- and full-rank group memberships from the data is proposed, and then, conditionally on this allocation, the corresponding (reduced) rank is estimated. Both steps are carried out in a Bayesian approach, allowing for full uncertainty quantification and based on a partially collapsed Gibbs sampler. It relies on a Laplace approximation of the marginal likelihood and the Metropolized Shotgun Stochastic Search to estimate the group allocation efficiently. Applications to synthetic and real-world data reveal the potential of the proposed method to reveal hidden structures in the data.</summary></entry><entry><title type="html">Bayesian temporal biclustering with applications to multi-subject neuroscience studies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Bayesiantemporalbiclusteringwithapplicationstomultisubjectneurosciencestudies.html" rel="alternate" type="text/html" title="Bayesian temporal biclustering with applications to multi-subject neuroscience studies" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Bayesiantemporalbiclusteringwithapplicationstomultisubjectneurosciencestudies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Bayesiantemporalbiclusteringwithapplicationstomultisubjectneurosciencestudies.html">&lt;p&gt;We consider the problem of analyzing multivariate time series collected on multiple subjects, with the goal of identifying groups of subjects exhibiting similar trends in their recorded measurements over time as well as time-varying groups of associated measurements. To this end, we propose a Bayesian model for temporal biclustering featuring nested partitions, where a time-invariant partition of subjects induces a time-varying partition of measurements. Our approach allows for data-driven determination of the number of subject and measurement clusters as well as estimation of the number and location of changepoints in measurement partitions. To efficiently perform model fitting and posterior estimation with Markov Chain Monte Carlo, we derive a blocked update of measurements’ cluster-assignment sequences. We illustrate the performance of our model in two applications to functional magnetic resonance imaging data and to an electroencephalogram dataset. The results indicate that the proposed model can combine information from potentially many subjects to discover a set of interpretable, dynamic patterns. Experiments on simulated data compare the estimation performance of the proposed model against ground-truth values and other statistical methods, showing that it performs well at identifying ground-truth subject and measurement clusters even when no subject or time dependence is present.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17131&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Federica Zoe Ricci, Erik B. Sudderth, Jaylen Lee, Megan A. K. Peters, Marina Vannucci, Michele Guindani</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">We consider the problem of analyzing multivariate time series collected on multiple subjects, with the goal of identifying groups of subjects exhibiting similar trends in their recorded measurements over time as well as time-varying groups of associated measurements. To this end, we propose a Bayesian model for temporal biclustering featuring nested partitions, where a time-invariant partition of subjects induces a time-varying partition of measurements. Our approach allows for data-driven determination of the number of subject and measurement clusters as well as estimation of the number and location of changepoints in measurement partitions. To efficiently perform model fitting and posterior estimation with Markov Chain Monte Carlo, we derive a blocked update of measurements’ cluster-assignment sequences. We illustrate the performance of our model in two applications to functional magnetic resonance imaging data and to an electroencephalogram dataset. The results indicate that the proposed model can combine information from potentially many subjects to discover a set of interpretable, dynamic patterns. Experiments on simulated data compare the estimation performance of the proposed model against ground-truth values and other statistical methods, showing that it performs well at identifying ground-truth subject and measurement clusters even when no subject or time dependence is present.</summary></entry><entry><title type="html">Benchmarking mortality risk prediction from electrocardiograms</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Benchmarkingmortalityriskpredictionfromelectrocardiograms.html" rel="alternate" type="text/html" title="Benchmarking mortality risk prediction from electrocardiograms" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Benchmarkingmortalityriskpredictionfromelectrocardiograms</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Benchmarkingmortalityriskpredictionfromelectrocardiograms.html">&lt;p&gt;Several recent high-impact studies leverage large hospital-owned electrocardiographic (ECG) databases to model and predict patient mortality. MIMIC-IV, released September 2023, is the first comparable public dataset and includes 800,000 ECGs from a U.S. hospital system. Previously, the largest public ECG dataset was Code-15, containing 345,000 ECGs collected during routine care in Brazil. These datasets now provide an excellent resource for a broader audience to explore ECG survival modeling. Here, we benchmark survival model performance on Code-15 and MIMIC-IV with two neural network architectures, compare four deep survival modeling approaches to Cox regressions trained on classifier outputs, and evaluate performance at one to ten years. Our results yield AUROC and concordance scores comparable to past work (circa 0.8) and reasonable AUPRC scores (MIMIC-IV: 0.4-0.5, Code-15: 0.05-0.13) considering the fraction of ECG samples linked to a mortality (MIMIC-IV: 27\%, Code-15: 4\%). When evaluating models on the opposite dataset, AUROC and concordance values drop by 0.1-0.15, which may be due to cohort differences. All code and results are made public.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17002&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Platon Lukyanenko, Joshua Mayourian, Mingxuan Liua, John K. Triedman, Sunil J. Ghelani, William G. La Cava</name></author><category term="stat.AP" /><summary type="html">Several recent high-impact studies leverage large hospital-owned electrocardiographic (ECG) databases to model and predict patient mortality. MIMIC-IV, released September 2023, is the first comparable public dataset and includes 800,000 ECGs from a U.S. hospital system. Previously, the largest public ECG dataset was Code-15, containing 345,000 ECGs collected during routine care in Brazil. These datasets now provide an excellent resource for a broader audience to explore ECG survival modeling. Here, we benchmark survival model performance on Code-15 and MIMIC-IV with two neural network architectures, compare four deep survival modeling approaches to Cox regressions trained on classifier outputs, and evaluate performance at one to ten years. Our results yield AUROC and concordance scores comparable to past work (circa 0.8) and reasonable AUPRC scores (MIMIC-IV: 0.4-0.5, Code-15: 0.05-0.13) considering the fraction of ECG samples linked to a mortality (MIMIC-IV: 27\%, Code-15: 4\%). When evaluating models on the opposite dataset, AUROC and concordance values drop by 0.1-0.15, which may be due to cohort differences. All code and results are made public.</summary></entry><entry><title type="html">Causal Inference on Process Graphs, Part II: Causal Structure and Effect Identification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/CausalInferenceonProcessGraphsPartIICausalStructureandEffectIdentification.html" rel="alternate" type="text/html" title="Causal Inference on Process Graphs, Part II: Causal Structure and Effect Identification" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/CausalInferenceonProcessGraphsPartIICausalStructureandEffectIdentification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/CausalInferenceonProcessGraphsPartIICausalStructureandEffectIdentification.html">&lt;p&gt;A structural vector autoregressive (SVAR) process is a linear causal model for variables that evolve over a discrete set of time points and between which there may be lagged and instantaneous effects. The qualitative causal structure of an SVAR process can be represented by its finite and directed process graph, in which a directed link connects two processes whenever there is a lagged or instantaneous effect between them. At the process graph level, the causal structure of SVAR processes is compactly parameterised in the frequency domain. In this paper, we consider the problem of causal discovery and causal effect estimation from the spectral density, the frequency domain analogue of the auto covariance, of the SVAR process. Causal discovery concerns the recovery of the process graph and causal effect estimation concerns the identification and estimation of causal effects in the frequency domain.
  We show that information about the process graph, in terms of $d$- and $t$-separation statements, can be identified by verifying algebraic constraints on the spectral density. Furthermore, we introduce a notion of rational identifiability for frequency causal effects that may be confounded by exogenous latent processes, and show that the recent graphical latent factor half-trek criterion can be used on the process graph to assess whether a given (confounded) effect can be identified by rational operations on the entries of the spectral density.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17422&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nicolas-Domenic Reiter, Jonas Wahl, Andreas Gerhardus, Jakob Runge</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">A structural vector autoregressive (SVAR) process is a linear causal model for variables that evolve over a discrete set of time points and between which there may be lagged and instantaneous effects. The qualitative causal structure of an SVAR process can be represented by its finite and directed process graph, in which a directed link connects two processes whenever there is a lagged or instantaneous effect between them. At the process graph level, the causal structure of SVAR processes is compactly parameterised in the frequency domain. In this paper, we consider the problem of causal discovery and causal effect estimation from the spectral density, the frequency domain analogue of the auto covariance, of the SVAR process. Causal discovery concerns the recovery of the process graph and causal effect estimation concerns the identification and estimation of causal effects in the frequency domain. We show that information about the process graph, in terms of $d$- and $t$-separation statements, can be identified by verifying algebraic constraints on the spectral density. Furthermore, we introduce a notion of rational identifiability for frequency causal effects that may be confounded by exogenous latent processes, and show that the recent graphical latent factor half-trek criterion can be used on the process graph to assess whether a given (confounded) effect can be identified by rational operations on the entries of the spectral density.</summary></entry><entry><title type="html">Causal Responder Detection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/CausalResponderDetection.html" rel="alternate" type="text/html" title="Causal Responder Detection" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/CausalResponderDetection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/CausalResponderDetection.html">&lt;p&gt;We introduce the causal responders detection (CARD), a novel method for responder analysis that identifies treated subjects who significantly respond to a treatment. Leveraging recent advances in conformal prediction, CARD employs machine learning techniques to accurately identify responders while controlling the false discovery rate in finite sample sizes. Additionally, we incorporate a propensity score adjustment to mitigate bias arising from non-random treatment allocation, enhancing the robustness of our method in observational settings. Simulation studies demonstrate that CARD effectively detects responders with high power in diverse scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17571&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tzviel Frostig, Oshri Machluf, Amitay Kamber, Elad Berkman, Raviv Pryluk</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.ML" /><summary type="html">We introduce the causal responders detection (CARD), a novel method for responder analysis that identifies treated subjects who significantly respond to a treatment. Leveraging recent advances in conformal prediction, CARD employs machine learning techniques to accurately identify responders while controlling the false discovery rate in finite sample sizes. Additionally, we incorporate a propensity score adjustment to mitigate bias arising from non-random treatment allocation, enhancing the robustness of our method in observational settings. Simulation studies demonstrate that CARD effectively detects responders with high power in diverse scenarios.</summary></entry><entry><title type="html">Compositional Models for Estimating Causal Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/CompositionalModelsforEstimatingCausalEffects.html" rel="alternate" type="text/html" title="Compositional Models for Estimating Causal Effects" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/CompositionalModelsforEstimatingCausalEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/CompositionalModelsforEstimatingCausalEffects.html">&lt;p&gt;Many real-world systems can be represented as sets of interacting components. Examples of such systems include computational systems such as query processors, natural systems such as cells, and social systems such as families. Many approaches have been proposed in traditional (associational) machine learning to model such structured systems, including statistical relational models and graph neural networks. Despite this prior work, existing approaches to estimating causal effects typically treat such systems as single units, represent them with a fixed set of variables and assume a homogeneous data-generating process. We study a compositional approach for estimating individual treatment effects (ITE) in structured systems, where each unit is represented by the composition of multiple heterogeneous components. This approach uses a modular architecture to model potential outcomes at each component and aggregates component-level potential outcomes to obtain the unit-level potential outcomes. We discover novel benefits of the compositional approach in causal inference - systematic generalization to estimate counterfactual outcomes of unseen combinations of components and improved overlap guarantees between treatment and control groups compared to the classical methods for causal effect estimation. We also introduce a set of novel environments for empirically evaluating the compositional approach and demonstrate the effectiveness of our approach using both simulated and real-world data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17714&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Purva Pruthi, David Jensen</name></author><category term="stat.ME" /><summary type="html">Many real-world systems can be represented as sets of interacting components. Examples of such systems include computational systems such as query processors, natural systems such as cells, and social systems such as families. Many approaches have been proposed in traditional (associational) machine learning to model such structured systems, including statistical relational models and graph neural networks. Despite this prior work, existing approaches to estimating causal effects typically treat such systems as single units, represent them with a fixed set of variables and assume a homogeneous data-generating process. We study a compositional approach for estimating individual treatment effects (ITE) in structured systems, where each unit is represented by the composition of multiple heterogeneous components. This approach uses a modular architecture to model potential outcomes at each component and aggregates component-level potential outcomes to obtain the unit-level potential outcomes. We discover novel benefits of the compositional approach in causal inference - systematic generalization to estimate counterfactual outcomes of unseen combinations of components and improved overlap guarantees between treatment and control groups compared to the classical methods for causal effect estimation. We also introduce a set of novel environments for empirically evaluating the compositional approach and demonstrate the effectiveness of our approach using both simulated and real-world data.</summary></entry><entry><title type="html">Controlling Moments with Kernel Stein Discrepancies</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/ControllingMomentswithKernelSteinDiscrepancies.html" rel="alternate" type="text/html" title="Controlling Moments with Kernel Stein Discrepancies" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/ControllingMomentswithKernelSteinDiscrepancies</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/ControllingMomentswithKernelSteinDiscrepancies.html">&lt;p&gt;Kernel Stein discrepancies (KSDs) measure the quality of a distributional approximation and can be computed even when the target density has an intractable normalizing constant. Notable applications include the diagnosis of approximate MCMC samplers and goodness-of-fit tests for unnormalized statistical models. The present work analyzes the convergence control properties of KSDs. We first show that standard KSDs used for weak convergence control fail to control moment convergence. To address this limitation, we next provide sufficient conditions under which alternative diffusion KSDs control both moment and weak convergence. As an immediate consequence we develop, for each $q &amp;gt; 0$, the first KSDs known to exactly characterize $q$-Wasserstein convergence.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2211.05408&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Heishiro Kanagawa, Alessandro Barp, Arthur Gretton, Lester Mackey</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">Kernel Stein discrepancies (KSDs) measure the quality of a distributional approximation and can be computed even when the target density has an intractable normalizing constant. Notable applications include the diagnosis of approximate MCMC samplers and goodness-of-fit tests for unnormalized statistical models. The present work analyzes the convergence control properties of KSDs. We first show that standard KSDs used for weak convergence control fail to control moment convergence. To address this limitation, we next provide sufficient conditions under which alternative diffusion KSDs control both moment and weak convergence. As an immediate consequence we develop, for each $q &amp;gt; 0$, the first KSDs known to exactly characterize $q$-Wasserstein convergence.</summary></entry><entry><title type="html">Copula-Based Estimation of Causal Effects in Multiple Linear and Path Analysis Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/CopulaBasedEstimationofCausalEffectsinMultipleLinearandPathAnalysisModels.html" rel="alternate" type="text/html" title="Copula-Based Estimation of Causal Effects in Multiple Linear and Path Analysis Models" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/CopulaBasedEstimationofCausalEffectsinMultipleLinearandPathAnalysisModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/CopulaBasedEstimationofCausalEffectsinMultipleLinearandPathAnalysisModels.html">&lt;p&gt;Regression analysis is one of the most popularly used statistical technique which only measures the direct effect of independent variables on dependent variable. Path analysis looks for both direct and indirect effects of independent variables and may overcome several hurdles allied with regression models. It utilizes one or more structural regression equations in the model which are used to estimate the unknown parameters. The aim of this work is to study the path analysis models when the endogenous (dependent) variable and exogenous (independent) variables are linked through the elliptical copulas. Using well-organized numerical schemes, we investigate the performance of path models when direct and indirect effects are estimated applying classical ordinary least squares and copula-based regression approaches in different scenarios. Finally, two real data applications are also presented to demonstrate the performance of path analysis using copula approach.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17445&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alam Ali, Ashok Kumar Pathak, Mohd Arshad, Ayyub Sheikhi</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Regression analysis is one of the most popularly used statistical technique which only measures the direct effect of independent variables on dependent variable. Path analysis looks for both direct and indirect effects of independent variables and may overcome several hurdles allied with regression models. It utilizes one or more structural regression equations in the model which are used to estimate the unknown parameters. The aim of this work is to study the path analysis models when the endogenous (dependent) variable and exogenous (independent) variables are linked through the elliptical copulas. Using well-organized numerical schemes, we investigate the performance of path models when direct and indirect effects are estimated applying classical ordinary least squares and copula-based regression approaches in different scenarios. Finally, two real data applications are also presented to demonstrate the performance of path analysis using copula approach.</summary></entry><entry><title type="html">Distance-based Chatterjee correlation: a new generalized robust measure of directed association for multivariate real and complex-valued data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/DistancebasedChatterjeecorrelationanewgeneralizedrobustmeasureofdirectedassociationformultivariaterealandcomplexvalueddata.html" rel="alternate" type="text/html" title="Distance-based Chatterjee correlation: a new generalized robust measure of directed association for multivariate real and complex-valued data" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/DistancebasedChatterjeecorrelationanewgeneralizedrobustmeasureofdirectedassociationformultivariaterealandcomplexvalueddata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/DistancebasedChatterjeecorrelationanewgeneralizedrobustmeasureofdirectedassociationformultivariaterealandcomplexvalueddata.html">&lt;p&gt;Building upon the Chatterjee correlation (2021: J. Am. Stat. Assoc. 116, p2009) for two real-valued variables, this study introduces a generalized measure of directed association between two vector variables, real or complex-valued, and of possibly different dimensions. The new measure is denoted as the “distance-based Chatterjee correlation”, owing to the use here of the “distance transformed data” defined in Szekely et al (2007: Ann. Statist. 35, p2769) for the distance correlation. A main property of the new measure, inherited from the original Chatterjee correlation, is its predictive and asymmetric nature: it measures how well one variable can be predicted by the other, asymmetrically. This allows for inferring the causal direction of the association, by using the method of Blobaum et al (2019: PeerJ Comput. Sci. 1, e169). Since the original Chatterjee correlation is based on ranks, it is not available for complex variables, nor for general multivariate data. The novelty of our work is the extension to multivariate real and complex-valued pairs of vectors, offering a robust measure of directed association in a completely non-parametric setting. Informally, the intuitive assumption used here is that distance correlation is mathematically equivalent to Pearson’s correlation when applied to “distance transformed” data. The next logical step is to compute Chatterjee’s correlation on the same “distance transformed” data, thereby extending the analysis to multivariate vectors of real and complex valued data. As a bonus, the new measure here is robust to outliers, which is not true for the distance correlation of Szekely et al. Additionally, this approach allows for inference regarding the causal direction of the association between the variables.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16458&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Roberto D. Pascual-Marqui, Kieko Kochi, Toshihiko Kinoshita</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Building upon the Chatterjee correlation (2021: J. Am. Stat. Assoc. 116, p2009) for two real-valued variables, this study introduces a generalized measure of directed association between two vector variables, real or complex-valued, and of possibly different dimensions. The new measure is denoted as the “distance-based Chatterjee correlation”, owing to the use here of the “distance transformed data” defined in Szekely et al (2007: Ann. Statist. 35, p2769) for the distance correlation. A main property of the new measure, inherited from the original Chatterjee correlation, is its predictive and asymmetric nature: it measures how well one variable can be predicted by the other, asymmetrically. This allows for inferring the causal direction of the association, by using the method of Blobaum et al (2019: PeerJ Comput. Sci. 1, e169). Since the original Chatterjee correlation is based on ranks, it is not available for complex variables, nor for general multivariate data. The novelty of our work is the extension to multivariate real and complex-valued pairs of vectors, offering a robust measure of directed association in a completely non-parametric setting. Informally, the intuitive assumption used here is that distance correlation is mathematically equivalent to Pearson’s correlation when applied to “distance transformed” data. The next logical step is to compute Chatterjee’s correlation on the same “distance transformed” data, thereby extending the analysis to multivariate vectors of real and complex valued data. As a bonus, the new measure here is robust to outliers, which is not true for the distance correlation of Szekely et al. Additionally, this approach allows for inference regarding the causal direction of the association between the variables.</summary></entry><entry><title type="html">Estimation and Inference for CP Tensor Factor Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/EstimationandInferenceforCPTensorFactorModels.html" rel="alternate" type="text/html" title="Estimation and Inference for CP Tensor Factor Models" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/EstimationandInferenceforCPTensorFactorModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/EstimationandInferenceforCPTensorFactorModels.html">&lt;p&gt;High-dimensional tensor-valued data have recently gained attention from researchers in economics and finance. We consider the estimation and inference of high-dimensional tensor factor models, where each dimension of the tensor diverges. Our focus is on a factor model that admits CP-type tensor decomposition, which allows for non-orthogonal loading vectors. Based on the contemporary covariance matrix, we propose an iterative simultaneous projection estimation method. Our estimator is robust to weak dependence among factors and weak correlation across different dimensions in the idiosyncratic shocks. We establish an inferential theory, demonstrating both consistency and asymptotic normality under relaxed assumptions. Within a unified framework, we consider two eigenvalue ratio-based estimators for the number of factors in a tensor factor model and justify their consistency. Through a simulation study and two empirical applications featuring sorted portfolios and international trade flows, we illustrate the advantages of our proposed estimator over existing methodologies in the literature.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17278&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bin Chen, Yuefeng Han, Qiyang Yu</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">High-dimensional tensor-valued data have recently gained attention from researchers in economics and finance. We consider the estimation and inference of high-dimensional tensor factor models, where each dimension of the tensor diverges. Our focus is on a factor model that admits CP-type tensor decomposition, which allows for non-orthogonal loading vectors. Based on the contemporary covariance matrix, we propose an iterative simultaneous projection estimation method. Our estimator is robust to weak dependence among factors and weak correlation across different dimensions in the idiosyncratic shocks. We establish an inferential theory, demonstrating both consistency and asymptotic normality under relaxed assumptions. Within a unified framework, we consider two eigenvalue ratio-based estimators for the number of factors in a tensor factor model and justify their consistency. Through a simulation study and two empirical applications featuring sorted portfolios and international trade flows, we illustrate the advantages of our proposed estimator over existing methodologies in the literature.</summary></entry><entry><title type="html">Estimation of the Number Needed to Treat, the Number Needed to be Exposed, and the Exposure Impact Number with Instrumental Variables</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/EstimationoftheNumberNeededtoTreattheNumberNeededtobeExposedandtheExposureImpactNumberwithInstrumentalVariables.html" rel="alternate" type="text/html" title="Estimation of the Number Needed to Treat, the Number Needed to be Exposed, and the Exposure Impact Number with Instrumental Variables" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/EstimationoftheNumberNeededtoTreattheNumberNeededtobeExposedandtheExposureImpactNumberwithInstrumentalVariables</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/EstimationoftheNumberNeededtoTreattheNumberNeededtobeExposedandtheExposureImpactNumberwithInstrumentalVariables.html">&lt;p&gt;The Number needed to treat (NNT) is an efficacy index defined as the average number of patients needed to treat to attain one additional treatment benefit. In observational studies, specifically in epidemiology, the adequacy of the populationwise NNT is questionable since the exposed group characteristics may substantially differ from the unexposed. To address this issue, groupwise efficacy indices were defined: the Exposure Impact Number (EIN) for the exposed group and the Number Needed to be Exposed (NNE) for the unexposed. Each defined index answers a unique research question since it targets a unique sub-population. In observational studies, the group allocation is typically affected by confounders that might be unmeasured. The available estimation methods that rely either on randomization or the sufficiency of the measured covariates for confounding control will result in inconsistent estimators of the true NNT (EIN, NNE) in such settings. Using Rubin’s potential outcomes framework, we explicitly define the NNT and its derived indices as causal contrasts. Next, we introduce a novel method that uses instrumental variables to estimate the three aforementioned indices in observational studies. We present two analytical examples and a corresponding simulation study. The simulation study illustrates that the novel estimators are statistically consistent, unlike the previously available methods, and their analytical confidence intervals’ empirical coverage rates converge to their nominal values. Finally, a real-world data example of an analysis of the effect of vitamin D deficiency on the mortality rate is presented.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.09319&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Valentin Vancak, Arvid Sjölander</name></author><category term="stat.ME" /><summary type="html">The Number needed to treat (NNT) is an efficacy index defined as the average number of patients needed to treat to attain one additional treatment benefit. In observational studies, specifically in epidemiology, the adequacy of the populationwise NNT is questionable since the exposed group characteristics may substantially differ from the unexposed. To address this issue, groupwise efficacy indices were defined: the Exposure Impact Number (EIN) for the exposed group and the Number Needed to be Exposed (NNE) for the unexposed. Each defined index answers a unique research question since it targets a unique sub-population. In observational studies, the group allocation is typically affected by confounders that might be unmeasured. The available estimation methods that rely either on randomization or the sufficiency of the measured covariates for confounding control will result in inconsistent estimators of the true NNT (EIN, NNE) in such settings. Using Rubin’s potential outcomes framework, we explicitly define the NNT and its derived indices as causal contrasts. Next, we introduce a novel method that uses instrumental variables to estimate the three aforementioned indices in observational studies. We present two analytical examples and a corresponding simulation study. The simulation study illustrates that the novel estimators are statistically consistent, unlike the previously available methods, and their analytical confidence intervals’ empirical coverage rates converge to their nominal values. Finally, a real-world data example of an analysis of the effect of vitamin D deficiency on the mortality rate is presented.</summary></entry><entry><title type="html">Gaussian and Student’s $t$ mixture vector autoregressive model with application to the effects of the Euro area monetary policy shock</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/GaussianandStudentstmixturevectorautoregressivemodelwithapplicationtotheeffectsoftheEuroareamonetarypolicyshock.html" rel="alternate" type="text/html" title="Gaussian and Student’s $t$ mixture vector autoregressive model with application to the effects of the Euro area monetary policy shock" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/GaussianandStudentstmixturevectorautoregressivemodelwithapplicationtotheeffectsoftheEuroareamonetarypolicyshock</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/GaussianandStudentstmixturevectorautoregressivemodelwithapplicationtotheeffectsoftheEuroareamonetarypolicyshock.html">&lt;p&gt;A new mixture vector autoregressive model based on Gaussian and Student’s $t$ distributions is introduced. As its mixture components, our model incorporates conditionally homoskedastic linear Gaussian vector autoregressions and conditionally heteroskedastic linear Student’s $t$ vector autoregressions. For a $p$th order model, the mixing weights depend on the full distribution of the preceding $p$ observations, which leads to attractive practical and theoretical properties such as ergodicity and full knowledge of the stationary distribution of $p+1$ consecutive observations. A structural version of the model with statistically identified shocks is also proposed. The empirical application studies the effects of the Euro area monetary policy shock. We fit a two-regime model to the data and find the effects, particularly on inflation, stronger in the regime that mainly prevails before the Financial crisis than in the regime that mainly dominates after it. The introduced methods are implemented in the accompanying R package gmvarkit.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2109.13648&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Savi Virolainen</name></author><category term="stat.ME" /><summary type="html">A new mixture vector autoregressive model based on Gaussian and Student’s $t$ distributions is introduced. As its mixture components, our model incorporates conditionally homoskedastic linear Gaussian vector autoregressions and conditionally heteroskedastic linear Student’s $t$ vector autoregressions. For a $p$th order model, the mixing weights depend on the full distribution of the preceding $p$ observations, which leads to attractive practical and theoretical properties such as ergodicity and full knowledge of the stationary distribution of $p+1$ consecutive observations. A structural version of the model with statistically identified shocks is also proposed. The empirical application studies the effects of the Euro area monetary policy shock. We fit a two-regime model to the data and find the effects, particularly on inflation, stronger in the regime that mainly prevails before the Financial crisis than in the regime that mainly dominates after it. The introduced methods are implemented in the accompanying R package gmvarkit.</summary></entry><entry><title type="html">Mask-Guided Attention U-Net for Enhanced Neonatal Brain Extraction and Image Preprocessing</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/MaskGuidedAttentionUNetforEnhancedNeonatalBrainExtractionandImagePreprocessing.html" rel="alternate" type="text/html" title="Mask-Guided Attention U-Net for Enhanced Neonatal Brain Extraction and Image Preprocessing" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/MaskGuidedAttentionUNetforEnhancedNeonatalBrainExtractionandImagePreprocessing</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/MaskGuidedAttentionUNetforEnhancedNeonatalBrainExtractionandImagePreprocessing.html">&lt;p&gt;In this study, we introduce MGA-Net, a novel mask-guided attention neural network, which extends the U-net model for precision neonatal brain imaging. MGA-Net is designed to extract the brain from other structures and reconstruct high-quality brain images. The network employs a common encoder and two decoders: one for brain mask extraction and the other for brain region reconstruction. A key feature of MGA-Net is its high-level mask-guided attention module, which leverages features from the brain mask decoder to enhance image reconstruction. To enable the same encoder and decoder to process both MRI and ultrasound (US) images, MGA-Net integrates sinusoidal positional encoding. This encoding assigns distinct positional values to MRI and US images, allowing the model to effectively learn from both modalities. Consequently, features learned from a single modality can aid in learning a modality with less available data, such as US. We extensively validated the proposed MGA-Net on diverse datasets from varied clinical settings and neonatal age groups. The metrics used for assessment included the DICE similarity coefficient, recall, and accuracy for image segmentation; structural similarity for image reconstruction; and root mean squared error for total brain volume estimation from 3D ultrasound images. Our results demonstrate that MGA-Net significantly outperforms traditional methods, offering superior performance in brain extraction and segmentation while achieving high precision in image reconstruction and volumetric analysis. Thus, MGA-Net represents a robust and effective preprocessing tool for MRI and 3D ultrasound images, marking a significant advance in neuroimaging that enhances both research and clinical diagnostics in the neonatal period and beyond.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17709&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bahram Jafrasteh, Simon Pedro Lubian-Lopez, Emiliano Trimarco, Macarena Roman Ruiz, Carmen Rodriguez Barrios, Yolanda Marin Almagro, Isabel Benavente-Fernandez</name></author><category term="stat.CO" /><summary type="html">In this study, we introduce MGA-Net, a novel mask-guided attention neural network, which extends the U-net model for precision neonatal brain imaging. MGA-Net is designed to extract the brain from other structures and reconstruct high-quality brain images. The network employs a common encoder and two decoders: one for brain mask extraction and the other for brain region reconstruction. A key feature of MGA-Net is its high-level mask-guided attention module, which leverages features from the brain mask decoder to enhance image reconstruction. To enable the same encoder and decoder to process both MRI and ultrasound (US) images, MGA-Net integrates sinusoidal positional encoding. This encoding assigns distinct positional values to MRI and US images, allowing the model to effectively learn from both modalities. Consequently, features learned from a single modality can aid in learning a modality with less available data, such as US. We extensively validated the proposed MGA-Net on diverse datasets from varied clinical settings and neonatal age groups. The metrics used for assessment included the DICE similarity coefficient, recall, and accuracy for image segmentation; structural similarity for image reconstruction; and root mean squared error for total brain volume estimation from 3D ultrasound images. Our results demonstrate that MGA-Net significantly outperforms traditional methods, offering superior performance in brain extraction and segmentation while achieving high precision in image reconstruction and volumetric analysis. Thus, MGA-Net represents a robust and effective preprocessing tool for MRI and 3D ultrasound images, marking a significant advance in neuroimaging that enhances both research and clinical diagnostics in the neonatal period and beyond.</summary></entry><entry><title type="html">Minimax-Regret Sample Selection in Randomized Experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/MinimaxRegretSampleSelectioninRandomizedExperiments.html" rel="alternate" type="text/html" title="Minimax-Regret Sample Selection in Randomized Experiments" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/MinimaxRegretSampleSelectioninRandomizedExperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/MinimaxRegretSampleSelectioninRandomizedExperiments.html">&lt;p&gt;Randomized controlled trials are often run in settings with many subpopulations that may have differential benefits from the treatment being evaluated. We consider the problem of sample selection, i.e., whom to enroll in a randomized trial, such as to optimize welfare in a heterogeneous population. We formalize this problem within the minimax-regret framework, and derive optimal sample-selection schemes under a variety of conditions. Using data from a COVID-19 vaccine trial, we also highlight how different objectives and decision rules can lead to meaningfully different guidance regarding optimal sample allocation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.01386&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuchen Hu, Henry Zhu, Emma Brunskill, Stefan Wager</name></author><category term="stat.ME" /><summary type="html">Randomized controlled trials are often run in settings with many subpopulations that may have differential benefits from the treatment being evaluated. We consider the problem of sample selection, i.e., whom to enroll in a randomized trial, such as to optimize welfare in a heterogeneous population. We formalize this problem within the minimax-regret framework, and derive optimal sample-selection schemes under a variety of conditions. Using data from a COVID-19 vaccine trial, we also highlight how different objectives and decision rules can lead to meaningfully different guidance regarding optimal sample allocation.</summary></entry><entry><title type="html">Model Uncertainty in Latent Gaussian Models with Univariate Link Function</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/ModelUncertaintyinLatentGaussianModelswithUnivariateLinkFunction.html" rel="alternate" type="text/html" title="Model Uncertainty in Latent Gaussian Models with Univariate Link Function" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/ModelUncertaintyinLatentGaussianModelswithUnivariateLinkFunction</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/ModelUncertaintyinLatentGaussianModelswithUnivariateLinkFunction.html">&lt;p&gt;We consider a class of latent Gaussian models with a univariate link function (ULLGMs). These are based on standard likelihood specifications (such as Poisson, Binomial, Bernoulli, Erlang, etc.) but incorporate a latent normal linear regression framework on a transformation of a key scalar parameter. We allow for model uncertainty regarding the covariates included in the regression. The ULLGM class typically accommodates extra dispersion in the data and has clear advantages for deriving theoretical properties and designing computational procedures. We formally characterize posterior existence under a convenient and popular improper prior and propose an efficient Markov chain Monte Carlo algorithm for Bayesian model averaging in ULLGMs. Simulation results suggest that the framework provides accurate results that are robust to some degree of misspecification. The methodology is successfully applied to measles vaccination coverage data from Ethiopia and to data on bilateral migration flows between OECD countries.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17318&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mark F. J. Steel, Gregor Zens</name></author><category term="stat.ME" /><summary type="html">We consider a class of latent Gaussian models with a univariate link function (ULLGMs). These are based on standard likelihood specifications (such as Poisson, Binomial, Bernoulli, Erlang, etc.) but incorporate a latent normal linear regression framework on a transformation of a key scalar parameter. We allow for model uncertainty regarding the covariates included in the regression. The ULLGM class typically accommodates extra dispersion in the data and has clear advantages for deriving theoretical properties and designing computational procedures. We formally characterize posterior existence under a convenient and popular improper prior and propose an efficient Markov chain Monte Carlo algorithm for Bayesian model averaging in ULLGMs. Simulation results suggest that the framework provides accurate results that are robust to some degree of misspecification. The methodology is successfully applied to measles vaccination coverage data from Ethiopia and to data on bilateral migration flows between OECD countries.</summary></entry><entry><title type="html">Model-assisted analysis of covariance estimators for stepped wedge cluster randomized experiments</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Modelassistedanalysisofcovarianceestimatorsforsteppedwedgeclusterrandomizedexperiments.html" rel="alternate" type="text/html" title="Model-assisted analysis of covariance estimators for stepped wedge cluster randomized experiments" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Modelassistedanalysisofcovarianceestimatorsforsteppedwedgeclusterrandomizedexperiments</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Modelassistedanalysisofcovarianceestimatorsforsteppedwedgeclusterrandomizedexperiments.html">&lt;p&gt;Stepped wedge cluster randomized experiments (SW-CREs) represent a class of unidirectional crossover designs. Although SW-CREs have become popular, definitions of estimands and robust methods to target estimands under the potential outcomes framework remain insufficient. To address this gap, we describe a class of estimands that explicitly acknowledge the multilevel data structure in SW-CREs and highlight three typical members of the estimand class that are interpretable. We then introduce four analysis of covariance (ANCOVA) working models to achieve estimand-aligned analyses with covariate adjustment. Each ANCOVA estimator is model-assisted, as its point estimator is consistent even when the working model is misspecified. Under the stepped wedge randomization scheme, we establish the finite population Central Limit Theorem for each estimator. We study the finite-sample operating characteristics of the ANCOVA estimators in simulations and illustrate their application by analyzing the Washington State Expedited Partner Therapy study.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.11267&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xinyuan Chen, Fan Li</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Stepped wedge cluster randomized experiments (SW-CREs) represent a class of unidirectional crossover designs. Although SW-CREs have become popular, definitions of estimands and robust methods to target estimands under the potential outcomes framework remain insufficient. To address this gap, we describe a class of estimands that explicitly acknowledge the multilevel data structure in SW-CREs and highlight three typical members of the estimand class that are interpretable. We then introduce four analysis of covariance (ANCOVA) working models to achieve estimand-aligned analyses with covariate adjustment. Each ANCOVA estimator is model-assisted, as its point estimator is consistent even when the working model is misspecified. Under the stepped wedge randomization scheme, we establish the finite population Central Limit Theorem for each estimator. We study the finite-sample operating characteristics of the ANCOVA estimators in simulations and illustrate their application by analyzing the Washington State Expedited Partner Therapy study.</summary></entry><entry><title type="html">NFL Ghosts: A framework for evaluating defender positioning with conditional density estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/NFLGhostsAframeworkforevaluatingdefenderpositioningwithconditionaldensityestimation.html" rel="alternate" type="text/html" title="NFL Ghosts: A framework for evaluating defender positioning with conditional density estimation" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/NFLGhostsAframeworkforevaluatingdefenderpositioningwithconditionaldensityestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/NFLGhostsAframeworkforevaluatingdefenderpositioningwithconditionaldensityestimation.html">&lt;p&gt;Player attribution in American football remains an open problem due to the complex nature of twenty-two players interacting on the field, but the granularity of player tracking data provides ample opportunity for novel approaches. In this work, we introduce the first public framework to evaluate spatial and trajectory tracking data of players relative to a baseline distribution of “ghost” defenders. We demonstrate our framework in the context of modeling the nearest defender positioning at the moment of catch. In particular, we provide estimates of how much better or worse their observed positioning and trajectory compared to the expected play value of ghost defenders. Our framework leverages high-dimensional tracking data features through flexible random forests for conditional density estimation in two ways: (1) to model the distribution of receiver yards gained enabling the estimation of within-play expected value, and (2) to model the 2D spatial distribution of baseline ghost defenders. We present novel metrics for measuring player and team performance based on tracking data, and discuss challenges that remain in extending our framework to other aspects of American football.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17220&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ronald Yurko, Quang Nguyen, Konstantinos Pelechrinis</name></author><category term="stat.AP" /><summary type="html">Player attribution in American football remains an open problem due to the complex nature of twenty-two players interacting on the field, but the granularity of player tracking data provides ample opportunity for novel approaches. In this work, we introduce the first public framework to evaluate spatial and trajectory tracking data of players relative to a baseline distribution of “ghost” defenders. We demonstrate our framework in the context of modeling the nearest defender positioning at the moment of catch. In particular, we provide estimates of how much better or worse their observed positioning and trajectory compared to the expected play value of ghost defenders. Our framework leverages high-dimensional tracking data features through flexible random forests for conditional density estimation in two ways: (1) to model the distribution of receiver yards gained enabling the estimation of within-play expected value, and (2) to model the 2D spatial distribution of baseline ghost defenders. We present novel metrics for measuring player and team performance based on tracking data, and discuss challenges that remain in extending our framework to other aspects of American football.</summary></entry><entry><title type="html">Nowcasting in triple-system estimation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Nowcastingintriplesystemestimation.html" rel="alternate" type="text/html" title="Nowcasting in triple-system estimation" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Nowcastingintriplesystemestimation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Nowcastingintriplesystemestimation.html">&lt;p&gt;When samples that each cover part of a population for a certain reference date become available slowly over time, an estimate of the population size can be obtained when at least two samples are available. Ideally one uses all the available samples, but if some samples become available much later one may want to use the samples that are available earlier, to obtain a preliminary or nowcast estimate. However, a limited number of samples may no longer lead to asymptotically unbiased estimates, in particularly in case of two early available samples that suffer from pairwise dependence. In this paper we propose a multiple system nowcasting model that deals with this issue by combining the early available samples with samples from a previous reference date and the expectation-maximisation algorithm. This leads to a nowcast estimate that is asymptotically unbiased under more relaxed assumptions than the dual-system estimator. The multiple system nowcasting model is applied to the problem of estimating the number of homeless people in The Netherlands, which leads to reasonably accurate nowcast estimates.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17637&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Daan B. Zult, Peter G. M. van der Heijden, Bart F. M. Bakker</name></author><category term="stat.ME" /><summary type="html">When samples that each cover part of a population for a certain reference date become available slowly over time, an estimate of the population size can be obtained when at least two samples are available. Ideally one uses all the available samples, but if some samples become available much later one may want to use the samples that are available earlier, to obtain a preliminary or nowcast estimate. However, a limited number of samples may no longer lead to asymptotically unbiased estimates, in particularly in case of two early available samples that suffer from pairwise dependence. In this paper we propose a multiple system nowcasting model that deals with this issue by combining the early available samples with samples from a previous reference date and the expectation-maximisation algorithm. This leads to a nowcast estimate that is asymptotically unbiased under more relaxed assumptions than the dual-system estimator. The multiple system nowcasting model is applied to the problem of estimating the number of homeless people in The Netherlands, which leads to reasonably accurate nowcast estimates.</summary></entry><entry><title type="html">Rapid Shear Capacity Prediction of TRM-Strengthened Unreinforced Masonry Walls through Interpretable Machine Learning using a Web App</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/RapidShearCapacityPredictionofTRMStrengthenedUnreinforcedMasonryWallsthroughInterpretableMachineLearningusingaWebApp.html" rel="alternate" type="text/html" title="Rapid Shear Capacity Prediction of TRM-Strengthened Unreinforced Masonry Walls through Interpretable Machine Learning using a Web App" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/RapidShearCapacityPredictionofTRMStrengthenedUnreinforcedMasonryWallsthroughInterpretableMachineLearningusingaWebApp</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/RapidShearCapacityPredictionofTRMStrengthenedUnreinforcedMasonryWallsthroughInterpretableMachineLearningusingaWebApp.html">&lt;p&gt;The presented study aims to provide an efficient and reliable tool for rapid estimation of the shear capacity of a TRM-strengthened masonry wall. For this purpose, a data-driven methodology based on a machine learning system is proposed using a dataset constituted of experimental results selected from the bibliography. The outlier points were detected using Cook’s distance methodology and removed from the raw dataset, which consisted of 113 examples and 11 input variables. In the processed dataset, 17 Machine Learning methods were trained, optimized through hyperparameter tuning, and compared on the test set. The most effective models are combined into a voting model to leverage the predictive capacity of more than a single regressor. The final blended model shows remarkable predicting capacity with the determination factor ($R^2$) equal to 0.95 and the mean absolute percentage error equal to 8.03\%. In sequence, machine learning interpretation methods are applied to find how the predictors influence the target output. $A_m$, $f_t$, and $n\cdot t_f$ were identified as the most significant predictors with a mainly positive influence on the shear capacity. Finally, the built ML system is employed in a user-friendly web app for easy access and usage by professionals and researchers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.16889&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Petros Lazaridis, Athanasia Thomoglou</name></author><category term="stat.AP" /><summary type="html">The presented study aims to provide an efficient and reliable tool for rapid estimation of the shear capacity of a TRM-strengthened masonry wall. For this purpose, a data-driven methodology based on a machine learning system is proposed using a dataset constituted of experimental results selected from the bibliography. The outlier points were detected using Cook’s distance methodology and removed from the raw dataset, which consisted of 113 examples and 11 input variables. In the processed dataset, 17 Machine Learning methods were trained, optimized through hyperparameter tuning, and compared on the test set. The most effective models are combined into a voting model to leverage the predictive capacity of more than a single regressor. The final blended model shows remarkable predicting capacity with the determination factor ($R^2$) equal to 0.95 and the mean absolute percentage error equal to 8.03\%. In sequence, machine learning interpretation methods are applied to find how the predictors influence the target output. $A_m$, $f_t$, and $n\cdot t_f$ were identified as the most significant predictors with a mainly positive influence on the shear capacity. Finally, the built ML system is employed in a user-friendly web app for easy access and usage by professionals and researchers.</summary></entry><entry><title type="html">Scalable Sampling of Truncated Multivariate Normals Using Sequential Nearest-Neighbor Approximation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/ScalableSamplingofTruncatedMultivariateNormalsUsingSequentialNearestNeighborApproximation.html" rel="alternate" type="text/html" title="Scalable Sampling of Truncated Multivariate Normals Using Sequential Nearest-Neighbor Approximation" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/ScalableSamplingofTruncatedMultivariateNormalsUsingSequentialNearestNeighborApproximation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/ScalableSamplingofTruncatedMultivariateNormalsUsingSequentialNearestNeighborApproximation.html">&lt;p&gt;We propose a linear-complexity method for sampling from truncated multivariate normal (TMVN) distributions with high fidelity by applying nearest-neighbor approximations to a product-of-conditionals decomposition of the TMVN density. To make the sequential sampling based on the decomposition feasible, we introduce a novel method that avoids the intractable high-dimensional TMVN distribution by sampling sequentially from $m$-dimensional TMVN distributions, where $m$ is a tuning parameter controlling the fidelity. This allows us to overcome the existing methods’ crucial problem of rapidly decreasing acceptance rates for increasing dimension. Throughout our experiments with up to tens of thousands of dimensions, we can produce high-fidelity samples with $m$ in the dozens, achieving superior scalability compared to existing state-of-the-art methods. We study a tetrachloroethylene concentration dataset that has $3{,}971$ observed responses and $20{,}730$ undetected responses, together modeled as a partially censored Gaussian process, where our method enables posterior inference for the censored responses through sampling a $20{,}730$-dimensional TMVN distribution.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17307&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jian Cao, Matthias Katzfuss</name></author><category term="stat.CO" /><summary type="html">We propose a linear-complexity method for sampling from truncated multivariate normal (TMVN) distributions with high fidelity by applying nearest-neighbor approximations to a product-of-conditionals decomposition of the TMVN density. To make the sequential sampling based on the decomposition feasible, we introduce a novel method that avoids the intractable high-dimensional TMVN distribution by sampling sequentially from $m$-dimensional TMVN distributions, where $m$ is a tuning parameter controlling the fidelity. This allows us to overcome the existing methods’ crucial problem of rapidly decreasing acceptance rates for increasing dimension. Throughout our experiments with up to tens of thousands of dimensions, we can produce high-fidelity samples with $m$ in the dozens, achieving superior scalability compared to existing state-of-the-art methods. We study a tetrachloroethylene concentration dataset that has $3{,}971$ observed responses and $20{,}730$ undetected responses, together modeled as a partially censored Gaussian process, where our method enables posterior inference for the censored responses through sampling a $20{,}730$-dimensional TMVN distribution.</summary></entry><entry><title type="html">The Influence of Nuisance Parameter Uncertainty on Statistical Inference in Practical Data Science Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/TheInfluenceofNuisanceParameterUncertaintyonStatisticalInferenceinPracticalDataScienceModels.html" rel="alternate" type="text/html" title="The Influence of Nuisance Parameter Uncertainty on Statistical Inference in Practical Data Science Models" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/TheInfluenceofNuisanceParameterUncertaintyonStatisticalInferenceinPracticalDataScienceModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/TheInfluenceofNuisanceParameterUncertaintyonStatisticalInferenceinPracticalDataScienceModels.html">&lt;p&gt;For multiple reasons – such as avoiding overtraining from one data set or because of having received numerical estimates for some parameters in a model from an alternative source – it is sometimes useful to divide a model’s parameters into one group of primary parameters and one group of nuisance parameters. However, uncertainty in the values of nuisance parameters is an inevitable factor that impacts the model’s reliability. This paper examines the issue of uncertainty calculation for primary parameters of interest in the presence of nuisance parameters. We illustrate a general procedure on two distinct model forms: 1) the GARCH time series model with univariate nuisance parameter and 2) multiple hidden layer feed-forward neural network models with multivariate nuisance parameters. Leveraging an existing theoretical framework for nuisance parameter uncertainty, we show how to modify the confidence regions for the primary parameters while considering the inherent uncertainty introduced by nuisance parameters. Furthermore, our study validates the practical effectiveness of adjusted confidence regions that properly account for uncertainty in nuisance parameters. Such an adjustment helps data scientists produce results that more honestly reflect the overall uncertainty.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.15078&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yunrong Wan</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">For multiple reasons – such as avoiding overtraining from one data set or because of having received numerical estimates for some parameters in a model from an alternative source – it is sometimes useful to divide a model’s parameters into one group of primary parameters and one group of nuisance parameters. However, uncertainty in the values of nuisance parameters is an inevitable factor that impacts the model’s reliability. This paper examines the issue of uncertainty calculation for primary parameters of interest in the presence of nuisance parameters. We illustrate a general procedure on two distinct model forms: 1) the GARCH time series model with univariate nuisance parameter and 2) multiple hidden layer feed-forward neural network models with multivariate nuisance parameters. Leveraging an existing theoretical framework for nuisance parameter uncertainty, we show how to modify the confidence regions for the primary parameters while considering the inherent uncertainty introduced by nuisance parameters. Furthermore, our study validates the practical effectiveness of adjusted confidence regions that properly account for uncertainty in nuisance parameters. Such an adjustment helps data scientists produce results that more honestly reflect the overall uncertainty.</summary></entry><entry><title type="html">Transfer Learning for High Dimensional Robust Regression</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/TransferLearningforHighDimensionalRobustRegression.html" rel="alternate" type="text/html" title="Transfer Learning for High Dimensional Robust Regression" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/TransferLearningforHighDimensionalRobustRegression</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/TransferLearningforHighDimensionalRobustRegression.html">&lt;p&gt;Transfer learning has become an essential technique for utilizing information from source datasets to improve the performance of the target task. However, in the context of high-dimensional data, heterogeneity arises due to heteroscedastic variance or inhomogeneous covariate effects. To solve this problem, this paper proposes a robust transfer learning based on the Huber regression, specifically designed for scenarios where the transferable source data set is known. This method effectively mitigates the impact of data heteroscedasticity, leading to improvements in estimation and prediction accuracy. Moreover, when the transferable source data set is unknown, the paper introduces an efficient detection algorithm to identify informative sources. The effectiveness of the proposed method is proved through numerical simulation and empirical analysis using superconductor data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17567&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xiaohui Yuan, Shujie Ren</name></author><category term="stat.ME," /><category term="stat.OT" /><summary type="html">Transfer learning has become an essential technique for utilizing information from source datasets to improve the performance of the target task. However, in the context of high-dimensional data, heterogeneity arises due to heteroscedastic variance or inhomogeneous covariate effects. To solve this problem, this paper proposes a robust transfer learning based on the Huber regression, specifically designed for scenarios where the transferable source data set is known. This method effectively mitigates the impact of data heteroscedasticity, leading to improvements in estimation and prediction accuracy. Moreover, when the transferable source data set is unknown, the paper introduces an efficient detection algorithm to identify informative sources. The effectiveness of the proposed method is proved through numerical simulation and empirical analysis using superconductor data.</summary></entry><entry><title type="html">Tree-based variational inference for Poisson log-normal models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/TreebasedvariationalinferenceforPoissonlognormalmodels.html" rel="alternate" type="text/html" title="Tree-based variational inference for Poisson log-normal models" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/TreebasedvariationalinferenceforPoissonlognormalmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/TreebasedvariationalinferenceforPoissonlognormalmodels.html">&lt;p&gt;When studying ecosystems, hierarchical trees are often used to organize entities based on proximity criteria, such as the taxonomy in microbiology, social classes in geography, or product types in retail businesses, offering valuable insights into entity relationships. Despite their significance, current count-data models do not leverage this structured information. In particular, the widely used Poisson log-normal (PLN) model, known for its ability to model interactions between entities from count data, lacks the possibility to incorporate such hierarchical tree structures, limiting its applicability in domains characterized by such complexities. To address this matter, we introduce the PLN-Tree model as an extension of the PLN model, specifically designed for modeling hierarchical count data. By integrating structured variational inference techniques, we propose an adapted training procedure and establish identifiability results, enhancisng both theoretical foundations and practical interpretability. Additionally, we extend our framework to classification tasks as a preprocessing pipeline, showcasing its versatility. Experimental evaluations on synthetic datasets as well as real-world microbiome data demonstrate the superior performance of the PLN-Tree model in capturing hierarchical dependencies and providing valuable insights into complex data structures, showing the practical interest of knowledge graphs like the taxonomy in ecosystems modeling.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17361&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexandre Chaussard , Anna Bonnet , Elisabeth Gassiat , Sylvain Le Corff</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">When studying ecosystems, hierarchical trees are often used to organize entities based on proximity criteria, such as the taxonomy in microbiology, social classes in geography, or product types in retail businesses, offering valuable insights into entity relationships. Despite their significance, current count-data models do not leverage this structured information. In particular, the widely used Poisson log-normal (PLN) model, known for its ability to model interactions between entities from count data, lacks the possibility to incorporate such hierarchical tree structures, limiting its applicability in domains characterized by such complexities. To address this matter, we introduce the PLN-Tree model as an extension of the PLN model, specifically designed for modeling hierarchical count data. By integrating structured variational inference techniques, we propose an adapted training procedure and establish identifiability results, enhancisng both theoretical foundations and practical interpretability. Additionally, we extend our framework to classification tasks as a preprocessing pipeline, showcasing its versatility. Experimental evaluations on synthetic datasets as well as real-world microbiome data demonstrate the superior performance of the PLN-Tree model in capturing hierarchical dependencies and providing valuable insights into complex data structures, showing the practical interest of knowledge graphs like the taxonomy in ecosystems modeling.</summary></entry><entry><title type="html">Two-Stage Testing in a high dimensional setting</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/TwoStageTestinginahighdimensionalsetting.html" rel="alternate" type="text/html" title="Two-Stage Testing in a high dimensional setting" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/TwoStageTestinginahighdimensionalsetting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/TwoStageTestinginahighdimensionalsetting.html">&lt;p&gt;In a high dimensional regression setting in which the number of variables ($p$) is much larger than the sample size ($n$), the number of possible two-way interactions between the variables is immense. If the number of variables is in the order of one million, which is usually the case in e.g., genetics, the number of two-way interactions is of the order one million squared. In the pursuit of detecting two-way interactions, testing all pairs for interactions one-by-one is computational unfeasible and the multiple testing correction will be severe. In this paper we describe a two-stage testing procedure consisting of a screening and an evaluation stage. It is proven that, under some assumptions, the tests-statistics in the two stages are asymptotically independent. As a result, multiplicity correction in the second stage is only needed for the number of statistical tests that are actually performed in that stage. This increases the power of the testing procedure. Also, since the testing procedure in the first stage is computational simple, the computational burden is lowered. Simulations have been performed for multiple settings and regression models (generalized linear models and Cox PH model) to study the performance of the two-stage testing procedure. The results show type I error control and an increase in power compared to the procedure in which the pairs are tested one-by-one.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17466&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Marianne A Jonker, Luc van Schijndel, Eric Cator</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In a high dimensional regression setting in which the number of variables ($p$) is much larger than the sample size ($n$), the number of possible two-way interactions between the variables is immense. If the number of variables is in the order of one million, which is usually the case in e.g., genetics, the number of two-way interactions is of the order one million squared. In the pursuit of detecting two-way interactions, testing all pairs for interactions one-by-one is computational unfeasible and the multiple testing correction will be severe. In this paper we describe a two-stage testing procedure consisting of a screening and an evaluation stage. It is proven that, under some assumptions, the tests-statistics in the two stages are asymptotically independent. As a result, multiplicity correction in the second stage is only needed for the number of statistical tests that are actually performed in that stage. This increases the power of the testing procedure. Also, since the testing procedure in the first stage is computational simple, the computational burden is lowered. Simulations have been performed for multiple settings and regression models (generalized linear models and Cox PH model) to study the performance of the two-stage testing procedure. The results show type I error control and an increase in power compared to the procedure in which the pairs are tested one-by-one.</summary></entry><entry><title type="html">Unraveling the Dynamics of SPY Trading Volumes: A Comprehensive Analysis of Daily and Intraday Liquidity Trends</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/UnravelingtheDynamicsofSPYTradingVolumesAComprehensiveAnalysisofDailyandIntradayLiquidityTrends.html" rel="alternate" type="text/html" title="Unraveling the Dynamics of SPY Trading Volumes: A Comprehensive Analysis of Daily and Intraday Liquidity Trends" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/UnravelingtheDynamicsofSPYTradingVolumesAComprehensiveAnalysisofDailyandIntradayLiquidityTrends</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/UnravelingtheDynamicsofSPYTradingVolumesAComprehensiveAnalysisofDailyandIntradayLiquidityTrends.html">&lt;p&gt;In this project, we investigate the accuracy of forecasting intraday and daily trading volume of the exchange-traded fund SPY. The ability to forecast volume over varying time intervals with high accuracy is a critical element to many trading strategies. After performing exploratory data analysis on intraday and daily SPY data we identify three methods for our analysis: ARIMA and ARIMAX models, with or without seasonality, as well as a Frequency Domain Process Representation. To evaluate predictive power of our models, we use mean squared error, mean absolute percentage error, and volume weighted average price (VWAP) tracking error. All models for both intraday and daily data output strong VWAP predictions in comparison to the VWAP estimates produced by naive baseline methodologies. In both cases volume is most accurately forecasted using ARIMA models with exogenous variables in the form of technical indicators, with intraday incorporating a seasonal component and daily not.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17198&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Ananya Krishnan, Martin Pollack, Alma Cooper</name></author><category term="stat.AP" /><summary type="html">In this project, we investigate the accuracy of forecasting intraday and daily trading volume of the exchange-traded fund SPY. The ability to forecast volume over varying time intervals with high accuracy is a critical element to many trading strategies. After performing exploratory data analysis on intraday and daily SPY data we identify three methods for our analysis: ARIMA and ARIMAX models, with or without seasonality, as well as a Frequency Domain Process Representation. To evaluate predictive power of our models, we use mean squared error, mean absolute percentage error, and volume weighted average price (VWAP) tracking error. All models for both intraday and daily data output strong VWAP predictions in comparison to the VWAP estimates produced by naive baseline methodologies. In both cases volume is most accurately forecasted using ARIMA models with exogenous variables in the form of technical indicators, with intraday incorporating a seasonal component and daily not.</summary></entry><entry><title type="html">Using iterated local alignment to aggregate GPS trajectories into a traffic flow map</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/UsingiteratedlocalalignmenttoaggregateGPStrajectoriesintoatrafficflowmap.html" rel="alternate" type="text/html" title="Using iterated local alignment to aggregate GPS trajectories into a traffic flow map" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/UsingiteratedlocalalignmenttoaggregateGPStrajectoriesintoatrafficflowmap</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/UsingiteratedlocalalignmenttoaggregateGPStrajectoriesintoatrafficflowmap.html">&lt;p&gt;Desire line maps are widely deployed for traffic flow analysis by virtue of their ease of interpretation and computation. They can be considered to be simplified traffic flow maps, whereas the computational challenges in aggregating small scale traffic flows prevent the wider dissemination of high resolution flow maps. GPS trajectories are a promising data source to solve this challenging problem. The solution begins with the alignment (or map matching) of the GPS trajectories to the road network. However even the state-of-the-art map matching APIs produce sub-optimal results with small misalignments. While these misalignments are negligible for large scale flow aggregation in desire line maps, they pose substantial obstacles for small scale flow aggregation in high resolution maps. To remove these remaining misalignments, we introduce innovative local alignment algorithms, where we infer road segments to serve as local reference segments, and proceed to align nearby road segments to them. With each local alignment iteration, the misalignments of the GPS trajectories with each other and with the road network are reduced, and so converge closer to a minimal flow map. By analysing a set of empirical GPS trajectories collected in Hannover, Germany, we confirm that our minimal flow map has high levels of spatial resolution, accuracy and coverage.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2406.17500&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tarn Duong</name></author><category term="stat.AP" /><summary type="html">Desire line maps are widely deployed for traffic flow analysis by virtue of their ease of interpretation and computation. They can be considered to be simplified traffic flow maps, whereas the computational challenges in aggregating small scale traffic flows prevent the wider dissemination of high resolution flow maps. GPS trajectories are a promising data source to solve this challenging problem. The solution begins with the alignment (or map matching) of the GPS trajectories to the road network. However even the state-of-the-art map matching APIs produce sub-optimal results with small misalignments. While these misalignments are negligible for large scale flow aggregation in desire line maps, they pose substantial obstacles for small scale flow aggregation in high resolution maps. To remove these remaining misalignments, we introduce innovative local alignment algorithms, where we infer road segments to serve as local reference segments, and proceed to align nearby road segments to them. With each local alignment iteration, the misalignments of the GPS trajectories with each other and with the road network are reduced, and so converge closer to a minimal flow map. By analysing a set of empirical GPS trajectories collected in Hannover, Germany, we confirm that our minimal flow map has high levels of spatial resolution, accuracy and coverage.</summary></entry><entry><title type="html">Using spatial extreme-value theory with machine learning to model and understand spatially compounding weather extremes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Usingspatialextremevaluetheorywithmachinelearningtomodelandunderstandspatiallycompoundingweatherextremes.html" rel="alternate" type="text/html" title="Using spatial extreme-value theory with machine learning to model and understand spatially compounding weather extremes" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Usingspatialextremevaluetheorywithmachinelearningtomodelandunderstandspatiallycompoundingweatherextremes</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/Usingspatialextremevaluetheorywithmachinelearningtomodelandunderstandspatiallycompoundingweatherextremes.html">&lt;p&gt;When extreme weather events affect large areas, their regional to sub-continental spatial scale is important for their impacts. We propose a novel machine learning (ML) framework that integrates spatial extreme-value theory to model weather extremes and to quantify probabilities associated with the occurrence, intensity, and spatial extent of these events. Our approach employs new loss functions adapted to extreme values, enabling our model to prioritize the tail instead of the bulk of the data distribution. Applied to a case study of Western European summertime heat extremes, we use daily 500-hPa geopotential height fields and local soil moisture as predictors to capture the complex interplay between local and remote physical processes. Our generative model reveals the importance of individual circulation features in determining different facets of heat extremes, thereby enriching our process understanding from a data-driven perspective. The occurrence, intensity, and spatial extent of heat extremes are sensitive to the relative position of upper-level ridges and troughs that are part of a large-scale wave pattern. Our approach is able to extrapolate beyond the range of the data to make risk-related probabilistic statements, applies more generally to other weather extremes, and offers an attractive alternative to traditional physical and ML-based techniques that focus less on the extremal aspects of weather data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.12195&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jonathan Koh, Daniel Steinfeld, Olivia Martius</name></author><category term="stat.AP" /><summary type="html">When extreme weather events affect large areas, their regional to sub-continental spatial scale is important for their impacts. We propose a novel machine learning (ML) framework that integrates spatial extreme-value theory to model weather extremes and to quantify probabilities associated with the occurrence, intensity, and spatial extent of these events. Our approach employs new loss functions adapted to extreme values, enabling our model to prioritize the tail instead of the bulk of the data distribution. Applied to a case study of Western European summertime heat extremes, we use daily 500-hPa geopotential height fields and local soil moisture as predictors to capture the complex interplay between local and remote physical processes. Our generative model reveals the importance of individual circulation features in determining different facets of heat extremes, thereby enriching our process understanding from a data-driven perspective. The occurrence, intensity, and spatial extent of heat extremes are sensitive to the relative position of upper-level ridges and troughs that are part of a large-scale wave pattern. Our approach is able to extrapolate beyond the range of the data to make risk-related probabilistic statements, applies more generally to other weather extremes, and offers an attractive alternative to traditional physical and ML-based techniques that focus less on the extremal aspects of weather data.</summary></entry><entry><title type="html">Weighted Particle-Based Optimization for Efficient Generalized Posterior Calibration</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/WeightedParticleBasedOptimizationforEfficientGeneralizedPosteriorCalibration.html" rel="alternate" type="text/html" title="Weighted Particle-Based Optimization for Efficient Generalized Posterior Calibration" /><published>2024-06-26T00:00:00+00:00</published><updated>2024-06-26T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/WeightedParticleBasedOptimizationforEfficientGeneralizedPosteriorCalibration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/06/26/WeightedParticleBasedOptimizationforEfficientGeneralizedPosteriorCalibration.html">&lt;p&gt;In the realm of statistical learning, the increasing volume of accessible data and increasing model complexity necessitate robust methodologies. This paper explores two branches of robust Bayesian methods in response to this trend. The first is generalized Bayesian inference, which introduces a learning rate parameter to enhance robustness against model misspecifications. The second is Gibbs posterior inference, which formulates inferential problems using generic loss functions rather than probabilistic models. In such approaches, it is necessary to calibrate the spread of the posterior distribution by selecting a learning rate parameter. The study aims to enhance the generalized posterior calibration (GPC) algorithm proposed by [1]. Their algorithm chooses the learning rate to achieve the nominal frequentist coverage probability, but it is computationally intensive because it requires repeated posterior simulations for bootstrap samples. We propose a more efficient version of the GPC inspired by sequential Monte Carlo (SMC) samplers. A target distribution with a different learning rate is evaluated without posterior simulation as in the reweighting step in SMC sampling. Thus, the proposed algorithm can reach the desirable value within a few iterations. This improvement substantially reduces the computational cost of the GPC. Its efficacy is demonstrated through synthetic and real data applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.04845&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Masahiro Tanaka</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">In the realm of statistical learning, the increasing volume of accessible data and increasing model complexity necessitate robust methodologies. This paper explores two branches of robust Bayesian methods in response to this trend. The first is generalized Bayesian inference, which introduces a learning rate parameter to enhance robustness against model misspecifications. The second is Gibbs posterior inference, which formulates inferential problems using generic loss functions rather than probabilistic models. In such approaches, it is necessary to calibrate the spread of the posterior distribution by selecting a learning rate parameter. The study aims to enhance the generalized posterior calibration (GPC) algorithm proposed by [1]. Their algorithm chooses the learning rate to achieve the nominal frequentist coverage probability, but it is computationally intensive because it requires repeated posterior simulations for bootstrap samples. We propose a more efficient version of the GPC inspired by sequential Monte Carlo (SMC) samplers. A target distribution with a different learning rate is evaluated without posterior simulation as in the reweighting step in SMC sampling. Thus, the proposed algorithm can reach the desirable value within a few iterations. This improvement substantially reduces the computational cost of the GPC. Its efficacy is demonstrated through synthetic and real data applications.</summary></entry></feed>