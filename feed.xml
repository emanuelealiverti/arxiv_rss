<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-30T07:14:34+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Return to Biased Nets: New Specifications and Approximate Bayesian Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/AReturntoBiasedNetsNewSpecificationsandApproximateBayesianInference.html" rel="alternate" type="text/html" title="A Return to Biased Nets: New Specifications and Approximate Bayesian Inference" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/AReturntoBiasedNetsNewSpecificationsandApproximateBayesianInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/AReturntoBiasedNetsNewSpecificationsandApproximateBayesianInference.html">&lt;p&gt;The biased net paradigm was the first general and empirically tractable scheme for parameterizing complex patterns of dependence in networks, expressing deviations from uniform random graph structure in terms of latent ``bias events,’’ whose realizations enhance reciprocity, transitivity, or other structural features. Subsequent developments have introduced local specifications of biased nets, which reduce the need for approximations required in early specifications based on tracing processes. Here, we show that while one such specification leads to inconsistencies, a closely related Markovian specification both evades these difficulties and can be extended to incorporate new types of effects. We introduce the notion of inhibitory bias events, with satiation as an example, which are useful for avoiding degeneracies that can arise from closure bias terms. Although our approach does not lead to a computable likelihood, we provide a strategy for approximate Bayesian inference using random forest prevision. We demonstrate our approach on a network of friendship ties among college students, recapitulating a relationship between the sibling bias and tie strength posited in earlier work by Fararo.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18873&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carter T. Butts</name></author><category term="stat.ME" /><summary type="html">The biased net paradigm was the first general and empirically tractable scheme for parameterizing complex patterns of dependence in networks, expressing deviations from uniform random graph structure in terms of latent ``bias events,’’ whose realizations enhance reciprocity, transitivity, or other structural features. Subsequent developments have introduced local specifications of biased nets, which reduce the need for approximations required in early specifications based on tracing processes. Here, we show that while one such specification leads to inconsistencies, a closely related Markovian specification both evades these difficulties and can be extended to incorporate new types of effects. We introduce the notion of inhibitory bias events, with satiation as an example, which are useful for avoiding degeneracies that can arise from closure bias terms. Although our approach does not lead to a computable likelihood, we provide a strategy for approximate Bayesian inference using random forest prevision. We demonstrate our approach on a network of friendship ties among college students, recapitulating a relationship between the sibling bias and tie strength posited in earlier work by Fararo.</summary></entry><entry><title type="html">Active Statistical Inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ActiveStatisticalInference.html" rel="alternate" type="text/html" title="Active Statistical Inference" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ActiveStatisticalInference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ActiveStatisticalInference.html">&lt;p&gt;Inspired by the concept of active learning, we propose active inference$\unicode{x2013}$a methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model’s predictions where it is confident. Active inference constructs provably valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful p-values. We evaluate active inference on datasets from public opinion research, census analysis, and proteomics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.03208&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tijana Zrnic, Emmanuel J. Candès</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Inspired by the concept of active learning, we propose active inference$\unicode{x2013}$a methodology for statistical inference with machine-learning-assisted data collection. Assuming a budget on the number of labels that can be collected, the methodology uses a machine learning model to identify which data points would be most beneficial to label, thus effectively utilizing the budget. It operates on a simple yet powerful intuition: prioritize the collection of labels for data points where the model exhibits uncertainty, and rely on the model’s predictions where it is confident. Active inference constructs provably valid confidence intervals and hypothesis tests while leveraging any black-box machine learning model and handling any data distribution. The key point is that it achieves the same level of accuracy with far fewer samples than existing baselines relying on non-adaptively-collected data. This means that for the same number of collected samples, active inference enables smaller confidence intervals and more powerful p-values. We evaluate active inference on datasets from public opinion research, census analysis, and proteomics.</summary></entry><entry><title type="html">Adaptive Generalized Neyman Allocation: Local Asymptotic Minimax Optimal Best Arm Identification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/AdaptiveGeneralizedNeymanAllocationLocalAsymptoticMinimaxOptimalBestArmIdentification.html" rel="alternate" type="text/html" title="Adaptive Generalized Neyman Allocation: Local Asymptotic Minimax Optimal Best Arm Identification" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/AdaptiveGeneralizedNeymanAllocationLocalAsymptoticMinimaxOptimalBestArmIdentification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/AdaptiveGeneralizedNeymanAllocationLocalAsymptoticMinimaxOptimalBestArmIdentification.html">&lt;p&gt;This study investigates a local asymptotic minimax optimal strategy for fixed-budget best arm identification (BAI). We propose the Adaptive Generalized Neyman Allocation (AGNA) strategy and show that its worst-case upper bound of the probability of misidentifying the best arm aligns with the worst-case lower bound under the small-gap regime, where the gap between the expected outcomes of the best and suboptimal arms is small. Our strategy corresponds to a generalization of the Neyman allocation for two-armed bandits (Neyman, 1934; Kaufmann et al., 2016) and a refinement of existing strategies such as the ones proposed by Glynn &amp;amp; Juneja (2004) and Shin et al. (2018). Compared to Komiyama et al. (2022), which proposes a minimax rate-optimal strategy, our proposed strategy has a tighter upper bound that exactly matches the lower bound, including the constant terms, by restricting the class of distributions to the class of small-gap distributions. Our result contributes to the longstanding open issue about the existence of asymptotically optimal strategies in fixed-budget BAI, by presenting the local asymptotic minimax optimal strategy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19317&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Masahiro Kato</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">This study investigates a local asymptotic minimax optimal strategy for fixed-budget best arm identification (BAI). We propose the Adaptive Generalized Neyman Allocation (AGNA) strategy and show that its worst-case upper bound of the probability of misidentifying the best arm aligns with the worst-case lower bound under the small-gap regime, where the gap between the expected outcomes of the best and suboptimal arms is small. Our strategy corresponds to a generalization of the Neyman allocation for two-armed bandits (Neyman, 1934; Kaufmann et al., 2016) and a refinement of existing strategies such as the ones proposed by Glynn &amp;amp; Juneja (2004) and Shin et al. (2018). Compared to Komiyama et al. (2022), which proposes a minimax rate-optimal strategy, our proposed strategy has a tighter upper bound that exactly matches the lower bound, including the constant terms, by restricting the class of distributions to the class of small-gap distributions. Our result contributes to the longstanding open issue about the existence of asymptotically optimal strategies in fixed-budget BAI, by presenting the local asymptotic minimax optimal strategy.</summary></entry><entry><title type="html">Adaptive and Efficient Learning with Blockwise Missing and Semi-Supervised Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/AdaptiveandEfficientLearningwithBlockwiseMissingandSemiSupervisedData.html" rel="alternate" type="text/html" title="Adaptive and Efficient Learning with Blockwise Missing and Semi-Supervised Data" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/AdaptiveandEfficientLearningwithBlockwiseMissingandSemiSupervisedData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/AdaptiveandEfficientLearningwithBlockwiseMissingandSemiSupervisedData.html">&lt;p&gt;Data fusion is an important way to realize powerful and generalizable analyses across multiple sources. However, different capability of data collection across the sources has become a prominent issue in practice. This could result in the blockwise missingness (BM) of covariates troublesome for integration. Meanwhile, the high cost of obtaining gold-standard labels can cause the missingness of response on a large proportion of samples, known as the semi-supervised (SS) problem. In this paper, we consider a challenging scenario confronting both the BM and SS issues, and propose a novel Data-adaptive projecting Estimation approach for data FUsion in the SEmi-supervised setting (DEFUSE). Starting with a complete-data-only estimator, it involves two successive projection steps to reduce its variance without incurring bias. Compared to existing approaches, DEFUSE achieves a two-fold improvement. First, it leverages the BM labeled sample more efficiently through a novel data-adaptive projection approach robust to model misspecification on the missing covariates, leading to better variance reduction. Second, our method further incorporates the large unlabeled sample to enhance the estimation efficiency through imputation and projection. Compared to the previous SS setting with complete covariates, our work reveals a more essential role of the unlabeled sample in the BM setting. These advantages are justified in asymptotic and simulation studies. We also apply DEFUSE for the risk modeling and inference of heart diseases with the MIMIC-III electronic medical record (EMR) data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18722&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yiming Li, Xuehan Yang, Ying Wei, Molei Liu</name></author><category term="stat.ME" /><summary type="html">Data fusion is an important way to realize powerful and generalizable analyses across multiple sources. However, different capability of data collection across the sources has become a prominent issue in practice. This could result in the blockwise missingness (BM) of covariates troublesome for integration. Meanwhile, the high cost of obtaining gold-standard labels can cause the missingness of response on a large proportion of samples, known as the semi-supervised (SS) problem. In this paper, we consider a challenging scenario confronting both the BM and SS issues, and propose a novel Data-adaptive projecting Estimation approach for data FUsion in the SEmi-supervised setting (DEFUSE). Starting with a complete-data-only estimator, it involves two successive projection steps to reduce its variance without incurring bias. Compared to existing approaches, DEFUSE achieves a two-fold improvement. First, it leverages the BM labeled sample more efficiently through a novel data-adaptive projection approach robust to model misspecification on the missing covariates, leading to better variance reduction. Second, our method further incorporates the large unlabeled sample to enhance the estimation efficiency through imputation and projection. Compared to the previous SS setting with complete covariates, our work reveals a more essential role of the unlabeled sample in the BM setting. These advantages are justified in asymptotic and simulation studies. We also apply DEFUSE for the risk modeling and inference of heart diseases with the MIMIC-III electronic medical record (EMR) data.</summary></entry><entry><title type="html">Bayesian Time-Varying Tensor Vector Autoregressive Models for Dynamic Effective Connectivity</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/BayesianTimeVaryingTensorVectorAutoregressiveModelsforDynamicEffectiveConnectivity.html" rel="alternate" type="text/html" title="Bayesian Time-Varying Tensor Vector Autoregressive Models for Dynamic Effective Connectivity" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/BayesianTimeVaryingTensorVectorAutoregressiveModelsforDynamicEffectiveConnectivity</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/BayesianTimeVaryingTensorVectorAutoregressiveModelsforDynamicEffectiveConnectivity.html">&lt;p&gt;In contemporary neuroscience, a key area of interest is dynamic effective connectivity, which is crucial for understanding the dynamic interactions and causal relationships between different brain regions. Dynamic effective connectivity can provide insights into how brain network interactions are altered in neurological disorders such as dyslexia. Time-varying vector autoregressive (TV-VAR) models have been employed to draw inferences for this purpose. However, their significant computational requirements pose challenges, since the number of parameters to be estimated increases quadratically with the number of time series. In this paper, we propose a computationally efficient Bayesian time-varying VAR approach. For dealing with large-dimensional time series, the proposed framework employs a tensor decomposition for the VAR coefficient matrices at different lags. Dynamically varying connectivity patterns are captured by assuming that at any given time only a subset of components in the tensor decomposition is active. Latent binary time series select the active components at each time via an innovative and parsimonious Ising model in the time-domain. Furthermore, we propose parsity-inducing priors to achieve global-local shrinkage of the VAR coefficients, determine automatically the rank of the tensor decomposition and guide the selection of the lags of the auto-regression. We show the performances of our model formulation via simulation studies and data from a real fMRI study involving a book reading experiment.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.14083&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wei Zhang, Ivor Cribben, sonia Petrone, Michele Guindani</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">In contemporary neuroscience, a key area of interest is dynamic effective connectivity, which is crucial for understanding the dynamic interactions and causal relationships between different brain regions. Dynamic effective connectivity can provide insights into how brain network interactions are altered in neurological disorders such as dyslexia. Time-varying vector autoregressive (TV-VAR) models have been employed to draw inferences for this purpose. However, their significant computational requirements pose challenges, since the number of parameters to be estimated increases quadratically with the number of time series. In this paper, we propose a computationally efficient Bayesian time-varying VAR approach. For dealing with large-dimensional time series, the proposed framework employs a tensor decomposition for the VAR coefficient matrices at different lags. Dynamically varying connectivity patterns are captured by assuming that at any given time only a subset of components in the tensor decomposition is active. Latent binary time series select the active components at each time via an innovative and parsimonious Ising model in the time-domain. Furthermore, we propose parsity-inducing priors to achieve global-local shrinkage of the VAR coefficients, determine automatically the rank of the tensor decomposition and guide the selection of the lags of the auto-regression. We show the performances of our model formulation via simulation studies and data from a real fMRI study involving a book reading experiment.</summary></entry><entry><title type="html">Categorization of 31 computational methods to detect spatially variable genes from spatially resolved transcriptomics data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Categorizationof31computationalmethodstodetectspatiallyvariablegenesfromspatiallyresolvedtranscriptomicsdata.html" rel="alternate" type="text/html" title="Categorization of 31 computational methods to detect spatially variable genes from spatially resolved transcriptomics data" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Categorizationof31computationalmethodstodetectspatiallyvariablegenesfromspatiallyresolvedtranscriptomicsdata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Categorizationof31computationalmethodstodetectspatiallyvariablegenesfromspatiallyresolvedtranscriptomicsdata.html">&lt;p&gt;In the analysis of spatially resolved transcriptomics data, detecting spatially variable genes (SVGs) is crucial. Numerous computational methods exist, but varying SVG definitions and methodologies lead to incomparable results. We review 31 state-of-the-art methods, categorizing SVGs into three types: overall, cell-type-specific, and spatial-domain-marker SVGs. Our review explains the intuitions underlying these methods, summarizes their applications, and categorizes the hypothesis tests they use in the trade-off between generality and specificity for SVG detection. We discuss challenges in SVG detection and propose future directions for improvement. Our review offers insights for method developers and users, advocating for category-specific benchmarking.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18779&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Guanao Yan, Shuo Harper Hua, Jingyi Jessica Li</name></author><category term="stat.AP" /><summary type="html">In the analysis of spatially resolved transcriptomics data, detecting spatially variable genes (SVGs) is crucial. Numerous computational methods exist, but varying SVG definitions and methodologies lead to incomparable results. We review 31 state-of-the-art methods, categorizing SVGs into three types: overall, cell-type-specific, and spatial-domain-marker SVGs. Our review explains the intuitions underlying these methods, summarizes their applications, and categorizes the hypothesis tests they use in the trade-off between generality and specificity for SVG detection. We discuss challenges in SVG detection and propose future directions for improvement. Our review offers insights for method developers and users, advocating for category-specific benchmarking.</summary></entry><entry><title type="html">Causal Inference for Balanced Incomplete Block Designs</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/CausalInferenceforBalancedIncompleteBlockDesigns.html" rel="alternate" type="text/html" title="Causal Inference for Balanced Incomplete Block Designs" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/CausalInferenceforBalancedIncompleteBlockDesigns</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/CausalInferenceforBalancedIncompleteBlockDesigns.html">&lt;p&gt;Researchers often turn to block randomization to increase the precision of their inference or due to practical considerations, such as in multi-site trials. However, if the number of treatments under consideration is large it might not be practical or even feasible to assign all treatments within each block. We develop novel inference results under the finite-population design-based framework for a natural alternative to the complete block design that does not require reducing the number of treatment arms, the balanced incomplete block design (BIBD). This includes deriving the properties of two estimators for BIBDs and proposing conservative variance estimators. To assist practitioners in understanding the trade-offs of using BIBDs over other designs, the precisions of resulting estimators are compared to standard estimators for the complete block design, the cluster-randomized design, and the completely randomized design. Simulations and a data illustration demonstrate the strengths and weaknesses of using BIBDs. This work highlights BIBDs as practical and currently underutilized designs.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19312&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Taehyeon Koo, Nicole E. Pashley</name></author><category term="stat.ME" /><summary type="html">Researchers often turn to block randomization to increase the precision of their inference or due to practical considerations, such as in multi-site trials. However, if the number of treatments under consideration is large it might not be practical or even feasible to assign all treatments within each block. We develop novel inference results under the finite-population design-based framework for a natural alternative to the complete block design that does not require reducing the number of treatment arms, the balanced incomplete block design (BIBD). This includes deriving the properties of two estimators for BIBDs and proposing conservative variance estimators. To assist practitioners in understanding the trade-offs of using BIBDs over other designs, the precisions of resulting estimators are compared to standard estimators for the complete block design, the cluster-randomized design, and the completely randomized design. Simulations and a data illustration demonstrate the strengths and weaknesses of using BIBDs. This work highlights BIBDs as practical and currently underutilized designs.</summary></entry><entry><title type="html">Causal inference in the closed-loop: marginal structural models for sequential excursion effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Causalinferenceintheclosedloopmarginalstructuralmodelsforsequentialexcursioneffects.html" rel="alternate" type="text/html" title="Causal inference in the closed-loop: marginal structural models for sequential excursion effects" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Causalinferenceintheclosedloopmarginalstructuralmodelsforsequentialexcursioneffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Causalinferenceintheclosedloopmarginalstructuralmodelsforsequentialexcursioneffects.html">&lt;p&gt;Optogenetics is widely used to study the effects of neural circuit manipulation on behavior. However, the paucity of causal inference methodological work on this topic has resulted in analysis conventions that discard information, and constrain the scientific questions that can be posed. To fill this gap, we introduce a nonparametric causal inference framework for analyzing “closed-loop” designs, which use dynamic policies that assign treatment based on covariates. In this setting, standard methods can introduce bias and occlude causal effects. Building on the sequentially randomized experiments literature in causal inference, our approach extends history-restricted marginal structural models for dynamic regimes. In practice, our framework can identify a wide range of causal effects of optogenetics on trial-by-trial behavior, such as, fast/slow-acting, dose-response, additive/antagonistic, and floor/ceiling. Importantly, it does so without requiring negative controls, and can estimate how causal effect magnitudes evolve across time points. From another view, our work extends “excursion effect” methods–popular in the mobile health literature–to enable estimation of causal contrasts for treatment sequences greater than length one, in the presence of positivity violations. We derive rigorous statistical guarantees, enabling hypothesis testing of these causal effects. We demonstrate our approach on data from a recent study of dopaminergic activity on learning, and show how our method reveals relevant effects obscured in standard analyses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18597&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Alexander W. Levis, Gabriel Loewinger, Francisco Pereira</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Optogenetics is widely used to study the effects of neural circuit manipulation on behavior. However, the paucity of causal inference methodological work on this topic has resulted in analysis conventions that discard information, and constrain the scientific questions that can be posed. To fill this gap, we introduce a nonparametric causal inference framework for analyzing “closed-loop” designs, which use dynamic policies that assign treatment based on covariates. In this setting, standard methods can introduce bias and occlude causal effects. Building on the sequentially randomized experiments literature in causal inference, our approach extends history-restricted marginal structural models for dynamic regimes. In practice, our framework can identify a wide range of causal effects of optogenetics on trial-by-trial behavior, such as, fast/slow-acting, dose-response, additive/antagonistic, and floor/ceiling. Importantly, it does so without requiring negative controls, and can estimate how causal effect magnitudes evolve across time points. From another view, our work extends “excursion effect” methods–popular in the mobile health literature–to enable estimation of causal contrasts for treatment sequences greater than length one, in the presence of positivity violations. We derive rigorous statistical guarantees, enabling hypothesis testing of these causal effects. We demonstrate our approach on data from a recent study of dopaminergic activity on learning, and show how our method reveals relevant effects obscured in standard analyses.</summary></entry><entry><title type="html">Continuously Optimizing Radar Placement with Model Predictive Path Integrals</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ContinuouslyOptimizingRadarPlacementwithModelPredictivePathIntegrals.html" rel="alternate" type="text/html" title="Continuously Optimizing Radar Placement with Model Predictive Path Integrals" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ContinuouslyOptimizingRadarPlacementwithModelPredictivePathIntegrals</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ContinuouslyOptimizingRadarPlacementwithModelPredictivePathIntegrals.html">&lt;p&gt;Continuously optimizing sensor placement is essential for precise target localization in various military and civilian applications. While information theory has shown promise in optimizing sensor placement, many studies oversimplify sensor measurement models or neglect dynamic constraints of mobile sensors. To address these challenges, we employ a range measurement model that incorporates radar parameters and radar-target distance, coupled with Model Predictive Path Integral (MPPI) control to manage complex environmental obstacles and dynamic constraints. We compare the proposed approach against stationary radars or simplified range measurement models based on the root mean squared error (RMSE) of the Cubature Kalman Filter (CKF) estimator for the targets’ state. Additionally, we visualize the evolving geometry of radars and targets over time, highlighting areas of highest measurement information gain, demonstrating the strengths of the approach. The proposed strategy outperforms stationary radars and simplified range measurement models in target localization, achieving a 38-74% reduction in mean RMSE and a 33-79% reduction in the upper tail of the 90% Highest Density Interval (HDI) over 500 Monte Carl (MC) trials across all time steps.
  Code will be made publicly available upon acceptance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18999&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Michael Potter, Shuo Tang, Paul Ghanem, Milica Stojanovic, Pau Closas, Murat Akcakaya, Ben Wright, Marius Necsoiu, Deniz Erdogmus, Michael Everett, Talees Imbiriba</name></author><category term="stat.AP" /><summary type="html">Continuously optimizing sensor placement is essential for precise target localization in various military and civilian applications. While information theory has shown promise in optimizing sensor placement, many studies oversimplify sensor measurement models or neglect dynamic constraints of mobile sensors. To address these challenges, we employ a range measurement model that incorporates radar parameters and radar-target distance, coupled with Model Predictive Path Integral (MPPI) control to manage complex environmental obstacles and dynamic constraints. We compare the proposed approach against stationary radars or simplified range measurement models based on the root mean squared error (RMSE) of the Cubature Kalman Filter (CKF) estimator for the targets’ state. Additionally, we visualize the evolving geometry of radars and targets over time, highlighting areas of highest measurement information gain, demonstrating the strengths of the approach. The proposed strategy outperforms stationary radars and simplified range measurement models in target localization, achieving a 38-74% reduction in mean RMSE and a 33-79% reduction in the upper tail of the 90% Highest Density Interval (HDI) over 500 Monte Carl (MC) trials across all time steps. Code will be made publicly available upon acceptance.</summary></entry><entry><title type="html">Counterfactual Explanations for Multivariate Time-Series without Training Datasets</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/CounterfactualExplanationsforMultivariateTimeSerieswithoutTrainingDatasets.html" rel="alternate" type="text/html" title="Counterfactual Explanations for Multivariate Time-Series without Training Datasets" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/CounterfactualExplanationsforMultivariateTimeSerieswithoutTrainingDatasets</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/CounterfactualExplanationsforMultivariateTimeSerieswithoutTrainingDatasets.html">&lt;p&gt;Machine learning (ML) methods have experienced significant growth in the past decade, yet their practical application in high-impact real-world domains has been hindered by their opacity. When ML methods are responsible for making critical decisions, stakeholders often require insights into how to alter these decisions. Counterfactual explanations (CFEs) have emerged as a solution, offering interpretations of opaque ML models and providing a pathway to transition from one decision to another. However, most existing CFE methods require access to the model’s training dataset, few methods can handle multivariate time-series, and none can handle multivariate time-series without training datasets. These limitations can be formidable in many scenarios. In this paper, we present CFWoT, a novel reinforcement-learning-based CFE method that generates CFEs when training datasets are unavailable. CFWoT is model-agnostic and suitable for both static and multivariate time-series datasets with continuous and discrete features. Users have the flexibility to specify non-actionable, immutable, and preferred features, as well as causal constraints which CFWoT guarantees will be respected. We demonstrate the performance of CFWoT against four baselines on several datasets and find that, despite not having access to a training dataset, CFWoT finds CFEs that make significantly fewer and significantly smaller changes to the input time-series. These properties make CFEs more actionable, as the magnitude of change required to alter an outcome is vastly reduced.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18563&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xiangyu Sun, Raquel Aoki, Kevin H. Wilson</name></author><category term="stat.ME" /><summary type="html">Machine learning (ML) methods have experienced significant growth in the past decade, yet their practical application in high-impact real-world domains has been hindered by their opacity. When ML methods are responsible for making critical decisions, stakeholders often require insights into how to alter these decisions. Counterfactual explanations (CFEs) have emerged as a solution, offering interpretations of opaque ML models and providing a pathway to transition from one decision to another. However, most existing CFE methods require access to the model’s training dataset, few methods can handle multivariate time-series, and none can handle multivariate time-series without training datasets. These limitations can be formidable in many scenarios. In this paper, we present CFWoT, a novel reinforcement-learning-based CFE method that generates CFEs when training datasets are unavailable. CFWoT is model-agnostic and suitable for both static and multivariate time-series datasets with continuous and discrete features. Users have the flexibility to specify non-actionable, immutable, and preferred features, as well as causal constraints which CFWoT guarantees will be respected. We demonstrate the performance of CFWoT against four baselines on several datasets and find that, despite not having access to a training dataset, CFWoT finds CFEs that make significantly fewer and significantly smaller changes to the input time-series. These properties make CFEs more actionable, as the magnitude of change required to alter an outcome is vastly reduced.</summary></entry><entry><title type="html">Covariate Shift Corrected Conditional Randomization Test</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/CovariateShiftCorrectedConditionalRandomizationTest.html" rel="alternate" type="text/html" title="Covariate Shift Corrected Conditional Randomization Test" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/CovariateShiftCorrectedConditionalRandomizationTest</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/CovariateShiftCorrectedConditionalRandomizationTest.html">&lt;p&gt;Conditional independence tests are crucial across various disciplines in determining the independence of an outcome variable $Y$ from a treatment variable $X$, conditioning on a set of confounders $Z$. The Conditional Randomization Test (CRT) offers a powerful framework for such testing by assuming known distributions of $X \mid Z$; it controls the Type-I error exactly, allowing for the use of flexible, black-box test statistics. In practice, testing for conditional independence often involves using data from a source population to draw conclusions about a target population. This can be challenging due to covariate shift – differences in the distribution of $X$, $Z$, and surrogate variables, which can affect the conditional distribution of $Y \mid X, Z$ – rendering traditional CRT approaches invalid. To address this issue, we propose a novel Covariate Shift Corrected Pearson Chi-squared Conditional Randomization (csPCR) test. This test adapts to covariate shifts by integrating importance weights and employing the control variates method to reduce variance in the test statistics and thus enhance power. Theoretically, we establish that the csPCR test controls the Type-I error asymptotically. Empirically, through simulation studies, we demonstrate that our method not only maintains control over Type-I errors but also exhibits superior power, confirming its efficacy and practical utility in real-world scenarios where covariate shifts are prevalent. Finally, we apply our methodology to a real-world dataset to assess the impact of a COVID-19 treatment on the 90-day mortality rate among patients.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19231&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bowen Xu, Yiwen Huang, Chuan Hong, Shuangning Li, Molei Liu</name></author><category term="stat.ME" /><summary type="html">Conditional independence tests are crucial across various disciplines in determining the independence of an outcome variable $Y$ from a treatment variable $X$, conditioning on a set of confounders $Z$. The Conditional Randomization Test (CRT) offers a powerful framework for such testing by assuming known distributions of $X \mid Z$; it controls the Type-I error exactly, allowing for the use of flexible, black-box test statistics. In practice, testing for conditional independence often involves using data from a source population to draw conclusions about a target population. This can be challenging due to covariate shift – differences in the distribution of $X$, $Z$, and surrogate variables, which can affect the conditional distribution of $Y \mid X, Z$ – rendering traditional CRT approaches invalid. To address this issue, we propose a novel Covariate Shift Corrected Pearson Chi-squared Conditional Randomization (csPCR) test. This test adapts to covariate shifts by integrating importance weights and employing the control variates method to reduce variance in the test statistics and thus enhance power. Theoretically, we establish that the csPCR test controls the Type-I error asymptotically. Empirically, through simulation studies, we demonstrate that our method not only maintains control over Type-I errors but also exhibits superior power, confirming its efficacy and practical utility in real-world scenarios where covariate shifts are prevalent. Finally, we apply our methodology to a real-world dataset to assess the impact of a COVID-19 treatment on the 90-day mortality rate among patients.</summary></entry><entry><title type="html">Difference-in-Discontinuities: Estimation, Inference and Validity Tests</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/DifferenceinDiscontinuitiesEstimationInferenceandValidityTests.html" rel="alternate" type="text/html" title="Difference-in-Discontinuities: Estimation, Inference and Validity Tests" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/DifferenceinDiscontinuitiesEstimationInferenceandValidityTests</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/DifferenceinDiscontinuitiesEstimationInferenceandValidityTests.html">&lt;p&gt;This paper investigates the econometric theory behind the newly developed difference-in-discontinuities design (DiDC). Despite its increasing use in applied research, there are currently limited studies of its properties. The method combines elements of regression discontinuity (RDD) and difference-in-differences (DiD) designs, allowing researchers to eliminate the effects of potential confounders at the discontinuity. We formalize the difference-in-discontinuity theory by stating the identification assumptions and proposing a nonparametric estimator, deriving its asymptotic properties and examining the scenarios in which the DiDC has desirable bias properties when compared to the standard RDD. We also provide comprehensive tests for one of the identification assumption of the DiDC. Monte Carlo simulation studies show that the estimators have good performance in finite samples. Finally, we revisit Grembi et al. (2016), that studies the effects of relaxing fiscal rules on public finance outcomes in Italian municipalities. The results show that the proposed estimator exhibits substantially smaller confidence intervals for the estimated effects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18531&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Pedro Picchetti, Cristine C. X. Pinto, Stephanie T. Shinoki</name></author><category term="stat.AP" /><summary type="html">This paper investigates the econometric theory behind the newly developed difference-in-discontinuities design (DiDC). Despite its increasing use in applied research, there are currently limited studies of its properties. The method combines elements of regression discontinuity (RDD) and difference-in-differences (DiD) designs, allowing researchers to eliminate the effects of potential confounders at the discontinuity. We formalize the difference-in-discontinuity theory by stating the identification assumptions and proposing a nonparametric estimator, deriving its asymptotic properties and examining the scenarios in which the DiDC has desirable bias properties when compared to the standard RDD. We also provide comprehensive tests for one of the identification assumption of the DiDC. Monte Carlo simulation studies show that the estimators have good performance in finite samples. Finally, we revisit Grembi et al. (2016), that studies the effects of relaxing fiscal rules on public finance outcomes in Italian municipalities. The results show that the proposed estimator exhibits substantially smaller confidence intervals for the estimated effects.</summary></entry><entry><title type="html">Diffusive Gibbs Sampling</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/DiffusiveGibbsSampling.html" rel="alternate" type="text/html" title="Diffusive Gibbs Sampling" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/DiffusiveGibbsSampling</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/DiffusiveGibbsSampling.html">&lt;p&gt;The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. A novel Metropolis-within-Gibbs scheme is proposed to enhance mixing in the denoising sampling step. DiGS exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering, attaining substantially improved performance across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.03008&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Wenlin Chen, Mingtian Zhang, Brooks Paige, José Miguel Hernández-Lobato, David Barber</name></author><category term="stat.ML," /><category term="stat.CO" /><summary type="html">The inadequate mixing of conventional Markov Chain Monte Carlo (MCMC) methods for multi-modal distributions presents a significant challenge in practical applications such as Bayesian inference and molecular dynamics. Addressing this, we propose Diffusive Gibbs Sampling (DiGS), an innovative family of sampling methods designed for effective sampling from distributions characterized by distant and disconnected modes. DiGS integrates recent developments in diffusion models, leveraging Gaussian convolution to create an auxiliary noisy distribution that bridges isolated modes in the original space and applying Gibbs sampling to alternately draw samples from both spaces. A novel Metropolis-within-Gibbs scheme is proposed to enhance mixing in the denoising sampling step. DiGS exhibits a better mixing property for sampling multi-modal distributions than state-of-the-art methods such as parallel tempering, attaining substantially improved performance across various tasks, including mixtures of Gaussians, Bayesian neural networks and molecular dynamics.</summary></entry><entry><title type="html">Discovering deposition process regimes: leveraging unsupervised learning for process insights, surrogate modeling, and sensitivity analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Discoveringdepositionprocessregimesleveragingunsupervisedlearningforprocessinsightssurrogatemodelingandsensitivityanalysis.html" rel="alternate" type="text/html" title="Discovering deposition process regimes: leveraging unsupervised learning for process insights, surrogate modeling, and sensitivity analysis" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Discoveringdepositionprocessregimesleveragingunsupervisedlearningforprocessinsightssurrogatemodelingandsensitivityanalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Discoveringdepositionprocessregimesleveragingunsupervisedlearningforprocessinsightssurrogatemodelingandsensitivityanalysis.html">&lt;p&gt;This work introduces a comprehensive approach utilizing data-driven methods to elucidate the deposition process regimes in Chemical Vapor Deposition (CVD) reactors and the interplay of physical mechanism that dominate in each one of them. Through this work, we address three key objectives. Firstly, our methodology relies on process outcomes, derived by a detailed CFD model, to identify clusters of “outcomes” corresponding to distinct process regimes, wherein the relative influence of input variables undergoes notable shifts. This phenomenon is experimentally validated through Arrhenius plot analysis, affirming the efficacy of our approach. Secondly, we demonstrate the development of an efficient surrogate model, based on Polynomial Chaos Expansion (PCE), that maintains accuracy, facilitating streamlined computational analyses. Finally, as a result of PCE, sensitivity analysis is made possible by means of Sobol’ indices, that quantify the impact of process inputs across identified regimes. The insights gained from our analysis contribute to the formulation of hypotheses regarding phenomena occurring beyond the transition regime. Notably, the significance of temperature even in the diffusion-limited regime, as evidenced by the Arrhenius plot, suggests activation of gas phase reactions at elevated temperatures. Importantly, our proposed methods yield insights that align with experimental observations and theoretical principles, aiding decision-making in process design and optimization. By circumventing the need for costly and time-consuming experiments, our approach offers a pragmatic pathway towards enhanced process efficiency. Moreover, this study underscores the potential of data-driven computational methods for innovating reactor design paradigms.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18444&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Geremy Loachamín Suntaxi, Paris Papavasileiou, Eleni D. Koronaki, Dimitrios G. Giovanis, Georgios Gakis, Ioannis G. Aviziotis, Martin Kathrein, Gabriele Pozzetti, Christoph Czettl, Stéphane P. A. Bordas, Andreas G. Boudouvis</name></author><category term="stat.AP" /><summary type="html">This work introduces a comprehensive approach utilizing data-driven methods to elucidate the deposition process regimes in Chemical Vapor Deposition (CVD) reactors and the interplay of physical mechanism that dominate in each one of them. Through this work, we address three key objectives. Firstly, our methodology relies on process outcomes, derived by a detailed CFD model, to identify clusters of “outcomes” corresponding to distinct process regimes, wherein the relative influence of input variables undergoes notable shifts. This phenomenon is experimentally validated through Arrhenius plot analysis, affirming the efficacy of our approach. Secondly, we demonstrate the development of an efficient surrogate model, based on Polynomial Chaos Expansion (PCE), that maintains accuracy, facilitating streamlined computational analyses. Finally, as a result of PCE, sensitivity analysis is made possible by means of Sobol’ indices, that quantify the impact of process inputs across identified regimes. The insights gained from our analysis contribute to the formulation of hypotheses regarding phenomena occurring beyond the transition regime. Notably, the significance of temperature even in the diffusion-limited regime, as evidenced by the Arrhenius plot, suggests activation of gas phase reactions at elevated temperatures. Importantly, our proposed methods yield insights that align with experimental observations and theoretical principles, aiding decision-making in process design and optimization. By circumventing the need for costly and time-consuming experiments, our approach offers a pragmatic pathway towards enhanced process efficiency. Moreover, this study underscores the potential of data-driven computational methods for innovating reactor design paradigms.</summary></entry><entry><title type="html">Do Finetti: On Causal Effects for Exchangeable Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/DoFinettiOnCausalEffectsforExchangeableData.html" rel="alternate" type="text/html" title="Do Finetti: On Causal Effects for Exchangeable Data" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/DoFinettiOnCausalEffectsforExchangeableData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/DoFinettiOnCausalEffectsforExchangeableData.html">&lt;p&gt;We study causal effect estimation in a setting where the data are not i.i.d. (independent and identically distributed). We focus on exchangeable data satisfying an assumption of independent causal mechanisms. Traditional causal effect estimation frameworks, e.g., relying on structural causal models and do-calculus, are typically limited to i.i.d. data and do not extend to more general exchangeable generative processes, which naturally arise in multi-environment data. To address this gap, we develop a generalized framework for exchangeable data and introduce a truncated factorization formula that facilitates both the identification and estimation of causal effects in our setting. To illustrate potential applications, we introduce a causal P&apos;olya urn model and demonstrate how intervention propagates effects in exchangeable data settings. Finally, we develop an algorithm that performs simultaneous causal discovery and effect estimation given multi-environment data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18836&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Siyuan Guo, Chi Zhang, Karthika Mohan, Ferenc Huszár, Bernhard Schölkopf</name></author><category term="stat.ME" /><summary type="html">We study causal effect estimation in a setting where the data are not i.i.d. (independent and identically distributed). We focus on exchangeable data satisfying an assumption of independent causal mechanisms. Traditional causal effect estimation frameworks, e.g., relying on structural causal models and do-calculus, are typically limited to i.i.d. data and do not extend to more general exchangeable generative processes, which naturally arise in multi-environment data. To address this gap, we develop a generalized framework for exchangeable data and introduce a truncated factorization formula that facilitates both the identification and estimation of causal effects in our setting. To illustrate potential applications, we introduce a causal P&apos;olya urn model and demonstrate how intervention propagates effects in exchangeable data settings. Finally, we develop an algorithm that performs simultaneous causal discovery and effect estimation given multi-environment data.</summary></entry><entry><title type="html">Examining the development of attitude scales using Large Language Models (LLMs)</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ExaminingthedevelopmentofattitudescalesusingLargeLanguageModelsLLMs.html" rel="alternate" type="text/html" title="Examining the development of attitude scales using Large Language Models (LLMs)" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ExaminingthedevelopmentofattitudescalesusingLargeLanguageModelsLLMs</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ExaminingthedevelopmentofattitudescalesusingLargeLanguageModelsLLMs.html">&lt;p&gt;For nearly a century, social researchers and psychologists have debated the efficacy of psychometric scales for attitude measurement, focusing on Thurstone’s equal appearing interval scales and Likert’s summated rating scales. Thurstone scales fell out of favour due to the labour intensive process of gathering judges’ opinions on the initial items. However, advancements in technology have mitigated these challenges, nullifying the simplicity advantage of Likert scales, which have their own methodological issues. This study explores a methodological experiment to develop a Thurstone scale for assessing attitudes towards individuals living with AIDS. An electronic questionnaire was distributed to a group of judges, including undergraduate, postgraduate, and PhD students from disciplines such as social policy, law, medicine, and computer engineering, alongside established social researchers, and their responses were statistically analysed. The primary innovation of this study is the incorporation of an Artificial Intelligence (AI) Large Language Model (LLM) to evaluate the initial 63 items, comparing its assessments with those of the human judges. Interestingly, the AI provided also detailed explanations for its categorisation. Results showed no significant difference between AI and human judges for 35 items, minor differences for 23 items, and major differences for 5 items. This experiment demonstrates the potential of integrating AI with traditional psychometric methods to enhance the development of attitude measurement scales.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19011&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maria Symeonaki, Giorgos Stamou, Aggeliki Kazani, Eva Tsouparopoulou, Glykeria Stamatopoulou</name></author><category term="stat.AP" /><summary type="html">For nearly a century, social researchers and psychologists have debated the efficacy of psychometric scales for attitude measurement, focusing on Thurstone’s equal appearing interval scales and Likert’s summated rating scales. Thurstone scales fell out of favour due to the labour intensive process of gathering judges’ opinions on the initial items. However, advancements in technology have mitigated these challenges, nullifying the simplicity advantage of Likert scales, which have their own methodological issues. This study explores a methodological experiment to develop a Thurstone scale for assessing attitudes towards individuals living with AIDS. An electronic questionnaire was distributed to a group of judges, including undergraduate, postgraduate, and PhD students from disciplines such as social policy, law, medicine, and computer engineering, alongside established social researchers, and their responses were statistically analysed. The primary innovation of this study is the incorporation of an Artificial Intelligence (AI) Large Language Model (LLM) to evaluate the initial 63 items, comparing its assessments with those of the human judges. Interestingly, the AI provided also detailed explanations for its categorisation. Results showed no significant difference between AI and human judges for 35 items, minor differences for 23 items, and major differences for 5 items. This experiment demonstrates the potential of integrating AI with traditional psychometric methods to enhance the development of attitude measurement scales.</summary></entry><entry><title type="html">From Conformal Predictions to Confidence Regions</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/FromConformalPredictionstoConfidenceRegions.html" rel="alternate" type="text/html" title="From Conformal Predictions to Confidence Regions" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/FromConformalPredictionstoConfidenceRegions</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/FromConformalPredictionstoConfidenceRegions.html">&lt;p&gt;Conformal prediction methodologies have significantly advanced the quantification of uncertainties in predictive models. Yet, the construction of confidence regions for model parameters presents a notable challenge, often necessitating stringent assumptions regarding data distribution or merely providing asymptotic guarantees. We introduce a novel approach termed CCR, which employs a combination of conformal prediction intervals for the model outputs to establish confidence regions for model parameters. We present coverage guarantees under minimal assumptions on noise and that is valid in finite sample regime. Our approach is applicable to both split conformal predictions and black-box methodologies including full or cross-conformal approaches. In the specific case of linear models, the derived confidence region manifests as the feasible set of a Mixed-Integer Linear Program (MILP), facilitating the deduction of confidence intervals for individual parameters and enabling robust optimization. We empirically compare CCR to recent advancements in challenging settings such as with heteroskedastic and non-Gaussian noise.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18601&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Charles Guille-Escuret, Eugene Ndiaye</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Conformal prediction methodologies have significantly advanced the quantification of uncertainties in predictive models. Yet, the construction of confidence regions for model parameters presents a notable challenge, often necessitating stringent assumptions regarding data distribution or merely providing asymptotic guarantees. We introduce a novel approach termed CCR, which employs a combination of conformal prediction intervals for the model outputs to establish confidence regions for model parameters. We present coverage guarantees under minimal assumptions on noise and that is valid in finite sample regime. Our approach is applicable to both split conformal predictions and black-box methodologies including full or cross-conformal approaches. In the specific case of linear models, the derived confidence region manifests as the feasible set of a Mixed-Integer Linear Program (MILP), facilitating the deduction of confidence intervals for individual parameters and enabling robust optimization. We empirically compare CCR to recent advancements in challenging settings such as with heteroskedastic and non-Gaussian noise.</summary></entry><entry><title type="html">Guided sequential ABC schemes for intractable Bayesian models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/GuidedsequentialABCschemesforintractableBayesianmodels.html" rel="alternate" type="text/html" title="Guided sequential ABC schemes for intractable Bayesian models" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/GuidedsequentialABCschemesforintractableBayesianmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/GuidedsequentialABCschemesforintractableBayesianmodels.html">&lt;p&gt;Sequential algorithms such as sequential importance sampling (SIS) and sequential Monte Carlo (SMC) have proven fundamental in Bayesian inference for models not admitting a readily available likelihood function. For approximate Bayesian computation (ABC), SMC-ABC is the state-of-art sampler. However, since the ABC paradigm is intrinsically wasteful, sequential ABC schemes can benefit from well-targeted proposal samplers that efficiently avoid improbable parameter regions. We contribute to the ABC modeller’s toolbox with novel proposal samplers that are conditional to summary statistics of the data. In a sense, the proposed parameters are “guided” to rapidly reach regions of the posterior surface that are compatible with the observed data. This speeds up the convergence of these sequential samplers, thus reducing the computational effort, while preserving the accuracy in the inference. We provide a variety of guided Gaussian and copula-based samplers for both SIS-ABC and SMC-ABC easing inference for challenging case-studies, including multimodal posteriors, highly correlated posteriors, hierarchical models with about 20 parameters, and a simulation study of cell movements using more than 400 summary statistics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2206.12235&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Umberto Picchini, Massimiliano Tamborrino</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">Sequential algorithms such as sequential importance sampling (SIS) and sequential Monte Carlo (SMC) have proven fundamental in Bayesian inference for models not admitting a readily available likelihood function. For approximate Bayesian computation (ABC), SMC-ABC is the state-of-art sampler. However, since the ABC paradigm is intrinsically wasteful, sequential ABC schemes can benefit from well-targeted proposal samplers that efficiently avoid improbable parameter regions. We contribute to the ABC modeller’s toolbox with novel proposal samplers that are conditional to summary statistics of the data. In a sense, the proposed parameters are “guided” to rapidly reach regions of the posterior surface that are compatible with the observed data. This speeds up the convergence of these sequential samplers, thus reducing the computational effort, while preserving the accuracy in the inference. We provide a variety of guided Gaussian and copula-based samplers for both SIS-ABC and SMC-ABC easing inference for challenging case-studies, including multimodal posteriors, highly correlated posteriors, hierarchical models with about 20 parameters, and a simulation study of cell movements using more than 400 summary statistics.</summary></entry><entry><title type="html">How to Simulate Realistic Survival Data? A Simulation Study to Compare Realistic Simulation Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/HowtoSimulateRealisticSurvivalDataASimulationStudytoCompareRealisticSimulationModels.html" rel="alternate" type="text/html" title="How to Simulate Realistic Survival Data? A Simulation Study to Compare Realistic Simulation Models" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/HowtoSimulateRealisticSurvivalDataASimulationStudytoCompareRealisticSimulationModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/HowtoSimulateRealisticSurvivalDataASimulationStudytoCompareRealisticSimulationModels.html">&lt;p&gt;In statistics, it is important to have realistic data sets available for a particular context to allow an appropriate and objective method comparison. For many use cases, benchmark data sets for method comparison are already available online. However, in most medical applications and especially for clinical trials in oncology, there is a lack of adequate benchmark data sets, as patient data can be sensitive and therefore cannot be published. A potential solution for this are simulation studies. However, it is sometimes not clear, which simulation models are suitable for generating realistic data. A challenge is that potentially unrealistic assumptions have to be made about the distributions. Our approach is to use reconstructed benchmark data sets %can be used as a basis for the simulations, which has the following advantages: the actual properties are known and more realistic data can be simulated. There are several possibilities to simulate realistic data from benchmark data sets. We investigate simulation models based upon kernel density estimation, fitted distributions, case resampling and conditional bootstrapping. In order to make recommendations on which models are best suited for a specific survival setting, we conducted a comparative simulation study. Since it is not possible to provide recommendations for all possible survival settings in a single paper, we focus on providing realistic simulation models for two-armed phase III lung cancer studies. To this end we reconstructed benchmark data sets from recent studies. We used the runtime and different accuracy measures (effect sizes and p-values) as criteria for comparison.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.07842&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maria Thurow, Ina Dormuth, Christina Sauer, Marc Ditzhaus, Markus Pauly</name></author><category term="stat.AP" /><summary type="html">In statistics, it is important to have realistic data sets available for a particular context to allow an appropriate and objective method comparison. For many use cases, benchmark data sets for method comparison are already available online. However, in most medical applications and especially for clinical trials in oncology, there is a lack of adequate benchmark data sets, as patient data can be sensitive and therefore cannot be published. A potential solution for this are simulation studies. However, it is sometimes not clear, which simulation models are suitable for generating realistic data. A challenge is that potentially unrealistic assumptions have to be made about the distributions. Our approach is to use reconstructed benchmark data sets %can be used as a basis for the simulations, which has the following advantages: the actual properties are known and more realistic data can be simulated. There are several possibilities to simulate realistic data from benchmark data sets. We investigate simulation models based upon kernel density estimation, fitted distributions, case resampling and conditional bootstrapping. In order to make recommendations on which models are best suited for a specific survival setting, we conducted a comparative simulation study. Since it is not possible to provide recommendations for all possible survival settings in a single paper, we focus on providing realistic simulation models for two-armed phase III lung cancer studies. To this end we reconstructed benchmark data sets from recent studies. We used the runtime and different accuracy measures (effect sizes and p-values) as criteria for comparison.</summary></entry><entry><title type="html">Improving Harmonic Analysis using Multitapering: Precise frequency estimation of stellar oscillations using the harmonic F-test</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ImprovingHarmonicAnalysisusingMultitaperingPrecisefrequencyestimationofstellaroscillationsusingtheharmonicFtest.html" rel="alternate" type="text/html" title="Improving Harmonic Analysis using Multitapering: Precise frequency estimation of stellar oscillations using the harmonic F-test" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ImprovingHarmonicAnalysisusingMultitaperingPrecisefrequencyestimationofstellaroscillationsusingtheharmonicFtest</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ImprovingHarmonicAnalysisusingMultitaperingPrecisefrequencyestimationofstellaroscillationsusingtheharmonicFtest.html">&lt;p&gt;In Patil et. al 2024a, we developed a multitaper power spectrum estimation method, mtNUFFT, for analyzing time-series with quasi-regular spacing, and showed that it not only improves upon the statistical issues of the Lomb-Scargle periodogram, but also provides a factor of three speed up in some applications. In this paper, we combine mtNUFFT with the harmonic F-test to test the hypothesis that a strictly periodic signal or its harmonic (as opposed to e.g. a quasi-periodic signal) is present at a given frequency. This mtNUFFT/F-test combination shows that multitapering allows detection of periodic signals and precise estimation of their frequencies, thereby improving both power spectrum estimation and harmonic analysis. Using asteroseismic time-series data for the Kepler-91 red giant, we show that the F-test automatically picks up the harmonics of its transiting exoplanet as well as certain dipole ($l=1$) mixed modes. We use this example to highlight that we can distinguish between different types of stellar oscillations, e.g., transient (damped, stochastically-excited) and strictly periodic (undamped, heat-driven). We also illustrate the technique of dividing a time-series into chunks to further examine the transient versus periodic nature of stellar oscillations. The harmonic F-test combined with mtNUFFT is implemented in the public Python package tapify (https://github.com/aaryapatil/tapify), which opens opportunities to perform detailed investigations of periodic signals in time-domain astronomy.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18509&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aarya A. Patil, Gwendolyn M. Eadie, Joshua S. Speagle, David J. Thomson</name></author><category term="stat.AP" /><summary type="html">In Patil et. al 2024a, we developed a multitaper power spectrum estimation method, mtNUFFT, for analyzing time-series with quasi-regular spacing, and showed that it not only improves upon the statistical issues of the Lomb-Scargle periodogram, but also provides a factor of three speed up in some applications. In this paper, we combine mtNUFFT with the harmonic F-test to test the hypothesis that a strictly periodic signal or its harmonic (as opposed to e.g. a quasi-periodic signal) is present at a given frequency. This mtNUFFT/F-test combination shows that multitapering allows detection of periodic signals and precise estimation of their frequencies, thereby improving both power spectrum estimation and harmonic analysis. Using asteroseismic time-series data for the Kepler-91 red giant, we show that the F-test automatically picks up the harmonics of its transiting exoplanet as well as certain dipole ($l=1$) mixed modes. We use this example to highlight that we can distinguish between different types of stellar oscillations, e.g., transient (damped, stochastically-excited) and strictly periodic (undamped, heat-driven). We also illustrate the technique of dividing a time-series into chunks to further examine the transient versus periodic nature of stellar oscillations. The harmonic F-test combined with mtNUFFT is implemented in the public Python package tapify (https://github.com/aaryapatil/tapify), which opens opportunities to perform detailed investigations of periodic signals in time-domain astronomy.</summary></entry><entry><title type="html">Improving Power Spectrum Estimation using Multitapering: Efficient asteroseismic analyses for understanding stars, the Milky Way, and beyond</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ImprovingPowerSpectrumEstimationusingMultitaperingEfficientasteroseismicanalysesforunderstandingstarstheMilkyWayandbeyond.html" rel="alternate" type="text/html" title="Improving Power Spectrum Estimation using Multitapering: Efficient asteroseismic analyses for understanding stars, the Milky Way, and beyond" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ImprovingPowerSpectrumEstimationusingMultitaperingEfficientasteroseismicanalysesforunderstandingstarstheMilkyWayandbeyond</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ImprovingPowerSpectrumEstimationusingMultitaperingEfficientasteroseismicanalysesforunderstandingstarstheMilkyWayandbeyond.html">&lt;p&gt;Asteroseismic time-series data have imprints of stellar oscillation modes, whose detection and characterization through time-series analysis allows us to probe stellar interior physics. Such analyses usually occur in the Fourier domain by computing the Lomb-Scargle (LS) periodogram, an estimator of the power spectrum underlying unevenly-sampled time-series data. However, the LS periodogram suffers from the statistical problems of (1) inconsistency (or noise) and (2) bias due to high spectral leakage. Here, we develop a multitaper power spectrum estimator using the Non-Uniform Fast Fourier Transform (mtNUFFT) to tackle the inconsistency and bias problems of the LS periodogram. Using a simulated light curve, we show that the mtNUFFT power spectrum estimate of solar-like oscillations has lower variance and bias than the LS estimate. We also apply our method to the Kepler-91 red giant, and combine it with PBjam peakbagging to obtain mode parameters and a derived age estimate of $3.97 \pm 0.52$ Gyr. PBjam allows the improvement of age precision relative to the $4.27 \pm 0.75$ Gyr APOKASC-2 (uncorrected) estimate, whereas partnering mtNUFFT with PBjam speeds up peakbagging thrice as much as LS. This increase in efficiency has promising implications for Galactic archaeology, in addition to stellar structure and evolution studies. Our new method generally applies to time-domain astronomy and is implemented in the public Python package tapify, available at https://github.com/aaryapatil/tapify.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2209.15027&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aarya A. Patil, Gwendolyn M. Eadie, Joshua S. Speagle, David J. Thomson</name></author><category term="stat.AP" /><summary type="html">Asteroseismic time-series data have imprints of stellar oscillation modes, whose detection and characterization through time-series analysis allows us to probe stellar interior physics. Such analyses usually occur in the Fourier domain by computing the Lomb-Scargle (LS) periodogram, an estimator of the power spectrum underlying unevenly-sampled time-series data. However, the LS periodogram suffers from the statistical problems of (1) inconsistency (or noise) and (2) bias due to high spectral leakage. Here, we develop a multitaper power spectrum estimator using the Non-Uniform Fast Fourier Transform (mtNUFFT) to tackle the inconsistency and bias problems of the LS periodogram. Using a simulated light curve, we show that the mtNUFFT power spectrum estimate of solar-like oscillations has lower variance and bias than the LS estimate. We also apply our method to the Kepler-91 red giant, and combine it with PBjam peakbagging to obtain mode parameters and a derived age estimate of $3.97 \pm 0.52$ Gyr. PBjam allows the improvement of age precision relative to the $4.27 \pm 0.75$ Gyr APOKASC-2 (uncorrected) estimate, whereas partnering mtNUFFT with PBjam speeds up peakbagging thrice as much as LS. This increase in efficiency has promising implications for Galactic archaeology, in addition to stellar structure and evolution studies. Our new method generally applies to time-domain astronomy and is implemented in the public Python package tapify, available at https://github.com/aaryapatil/tapify.</summary></entry><entry><title type="html">Inference under covariate-adaptive randomization with many strata</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Inferenceundercovariateadaptiverandomizationwithmanystrata.html" rel="alternate" type="text/html" title="Inference under covariate-adaptive randomization with many strata" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Inferenceundercovariateadaptiverandomizationwithmanystrata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Inferenceundercovariateadaptiverandomizationwithmanystrata.html">&lt;p&gt;Covariate-adaptive randomization is widely employed to balance baseline covariates in interventional studies such as clinical trials and experiments in development economics. Recent years have witnessed substantial progress in inference under covariate-adaptive randomization with a fixed number of strata. However, concerns have been raised about the impact of a large number of strata on its design and analysis, which is a common scenario in practice, such as in multicenter randomized clinical trials. In this paper, we propose a general framework for inference under covariate-adaptive randomization, which extends the seminal works of Bugni et al. (2018, 2019) by allowing for a diverging number of strata. Furthermore, we introduce a novel weighted regression adjustment that ensures efficiency improvement. On top of establishing the asymptotic theory, practical algorithms for handling situations involving an extremely large number of strata are also developed. Moreover, by linking design balance and inference robustness, we highlight the advantages of stratified block randomization, which enforces better covariate balance within strata compared to simple randomization. This paper offers a comprehensive landscape of inference under covariate-adaptive randomization, spanning from fixed to diverging to extremely large numbers of strata.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18856&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiahui Xin, Hanzhong Liu, Wei Ma</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Covariate-adaptive randomization is widely employed to balance baseline covariates in interventional studies such as clinical trials and experiments in development economics. Recent years have witnessed substantial progress in inference under covariate-adaptive randomization with a fixed number of strata. However, concerns have been raised about the impact of a large number of strata on its design and analysis, which is a common scenario in practice, such as in multicenter randomized clinical trials. In this paper, we propose a general framework for inference under covariate-adaptive randomization, which extends the seminal works of Bugni et al. (2018, 2019) by allowing for a diverging number of strata. Furthermore, we introduce a novel weighted regression adjustment that ensures efficiency improvement. On top of establishing the asymptotic theory, practical algorithms for handling situations involving an extremely large number of strata are also developed. Moreover, by linking design balance and inference robustness, we highlight the advantages of stratified block randomization, which enforces better covariate balance within strata compared to simple randomization. This paper offers a comprehensive landscape of inference under covariate-adaptive randomization, spanning from fixed to diverging to extremely large numbers of strata.</summary></entry><entry><title type="html">L-Estimation in Instrumental Variables Regression for Censored Data in Presence of Endogeneity and Dependent Errors</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/LEstimationinInstrumentalVariablesRegressionforCensoredDatainPresenceofEndogeneityandDependentErrors.html" rel="alternate" type="text/html" title="L-Estimation in Instrumental Variables Regression for Censored Data in Presence of Endogeneity and Dependent Errors" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/LEstimationinInstrumentalVariablesRegressionforCensoredDatainPresenceofEndogeneityandDependentErrors</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/LEstimationinInstrumentalVariablesRegressionforCensoredDatainPresenceofEndogeneityandDependentErrors.html">&lt;p&gt;In this article, we propose L-estimators of the unknown parameters in the instrumental variables regression in the presence of censored data under endogeneity. We allow the random errors involved in the model to be dependent. The proposed estimation procedure is a two-stage procedure, and the large sample properties of the proposed estimators are established. The utility of the proposed methodology is demonstrated for various simulated data and a benchmark real data set.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19145&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Swati Shukla, Subhra Sankar Dhar,  Shalabh</name></author><category term="stat.ME" /><summary type="html">In this article, we propose L-estimators of the unknown parameters in the instrumental variables regression in the presence of censored data under endogeneity. We allow the random errors involved in the model to be dependent. The proposed estimation procedure is a two-stage procedure, and the large sample properties of the proposed estimators are established. The utility of the proposed methodology is demonstrated for various simulated data and a benchmark real data set.</summary></entry><entry><title type="html">LSTM-COX Model: A Concise and Efficient Deep Learning Approach for Handling Recurrent Events</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/LSTMCOXModelAConciseandEfficientDeepLearningApproachforHandlingRecurrentEvents.html" rel="alternate" type="text/html" title="LSTM-COX Model: A Concise and Efficient Deep Learning Approach for Handling Recurrent Events" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/LSTMCOXModelAConciseandEfficientDeepLearningApproachforHandlingRecurrentEvents</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/LSTMCOXModelAConciseandEfficientDeepLearningApproachforHandlingRecurrentEvents.html">&lt;p&gt;In the current field of clinical medicine, traditional methods for analyzing recurrent events have limitations when dealing with complex time-dependent data. This study combines Long Short-Term Memory networks (LSTM) with the Cox model to enhance the model’s performance in analyzing recurrent events with dynamic temporal information. Compared to classical models, the LSTM-Cox model significantly improves the accuracy of extracting clinical risk features and exhibits lower Akaike Information Criterion (AIC) values, while maintaining good performance on simulated datasets. In an empirical analysis of bladder cancer recurrence data, the model successfully reduced the mean squared error during the training phase and achieved a Concordance index of up to 0.90 on the test set. Furthermore, the model effectively distinguished between high and low-risk patient groups, and the identified recurrence risk features such as the number of tumor recurrences and maximum size were consistent with other research and clinical trial results. This study not only provides a straightforward and efficient method for analyzing recurrent data and extracting features but also offers a convenient pathway for integrating deep learning techniques into clinical risk prediction systems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18518&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhang Runquan, Shi Xiaoping</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">In the current field of clinical medicine, traditional methods for analyzing recurrent events have limitations when dealing with complex time-dependent data. This study combines Long Short-Term Memory networks (LSTM) with the Cox model to enhance the model’s performance in analyzing recurrent events with dynamic temporal information. Compared to classical models, the LSTM-Cox model significantly improves the accuracy of extracting clinical risk features and exhibits lower Akaike Information Criterion (AIC) values, while maintaining good performance on simulated datasets. In an empirical analysis of bladder cancer recurrence data, the model successfully reduced the mean squared error during the training phase and achieved a Concordance index of up to 0.90 on the test set. Furthermore, the model effectively distinguished between high and low-risk patient groups, and the identified recurrence risk features such as the number of tumor recurrences and maximum size were consistent with other research and clinical trial results. This study not only provides a straightforward and efficient method for analyzing recurrent data and extracting features but also offers a convenient pathway for integrating deep learning techniques into clinical risk prediction systems.</summary></entry><entry><title type="html">Multi-Armed Bandits with Network Interference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/MultiArmedBanditswithNetworkInterference.html" rel="alternate" type="text/html" title="Multi-Armed Bandits with Network Interference" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/MultiArmedBanditswithNetworkInterference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/MultiArmedBanditswithNetworkInterference.html">&lt;p&gt;Online experimentation with interference is a common challenge in modern applications such as e-commerce and adaptive clinical trials in medicine. For example, in online marketplaces, the revenue of a good depends on discounts applied to competing goods. Statistical inference with interference is widely studied in the offline setting, but far less is known about how to adaptively assign treatments to minimize regret. We address this gap by studying a multi-armed bandit (MAB) problem where a learner (e-commerce platform) sequentially assigns one of possible $\mathcal{A}$ actions (discounts) to $N$ units (goods) over $T$ rounds to minimize regret (maximize revenue). Unlike traditional MAB problems, the reward of each unit depends on the treatments assigned to other units, i.e., there is interference across the underlying network of units. With $\mathcal{A}$ actions and $N$ units, minimizing regret is combinatorially difficult since the action space grows as $\mathcal{A}^N$. To overcome this issue, we study a sparse network interference model, where the reward of a unit is only affected by the treatments assigned to $s$ neighboring units. We use tools from discrete Fourier analysis to develop a sparse linear representation of the unit-specific reward $r_n: [\mathcal{A}]^N \rightarrow \mathbb{R} $, and propose simple, linear regression-based algorithms to minimize regret. Importantly, our algorithms achieve provably low regret both when the learner observes the interference neighborhood for all units and when it is unknown. This significantly generalizes other works on this topic which impose strict conditions on the strength of interference on a known network, and also compare regret to a markedly weaker optimal action. Empirically, we corroborate our theoretical findings via numerical simulations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18621&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Abhineet Agarwal, Anish Agarwal, Lorenzo Masoero, Justin Whitehouse</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Online experimentation with interference is a common challenge in modern applications such as e-commerce and adaptive clinical trials in medicine. For example, in online marketplaces, the revenue of a good depends on discounts applied to competing goods. Statistical inference with interference is widely studied in the offline setting, but far less is known about how to adaptively assign treatments to minimize regret. We address this gap by studying a multi-armed bandit (MAB) problem where a learner (e-commerce platform) sequentially assigns one of possible $\mathcal{A}$ actions (discounts) to $N$ units (goods) over $T$ rounds to minimize regret (maximize revenue). Unlike traditional MAB problems, the reward of each unit depends on the treatments assigned to other units, i.e., there is interference across the underlying network of units. With $\mathcal{A}$ actions and $N$ units, minimizing regret is combinatorially difficult since the action space grows as $\mathcal{A}^N$. To overcome this issue, we study a sparse network interference model, where the reward of a unit is only affected by the treatments assigned to $s$ neighboring units. We use tools from discrete Fourier analysis to develop a sparse linear representation of the unit-specific reward $r_n: [\mathcal{A}]^N \rightarrow \mathbb{R} $, and propose simple, linear regression-based algorithms to minimize regret. Importantly, our algorithms achieve provably low regret both when the learner observes the interference neighborhood for all units and when it is unknown. This significantly generalizes other works on this topic which impose strict conditions on the strength of interference on a known network, and also compare regret to a markedly weaker optimal action. Empirically, we corroborate our theoretical findings via numerical simulations.</summary></entry><entry><title type="html">Nesting Particle Filters for Experimental Design in Dynamical Systems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/NestingParticleFiltersforExperimentalDesigninDynamicalSystems.html" rel="alternate" type="text/html" title="Nesting Particle Filters for Experimental Design in Dynamical Systems" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/NestingParticleFiltersforExperimentalDesigninDynamicalSystems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/NestingParticleFiltersforExperimentalDesigninDynamicalSystems.html">&lt;p&gt;In this paper, we propose a novel approach to Bayesian experimental design for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential Monte Carlo technique to infer optimal designs, and embed it into a particle Markov chain Monte Carlo framework to perform gradient-based policy amortization. Our approach is distinct from other amortized experimental design techniques, as it does not rely on contrastive estimators. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.07868&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sahel Iqbal, Adrien Corenflos, Simo Särkkä, Hany Abdulsamad</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">In this paper, we propose a novel approach to Bayesian experimental design for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential Monte Carlo technique to infer optimal designs, and embed it into a particle Markov chain Monte Carlo framework to perform gradient-based policy amortization. Our approach is distinct from other amortized experimental design techniques, as it does not rely on contrastive estimators. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.</summary></entry><entry><title type="html">Non-Log-Concave and Nonsmooth Sampling via Langevin Monte Carlo Algorithms</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/NonLogConcaveandNonsmoothSamplingviaLangevinMonteCarloAlgorithms.html" rel="alternate" type="text/html" title="Non-Log-Concave and Nonsmooth Sampling via Langevin Monte Carlo Algorithms" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/NonLogConcaveandNonsmoothSamplingviaLangevinMonteCarloAlgorithms</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/NonLogConcaveandNonsmoothSamplingviaLangevinMonteCarloAlgorithms.html">&lt;p&gt;We study the problem of approximate sampling from non-log-concave distributions, e.g., Gaussian mixtures, which is often challenging even in low dimensions due to their multimodality. We focus on performing this task via Markov chain Monte Carlo (MCMC) methods derived from discretizations of the overdamped Langevin diffusions, which are commonly known as Langevin Monte Carlo algorithms. Furthermore, we are also interested in two nonsmooth cases for which a large class of proximal MCMC methods have been developed: (i) a nonsmooth prior is considered with a Gaussian mixture likelihood; (ii) a Laplacian mixture distribution. Such nonsmooth and non-log-concave sampling tasks arise from a wide range of applications to Bayesian inference and imaging inverse problems such as image deconvolution. We perform numerical simulations to compare the performance of most commonly used Langevin Monte Carlo algorithms.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.15988&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tim Tsz-Kit Lau, Han Liu, Thomas Pock</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">We study the problem of approximate sampling from non-log-concave distributions, e.g., Gaussian mixtures, which is often challenging even in low dimensions due to their multimodality. We focus on performing this task via Markov chain Monte Carlo (MCMC) methods derived from discretizations of the overdamped Langevin diffusions, which are commonly known as Langevin Monte Carlo algorithms. Furthermore, we are also interested in two nonsmooth cases for which a large class of proximal MCMC methods have been developed: (i) a nonsmooth prior is considered with a Gaussian mixture likelihood; (ii) a Laplacian mixture distribution. Such nonsmooth and non-log-concave sampling tasks arise from a wide range of applications to Bayesian inference and imaging inverse problems such as image deconvolution. We perform numerical simulations to compare the performance of most commonly used Langevin Monte Carlo algorithms.</summary></entry><entry><title type="html">Parallel Affine Transformation Tuning of Markov Chain Monte Carlo</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ParallelAffineTransformationTuningofMarkovChainMonteCarlo.html" rel="alternate" type="text/html" title="Parallel Affine Transformation Tuning of Markov Chain Monte Carlo" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ParallelAffineTransformationTuningofMarkovChainMonteCarlo</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ParallelAffineTransformationTuningofMarkovChainMonteCarlo.html">&lt;p&gt;The performance of Markov chain Monte Carlo samplers strongly depends on the properties of the target distribution such as its covariance structure, the location of its probability mass and its tail behavior. We explore the use of bijective affine transformations of the sample space to improve the properties of the target distribution and thereby the performance of samplers running in the transformed space. In particular, we propose a flexible and user-friendly scheme for adaptively learning the affine transformation during sampling. Moreover, the combination of our scheme with Gibbsian polar slice sampling is shown to produce samples of high quality at comparatively low computational cost in several settings based on real-world data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.16567&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Philip Schär, Michael Habeck, Daniel Rudolf</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">The performance of Markov chain Monte Carlo samplers strongly depends on the properties of the target distribution such as its covariance structure, the location of its probability mass and its tail behavior. We explore the use of bijective affine transformations of the sample space to improve the properties of the target distribution and thereby the performance of samplers running in the transformed space. In particular, we propose a flexible and user-friendly scheme for adaptively learning the affine transformation during sampling. Moreover, the combination of our scheme with Gibbsian polar slice sampling is shown to produce samples of high quality at comparatively low computational cost in several settings based on real-world data.</summary></entry><entry><title type="html">Parameter identifiability, parameter estimation and model prediction for differential equation models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Parameteridentifiabilityparameterestimationandmodelpredictionfordifferentialequationmodels.html" rel="alternate" type="text/html" title="Parameter identifiability, parameter estimation and model prediction for differential equation models" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Parameteridentifiabilityparameterestimationandmodelpredictionfordifferentialequationmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Parameteridentifiabilityparameterestimationandmodelpredictionfordifferentialequationmodels.html">&lt;p&gt;Interpreting data with mathematical models is an important aspect of real-world applied mathematical modeling. Very often we are interested to understand the extent to which a particular data set informs and constrains model parameters. This question is closely related to the concept of parameter identifiability, and in this article we present a series of computational exercises to introduce tools that can be used to assess parameter identifiability, estimate parameters and generate model predictions. Taking a likelihood-based approach, we show that very similar ideas and algorithms can be used to deal with a range of different mathematical modelling frameworks. The exercises and results presented in this article are supported by a suite of open access codes that can be accessed on GitHub.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.08177&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Matthew J Simpson, Ruth E Baker</name></author><category term="stat.ME" /><summary type="html">Interpreting data with mathematical models is an important aspect of real-world applied mathematical modeling. Very often we are interested to understand the extent to which a particular data set informs and constrains model parameters. This question is closely related to the concept of parameter identifiability, and in this article we present a series of computational exercises to introduce tools that can be used to assess parameter identifiability, estimate parameters and generate model predictions. Taking a likelihood-based approach, we show that very similar ideas and algorithms can be used to deal with a range of different mathematical modelling frameworks. The exercises and results presented in this article are supported by a suite of open access codes that can be accessed on GitHub.</summary></entry><entry><title type="html">Participation bias in the estimation of heritability and genetic correlation</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Participationbiasintheestimationofheritabilityandgeneticcorrelation.html" rel="alternate" type="text/html" title="Participation bias in the estimation of heritability and genetic correlation" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Participationbiasintheestimationofheritabilityandgeneticcorrelation</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Participationbiasintheestimationofheritabilityandgeneticcorrelation.html">&lt;p&gt;It is increasingly recognized that participation bias can pose problems for genetic studies. Recently, to overcome the challenge that genetic information of non-participants is unavailable, it is shown that by comparing the IBD (identity by descent) shared and not-shared segments among the participants, one can estimate the genetic component underlying participation. That, however, does not directly address how to adjust estimates of heritability and genetic correlation for phenotypes correlated with participation. Here, for phenotypes whose mean differences between population and sample are known, we demonstrate a way to do so by adopting a statistical framework that separates out the genetic and non-genetic correlations between participation and these phenotypes. Crucially, our method avoids making the assumption that the effect of the genetic component underlying participation is manifested entirely through these other phenotypes. Applying the method to 12 UK Biobank phenotypes, we found 8 have significant genetic correlations with participation, including body mass index, educational attainment, and smoking status. For most of these phenotypes, without adjustments, estimates of heritability and the absolute value of genetic correlation would have underestimation biases.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19058&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Shuang Song, Stefania Benonisdottir, Jun S. Liu, Augustine Kong</name></author><category term="stat.ME" /><summary type="html">It is increasingly recognized that participation bias can pose problems for genetic studies. Recently, to overcome the challenge that genetic information of non-participants is unavailable, it is shown that by comparing the IBD (identity by descent) shared and not-shared segments among the participants, one can estimate the genetic component underlying participation. That, however, does not directly address how to adjust estimates of heritability and genetic correlation for phenotypes correlated with participation. Here, for phenotypes whose mean differences between population and sample are known, we demonstrate a way to do so by adopting a statistical framework that separates out the genetic and non-genetic correlations between participation and these phenotypes. Crucially, our method avoids making the assumption that the effect of the genetic component underlying participation is manifested entirely through these other phenotypes. Applying the method to 12 UK Biobank phenotypes, we found 8 have significant genetic correlations with participation, including body mass index, educational attainment, and smoking status. For most of these phenotypes, without adjustments, estimates of heritability and the absolute value of genetic correlation would have underestimation biases.</summary></entry><entry><title type="html">Probing the Information Theoretical Roots of Spatial Dependence Measures</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ProbingtheInformationTheoreticalRootsofSpatialDependenceMeasures.html" rel="alternate" type="text/html" title="Probing the Information Theoretical Roots of Spatial Dependence Measures" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ProbingtheInformationTheoreticalRootsofSpatialDependenceMeasures</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ProbingtheInformationTheoreticalRootsofSpatialDependenceMeasures.html">&lt;p&gt;Intuitively, there is a relation between measures of spatial dependence and information theoretical measures of entropy. For instance, we can provide an intuition of why spatial data is special by stating that, on average, spatial data samples contain less than expected information. Similarly, spatial data, e.g., remotely sensed imagery, that is easy to compress is also likely to show significant spatial autocorrelation. Formulating our (highly specific) core concepts of spatial information theory in the widely used language of information theory opens new perspectives on their differences and similarities and also fosters cross-disciplinary collaboration, e.g., with the broader AI/ML communities. Interestingly, however, this intuitive relation is challenging to formalize and generalize, leading prior work to rely mostly on experimental results, e.g., for describing landscape patterns. In this work, we will explore the information theoretical roots of spatial autocorrelation, more specifically Moran’s I, through the lens of self-information (also known as surprisal) and provide both formal proofs and experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18459&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhangyu Wang, Krzysztof Janowicz, Gengchen Mai, Ivan Majic</name></author><category term="stat.ME" /><summary type="html">Intuitively, there is a relation between measures of spatial dependence and information theoretical measures of entropy. For instance, we can provide an intuition of why spatial data is special by stating that, on average, spatial data samples contain less than expected information. Similarly, spatial data, e.g., remotely sensed imagery, that is easy to compress is also likely to show significant spatial autocorrelation. Formulating our (highly specific) core concepts of spatial information theory in the widely used language of information theory opens new perspectives on their differences and similarities and also fosters cross-disciplinary collaboration, e.g., with the broader AI/ML communities. Interestingly, however, this intuitive relation is challenging to formalize and generalize, leading prior work to rely mostly on experimental results, e.g., for describing landscape patterns. In this work, we will explore the information theoretical roots of spatial autocorrelation, more specifically Moran’s I, through the lens of self-information (also known as surprisal) and provide both formal proofs and experiments.</summary></entry><entry><title type="html">Provable Contrastive Continual Learning</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ProvableContrastiveContinualLearning.html" rel="alternate" type="text/html" title="Provable Contrastive Continual Learning" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ProvableContrastiveContinualLearning</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ProvableContrastiveContinualLearning.html">&lt;p&gt;Continual learning requires learning incremental tasks with dynamic data distributions. So far, it has been observed that employing a combination of contrastive loss and distillation loss for training in continual learning yields strong performance. To the best of our knowledge, however, this contrastive continual learning framework lacks convincing theoretical explanations. In this work, we fill this gap by establishing theoretical performance guarantees, which reveal how the performance of the model is bounded by training losses of previous tasks in the contrastive continual learning framework. Our theoretical explanations further support the idea that pre-training can benefit continual learning. Inspired by our theoretical analysis of these guarantees, we propose a novel contrastive continual learning algorithm called CILA, which uses adaptive distillation coefficients for different tasks. These distillation coefficients are easily computed by the ratio between average distillation losses and average contrastive losses from previous tasks. Our method shows great improvement on standard benchmarks and achieves new state-of-the-art performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18756&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yichen Wen, Zhiquan Tan, Kaipeng Zheng, Chuanlong Xie, Weiran Huang</name></author><category term="stat.AP," /><category term="stat.ML" /><summary type="html">Continual learning requires learning incremental tasks with dynamic data distributions. So far, it has been observed that employing a combination of contrastive loss and distillation loss for training in continual learning yields strong performance. To the best of our knowledge, however, this contrastive continual learning framework lacks convincing theoretical explanations. In this work, we fill this gap by establishing theoretical performance guarantees, which reveal how the performance of the model is bounded by training losses of previous tasks in the contrastive continual learning framework. Our theoretical explanations further support the idea that pre-training can benefit continual learning. Inspired by our theoretical analysis of these guarantees, we propose a novel contrastive continual learning algorithm called CILA, which uses adaptive distillation coefficients for different tasks. These distillation coefficients are easily computed by the ratio between average distillation losses and average contrastive losses from previous tasks. Our method shows great improvement on standard benchmarks and achieves new state-of-the-art performance.</summary></entry><entry><title type="html">Sampling metastable systems using collective variables and Jarzynski-Crooks paths</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/SamplingmetastablesystemsusingcollectivevariablesandJarzynskiCrookspaths.html" rel="alternate" type="text/html" title="Sampling metastable systems using collective variables and Jarzynski-Crooks paths" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/SamplingmetastablesystemsusingcollectivevariablesandJarzynskiCrookspaths</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/SamplingmetastablesystemsusingcollectivevariablesandJarzynskiCrookspaths.html">&lt;p&gt;We consider the problem of sampling a high dimensional multimodal target probability measure. We assume that a good proposal kernel to move only a subset of the degrees of freedoms (also known as collective variables) is known a priori. This proposal kernel can for example be built using normalizing flows. We show how to extend the move from the collective variable space to the full space and how to implement an accept-reject step in order to get a reversible chain with respect to a target probability measure. The accept-reject step does not require to know the marginal of the original measure in the collective variable (namely to know the free energy). The obtained algorithm admits several variants, some of them being very close to methods which have been proposed previously in the literature. We show how the obtained acceptance ratio can be expressed in terms of the work which appears in the Jarzynski-Crooks equality, at least for some variants. Numerical illustrations demonstrate the efficiency of the approach on various simple test cases, and allow us to compare the variants of the algorithm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18160&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Christoph Schönle, Marylou Gabrié, Tony Lelièvre, Gabriel Stoltz</name></author><category term="cond-mat.stat-mech," /><category term="stat.CO" /><summary type="html">We consider the problem of sampling a high dimensional multimodal target probability measure. We assume that a good proposal kernel to move only a subset of the degrees of freedoms (also known as collective variables) is known a priori. This proposal kernel can for example be built using normalizing flows. We show how to extend the move from the collective variable space to the full space and how to implement an accept-reject step in order to get a reversible chain with respect to a target probability measure. The accept-reject step does not require to know the marginal of the original measure in the collective variable (namely to know the free energy). The obtained algorithm admits several variants, some of them being very close to methods which have been proposed previously in the literature. We show how the obtained acceptance ratio can be expressed in terms of the work which appears in the Jarzynski-Crooks equality, at least for some variants. Numerical illustrations demonstrate the efficiency of the approach on various simple test cases, and allow us to compare the variants of the algorithm.</summary></entry><entry><title type="html">Synthetic Potential Outcomes for Mixtures of Treatment Effects</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/SyntheticPotentialOutcomesforMixturesofTreatmentEffects.html" rel="alternate" type="text/html" title="Synthetic Potential Outcomes for Mixtures of Treatment Effects" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/SyntheticPotentialOutcomesforMixturesofTreatmentEffects</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/SyntheticPotentialOutcomesforMixturesofTreatmentEffects.html">&lt;p&gt;Modern data analysis frequently relies on the use of large datasets, often constructed as amalgamations of diverse populations or data-sources. Heterogeneity across these smaller datasets constitutes two major challenges for causal inference: (1) the source of each sample can introduce latent confounding between treatment and effect, and (2) diverse populations may respond differently to the same treatment, giving rise to heterogeneous treatment effects (HTEs). The issues of latent confounding and HTEs have been studied separately but not in conjunction. In particular, previous works only report the conditional average treatment effect (CATE) among similar individuals (with respect to the measured covariates). CATEs cannot resolve mixtures of potential treatment effects driven by latent heterogeneity, which we call mixtures of treatment effects (MTEs). Inspired by method of moment approaches to mixture models, we propose “synthetic potential outcomes” (SPOs). Our new approach deconfounds heterogeneity while also guaranteeing the identifiability of MTEs. This technique bypasses full recovery of a mixture, which significantly simplifies its requirements for identifiability. We demonstrate the efficacy of SPOs on synthetic data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.19225&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Bijan Mazaheri, Chandler Squires, Caroline Uhler</name></author><category term="stat.ME" /><summary type="html">Modern data analysis frequently relies on the use of large datasets, often constructed as amalgamations of diverse populations or data-sources. Heterogeneity across these smaller datasets constitutes two major challenges for causal inference: (1) the source of each sample can introduce latent confounding between treatment and effect, and (2) diverse populations may respond differently to the same treatment, giving rise to heterogeneous treatment effects (HTEs). The issues of latent confounding and HTEs have been studied separately but not in conjunction. In particular, previous works only report the conditional average treatment effect (CATE) among similar individuals (with respect to the measured covariates). CATEs cannot resolve mixtures of potential treatment effects driven by latent heterogeneity, which we call mixtures of treatment effects (MTEs). Inspired by method of moment approaches to mixture models, we propose “synthetic potential outcomes” (SPOs). Our new approach deconfounds heterogeneity while also guaranteeing the identifiability of MTEs. This technique bypasses full recovery of a mixture, which significantly simplifies its requirements for identifiability. We demonstrate the efficacy of SPOs on synthetic data.</summary></entry><entry><title type="html">The Causal Roadmap and Simulations to Improve the Rigor and Reproducibility of Real-Data Applications</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TheCausalRoadmapandSimulationstoImprovetheRigorandReproducibilityofRealDataApplications.html" rel="alternate" type="text/html" title="The Causal Roadmap and Simulations to Improve the Rigor and Reproducibility of Real-Data Applications" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TheCausalRoadmapandSimulationstoImprovetheRigorandReproducibilityofRealDataApplications</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TheCausalRoadmapandSimulationstoImprovetheRigorandReproducibilityofRealDataApplications.html">&lt;p&gt;The Causal Roadmap outlines a systematic approach to asking and answering questions of cause-and-effect: define the quantity of interest, evaluate needed assumptions, conduct statistical estimation, and carefully interpret results. To protect research integrity, it is essential that the algorithm for statistical estimation and inference be pre-specified prior to conducting any effectiveness analyses. However, it is often unclear which algorithm will perform optimally for the real-data application. Instead, there is a temptation to simply implement one’s favorite algorithm – recycling prior code or relying on the default settings of a computing package. Here, we call for the use of simulations that realistically reflect the application, including key characteristics such as strong confounding and dependent or missing outcomes, to objectively compare candidate estimators and facilitate full specification of the Statistical Analysis Plan. Such simulations are informed by the Causal Roadmap and conducted after data collection but prior to effect estimation. We illustrate with two worked examples. First, in an observational longitudinal study, outcome-blind simulations are used to inform nuisance parameter estimation and variance estimation for longitudinal targeted minimum loss-based estimation (TMLE). Second, in a cluster randomized trial with missing outcomes, treatment-blind simulations are used to examine Type-I error control in Two-Stage TMLE. In both examples, realistic simulations empower us to pre-specify an estimation approach that is expected to have strong finite sample performance and also yield quality-controlled computing code for the actual analysis. Together, this process helps to improve the rigor and reproducibility of our research.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.03952&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nerissa Nance, Maya L. Petersen, Mark van der Laan, Laura B. Balzer</name></author><category term="stat.ME" /><summary type="html">The Causal Roadmap outlines a systematic approach to asking and answering questions of cause-and-effect: define the quantity of interest, evaluate needed assumptions, conduct statistical estimation, and carefully interpret results. To protect research integrity, it is essential that the algorithm for statistical estimation and inference be pre-specified prior to conducting any effectiveness analyses. However, it is often unclear which algorithm will perform optimally for the real-data application. Instead, there is a temptation to simply implement one’s favorite algorithm – recycling prior code or relying on the default settings of a computing package. Here, we call for the use of simulations that realistically reflect the application, including key characteristics such as strong confounding and dependent or missing outcomes, to objectively compare candidate estimators and facilitate full specification of the Statistical Analysis Plan. Such simulations are informed by the Causal Roadmap and conducted after data collection but prior to effect estimation. We illustrate with two worked examples. First, in an observational longitudinal study, outcome-blind simulations are used to inform nuisance parameter estimation and variance estimation for longitudinal targeted minimum loss-based estimation (TMLE). Second, in a cluster randomized trial with missing outcomes, treatment-blind simulations are used to examine Type-I error control in Two-Stage TMLE. In both examples, realistic simulations empower us to pre-specify an estimation approach that is expected to have strong finite sample performance and also yield quality-controlled computing code for the actual analysis. Together, this process helps to improve the rigor and reproducibility of our research.</summary></entry><entry><title type="html">The Importance of Discussing Assumptions when Teaching Bootstrapping</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TheImportanceofDiscussingAssumptionswhenTeachingBootstrapping.html" rel="alternate" type="text/html" title="The Importance of Discussing Assumptions when Teaching Bootstrapping" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TheImportanceofDiscussingAssumptionswhenTeachingBootstrapping</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TheImportanceofDiscussingAssumptionswhenTeachingBootstrapping.html">&lt;p&gt;Bootstrapping and other resampling methods are increasingly appearing in the textbooks and curricula of courses that introduce undergraduate students to statistical methods. In order to teach the bootstrap well, students and instructors need to be aware of the assumptions behind these intervals. In this article we discuss important assumptions about simple non-parametric bootstrap intervals and their corresponding hypothesis tests. We present simulations that instructors can use to help students understand some of the assumptions behind these methods. The simulations will be especially relevant to instructors who desire to increase accessibility for students from non-mathematical backgrounds, including those with math anxiety.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.07737&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Njesa Totty , James Molyneux , Claudio Fuentes</name></author><category term="stat.OT," /><category term="stat.CO" /><summary type="html">Bootstrapping and other resampling methods are increasingly appearing in the textbooks and curricula of courses that introduce undergraduate students to statistical methods. In order to teach the bootstrap well, students and instructors need to be aware of the assumptions behind these intervals. In this article we discuss important assumptions about simple non-parametric bootstrap intervals and their corresponding hypothesis tests. We present simulations that instructors can use to help students understand some of the assumptions behind these methods. The simulations will be especially relevant to instructors who desire to increase accessibility for students from non-mathematical backgrounds, including those with math anxiety.</summary></entry><entry><title type="html">The Multi-Range Theory of Translation Quality Measurement: MQM scoring models and Statistical Quality Control</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TheMultiRangeTheoryofTranslationQualityMeasurementMQMscoringmodelsandStatisticalQualityControl.html" rel="alternate" type="text/html" title="The Multi-Range Theory of Translation Quality Measurement: MQM scoring models and Statistical Quality Control" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TheMultiRangeTheoryofTranslationQualityMeasurementMQMscoringmodelsandStatisticalQualityControl</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TheMultiRangeTheoryofTranslationQualityMeasurementMQMscoringmodelsandStatisticalQualityControl.html">&lt;p&gt;The year 2024 marks the 10th anniversary of the Multidimensional Quality Metrics (MQM) framework for analytic translation quality evaluation. The MQM error typology has been widely used by practitioners in the translation and localization industry and has served as the basis for many derivative projects. The annual Conference on Machine Translation (WMT) shared tasks on both human and automatic translation quality evaluations used the MQM error typology.
  The metric stands on two pillars: error typology and the scoring model. The scoring model calculates the quality score from annotation data, detailing how to convert error type and severity counts into numeric scores to determine if the content meets specifications. Previously, only the raw scoring model had been published. This April, the MQM Council published the Linear Calibrated Scoring Model, officially presented herein, along with the Non-Linear Scoring Model, which had not been published before.
  This paper details the latest MQM developments and presents a universal approach to translation quality measurement across three sample size ranges. It also explains why Statistical Quality Control should be used for very small sample sizes, starting from a single sentence.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.16969&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arle Lommel, Serge Gladkoff, Alan Melby, Sue Ellen Wright, Ingemar Strandvik, Katerina Gasova, Angelika Vaasa, Andy Benzo, Romina Marazzato Sparano, Monica Foresi, Johani Innis, Lifeng Han, Goran Nenadic</name></author><category term="stat.AP" /><summary type="html">The year 2024 marks the 10th anniversary of the Multidimensional Quality Metrics (MQM) framework for analytic translation quality evaluation. The MQM error typology has been widely used by practitioners in the translation and localization industry and has served as the basis for many derivative projects. The annual Conference on Machine Translation (WMT) shared tasks on both human and automatic translation quality evaluations used the MQM error typology. The metric stands on two pillars: error typology and the scoring model. The scoring model calculates the quality score from annotation data, detailing how to convert error type and severity counts into numeric scores to determine if the content meets specifications. Previously, only the raw scoring model had been published. This April, the MQM Council published the Linear Calibrated Scoring Model, officially presented herein, along with the Non-Linear Scoring Model, which had not been published before. This paper details the latest MQM developments and presents a universal approach to translation quality measurement across three sample size ranges. It also explains why Statistical Quality Control should be used for very small sample sizes, starting from a single sentence.</summary></entry><entry><title type="html">Towards a Low-SWaP 1024-beam Digital Array: A 32-beam Sub-system at 5.8 GHz</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TowardsaLowSWaP1024beamDigitalArrayA32beamSubsystemat58GHz.html" rel="alternate" type="text/html" title="Towards a Low-SWaP 1024-beam Digital Array: A 32-beam Sub-system at 5.8 GHz" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TowardsaLowSWaP1024beamDigitalArrayA32beamSubsystemat58GHz</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TowardsaLowSWaP1024beamDigitalArrayA32beamSubsystemat58GHz.html">&lt;p&gt;Millimeter wave communications require multibeam beamforming in order to utilize wireless channels that suffer from obstructions, path loss, and multi-path effects. Digital multibeam beamforming has maximum degrees of freedom compared to analog phased arrays. However, circuit complexity and power consumption are important constraints for digital multibeam systems. A low-complexity digital computing architecture is proposed for a multiplication-free 32-point linear transform that approximates multiple simultaneous RF beams similar to a discrete Fourier transform (DFT). Arithmetic complexity due to multiplication is reduced from the FFT complexity of $\mathcal{O}(N: \log N)$ for DFT realizations, down to zero, thus yielding a 46% and 55% reduction in chip area and dynamic power consumption, respectively, for the $N=32$ case considered. The paper describes the proposed 32-point DFT approximation targeting a 1024-beams using a 2D array, and shows the multiplierless approximation and its mapping to a 32-beam sub-system consisting of 5.8 GHz antennas that can be used for generating 1024 digital beams without multiplications. Real-time beam computation is achieved using a Xilinx FPGA at 120 MHz bandwidth per beam. Theoretical beam performance is compared with measured RF patterns from both a fixed-point FFT as well as the proposed multiplier-free algorithm and are in good agreement.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2207.09054&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Arjuna Madanayake, Viduneth Ariyarathna, Suresh Madishetty, Sravan Pulipati, R. J. Cintra, Diego Coelho, Raíza Oliveira, Fábio M. Bayer, Leonid Belostotski, Soumyajit Mandal, Theodore S. Rappaport</name></author><category term="stat.ME" /><summary type="html">Millimeter wave communications require multibeam beamforming in order to utilize wireless channels that suffer from obstructions, path loss, and multi-path effects. Digital multibeam beamforming has maximum degrees of freedom compared to analog phased arrays. However, circuit complexity and power consumption are important constraints for digital multibeam systems. A low-complexity digital computing architecture is proposed for a multiplication-free 32-point linear transform that approximates multiple simultaneous RF beams similar to a discrete Fourier transform (DFT). Arithmetic complexity due to multiplication is reduced from the FFT complexity of $\mathcal{O}(N: \log N)$ for DFT realizations, down to zero, thus yielding a 46% and 55% reduction in chip area and dynamic power consumption, respectively, for the $N=32$ case considered. The paper describes the proposed 32-point DFT approximation targeting a 1024-beams using a 2D array, and shows the multiplierless approximation and its mapping to a 32-beam sub-system consisting of 5.8 GHz antennas that can be used for generating 1024 digital beams without multiplications. Real-time beam computation is achieved using a Xilinx FPGA at 120 MHz bandwidth per beam. Theoretical beam performance is compared with measured RF patterns from both a fixed-point FFT as well as the proposed multiplier-free algorithm and are in good agreement.</summary></entry><entry><title type="html">Transmission Channel Analysis in Dynamic Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TransmissionChannelAnalysisinDynamicModels.html" rel="alternate" type="text/html" title="Transmission Channel Analysis in Dynamic Models" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TransmissionChannelAnalysisinDynamicModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/TransmissionChannelAnalysisinDynamicModels.html">&lt;p&gt;We propose a framework for the analysis of transmission channels in a large class of dynamic models. To this end, we formulate our approach both using graph theory and potential outcomes, which we show to be equivalent. Our method, labelled Transmission Channel Analysis (TCA), allows for the decomposition of total effects captured by impulse response functions into the effects flowing along transmission channels, thereby providing a quantitative assessment of the strength of various transmission channels. We establish that this requires no additional identification assumptions beyond the identification of the structural shock whose effects the researcher wants to decompose. Additionally, we prove that impulse response functions are sufficient statistics for the computation of transmission effects. We also demonstrate the empirical relevance of TCA for policy evaluation by decomposing the effects of various monetary policy shock measures into instantaneous implementation effects and effects that likely relate to forward guidance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18987&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Enrico Wegner, Lenard Lieb, Stephan Smeekes, Ines Wilms</name></author><category term="stat.ME" /><summary type="html">We propose a framework for the analysis of transmission channels in a large class of dynamic models. To this end, we formulate our approach both using graph theory and potential outcomes, which we show to be equivalent. Our method, labelled Transmission Channel Analysis (TCA), allows for the decomposition of total effects captured by impulse response functions into the effects flowing along transmission channels, thereby providing a quantitative assessment of the strength of various transmission channels. We establish that this requires no additional identification assumptions beyond the identification of the structural shock whose effects the researcher wants to decompose. Additionally, we prove that impulse response functions are sufficient statistics for the computation of transmission effects. We also demonstrate the empirical relevance of TCA for policy evaluation by decomposing the effects of various monetary policy shock measures into instantaneous implementation effects and effects that likely relate to forward guidance.</summary></entry><entry><title type="html">Visibility graph-based covariance functions for scalable spatial analysis in non-convex domains</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Visibilitygraphbasedcovariancefunctionsforscalablespatialanalysisinnonconvexdomains.html" rel="alternate" type="text/html" title="Visibility graph-based covariance functions for scalable spatial analysis in non-convex domains" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Visibilitygraphbasedcovariancefunctionsforscalablespatialanalysisinnonconvexdomains</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/Visibilitygraphbasedcovariancefunctionsforscalablespatialanalysisinnonconvexdomains.html">&lt;p&gt;We present a new method for constructing valid covariance functions of Gaussian processes for spatial analysis in irregular, non-convex domains such as bodies of water. Standard covariance functions based on geodesic distances are not guaranteed to be positive definite on such domains, while existing non-Euclidean approaches fail to respect the partially Euclidean nature of these domains where the geodesic distance agrees with the Euclidean distances for some pairs of points. Using a visibility graph on the domain, we propose a class of covariance functions that preserve Euclidean-based covariances between points that are connected in the domain while incorporating the non-convex geometry of the domain via conditional independence relationships. We show that the proposed method preserves the partially Euclidean nature of the intrinsic geometry on the domain while maintaining validity (positive definiteness) and marginal stationarity of the covariance function over the entire parameter space, properties which are not always fulfilled by existing approaches to construct covariance functions on non-convex domains. We provide useful approximations to improve computational efficiency, resulting in a scalable algorithm. We compare the performance of our method with those of competing state-of-the-art methods using simulation studies on synthetic non-convex domains. The method is applied to data regarding acidity levels in the Chesapeake Bay, showing its potential for ecological monitoring in real-world spatial applications on irregular domains.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.11941&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Brian Gilbert, Abhirup Datta</name></author><category term="stat.ME" /><summary type="html">We present a new method for constructing valid covariance functions of Gaussian processes for spatial analysis in irregular, non-convex domains such as bodies of water. Standard covariance functions based on geodesic distances are not guaranteed to be positive definite on such domains, while existing non-Euclidean approaches fail to respect the partially Euclidean nature of these domains where the geodesic distance agrees with the Euclidean distances for some pairs of points. Using a visibility graph on the domain, we propose a class of covariance functions that preserve Euclidean-based covariances between points that are connected in the domain while incorporating the non-convex geometry of the domain via conditional independence relationships. We show that the proposed method preserves the partially Euclidean nature of the intrinsic geometry on the domain while maintaining validity (positive definiteness) and marginal stationarity of the covariance function over the entire parameter space, properties which are not always fulfilled by existing approaches to construct covariance functions on non-convex domains. We provide useful approximations to improve computational efficiency, resulting in a scalable algorithm. We compare the performance of our method with those of competing state-of-the-art methods using simulation studies on synthetic non-convex domains. The method is applied to data regarding acidity levels in the Chesapeake Bay, showing its potential for ecological monitoring in real-world spatial applications on irregular domains.</summary></entry><entry><title type="html">Watermarking Counterfactual Explanations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/WatermarkingCounterfactualExplanations.html" rel="alternate" type="text/html" title="Watermarking Counterfactual Explanations" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/WatermarkingCounterfactualExplanations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/WatermarkingCounterfactualExplanations.html">&lt;p&gt;The field of Explainable Artificial Intelligence (XAI) focuses on techniques for providing explanations to end-users about the decision-making processes that underlie modern-day machine learning (ML) models. Within the vast universe of XAI techniques, counterfactual (CF) explanations are often preferred by end-users as they help explain the predictions of ML models by providing an easy-to-understand &amp;amp; actionable recourse (or contrastive) case to individual end-users who are adversely impacted by predicted outcomes. However, recent studies have shown significant security concerns with using CF explanations in real-world applications; in particular, malicious adversaries can exploit CF explanations to perform query-efficient model extraction attacks on proprietary ML models. In this paper, we propose a model-agnostic watermarking framework (for adding watermarks to CF explanations) that can be leveraged to detect unauthorized model extraction attacks (which rely on the watermarked CF explanations). Our novel framework solves a bi-level optimization problem to embed an indistinguishable watermark into the generated CF explanation such that any future model extraction attacks that rely on these watermarked CF explanations can be detected using a null hypothesis significance testing (NHST) scheme, while ensuring that these embedded watermarks do not compromise the quality of the generated CF explanations. We evaluate this framework’s performance across a diverse set of real-world datasets, CF explanation methods, and model extraction techniques, and show that our watermarking detection system can be used to accurately identify extracted ML models that are trained using the watermarked CF explanations. Our work paves the way for the secure adoption of CF explanations in real-world applications.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.18671&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hangzhi Guo, Amulya Yadav</name></author><category term="stat.ME" /><summary type="html">The field of Explainable Artificial Intelligence (XAI) focuses on techniques for providing explanations to end-users about the decision-making processes that underlie modern-day machine learning (ML) models. Within the vast universe of XAI techniques, counterfactual (CF) explanations are often preferred by end-users as they help explain the predictions of ML models by providing an easy-to-understand &amp;amp; actionable recourse (or contrastive) case to individual end-users who are adversely impacted by predicted outcomes. However, recent studies have shown significant security concerns with using CF explanations in real-world applications; in particular, malicious adversaries can exploit CF explanations to perform query-efficient model extraction attacks on proprietary ML models. In this paper, we propose a model-agnostic watermarking framework (for adding watermarks to CF explanations) that can be leveraged to detect unauthorized model extraction attacks (which rely on the watermarked CF explanations). Our novel framework solves a bi-level optimization problem to embed an indistinguishable watermark into the generated CF explanation such that any future model extraction attacks that rely on these watermarked CF explanations can be detected using a null hypothesis significance testing (NHST) scheme, while ensuring that these embedded watermarks do not compromise the quality of the generated CF explanations. We evaluate this framework’s performance across a diverse set of real-world datasets, CF explanation methods, and model extraction techniques, and show that our watermarking detection system can be used to accurately identify extracted ML models that are trained using the watermarked CF explanations. Our work paves the way for the secure adoption of CF explanations in real-world applications.</summary></entry><entry><title type="html">nhppp: Simulating Nonhomogeneous Poisson Point Processes in R</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/nhpppSimulatingNonhomogeneousPoissonPointProcessesinR.html" rel="alternate" type="text/html" title="nhppp: Simulating Nonhomogeneous Poisson Point Processes in R" /><published>2024-05-30T00:00:00+00:00</published><updated>2024-05-30T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/nhpppSimulatingNonhomogeneousPoissonPointProcessesinR</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/nhpppSimulatingNonhomogeneousPoissonPointProcessesinR.html">&lt;p&gt;We introduce the `nhppp’ package for simulating events from one-dimensional non-homogeneous Poisson point processes (NHPPPs) in R fast and with a small memory footprint. We developed it to facilitate the sampling of event times in discrete event and statistical simulations. The package’s functions are based on three algorithms that provably sample from a target NHPPP: the time-transformation of a homogeneous Poisson process (of intensity one) via the inverse of the integrated intensity function; the generation of a Poisson number of order statistics from a fixed density function; and the thinning of a majorizing NHPPP via an acceptance-rejection scheme. We present a study of numerical accuracy and time performance of the algorithms. We illustrate use with simple reproducible examples.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.00358&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Thomas A. Trikalinos, Yuliia Sereda</name></author><category term="stat.CO," /><category term="stat.ME" /><summary type="html">We introduce the `nhppp’ package for simulating events from one-dimensional non-homogeneous Poisson point processes (NHPPPs) in R fast and with a small memory footprint. We developed it to facilitate the sampling of event times in discrete event and statistical simulations. The package’s functions are based on three algorithms that provably sample from a target NHPPP: the time-transformation of a homogeneous Poisson process (of intensity one) via the inverse of the integrated intensity function; the generation of a Poisson number of order statistics from a fixed density function; and the thinning of a majorizing NHPPP via an acceptance-rejection scheme. We present a study of numerical accuracy and time performance of the algorithms. We illustrate use with simple reproducible examples.</summary></entry></feed>