<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-04-29T10:28:19+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Novel Context driven Critical Integrative Levels (CIL) Approach: Advancing Human-Centric and Integrative Lighting Asset Management in Public Libraries with Practical Thresholds</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/ANovelContextdrivenCriticalIntegrativeLevelsCILApproachAdvancingHumanCentricandIntegrativeLightingAssetManagementinPublicLibrarieswithPracticalThresholds.html" rel="alternate" type="text/html" title="A Novel Context driven Critical Integrative Levels (CIL) Approach: Advancing Human-Centric and Integrative Lighting Asset Management in Public Libraries with Practical Thresholds" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/ANovelContextdrivenCriticalIntegrativeLevelsCILApproachAdvancingHumanCentricandIntegrativeLightingAssetManagementinPublicLibrarieswithPracticalThresholds</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/ANovelContextdrivenCriticalIntegrativeLevelsCILApproachAdvancingHumanCentricandIntegrativeLightingAssetManagementinPublicLibrarieswithPracticalThresholds.html">&lt;p&gt;This paper proposes the context driven Critical Integrative Levels (CIL), a novel approach to lighting asset management in public libraries that aligns with the transformative vision of human-centric and integrative lighting. This approach encompasses not only the visual aspects of lighting performance but also prioritizes the physiological and psychological well-being of library users. Incorporating a newly defined metric, Mean Time of Exposure (MTOE), the approach quantifies user-light interaction, enabling tailored lighting strategies that respond to diverse activities and needs in library spaces. Case studies demonstrate how the CIL matrix can be practically applied, offering significant improvements over conventional methods by focusing on optimized user experiences from both visual impacts and non-visual effects.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17554&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jing Lin, Nina Mylly, Per Olof Hedekvist, Jingchun Shen</name></author><category term="stat.AP" /><summary type="html">This paper proposes the context driven Critical Integrative Levels (CIL), a novel approach to lighting asset management in public libraries that aligns with the transformative vision of human-centric and integrative lighting. This approach encompasses not only the visual aspects of lighting performance but also prioritizes the physiological and psychological well-being of library users. Incorporating a newly defined metric, Mean Time of Exposure (MTOE), the approach quantifies user-light interaction, enabling tailored lighting strategies that respond to diverse activities and needs in library spaces. Case studies demonstrate how the CIL matrix can be practically applied, offering significant improvements over conventional methods by focusing on optimized user experiences from both visual impacts and non-visual effects.</summary></entry><entry><title type="html">A Weibull Mixture Cure Frailty Model for High-dimensional Covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AWeibullMixtureCureFrailtyModelforHighdimensionalCovariates.html" rel="alternate" type="text/html" title="A Weibull Mixture Cure Frailty Model for High-dimensional Covariates" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AWeibullMixtureCureFrailtyModelforHighdimensionalCovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AWeibullMixtureCureFrailtyModelforHighdimensionalCovariates.html">&lt;p&gt;A novel mixture cure frailty model is introduced for handling censored survival data. Mixture cure models are preferable when the existence of a cured fraction among patients can be assumed. However, such models are heavily underexplored: frailty structures within cure models remain largely undeveloped, and furthermore, most existing methods do not work for high-dimensional datasets, when the number of predictors is significantly larger than the number of observations. In this study, we introduce a novel extension of the Weibull mixture cure model that incorporates a frailty component, employed to model an underlying latent population heterogeneity with respect to the outcome risk. Additionally, high-dimensional covariates are integrated into both the cure rate and survival part of the model, providing a comprehensive approach to employ the model in the context of high-dimensional omics data. We also perform variable selection via an adaptive elastic-net penalization, and propose a novel approach to inference using the expectation-maximization (EM) algorithm. Extensive simulation studies are conducted across various scenarios to demonstrate the performance of the model, and results indicate that our proposed method outperforms competitor models. We apply the novel approach to analyze RNAseq gene expression data from bulk breast cancer patients included in The Cancer Genome Atlas (TCGA) database. A set of prognostic biomarkers is then derived from selected genes, and subsequently validated via both functional enrichment analysis and comparison to the existing biological literature. Finally, a prognostic risk score index based on the identified biomarkers is proposed and validated by exploring the patients’ survival.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.06575&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fatih K{\i}z{\i}laslan, David Michael Swanson, Valeria Vitelli</name></author><category term="stat.ME," /><category term="stat.AP," /><category term="stat.CO" /><summary type="html">A novel mixture cure frailty model is introduced for handling censored survival data. Mixture cure models are preferable when the existence of a cured fraction among patients can be assumed. However, such models are heavily underexplored: frailty structures within cure models remain largely undeveloped, and furthermore, most existing methods do not work for high-dimensional datasets, when the number of predictors is significantly larger than the number of observations. In this study, we introduce a novel extension of the Weibull mixture cure model that incorporates a frailty component, employed to model an underlying latent population heterogeneity with respect to the outcome risk. Additionally, high-dimensional covariates are integrated into both the cure rate and survival part of the model, providing a comprehensive approach to employ the model in the context of high-dimensional omics data. We also perform variable selection via an adaptive elastic-net penalization, and propose a novel approach to inference using the expectation-maximization (EM) algorithm. Extensive simulation studies are conducted across various scenarios to demonstrate the performance of the model, and results indicate that our proposed method outperforms competitor models. We apply the novel approach to analyze RNAseq gene expression data from bulk breast cancer patients included in The Cancer Genome Atlas (TCGA) database. A set of prognostic biomarkers is then derived from selected genes, and subsequently validated via both functional enrichment analysis and comparison to the existing biological literature. Finally, a prognostic risk score index based on the identified biomarkers is proposed and validated by exploring the patients’ survival.</summary></entry><entry><title type="html">A comparison of the discrimination performance of lasso and maximum likelihood estimation in logistic regression model</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Acomparisonofthediscriminationperformanceoflassoandmaximumlikelihoodestimationinlogisticregressionmodel.html" rel="alternate" type="text/html" title="A comparison of the discrimination performance of lasso and maximum likelihood estimation in logistic regression model" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Acomparisonofthediscriminationperformanceoflassoandmaximumlikelihoodestimationinlogisticregressionmodel</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Acomparisonofthediscriminationperformanceoflassoandmaximumlikelihoodestimationinlogisticregressionmodel.html">&lt;p&gt;Logistic regression is widely used in many areas of knowledge. Several works compare the performance of lasso and maximum likelihood estimation in logistic regression. However, part of these works do not perform simulation studies and the remaining ones do not consider scenarios in which the ratio of the number of covariates to sample size is high. In this work, we compare the discrimination performance of lasso and maximum likelihood estimation in logistic regression using simulation studies and applications. Variable selection is done both by lasso and by stepwise when maximum likelihood estimation is used. We consider a wide range of values for the ratio of the number of covariates to sample size. The main conclusion of the work is that lasso has a better discrimination performance than maximum likelihood estimation when the ratio of the number of covariates to sample size is high.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17482&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Gilberto P. Alcântara Junior, Gustavo H. A. Pereira</name></author><category term="stat.ME," /><category term="stat.CO" /><summary type="html">Logistic regression is widely used in many areas of knowledge. Several works compare the performance of lasso and maximum likelihood estimation in logistic regression. However, part of these works do not perform simulation studies and the remaining ones do not consider scenarios in which the ratio of the number of covariates to sample size is high. In this work, we compare the discrimination performance of lasso and maximum likelihood estimation in logistic regression using simulation studies and applications. Variable selection is done both by lasso and by stepwise when maximum likelihood estimation is used. We consider a wide range of values for the ratio of the number of covariates to sample size. The main conclusion of the work is that lasso has a better discrimination performance than maximum likelihood estimation when the ratio of the number of covariates to sample size is high.</summary></entry><entry><title type="html">A comprehensive survey of the home advantage in American football</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AcomprehensivesurveyofthehomeadvantageinAmericanfootball.html" rel="alternate" type="text/html" title="A comprehensive survey of the home advantage in American football" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AcomprehensivesurveyofthehomeadvantageinAmericanfootball</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AcomprehensivesurveyofthehomeadvantageinAmericanfootball.html">&lt;p&gt;The existence and justification to the home advantage – the benefit a sports team receives when playing at home – has been studied across sport. The majority of research on this topic is limited to individual leagues in short time frames, which hinders extrapolation and a deeper understanding of possible causes. Using nearly two decades of data from the National Football League (NFL), the National Collegiate Athletic Association (NCAA), and high schools from across the United States, we provide a uniform approach to understanding the home advantage in American football. Our findings suggest home advantage is declining in the NFL and the highest levels of collegiate football, but not in amateur football. This increases the possibility that characteristics of the NCAA and NFL, such as travel improvements and instant replay, have helped level the playing field.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.16392&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Luke S. Benz, Thompson J. Bliss, Michael J. Lopez</name></author><category term="stat.AP" /><summary type="html">The existence and justification to the home advantage – the benefit a sports team receives when playing at home – has been studied across sport. The majority of research on this topic is limited to individual leagues in short time frames, which hinders extrapolation and a deeper understanding of possible causes. Using nearly two decades of data from the National Football League (NFL), the National Collegiate Athletic Association (NCAA), and high schools from across the United States, we provide a uniform approach to understanding the home advantage in American football. Our findings suggest home advantage is declining in the NFL and the highest levels of collegiate football, but not in amateur football. This increases the possibility that characteristics of the NCAA and NFL, such as travel improvements and instant replay, have helped level the playing field.</summary></entry><entry><title type="html">An adaptive standardisation methodology for Day-Ahead electricity price forecasting</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AnadaptivestandardisationmethodologyforDayAheadelectricitypriceforecasting.html" rel="alternate" type="text/html" title="An adaptive standardisation methodology for Day-Ahead electricity price forecasting" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AnadaptivestandardisationmethodologyforDayAheadelectricitypriceforecasting</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AnadaptivestandardisationmethodologyforDayAheadelectricitypriceforecasting.html">&lt;p&gt;The study of Day-Ahead prices in the electricity market is one of the most popular problems in time series forecasting. Previous research has focused on employing increasingly complex learning algorithms to capture the sophisticated dynamics of the market. However, there is a threshold where increased complexity fails to yield substantial improvements. In this work, we propose an alternative approach by introducing an adaptive standardisation to mitigate the effects of dataset shifts that commonly occur in the market. By doing so, learning algorithms can prioritize uncovering the true relationship between the target variable and the explanatory variables. We investigate five distinct markets, including two novel datasets, previously unexplored in the literature. These datasets provide a more realistic representation of the current market context, that conventional datasets do not show. The results demonstrate a significant improvement across all five markets using the widely accepted learning algorithms in the literature (LEAR and DNN). In particular, the combination of the proposed methodology with the methodology previously presented in the literature obtains the best results. This significant advancement unveils new lines of research in this field, highlighting the potential of adaptive transformations in enhancing the performance of forecasting models.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.02610&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Carlos Sebastián, Carlos E. González-Guillén, Jesús Juan</name></author><category term="stat.AP," /><category term="stat.ME" /><summary type="html">The study of Day-Ahead prices in the electricity market is one of the most popular problems in time series forecasting. Previous research has focused on employing increasingly complex learning algorithms to capture the sophisticated dynamics of the market. However, there is a threshold where increased complexity fails to yield substantial improvements. In this work, we propose an alternative approach by introducing an adaptive standardisation to mitigate the effects of dataset shifts that commonly occur in the market. By doing so, learning algorithms can prioritize uncovering the true relationship between the target variable and the explanatory variables. We investigate five distinct markets, including two novel datasets, previously unexplored in the literature. These datasets provide a more realistic representation of the current market context, that conventional datasets do not show. The results demonstrate a significant improvement across all five markets using the widely accepted learning algorithms in the literature (LEAR and DNN). In particular, the combination of the proposed methodology with the methodology previously presented in the literature obtains the best results. This significant advancement unveils new lines of research in this field, highlighting the potential of adaptive transformations in enhancing the performance of forecasting models.</summary></entry><entry><title type="html">Assigning Stationary Distributions to Sparse Stochastic Matrices</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AssigningStationaryDistributionstoSparseStochasticMatrices.html" rel="alternate" type="text/html" title="Assigning Stationary Distributions to Sparse Stochastic Matrices" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AssigningStationaryDistributionstoSparseStochasticMatrices</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/AssigningStationaryDistributionstoSparseStochasticMatrices.html">&lt;p&gt;The target stationary distribution problem (TSDP) is the following: given an irreducible stochastic matrix $G$ and a target stationary distribution $\hat \mu$, construct a minimum norm perturbation, $\Delta$, such that $\hat G = G+\Delta$ is also stochastic and has the prescribed target stationary distribution, $\hat \mu$. In this paper, we revisit the TSDP under a constraint on the support of $\Delta$, that is, on the set of non-zero entries of $\Delta$. This is particularly meaningful in practice since one cannot typically modify all entries of $G$. We first show how to construct a feasible solution $\hat G$ that has essentially the same support as the matrix $G$. Then we show how to compute globally optimal and sparse solutions using the component-wise $\ell_1$ norm and linear optimization. We propose an efficient implementation that relies on a column-generation approach which allows us to solve sparse problems of size up to $10^5 \times 10^5$ in a few minutes. We illustrate the proposed algorithms with several numerical experiments.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.16011&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Nicolas Gillis, Paul Van Dooren</name></author><category term="stat.CO" /><summary type="html">The target stationary distribution problem (TSDP) is the following: given an irreducible stochastic matrix $G$ and a target stationary distribution $\hat \mu$, construct a minimum norm perturbation, $\Delta$, such that $\hat G = G+\Delta$ is also stochastic and has the prescribed target stationary distribution, $\hat \mu$. In this paper, we revisit the TSDP under a constraint on the support of $\Delta$, that is, on the set of non-zero entries of $\Delta$. This is particularly meaningful in practice since one cannot typically modify all entries of $G$. We first show how to construct a feasible solution $\hat G$ that has essentially the same support as the matrix $G$. Then we show how to compute globally optimal and sparse solutions using the component-wise $\ell_1$ norm and linear optimization. We propose an efficient implementation that relies on a column-generation approach which allows us to solve sparse problems of size up to $10^5 \times 10^5$ in a few minutes. We illustrate the proposed algorithms with several numerical experiments.</summary></entry><entry><title type="html">A unified framework for bounding causal effects on the always-survivor and other populations</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Aunifiedframeworkforboundingcausaleffectsonthealwayssurvivorandotherpopulations.html" rel="alternate" type="text/html" title="A unified framework for bounding causal effects on the always-survivor and other populations" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Aunifiedframeworkforboundingcausaleffectsonthealwayssurvivorandotherpopulations</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/Aunifiedframeworkforboundingcausaleffectsonthealwayssurvivorandotherpopulations.html">&lt;p&gt;We investigate the bounding problem of causal effects in experimental studies in which the outcome is truncated by death, meaning that the subject dies before the outcome can be measured. Causal effects cannot be point identified without instruments and/or tight parametric assumptions but can be bounded under mild restrictions. Previous work on partial identification under the principal stratification framework has primarily focused on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;always-survivor&apos; subpopulation. In this paper, we present a novel nonparametric unified framework to provide sharp bounds on causal effects on discrete and continuous square-integrable outcomes. These bounds are derived on the &lt;/code&gt;always-survivor’, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;protected&apos;, and &lt;/code&gt;harmed’ subpopulations and on the entire population with/without assumptions of monotonicity and stochastic dominance. The main idea depends on rewriting the optimization problem in terms of the integrated tail probability expectation formula using a set of conditional probability distributions. The proposed procedure allows for settings with any type and number of covariates, and can be extended to incorporate average causal effects and complier average causal effects. Furthermore, we present several simulation studies conducted under various assumptions as well as the application of the proposed approach to a real dataset from the National Supported Work Demonstration.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.13398&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Aixian Chen, Xia Cui, Guangren Yang</name></author><category term="stat.ME" /><summary type="html">We investigate the bounding problem of causal effects in experimental studies in which the outcome is truncated by death, meaning that the subject dies before the outcome can be measured. Causal effects cannot be point identified without instruments and/or tight parametric assumptions but can be bounded under mild restrictions. Previous work on partial identification under the principal stratification framework has primarily focused on the always-survivor&apos; subpopulation. In this paper, we present a novel nonparametric unified framework to provide sharp bounds on causal effects on discrete and continuous square-integrable outcomes. These bounds are derived on the always-survivor’, protected&apos;, and harmed’ subpopulations and on the entire population with/without assumptions of monotonicity and stochastic dominance. The main idea depends on rewriting the optimization problem in terms of the integrated tail probability expectation formula using a set of conditional probability distributions. The proposed procedure allows for settings with any type and number of covariates, and can be extended to incorporate average causal effects and complier average causal effects. Furthermore, we present several simulation studies conducted under various assumptions as well as the application of the proposed approach to a real dataset from the National Supported Work Demonstration.</summary></entry><entry><title type="html">Bayesian Federated Inference for Survival Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BayesianFederatedInferenceforSurvivalModels.html" rel="alternate" type="text/html" title="Bayesian Federated Inference for Survival Models" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BayesianFederatedInferenceforSurvivalModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BayesianFederatedInferenceforSurvivalModels.html">&lt;p&gt;In cancer research, overall survival and progression free survival are often analyzed with the Cox model. To estimate accurately the parameters in the model, sufficient data and, more importantly, sufficient events need to be observed. In practice, this is often a problem. Merging data sets from different medical centers may help, but this is not always possible due to strict privacy legislation and logistic difficulties. Recently, the Bayesian Federated Inference (BFI) strategy for generalized linear models was proposed. With this strategy the statistical analyses are performed in the local centers where the data were collected (or stored) and only the inference results are combined to a single estimated model; merging data is not necessary. The BFI methodology aims to compute from the separate inference results in the local centers what would have been obtained if the analysis had been based on the merged data sets. In this paper we generalize the BFI methodology as initially developed for generalized linear models to survival models. Simulation studies and real data analyses show excellent performance; i.e., the results obtained with the BFI methodology are very similar to the results obtained by analyzing the merged data. An R package for doing the analyses is available.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17464&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hassan Pazira, Emanuele Massa, Jetty AM Weijers, Anthony CC Coolen, Marianne A Jonker</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">In cancer research, overall survival and progression free survival are often analyzed with the Cox model. To estimate accurately the parameters in the model, sufficient data and, more importantly, sufficient events need to be observed. In practice, this is often a problem. Merging data sets from different medical centers may help, but this is not always possible due to strict privacy legislation and logistic difficulties. Recently, the Bayesian Federated Inference (BFI) strategy for generalized linear models was proposed. With this strategy the statistical analyses are performed in the local centers where the data were collected (or stored) and only the inference results are combined to a single estimated model; merging data is not necessary. The BFI methodology aims to compute from the separate inference results in the local centers what would have been obtained if the analysis had been based on the merged data sets. In this paper we generalize the BFI methodology as initially developed for generalized linear models to survival models. Simulation studies and real data analyses show excellent performance; i.e., the results obtained with the BFI methodology are very similar to the results obtained by analyzing the merged data. An R package for doing the analyses is available.</summary></entry><entry><title type="html">Bayesian Machine Learning meets Formal Methods: An application to spatio-temporal data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BayesianMachineLearningmeetsFormalMethodsAnapplicationtospatiotemporaldata.html" rel="alternate" type="text/html" title="Bayesian Machine Learning meets Formal Methods: An application to spatio-temporal data" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BayesianMachineLearningmeetsFormalMethodsAnapplicationtospatiotemporaldata</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BayesianMachineLearningmeetsFormalMethodsAnapplicationtospatiotemporaldata.html">&lt;p&gt;We propose an interdisciplinary framework that combines Bayesian predictive inference, a well-established tool in Machine Learning, with Formal Methods rooted in the computer science community. Bayesian predictive inference allows for coherently incorporating uncertainty about unknown quantities by making use of methods or models that produce predictive distributions, which in turn inform decision problems. By formalizing these decision problems into properties with the help of spatio-temporal logic, we can formulate and predict how likely such properties are to be satisfied in the future at a certain location. Moreover, we can leverage our methodology to evaluate and compare models directly on their ability to predict the satisfaction of application-driven properties. The approach is illustrated in an urban mobility application, where the crowdedness in the center of Milan is proxied by aggregated mobile phone traffic data. We specify several desirable spatio-temporal properties related to city crowdedness such as a fault-tolerant network or the reachability of hospitals. After verifying these properties on draws from the posterior predictive distributions, we compare several spatio-temporal Bayesian models based on their overall and property-based predictive performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2110.01360&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Laura Vana, Ennio Visconti, Laura Nenzi, Annalisa Cadonna, Gregor Kastner</name></author><category term="stat.CO" /><summary type="html">We propose an interdisciplinary framework that combines Bayesian predictive inference, a well-established tool in Machine Learning, with Formal Methods rooted in the computer science community. Bayesian predictive inference allows for coherently incorporating uncertainty about unknown quantities by making use of methods or models that produce predictive distributions, which in turn inform decision problems. By formalizing these decision problems into properties with the help of spatio-temporal logic, we can formulate and predict how likely such properties are to be satisfied in the future at a certain location. Moreover, we can leverage our methodology to evaluate and compare models directly on their ability to predict the satisfaction of application-driven properties. The approach is illustrated in an urban mobility application, where the crowdedness in the center of Milan is proxied by aggregated mobile phone traffic data. We specify several desirable spatio-temporal properties related to city crowdedness such as a fault-tolerant network or the reachability of hospitals. After verifying these properties on draws from the posterior predictive distributions, we compare several spatio-temporal Bayesian models based on their overall and property-based predictive performance.</summary></entry><entry><title type="html">Boosting e-BH via conditional calibration</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BoostingeBHviaconditionalcalibration.html" rel="alternate" type="text/html" title="Boosting e-BH via conditional calibration" /><published>2024-04-29T00:00:00+00:00</published><updated>2024-04-29T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BoostingeBHviaconditionalcalibration</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/04/29/BoostingeBHviaconditionalcalibration.html">&lt;p&gt;The e-BH procedure is an e-value-based multiple testing procedure that provably controls the false discovery rate (FDR) under any dependence structure between the e-values. Despite this appealing theoretical FDR control guarantee, the e-BH procedure often suffers from low power in practice. In this paper, we propose a general framework that boosts the power of e-BH without sacrificing its FDR control under arbitrary dependence. This is achieved by the technique of conditional calibration, where we take as input the e-values and calibrate them to be a set of “boosted e-values” that are guaranteed to be no less – and are often more – powerful than the original ones. Our general framework is explicitly instantiated in three classes of multiple testing problems: (1) testing under parametric models, (2) conditional independence testing under the model-X setting, and (3) model-free conformalized selection. Extensive numerical experiments show that our proposed method significantly improves the power of e-BH while continuing to control the FDR. We also demonstrate the effectiveness of our method through an application to an observational study dataset for identifying individuals whose counterfactuals satisfy certain properties.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2404.17562&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Junu Lee, Zhimei Ren</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">The e-BH procedure is an e-value-based multiple testing procedure that provably controls the false discovery rate (FDR) under any dependence structure between the e-values. Despite this appealing theoretical FDR control guarantee, the e-BH procedure often suffers from low power in practice. In this paper, we propose a general framework that boosts the power of e-BH without sacrificing its FDR control under arbitrary dependence. This is achieved by the technique of conditional calibration, where we take as input the e-values and calibrate them to be a set of “boosted e-values” that are guaranteed to be no less – and are often more – powerful than the original ones. Our general framework is explicitly instantiated in three classes of multiple testing problems: (1) testing under parametric models, (2) conditional independence testing under the model-X setting, and (3) model-free conformalized selection. Extensive numerical experiments show that our proposed method significantly improves the power of e-BH while continuing to control the FDR. We also demonstrate the effectiveness of our method through an application to an observational study dataset for identifying individuals whose counterfactuals satisfy certain properties.</summary></entry></feed>