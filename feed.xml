<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" rel="self" type="application/atom+xml" /><link href="https://emanuelealiverti.github.io/arxiv_rss/" rel="alternate" type="text/html" /><updated>2024-05-27T07:15:08+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/feed.xml</id><title type="html">Stat Arxiv of Today</title><subtitle></subtitle><author><name>Emanuele Aliverti</name></author><entry><title type="html">A Bayesian Approach to GRAPPA Parallel FMRI Image Reconstruction Increases SNR and Power of Task Detection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ABayesianApproachtoGRAPPAParallelFMRIImageReconstructionIncreasesSNRandPowerofTaskDetection.html" rel="alternate" type="text/html" title="A Bayesian Approach to GRAPPA Parallel FMRI Image Reconstruction Increases SNR and Power of Task Detection" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ABayesianApproachtoGRAPPAParallelFMRIImageReconstructionIncreasesSNRandPowerofTaskDetection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ABayesianApproachtoGRAPPAParallelFMRIImageReconstructionIncreasesSNRandPowerofTaskDetection.html">&lt;p&gt;In fMRI, capturing brain activation during a task is dependent on how quickly k-space arrays are obtained. Acquiring full k-space arrays, which are reconstructed into images using the inverse Fourier transform (IFT), that make up volume images can take a considerable amount of scan time. Under-sampling k-space reduces the acquisition time, but results in aliased, or “folded,” images. GeneRalized Autocalibrating Partial Parallel Acquisition (GRAPPA) is a parallel imaging technique that yields full images from subsampled arrays of k-space. GRAPPA uses localized interpolation weights, which are estimated per-scan and fixed over time, to fill in the missing spatial frequencies of the subsampled k-space. Hence, we propose a Bayesian approach to GRAPPA (BGRAPPA) where space measurement uncertainty are assessed from the a priori calibration k-space arrays. The prior information is utilized to estimate the missing spatial frequency values from the posterior distribution and reconstruct into full field-of-view images. Our BGRAPPA technique successfully reconstructed both a simulated and experimental single slice image with less artifacts, reduced noise leading to an increased signal-to-noise ratio (SNR), and stronger power of task detection.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15003&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chase J Sakitis, Daniel B Rowe</name></author><category term="stat.AP" /><summary type="html">In fMRI, capturing brain activation during a task is dependent on how quickly k-space arrays are obtained. Acquiring full k-space arrays, which are reconstructed into images using the inverse Fourier transform (IFT), that make up volume images can take a considerable amount of scan time. Under-sampling k-space reduces the acquisition time, but results in aliased, or “folded,” images. GeneRalized Autocalibrating Partial Parallel Acquisition (GRAPPA) is a parallel imaging technique that yields full images from subsampled arrays of k-space. GRAPPA uses localized interpolation weights, which are estimated per-scan and fixed over time, to fill in the missing spatial frequencies of the subsampled k-space. Hence, we propose a Bayesian approach to GRAPPA (BGRAPPA) where space measurement uncertainty are assessed from the a priori calibration k-space arrays. The prior information is utilized to estimate the missing spatial frequency values from the posterior distribution and reconstruct into full field-of-view images. Our BGRAPPA technique successfully reconstructed both a simulated and experimental single slice image with less artifacts, reduced noise leading to an increased signal-to-noise ratio (SNR), and stronger power of task detection.</summary></entry><entry><title type="html">A Latent Variable Approach to Learning High-dimensional Multivariate longitudinal Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ALatentVariableApproachtoLearningHighdimensionalMultivariatelongitudinalData.html" rel="alternate" type="text/html" title="A Latent Variable Approach to Learning High-dimensional Multivariate longitudinal Data" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ALatentVariableApproachtoLearningHighdimensionalMultivariatelongitudinalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ALatentVariableApproachtoLearningHighdimensionalMultivariatelongitudinalData.html">&lt;p&gt;High-dimensional multivariate longitudinal data, which arise when many outcome variables are measured repeatedly over time, are becoming increasingly common in social, behavioral and health sciences. We propose a latent variable model for drawing statistical inferences on covariate effects and predicting future outcomes based on high-dimensional multivariate longitudinal data. This model introduces unobserved factors to account for the between-variable and across-time dependence and assist the prediction. Statistical inference and prediction tools are developed under a general setting that allows outcome variables to be of mixed types and possibly unobserved for certain time points, for example, due to right censoring. A central limit theorem is established for drawing statistical inferences on regression coefficients. Additionally, an information criterion is introduced to choose the number of factors. The proposed model is applied to customer grocery shopping records to predict and understand shopping behavior.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15053&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sze Ming Lee, Yunxiao Chen, Tony Sit</name></author><category term="stat.ME" /><summary type="html">High-dimensional multivariate longitudinal data, which arise when many outcome variables are measured repeatedly over time, are becoming increasingly common in social, behavioral and health sciences. We propose a latent variable model for drawing statistical inferences on covariate effects and predicting future outcomes based on high-dimensional multivariate longitudinal data. This model introduces unobserved factors to account for the between-variable and across-time dependence and assist the prediction. Statistical inference and prediction tools are developed under a general setting that allows outcome variables to be of mixed types and possibly unobserved for certain time points, for example, due to right censoring. A central limit theorem is established for drawing statistical inferences on regression coefficients. Additionally, an information criterion is introduced to choose the number of factors. The proposed model is applied to customer grocery shopping records to predict and understand shopping behavior.</summary></entry><entry><title type="html">A New Fit Assessment Framework for Common Factor Models Using Generalized Residuals</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ANewFitAssessmentFrameworkforCommonFactorModelsUsingGeneralizedResiduals.html" rel="alternate" type="text/html" title="A New Fit Assessment Framework for Common Factor Models Using Generalized Residuals" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ANewFitAssessmentFrameworkforCommonFactorModelsUsingGeneralizedResiduals</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ANewFitAssessmentFrameworkforCommonFactorModelsUsingGeneralizedResiduals.html">&lt;p&gt;Standard common factor models, such as the linear normal factor model, rely on strict parametric assumptions, which require rigorous model-data fit assessment to prevent fallacious inferences. However, overall goodness-of-fit diagnostics conventionally used in factor analysis do not offer diagnostic information on where the misfit originates. In the current work, we propose a new fit assessment framework for common factor models by extending the theory of generalized residuals (Haberman &amp;amp; Sinharay, 2013). This framework allows for the flexible adaptation of test statistics to identify various sources of misfit. In addition, the resulting goodness-of-fit tests provide more informative diagnostics, as the evaluation is performed conditionally on latent variables. Several examples of test statistics suitable for assessing various model assumptions are presented within this framework, and their performance is evaluated by simulation studies and a real data example.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15204&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Youjin Sung, Youngjin Han, Yang Liu</name></author><category term="stat.ME" /><summary type="html">Standard common factor models, such as the linear normal factor model, rely on strict parametric assumptions, which require rigorous model-data fit assessment to prevent fallacious inferences. However, overall goodness-of-fit diagnostics conventionally used in factor analysis do not offer diagnostic information on where the misfit originates. In the current work, we propose a new fit assessment framework for common factor models by extending the theory of generalized residuals (Haberman &amp;amp; Sinharay, 2013). This framework allows for the flexible adaptation of test statistics to identify various sources of misfit. In addition, the resulting goodness-of-fit tests provide more informative diagnostics, as the evaluation is performed conditionally on latent variables. Several examples of test statistics suitable for assessing various model assumptions are presented within this framework, and their performance is evaluated by simulation studies and a real data example.</summary></entry><entry><title type="html">Adaptive probabilistic forecasting of French electricity spot prices</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/AdaptiveprobabilisticforecastingofFrenchelectricityspotprices.html" rel="alternate" type="text/html" title="Adaptive probabilistic forecasting of French electricity spot prices" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/AdaptiveprobabilisticforecastingofFrenchelectricityspotprices</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/AdaptiveprobabilisticforecastingofFrenchelectricityspotprices.html">&lt;p&gt;Electricity price forecasting (EPF) plays a major role for electricity companies as a fundamental entry for trading decisions or energy management operations. As electricity can not be stored, electricity prices are highly volatile which make EPF a particularly difficult task. This is all the more true when dramatic fortuitous events disrupt the markets. Trading and more generally energy management decisions require risk management tools which are based on probabilistic EPF (PEPF). In this challenging context, we argue in favor of the deployment of highly adaptive black-boxes strategies allowing to turn any forecasts into a robust adaptive predictive interval, such as conformal prediction and online aggregation, as a fundamental last layer of any operational pipeline.
  We propose to investigate a novel data set containing the French electricity spot prices during the turbulent 2020-2021 years, and build a new explanatory feature revealing high predictive power, namely the nuclear availability. Benchmarking state-of-the-art PEPF on this data set highlights the difficulty of choosing a given model, as they all behave very differently in practice, and none of them is reliable. However, we propose an adequate conformalisation, OSSCP-horizon, that improves the performances of PEPF methods, even in the most hazardous period of late 2021. Finally, we emphasize that combining it with online aggregation significantly outperforms any other approaches, and should be the preferred pipeline, as it provides trustworthy probabilistic forecasts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15359&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Grégoire Dutot, Margaux Zaffran, Olivier Féron, Yannig Goude</name></author><category term="stat.AP" /><summary type="html">Electricity price forecasting (EPF) plays a major role for electricity companies as a fundamental entry for trading decisions or energy management operations. As electricity can not be stored, electricity prices are highly volatile which make EPF a particularly difficult task. This is all the more true when dramatic fortuitous events disrupt the markets. Trading and more generally energy management decisions require risk management tools which are based on probabilistic EPF (PEPF). In this challenging context, we argue in favor of the deployment of highly adaptive black-boxes strategies allowing to turn any forecasts into a robust adaptive predictive interval, such as conformal prediction and online aggregation, as a fundamental last layer of any operational pipeline. We propose to investigate a novel data set containing the French electricity spot prices during the turbulent 2020-2021 years, and build a new explanatory feature revealing high predictive power, namely the nuclear availability. Benchmarking state-of-the-art PEPF on this data set highlights the difficulty of choosing a given model, as they all behave very differently in practice, and none of them is reliable. However, we propose an adequate conformalisation, OSSCP-horizon, that improves the performances of PEPF methods, even in the most hazardous period of late 2021. Finally, we emphasize that combining it with online aggregation significantly outperforms any other approaches, and should be the preferred pipeline, as it provides trustworthy probabilistic forecasts.</summary></entry><entry><title type="html">Addressing Duplicated Data in Point Process Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/AddressingDuplicatedDatainPointProcessModels.html" rel="alternate" type="text/html" title="Addressing Duplicated Data in Point Process Models" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/AddressingDuplicatedDatainPointProcessModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/AddressingDuplicatedDatainPointProcessModels.html">&lt;p&gt;Spatial point process models are widely applied to point pattern data from various fields in the social and environmental sciences. However, a serious hurdle in fitting point process models is the presence of duplicated points, wherein multiple observations share identical spatial coordinates. This often occurs because of decisions made in the geo-coding process, such as assigning representative locations (e.g., aggregate-level centroids) to observations when data producers lack exact location information. Because spatial point process models like the Log-Gaussian Cox Process (LGCP) assume unique locations, researchers often employ {\it ad hoc} solutions (e.g., jittering) to address duplicated data before analysis. As an alternative, this study proposes a Modified Minimum Contrast (MMC) method that adapts the inference procedure to account for the effect of duplicates without needing to alter the data. The proposed MMC method is applied to LGCP models, with simulation results demonstrating the gains of our method relative to existing approaches in terms of parameter estimation. Interestingly, simulation results also show the effect of the geo-coding process on parameter estimates, which can be utilized in the implementation of the MMC method. The MMC approach is then used to infer the spatial clustering characteristics of conflict events in Afghanistan (2008-2009).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15192&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Lingling Chen, Mikyoung Jun, Scott J. Cook</name></author><category term="stat.ME" /><summary type="html">Spatial point process models are widely applied to point pattern data from various fields in the social and environmental sciences. However, a serious hurdle in fitting point process models is the presence of duplicated points, wherein multiple observations share identical spatial coordinates. This often occurs because of decisions made in the geo-coding process, such as assigning representative locations (e.g., aggregate-level centroids) to observations when data producers lack exact location information. Because spatial point process models like the Log-Gaussian Cox Process (LGCP) assume unique locations, researchers often employ {\it ad hoc} solutions (e.g., jittering) to address duplicated data before analysis. As an alternative, this study proposes a Modified Minimum Contrast (MMC) method that adapts the inference procedure to account for the effect of duplicates without needing to alter the data. The proposed MMC method is applied to LGCP models, with simulation results demonstrating the gains of our method relative to existing approaches in terms of parameter estimation. Interestingly, simulation results also show the effect of the geo-coding process on parameter estimates, which can be utilized in the implementation of the MMC method. The MMC approach is then used to infer the spatial clustering characteristics of conflict events in Afghanistan (2008-2009).</summary></entry><entry><title type="html">A flexible class of priors for orthonormal matrices with basis function-specific structure</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Aflexibleclassofpriorsfororthonormalmatriceswithbasisfunctionspecificstructure.html" rel="alternate" type="text/html" title="A flexible class of priors for orthonormal matrices with basis function-specific structure" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Aflexibleclassofpriorsfororthonormalmatriceswithbasisfunctionspecificstructure</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Aflexibleclassofpriorsfororthonormalmatriceswithbasisfunctionspecificstructure.html">&lt;p&gt;Statistical modeling of high-dimensional matrix-valued data motivates the use of a low-rank representation that simultaneously summarizes key characteristics of the data and enables dimension reduction. Low-rank representations commonly factor the original data into the product of orthonormal basis functions and weights, where each basis function represents an independent feature of the data. However, the basis functions in these factorizations are typically computed using algorithmic methods that cannot quantify uncertainty or account for basis function correlation structure a priori. While there exist Bayesian methods that allow for a common correlation structure across basis functions, empirical examples motivate the need for basis function-specific dependence structure. We propose a prior distribution for orthonormal matrices that can explicitly model basis function-specific structure. The prior is used within a general probabilistic model for singular value decomposition to conduct posterior inference on the basis functions while accounting for measurement error and fixed effects. We discuss how the prior specification can be used for various scenarios and demonstrate favorable model properties through synthetic data examples. Finally, we apply our method to two-meter air temperature data from the Pacific Northwest, enhancing our understanding of the Earth system’s internal variability.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.13627&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joshua S. North, Mark D. Risser, F. Jay Breidt</name></author><category term="stat.ME" /><summary type="html">Statistical modeling of high-dimensional matrix-valued data motivates the use of a low-rank representation that simultaneously summarizes key characteristics of the data and enables dimension reduction. Low-rank representations commonly factor the original data into the product of orthonormal basis functions and weights, where each basis function represents an independent feature of the data. However, the basis functions in these factorizations are typically computed using algorithmic methods that cannot quantify uncertainty or account for basis function correlation structure a priori. While there exist Bayesian methods that allow for a common correlation structure across basis functions, empirical examples motivate the need for basis function-specific dependence structure. We propose a prior distribution for orthonormal matrices that can explicitly model basis function-specific structure. The prior is used within a general probabilistic model for singular value decomposition to conduct posterior inference on the basis functions while accounting for measurement error and fixed effects. We discuss how the prior specification can be used for various scenarios and demonstrate favorable model properties through synthetic data examples. Finally, we apply our method to two-meter air temperature data from the Pacific Northwest, enhancing our understanding of the Earth system’s internal variability.</summary></entry><entry><title type="html">A graph-space optimal transport FWI approach based on \kappa-generalized Gaussian distribution</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/AgraphspaceoptimaltransportFWIapproachbasedonkappageneralizedGaussiandistribution.html" rel="alternate" type="text/html" title="A graph-space optimal transport FWI approach based on \kappa-generalized Gaussian distribution" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/AgraphspaceoptimaltransportFWIapproachbasedonkappageneralizedGaussiandistribution</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/AgraphspaceoptimaltransportFWIapproachbasedonkappageneralizedGaussiandistribution.html">&lt;p&gt;The statistical basis for conventional full-waveform inversion (FWI) approaches is commonly associated with Gaussian statistics. However, errors are rarely Gaussian in non-linear problems like FWI. In this work, we investigate the portability of a new objective function for FWI applications based on the graph-space optimal transport and $\kappa$-generalized Gaussian probability distribution. In particular, we demonstrate that the proposed objective function is robust in mitigating two critical problems in FWI, which are associated with cycle skipping issues and non-Gaussian errors. The results reveal that our proposal can mitigate the negative influence of cycle-skipping ambiguity and non-Gaussian noises and reduce the computational runtime for computing the transport plan associated with the optimal transport theory.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15536&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Sérgio Luiz E. F. da Silva, G. Kaniadakis</name></author><category term="stat.AP," /><category term="stat.TH" /><summary type="html">The statistical basis for conventional full-waveform inversion (FWI) approaches is commonly associated with Gaussian statistics. However, errors are rarely Gaussian in non-linear problems like FWI. In this work, we investigate the portability of a new objective function for FWI applications based on the graph-space optimal transport and $\kappa$-generalized Gaussian probability distribution. In particular, we demonstrate that the proposed objective function is robust in mitigating two critical problems in FWI, which are associated with cycle skipping issues and non-Gaussian errors. The results reveal that our proposal can mitigate the negative influence of cycle-skipping ambiguity and non-Gaussian noises and reduce the computational runtime for computing the transport plan associated with the optimal transport theory.</summary></entry><entry><title type="html">Auditing the Fairness of COVID-19 Forecast Hub Case Prediction Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/AuditingtheFairnessofCOVID19ForecastHubCasePredictionModels.html" rel="alternate" type="text/html" title="Auditing the Fairness of COVID-19 Forecast Hub Case Prediction Models" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/AuditingtheFairnessofCOVID19ForecastHubCasePredictionModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/AuditingtheFairnessofCOVID19ForecastHubCasePredictionModels.html">&lt;p&gt;The COVID-19 Forecast Hub, a repository of COVID-19 forecasts from over 50 independent research groups, is used by the Centers for Disease Control and Prevention (CDC) for their official COVID-19 communications. As such, the Forecast Hub is a critical centralized resource to promote transparent decision making. Nevertheless, by focusing exclusively on prediction accuracy, the Forecast Hub fails to evaluate whether the proposed models have similar performance across social determinants that have been known to play a role in the COVID-19 pandemic including race, ethnicity and urbanization level. In this paper, we carry out a comprehensive fairness analysis of the Forecast Hub model predictions and we show statistically significant diverse predictive performance across social determinants, with minority racial and ethnic groups as well as less urbanized areas often associated with higher prediction errors. We hope this work will encourage COVID-19 modelers and the CDC to report fairness metrics together with accuracy, and to reflect on the potential harms of the models on specific social groups and contexts.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14891&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Saad Mohammad Abrar, Naman Awasthi, Daniel Smolyak, Vanessa Frias-Martinez</name></author><category term="stat.AP" /><summary type="html">The COVID-19 Forecast Hub, a repository of COVID-19 forecasts from over 50 independent research groups, is used by the Centers for Disease Control and Prevention (CDC) for their official COVID-19 communications. As such, the Forecast Hub is a critical centralized resource to promote transparent decision making. Nevertheless, by focusing exclusively on prediction accuracy, the Forecast Hub fails to evaluate whether the proposed models have similar performance across social determinants that have been known to play a role in the COVID-19 pandemic including race, ethnicity and urbanization level. In this paper, we carry out a comprehensive fairness analysis of the Forecast Hub model predictions and we show statistically significant diverse predictive performance across social determinants, with minority racial and ethnic groups as well as less urbanized areas often associated with higher prediction errors. We hope this work will encourage COVID-19 modelers and the CDC to report fairness metrics together with accuracy, and to reflect on the potential harms of the models on specific social groups and contexts.</summary></entry><entry><title type="html">Beyond the noise: intrinsic dimension estimation with optimal neighbourhood identification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Beyondthenoiseintrinsicdimensionestimationwithoptimalneighbourhoodidentification.html" rel="alternate" type="text/html" title="Beyond the noise: intrinsic dimension estimation with optimal neighbourhood identification" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Beyondthenoiseintrinsicdimensionestimationwithoptimalneighbourhoodidentification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Beyondthenoiseintrinsicdimensionestimationwithoptimalneighbourhoodidentification.html">&lt;p&gt;The Intrinsic Dimension (ID) is a key concept in unsupervised learning and feature selection, as it is a lower bound to the number of variables which are necessary to describe a system. However, in almost any real-world dataset the ID depends on the scale at which the data are analysed. Quite typically at a small scale, the ID is very large, as the data are affected by measurement errors. At large scale, the ID can also be erroneously large, due to the curvature and the topology of the manifold containing the data. In this work, we introduce an automatic protocol to select the sweet spot, namely the correct range of scales in which the ID is meaningful and useful. This protocol is based on imposing that for distances smaller than the correct scale the density of the data is constant. Since to estimate the density it is necessary to know the ID, this condition is imposed self-consistently. We illustrate the usefulness and robustness of this procedure by benchmarks on artificial and real-world datasets.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15132&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Antonio Di Noia, Iuri Macocco, Aldo Glielmo, Alessandro Laio, Antonietta Mira</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME," /><category term="stat.TH" /><summary type="html">The Intrinsic Dimension (ID) is a key concept in unsupervised learning and feature selection, as it is a lower bound to the number of variables which are necessary to describe a system. However, in almost any real-world dataset the ID depends on the scale at which the data are analysed. Quite typically at a small scale, the ID is very large, as the data are affected by measurement errors. At large scale, the ID can also be erroneously large, due to the curvature and the topology of the manifold containing the data. In this work, we introduce an automatic protocol to select the sweet spot, namely the correct range of scales in which the ID is meaningful and useful. This protocol is based on imposing that for distances smaller than the correct scale the density of the data is constant. Since to estimate the density it is necessary to know the ID, this condition is imposed self-consistently. We illustrate the usefulness and robustness of this procedure by benchmarks on artificial and real-world datasets.</summary></entry><entry><title type="html">Bootstrap test procedure for variance components in nonlinear mixed effects models in the presence of nuisance parameters and a singular Fisher Information Matrix</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/BootstraptestprocedureforvariancecomponentsinnonlinearmixedeffectsmodelsinthepresenceofnuisanceparametersandasingularFisherInformationMatrix.html" rel="alternate" type="text/html" title="Bootstrap test procedure for variance components in nonlinear mixed effects models in the presence of nuisance parameters and a singular Fisher Information Matrix" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/BootstraptestprocedureforvariancecomponentsinnonlinearmixedeffectsmodelsinthepresenceofnuisanceparametersandasingularFisherInformationMatrix</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/BootstraptestprocedureforvariancecomponentsinnonlinearmixedeffectsmodelsinthepresenceofnuisanceparametersandasingularFisherInformationMatrix.html">&lt;p&gt;We examine the problem of variance components testing in general mixed effects models using the likelihood ratio test. We account for the presence of nuisance parameters, i.e. the fact that some untested variances might also be equal to zero. Two main issues arise in this context leading to a non regular setting. First, under the null hypothesis the true parameter value lies on the boundary of the parameter space. Moreover, due to the presence of nuisance parameters the exact location of these boundary points is not known, which prevents from using classical asymptotic theory of maximum likelihood estimation. Then, in the specific context of nonlinear mixed-effects models, the Fisher information matrix is singular at the true parameter value. We address these two points by proposing a shrinked parametric bootstrap procedure, which is straightforward to apply even for nonlinear models. We show that the procedure is consistent, solving both the boundary and the singularity issues, and we provide a verifiable criterion for the applicability of our theoretical results. We show through a simulation study that, compared to the asymptotic approach, our procedure has a better small sample performance and is more robust to the presence of nuisance parameters. A real data application is also provided.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.10779&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Tom Guédon, Charlotte Baey, Estelle Kuhn</name></author><category term="stat.ME" /><summary type="html">We examine the problem of variance components testing in general mixed effects models using the likelihood ratio test. We account for the presence of nuisance parameters, i.e. the fact that some untested variances might also be equal to zero. Two main issues arise in this context leading to a non regular setting. First, under the null hypothesis the true parameter value lies on the boundary of the parameter space. Moreover, due to the presence of nuisance parameters the exact location of these boundary points is not known, which prevents from using classical asymptotic theory of maximum likelihood estimation. Then, in the specific context of nonlinear mixed-effects models, the Fisher information matrix is singular at the true parameter value. We address these two points by proposing a shrinked parametric bootstrap procedure, which is straightforward to apply even for nonlinear models. We show that the procedure is consistent, solving both the boundary and the singularity issues, and we provide a verifiable criterion for the applicability of our theoretical results. We show through a simulation study that, compared to the asymptotic approach, our procedure has a better small sample performance and is more robust to the presence of nuisance parameters. A real data application is also provided.</summary></entry><entry><title type="html">Causal de Finetti: On the Identification of Invariant Causal Structure in Exchangeable Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/CausaldeFinettiOntheIdentificationofInvariantCausalStructureinExchangeableData.html" rel="alternate" type="text/html" title="Causal de Finetti: On the Identification of Invariant Causal Structure in Exchangeable Data" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/CausaldeFinettiOntheIdentificationofInvariantCausalStructureinExchangeableData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/CausaldeFinettiOntheIdentificationofInvariantCausalStructureinExchangeableData.html">&lt;p&gt;Constraint-based causal discovery methods leverage conditional independence tests to infer causal relationships in a wide variety of applications. Just as the majority of machine learning methods, existing work focuses on studying $\textit{independent and identically distributed}$ data. However, it is known that even with infinite i.i.d.$\ $ data, constraint-based methods can only identify causal structures up to broad Markov equivalence classes, posing a fundamental limitation for causal discovery. In this work, we observe that exchangeable data contains richer conditional independence structure than i.i.d.$\ $ data, and show how the richer structure can be leveraged for causal discovery. We first present causal de Finetti theorems, which state that exchangeable distributions with certain non-trivial conditional independences can always be represented as $\textit{independent causal mechanism (ICM)}$ generative processes. We then present our main identifiability theorem, which shows that given data from an ICM generative process, its unique causal structure can be identified through performing conditional independence tests. We finally develop a causal discovery algorithm and demonstrate its applicability to inferring causal relationships from multi-environment data. Our code and models are publicly available at: https://github.com/syguo96/Causal-de-Finetti&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.15756&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Siyuan Guo, Viktor Tóth, Bernhard Schölkopf, Ferenc Huszár</name></author><category term="stat.ML," /><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Constraint-based causal discovery methods leverage conditional independence tests to infer causal relationships in a wide variety of applications. Just as the majority of machine learning methods, existing work focuses on studying $\textit{independent and identically distributed}$ data. However, it is known that even with infinite i.i.d.$\ $ data, constraint-based methods can only identify causal structures up to broad Markov equivalence classes, posing a fundamental limitation for causal discovery. In this work, we observe that exchangeable data contains richer conditional independence structure than i.i.d.$\ $ data, and show how the richer structure can be leveraged for causal discovery. We first present causal de Finetti theorems, which state that exchangeable distributions with certain non-trivial conditional independences can always be represented as $\textit{independent causal mechanism (ICM)}$ generative processes. We then present our main identifiability theorem, which shows that given data from an ICM generative process, its unique causal structure can be identified through performing conditional independence tests. We finally develop a causal discovery algorithm and demonstrate its applicability to inferring causal relationships from multi-environment data. Our code and models are publicly available at: https://github.com/syguo96/Causal-de-Finetti</summary></entry><entry><title type="html">Causal machine learning methods and use of sample splitting in settings with high-dimensional confounding</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Causalmachinelearningmethodsanduseofsamplesplittinginsettingswithhighdimensionalconfounding.html" rel="alternate" type="text/html" title="Causal machine learning methods and use of sample splitting in settings with high-dimensional confounding" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Causalmachinelearningmethodsanduseofsamplesplittinginsettingswithhighdimensionalconfounding</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Causalmachinelearningmethodsanduseofsamplesplittinginsettingswithhighdimensionalconfounding.html">&lt;p&gt;Observational epidemiological studies commonly seek to estimate the causal effect of an exposure on an outcome. Adjustment for potential confounding bias in modern studies is challenging due to the presence of high-dimensional confounding, induced when there are many confounders relative to sample size, or complex relationships between continuous confounders and exposure and outcome. As a promising avenue to overcome this challenge, doubly robust methods (Augmented Inverse Probability Weighting (AIPW) and Targeted Maximum Likelihood Estimation (TMLE)) enable the use of data-adaptive approaches to fit the two models they involve. Biased standard errors may result when the data-adaptive approaches used are very complex. The coupling of doubly robust methods with cross-fitting has been proposed to tackle this. Despite advances, limited evaluation, comparison, and guidance are available on the implementation of AIPW and TMLE with data-adaptive approaches and cross-fitting in realistic settings where high-dimensional confounding is present. We conducted an extensive simulation study to compare the relative performance of AIPW and TMLE using data-adaptive approaches in estimating the average causal effect (ACE) and evaluated the benefits of using cross-fitting with a varying number of folds, as well as the impact of using a reduced versus full (larger, more diverse) library in the Super Learner (SL) ensemble learning approach used for the data-adaptive models. A range of scenarios in terms of data generation, and sample size were considered. We found that AIPW and TMLE performed similarly in most cases for estimating the ACE, but TMLE was more stable. Cross-fitting improved the performance of both methods, with the number of folds a less important consideration. Using a full SL library was important to reduce bias and variance in the complex scenarios typical of modern health research studies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15242&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Susan Ellul, John B. Carlin, Stijn Vansteelandt, Margarita Moreno-Betancur</name></author><category term="stat.ME" /><summary type="html">Observational epidemiological studies commonly seek to estimate the causal effect of an exposure on an outcome. Adjustment for potential confounding bias in modern studies is challenging due to the presence of high-dimensional confounding, induced when there are many confounders relative to sample size, or complex relationships between continuous confounders and exposure and outcome. As a promising avenue to overcome this challenge, doubly robust methods (Augmented Inverse Probability Weighting (AIPW) and Targeted Maximum Likelihood Estimation (TMLE)) enable the use of data-adaptive approaches to fit the two models they involve. Biased standard errors may result when the data-adaptive approaches used are very complex. The coupling of doubly robust methods with cross-fitting has been proposed to tackle this. Despite advances, limited evaluation, comparison, and guidance are available on the implementation of AIPW and TMLE with data-adaptive approaches and cross-fitting in realistic settings where high-dimensional confounding is present. We conducted an extensive simulation study to compare the relative performance of AIPW and TMLE using data-adaptive approaches in estimating the average causal effect (ACE) and evaluated the benefits of using cross-fitting with a varying number of folds, as well as the impact of using a reduced versus full (larger, more diverse) library in the Super Learner (SL) ensemble learning approach used for the data-adaptive models. A range of scenarios in terms of data generation, and sample size were considered. We found that AIPW and TMLE performed similarly in most cases for estimating the ACE, but TMLE was more stable. Cross-fitting improved the performance of both methods, with the number of folds a less important consideration. Using a full SL library was important to reduce bias and variance in the complex scenarios typical of modern health research studies.</summary></entry><entry><title type="html">Consistent Validation for Predictive Methods in Spatial Settings</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ConsistentValidationforPredictiveMethodsinSpatialSettings.html" rel="alternate" type="text/html" title="Consistent Validation for Predictive Methods in Spatial Settings" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ConsistentValidationforPredictiveMethodsinSpatialSettings</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ConsistentValidationforPredictiveMethodsinSpatialSettings.html">&lt;p&gt;Spatial prediction tasks are key to weather forecasting, studying air pollution, and other scientific endeavors. Determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions. Unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions. This mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than i.i.d. from two distributions. In the present work, we formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense. We show that classical and covariate-shift methods can fail this check. We instead propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data at hand. We prove that our proposal passes our check. And we demonstrate its advantages empirically on simulated and real data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.03527&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>David R. Burt, Yunyi Shen, Tamara Broderick</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Spatial prediction tasks are key to weather forecasting, studying air pollution, and other scientific endeavors. Determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions. Unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions. This mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than i.i.d. from two distributions. In the present work, we formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense. We show that classical and covariate-shift methods can fail this check. We instead propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data at hand. We prove that our proposal passes our check. And we demonstrate its advantages empirically on simulated and real data.</summary></entry><entry><title type="html">Constrained D-optimal Design for Paid Research Study</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ConstrainedDoptimalDesignforPaidResearchStudy.html" rel="alternate" type="text/html" title="Constrained D-optimal Design for Paid Research Study" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ConstrainedDoptimalDesignforPaidResearchStudy</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ConstrainedDoptimalDesignforPaidResearchStudy.html">&lt;p&gt;We consider constrained sampling problems in paid research studies or clinical trials. When qualified volunteers are more than the budget allowed, we recommend a D-optimal sampling strategy based on the optimal design theory and develop a constrained lift-one algorithm to find the optimal allocation. Unlike the literature which mainly deals with linear models, our solution solves the constrained sampling problem under fairly general statistical models, including generalized linear models and multinomial logistic models, and with more general constraints. We justify theoretically the optimality of our sampling strategy and show by simulation studies and real-world examples the advantages over simple random sampling and proportionally stratified sampling strategies.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2207.05281&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yifei Huang, Liping Tong, Jie Yang</name></author><category term="stat.ME" /><summary type="html">We consider constrained sampling problems in paid research studies or clinical trials. When qualified volunteers are more than the budget allowed, we recommend a D-optimal sampling strategy based on the optimal design theory and develop a constrained lift-one algorithm to find the optimal allocation. Unlike the literature which mainly deals with linear models, our solution solves the constrained sampling problem under fairly general statistical models, including generalized linear models and multinomial logistic models, and with more general constraints. We justify theoretically the optimality of our sampling strategy and show by simulation studies and real-world examples the advantages over simple random sampling and proportionally stratified sampling strategies.</summary></entry><entry><title type="html">Dispersion Modeling in Zero-inflated Tweedie Models with Applications to Insurance Claim Data Analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/DispersionModelinginZeroinflatedTweedieModelswithApplicationstoInsuranceClaimDataAnalysis.html" rel="alternate" type="text/html" title="Dispersion Modeling in Zero-inflated Tweedie Models with Applications to Insurance Claim Data Analysis" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/DispersionModelinginZeroinflatedTweedieModelswithApplicationstoInsuranceClaimDataAnalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/DispersionModelinginZeroinflatedTweedieModelswithApplicationstoInsuranceClaimDataAnalysis.html">&lt;p&gt;The Tweedie generalized linear models are commonly applied in the insurance industry to analyze semicontinuous claim data. For better prediction of the aggregated claim size, the mean and dispersion of the Tweedie model are often estimated together using the double generalized linear models. In some actuarial applications, it is common to observe an excessive percentage of zeros, which often results in a decline in the performance of the Tweedie model. The zero-inflated Tweedie model has been recently considered in the literature, which draws inspiration from the zero-inflated Poisson model. In this article, we consider the problem of dispersion modeling of the Tweedie state in the zero-inflated Tweedie model, in addition to the mean modeling. We also model the probability of the zero state based on the generalized expectation-maximization algorithm. To potentially incorporate nonlinear and interaction effects of the covariates, we estimate the mean, dispersion, and zero-state probability using decision-tree-based gradient boosting. We conduct extensive numerical studies to demonstrate the improved performance of our method over existing ones.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14990&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yuwen Gu</name></author><category term="stat.ME" /><summary type="html">The Tweedie generalized linear models are commonly applied in the insurance industry to analyze semicontinuous claim data. For better prediction of the aggregated claim size, the mean and dispersion of the Tweedie model are often estimated together using the double generalized linear models. In some actuarial applications, it is common to observe an excessive percentage of zeros, which often results in a decline in the performance of the Tweedie model. The zero-inflated Tweedie model has been recently considered in the literature, which draws inspiration from the zero-inflated Poisson model. In this article, we consider the problem of dispersion modeling of the Tweedie state in the zero-inflated Tweedie model, in addition to the mean modeling. We also model the probability of the zero state based on the generalized expectation-maximization algorithm. To potentially incorporate nonlinear and interaction effects of the covariates, we estimate the mean, dispersion, and zero-state probability using decision-tree-based gradient boosting. We conduct extensive numerical studies to demonstrate the improved performance of our method over existing ones.</summary></entry><entry><title type="html">Efficient stochastic generators with spherical harmonic transformation for high-resolution global climate simulations from CESM2-LENS2</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/EfficientstochasticgeneratorswithsphericalharmonictransformationforhighresolutionglobalclimatesimulationsfromCESM2LENS2.html" rel="alternate" type="text/html" title="Efficient stochastic generators with spherical harmonic transformation for high-resolution global climate simulations from CESM2-LENS2" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/EfficientstochasticgeneratorswithsphericalharmonictransformationforhighresolutionglobalclimatesimulationsfromCESM2LENS2</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/EfficientstochasticgeneratorswithsphericalharmonictransformationforhighresolutionglobalclimatesimulationsfromCESM2LENS2.html">&lt;p&gt;Earth system models (ESMs) are fundamental for understanding Earth’s complex climate system. However, the computational demands and storage requirements of ESM simulations limit their utility. For the newly published CESM2-LENS2 data, which suffer from this issue, we propose a novel stochastic generator (SG) as a practical complement to the CESM2, capable of rapidly producing emulations closely mirroring training simulations. Our SG leverages the spherical harmonic transformation (SHT) to shift from spatial to spectral domains, enabling efficient low-rank approximations that significantly reduce computational and storage costs. By accounting for axial symmetry and retaining distinct ranks for land and ocean regions, our SG captures intricate non-stationary spatial dependencies. Additionally, a modified Tukey g-and-h (TGH) transformation accommodates non-Gaussianity in high-temporal-resolution data. We apply the proposed SG to generate emulations for surface temperature simulations from the CESM2-LENS2 data across various scales, marking the first attempt of reproducing daily data. These emulations are then meticulously validated against training simulations. This work offers a promising complementary pathway for efficient climate modeling and analysis while overcoming computational and storage limitations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.02216&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yan Song, Zubair Khalid, Marc G. Genton</name></author><category term="stat.AP" /><summary type="html">Earth system models (ESMs) are fundamental for understanding Earth’s complex climate system. However, the computational demands and storage requirements of ESM simulations limit their utility. For the newly published CESM2-LENS2 data, which suffer from this issue, we propose a novel stochastic generator (SG) as a practical complement to the CESM2, capable of rapidly producing emulations closely mirroring training simulations. Our SG leverages the spherical harmonic transformation (SHT) to shift from spatial to spectral domains, enabling efficient low-rank approximations that significantly reduce computational and storage costs. By accounting for axial symmetry and retaining distinct ranks for land and ocean regions, our SG captures intricate non-stationary spatial dependencies. Additionally, a modified Tukey g-and-h (TGH) transformation accommodates non-Gaussianity in high-temporal-resolution data. We apply the proposed SG to generate emulations for surface temperature simulations from the CESM2-LENS2 data across various scales, marking the first attempt of reproducing daily data. These emulations are then meticulously validated against training simulations. This work offers a promising complementary pathway for efficient climate modeling and analysis while overcoming computational and storage limitations.</summary></entry><entry><title type="html">High Rank Path Development: an approach of learning the filtration of stochastic processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/HighRankPathDevelopmentanapproachoflearningthefiltrationofstochasticprocesses.html" rel="alternate" type="text/html" title="High Rank Path Development: an approach of learning the filtration of stochastic processes" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/HighRankPathDevelopmentanapproachoflearningthefiltrationofstochasticprocesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/HighRankPathDevelopmentanapproachoflearningthefiltrationofstochasticprocesses.html">&lt;p&gt;Since the weak convergence for stochastic processes does not account for the growth of information over time which is represented by the underlying filtration, a slightly erroneous stochastic model in weak topology may cause huge loss in multi-periods decision making problems. To address such discontinuities Aldous introduced the extended weak convergence, which can fully characterise all essential properties, including the filtration, of stochastic processes; however was considered to be hard to find efficient numerical implementations. In this paper, we introduce a novel metric called High Rank PCF Distance (HRPCFD) for extended weak convergence based on the high rank path development method from rough path theory, which also defines the characteristic function for measure-valued processes. We then show that such HRPCFD admits many favourable analytic properties which allows us to design an efficient algorithm for training HRPCFD from data and construct the HRPCF-GAN by using HRPCFD as the discriminator for conditional time series generation. Our numerical experiments on both hypothesis testing and generative modelling validate the out-performance of our approach compared with several state-of-the-art methods, highlighting its potential in broad applications of synthetic time series generation and in addressing classic financial and economic challenges, such as optimal stopping or utility maximisation problems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14913&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Jiajie Tao, Hao Ni, Chong Liu</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">Since the weak convergence for stochastic processes does not account for the growth of information over time which is represented by the underlying filtration, a slightly erroneous stochastic model in weak topology may cause huge loss in multi-periods decision making problems. To address such discontinuities Aldous introduced the extended weak convergence, which can fully characterise all essential properties, including the filtration, of stochastic processes; however was considered to be hard to find efficient numerical implementations. In this paper, we introduce a novel metric called High Rank PCF Distance (HRPCFD) for extended weak convergence based on the high rank path development method from rough path theory, which also defines the characteristic function for measure-valued processes. We then show that such HRPCFD admits many favourable analytic properties which allows us to design an efficient algorithm for training HRPCFD from data and construct the HRPCF-GAN by using HRPCFD as the discriminator for conditional time series generation. Our numerical experiments on both hypothesis testing and generative modelling validate the out-performance of our approach compared with several state-of-the-art methods, highlighting its potential in broad applications of synthetic time series generation and in addressing classic financial and economic challenges, such as optimal stopping or utility maximisation problems.</summary></entry><entry><title type="html">Improving and Evaluating Machine Learning Methods for Forensic Shoeprint Matching</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ImprovingandEvaluatingMachineLearningMethodsforForensicShoeprintMatching.html" rel="alternate" type="text/html" title="Improving and Evaluating Machine Learning Methods for Forensic Shoeprint Matching" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ImprovingandEvaluatingMachineLearningMethodsforForensicShoeprintMatching</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ImprovingandEvaluatingMachineLearningMethodsforForensicShoeprintMatching.html">&lt;p&gt;We propose a machine learning pipeline for forensic shoeprint pattern matching that improves on the accuracy and generalisability of existing methods. We extract 2D coordinates from shoeprint scans using edge detection and align the two shoeprints with iterative closest point (ICP). We then extract similarity metrics to quantify how well the two prints match and use these metrics to train a random forest that generates a probabilistic measurement of how likely two prints are to have originated from the same outsole. We assess the generalisability of machine learning methods trained on lab shoeprint scans to more realistic crime scene shoeprint data by evaluating the accuracy of our methods on several shoeprint scenarios: partial prints, prints with varying levels of blurriness, prints with different amounts of wear, and prints from different shoe models. We find that models trained on one type of shoeprint yield extremely high levels of accuracy when tested on shoeprint pairs of the same scenario but fail to generalise to other scenarios. We also discover that models trained on a variety of scenarios predict almost as accurately as models trained on specific scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14878&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Divij Jain, Saatvik Kher, Lena Liang, Yufeng Wu, Ashley Zheng, Xizhen Cai, Anna Plantinga, Elizabeth Upton</name></author><category term="stat.AP" /><summary type="html">We propose a machine learning pipeline for forensic shoeprint pattern matching that improves on the accuracy and generalisability of existing methods. We extract 2D coordinates from shoeprint scans using edge detection and align the two shoeprints with iterative closest point (ICP). We then extract similarity metrics to quantify how well the two prints match and use these metrics to train a random forest that generates a probabilistic measurement of how likely two prints are to have originated from the same outsole. We assess the generalisability of machine learning methods trained on lab shoeprint scans to more realistic crime scene shoeprint data by evaluating the accuracy of our methods on several shoeprint scenarios: partial prints, prints with varying levels of blurriness, prints with different amounts of wear, and prints from different shoe models. We find that models trained on one type of shoeprint yield extremely high levels of accuracy when tested on shoeprint pairs of the same scenario but fail to generalise to other scenarios. We also discover that models trained on a variety of scenarios predict almost as accurately as models trained on specific scenarios.</summary></entry><entry><title type="html">Item-Level Heterogeneous Treatment Effects of Selective Serotonin Reuptake Inhibitors (SSRIs) on Depression: Implications for Inference, Generalizability, and Identification</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ItemLevelHeterogeneousTreatmentEffectsofSelectiveSerotoninReuptakeInhibitorsSSRIsonDepressionImplicationsforInferenceGeneralizabilityandIdentification.html" rel="alternate" type="text/html" title="Item-Level Heterogeneous Treatment Effects of Selective Serotonin Reuptake Inhibitors (SSRIs) on Depression: Implications for Inference, Generalizability, and Identification" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ItemLevelHeterogeneousTreatmentEffectsofSelectiveSerotoninReuptakeInhibitorsSSRIsonDepressionImplicationsforInferenceGeneralizabilityandIdentification</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ItemLevelHeterogeneousTreatmentEffectsofSelectiveSerotoninReuptakeInhibitorsSSRIsonDepressionImplicationsforInferenceGeneralizabilityandIdentification.html">&lt;p&gt;In analysis of randomized controlled trials (RCTs) with patient-reported outcome measures (PROMs), Item Response Theory (IRT) models that allow for heterogeneity in the treatment effect at the item level merit consideration. These models for ``item-level heterogeneous treatment effects’’ (IL-HTE) can provide more accurate statistical inference, allow researchers to better generalize their results, and resolve critical identification problems in the estimation of interaction effects. In this study, we extend the IL-HTE model to polytomous data and apply the model to determine how the effect of selective serotonin reuptake inhibitors (SSRIs) on depression varies across the items on a depression rating scale. We first conduct a Monte Carlo simulation study to assess the performance of the polytomous IL-HTE model under a range of conditions. We then apply the IL-HTE model to item-level data from 28 RCTs measuring the effect of SSRIs on depression using the 17-item Hamilton Depression Rating Scale (HDRS-17) and estimate potential heterogeneity by subscale (HDRS-6). Our results show that the IL-HTE model provides more accurate statistical inference, allows for generalizability of results to out-of-sample items, and resolves identification problems in the estimation of interaction effects. Our empirical application shows that while the average effect of SSRIs on depression is beneficial (i.e., negative) and statistically significant, there is substantial IL-HTE, with estimates of the standard deviation of item-level effects nearly as large as the average effect. We show that this substantial IL-HTE is driven primarily by systematically larger effects on the HDRS-6 subscale items. The IL-HTE model has the potential to provide new insights for the inference, generalizability, and identification of treatment effects in clinical trials using patient reported outcome measures.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.04487&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Joshua B. Gilbert, Fredrik Hieronymus, Elias Eriksson, Benjamin W. Domingue</name></author><category term="stat.ME" /><summary type="html">In analysis of randomized controlled trials (RCTs) with patient-reported outcome measures (PROMs), Item Response Theory (IRT) models that allow for heterogeneity in the treatment effect at the item level merit consideration. These models for ``item-level heterogeneous treatment effects’’ (IL-HTE) can provide more accurate statistical inference, allow researchers to better generalize their results, and resolve critical identification problems in the estimation of interaction effects. In this study, we extend the IL-HTE model to polytomous data and apply the model to determine how the effect of selective serotonin reuptake inhibitors (SSRIs) on depression varies across the items on a depression rating scale. We first conduct a Monte Carlo simulation study to assess the performance of the polytomous IL-HTE model under a range of conditions. We then apply the IL-HTE model to item-level data from 28 RCTs measuring the effect of SSRIs on depression using the 17-item Hamilton Depression Rating Scale (HDRS-17) and estimate potential heterogeneity by subscale (HDRS-6). Our results show that the IL-HTE model provides more accurate statistical inference, allows for generalizability of results to out-of-sample items, and resolves identification problems in the estimation of interaction effects. Our empirical application shows that while the average effect of SSRIs on depression is beneficial (i.e., negative) and statistically significant, there is substantial IL-HTE, with estimates of the standard deviation of item-level effects nearly as large as the average effect. We show that this substantial IL-HTE is driven primarily by systematically larger effects on the HDRS-6 subscale items. The IL-HTE model has the potential to provide new insights for the inference, generalizability, and identification of treatment effects in clinical trials using patient reported outcome measures.</summary></entry><entry><title type="html">Likelihood distortion and Bayesian local robustness</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/LikelihooddistortionandBayesianlocalrobustness.html" rel="alternate" type="text/html" title="Likelihood distortion and Bayesian local robustness" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/LikelihooddistortionandBayesianlocalrobustness</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/LikelihooddistortionandBayesianlocalrobustness.html">&lt;p&gt;Robust Bayesian analysis has been mainly devoted to detecting and measuring robustness to the prior distribution. Indeed, many contributions in the literature aim to define suitable classes of priors which allow the computation of variations of quantities of interest while the prior changes within those classes. The literature has devoted much less attention to the robustness of Bayesian methods to the likelihood function due to mathematical and computational complexity, and because it is often arguably considered a more objective choice compared to the prior. In this contribution, a new approach to Bayesian local robustness to the likelihood function is proposed and extended to robustness to the prior and to both. This approach is based on the notion of distortion function introduced in the literature on risk theory, and then successfully adopted to build suitable classes of priors for Bayesian global robustness to the prior. The novel robustness measure is a local sensitivity measure that turns out to be very tractable and easy to compute for certain classes of distortion functions. Asymptotic properties are derived and numerical experiments illustrate the theory and its applicability for modelling purposes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15141&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Antonio Di Noia, Fabrizio Ruggeri, Antonietta Mira</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Robust Bayesian analysis has been mainly devoted to detecting and measuring robustness to the prior distribution. Indeed, many contributions in the literature aim to define suitable classes of priors which allow the computation of variations of quantities of interest while the prior changes within those classes. The literature has devoted much less attention to the robustness of Bayesian methods to the likelihood function due to mathematical and computational complexity, and because it is often arguably considered a more objective choice compared to the prior. In this contribution, a new approach to Bayesian local robustness to the likelihood function is proposed and extended to robustness to the prior and to both. This approach is based on the notion of distortion function introduced in the literature on risk theory, and then successfully adopted to build suitable classes of priors for Bayesian global robustness to the prior. The novel robustness measure is a local sensitivity measure that turns out to be very tractable and easy to compute for certain classes of distortion functions. Asymptotic properties are derived and numerical experiments illustrate the theory and its applicability for modelling purposes.</summary></entry><entry><title type="html">MMD Two-sample Testing in the Presence of Arbitrarily Missing Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/MMDTwosampleTestinginthePresenceofArbitrarilyMissingData.html" rel="alternate" type="text/html" title="MMD Two-sample Testing in the Presence of Arbitrarily Missing Data" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/MMDTwosampleTestinginthePresenceofArbitrarilyMissingData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/MMDTwosampleTestinginthePresenceofArbitrarilyMissingData.html">&lt;p&gt;In many real-world applications, it is common that a proportion of the data may be missing or only partially observed. We develop a novel two-sample testing method based on the Maximum Mean Discrepancy (MMD) which accounts for missing data in both samples, without making assumptions about the missingness mechanism. Our approach is based on deriving the mathematically precise bounds of the MMD test statistic after accounting for all possible missing values. To the best of our knowledge, it is the only two-sample testing method that is guaranteed to control the Type I error for both univariate and multivariate data where data may be arbitrarily missing. Simulation results show that our method has good statistical power, typically for cases where 5% to 10% of the data are missing. We highlight the value of our approach when the data are missing not at random, a context in which either ignoring the missing values or using common imputation methods may not control the Type I error.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15531&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Yijin Zeng, Niall M. Adams, Dean A. Bodenham</name></author><category term="stat.ME" /><summary type="html">In many real-world applications, it is common that a proportion of the data may be missing or only partially observed. We develop a novel two-sample testing method based on the Maximum Mean Discrepancy (MMD) which accounts for missing data in both samples, without making assumptions about the missingness mechanism. Our approach is based on deriving the mathematically precise bounds of the MMD test statistic after accounting for all possible missing values. To the best of our knowledge, it is the only two-sample testing method that is guaranteed to control the Type I error for both univariate and multivariate data where data may be arbitrarily missing. Simulation results show that our method has good statistical power, typically for cases where 5% to 10% of the data are missing. We highlight the value of our approach when the data are missing not at random, a context in which either ignoring the missing values or using common imputation methods may not control the Type I error.</summary></entry><entry><title type="html">Non-parametric inference on calibration of predicted risks</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Nonparametricinferenceoncalibrationofpredictedrisks.html" rel="alternate" type="text/html" title="Non-parametric inference on calibration of predicted risks" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Nonparametricinferenceoncalibrationofpredictedrisks</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Nonparametricinferenceoncalibrationofpredictedrisks.html">&lt;p&gt;Moderate calibration, the expected event probability among observations with predicted probability z being equal to z, is a desired property of risk prediction models. Current graphical and numerical techniques for evaluating moderate calibration of risk prediction models are mostly based on smoothing or grouping the data. As well, there is no widely accepted inferential method for the null hypothesis that a model is moderately calibrated. In this work, we discuss recently-developed, and propose novel, methods for the assessment of moderate calibration for binary responses. The methods are based on the limiting distributions of functions of standardized partial sums of prediction errors converging to the corresponding laws of Brownian motion. The novel method relies on well-known properties of the Brownian bridge which enables joint inference on mean and moderate calibration, leading to a unified “bridge” test for detecting miscalibration. Simulation studies indicate that the bridge test is more powerful, often substantially, than the alternative test. As a case study we consider a prediction model for short-term mortality after a heart attack, where we provide suggestions on graphical presentation and the interpretation of results. Moderate calibration can be assessed without requiring arbitrary grouping of data or using methods that require tuning of parameters. An accompanying R package implements this method (see https://github.com/resplab/cumulcalib/).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.09713&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Mohsen Sadatsafavi, John Petkau</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Moderate calibration, the expected event probability among observations with predicted probability z being equal to z, is a desired property of risk prediction models. Current graphical and numerical techniques for evaluating moderate calibration of risk prediction models are mostly based on smoothing or grouping the data. As well, there is no widely accepted inferential method for the null hypothesis that a model is moderately calibrated. In this work, we discuss recently-developed, and propose novel, methods for the assessment of moderate calibration for binary responses. The methods are based on the limiting distributions of functions of standardized partial sums of prediction errors converging to the corresponding laws of Brownian motion. The novel method relies on well-known properties of the Brownian bridge which enables joint inference on mean and moderate calibration, leading to a unified “bridge” test for detecting miscalibration. Simulation studies indicate that the bridge test is more powerful, often substantially, than the alternative test. As a case study we consider a prediction model for short-term mortality after a heart attack, where we provide suggestions on graphical presentation and the interpretation of results. Moderate calibration can be assessed without requiring arbitrary grouping of data or using methods that require tuning of parameters. An accompanying R package implements this method (see https://github.com/resplab/cumulcalib/).</summary></entry><entry><title type="html">Nonparametric quantile regression for spatio-temporal processes</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Nonparametricquantileregressionforspatiotemporalprocesses.html" rel="alternate" type="text/html" title="Nonparametric quantile regression for spatio-temporal processes" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Nonparametricquantileregressionforspatiotemporalprocesses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Nonparametricquantileregressionforspatiotemporalprocesses.html">&lt;p&gt;In this paper, we develop a new and effective approach to nonparametric quantile regression that accommodates ultrahigh-dimensional data arising from spatio-temporal processes. This approach proves advantageous in staving off computational challenges that constitute known hindrances to existing nonparametric quantile regression methods when the number of predictors is much larger than the available sample size. We investigate conditions under which estimation is feasible and of good overall quality and obtain sharp approximations that we employ to devising statistical inference methodology. These include simultaneous confidence intervals and tests of hypotheses, whose asymptotics is borne by a non-trivial functional central limit theorem tailored to martingale differences. Additionally, we provide finite-sample results through various simulations which, accompanied by an illustrative application to real-worldesque data (on electricity demand), offer guarantees on the performance of the proposed methodology.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.13783&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Soudeep Deb, Claudia Neves, Subhrajyoty Roy</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">In this paper, we develop a new and effective approach to nonparametric quantile regression that accommodates ultrahigh-dimensional data arising from spatio-temporal processes. This approach proves advantageous in staving off computational challenges that constitute known hindrances to existing nonparametric quantile regression methods when the number of predictors is much larger than the available sample size. We investigate conditions under which estimation is feasible and of good overall quality and obtain sharp approximations that we employ to devising statistical inference methodology. These include simultaneous confidence intervals and tests of hypotheses, whose asymptotics is borne by a non-trivial functional central limit theorem tailored to martingale differences. Additionally, we provide finite-sample results through various simulations which, accompanied by an illustrative application to real-worldesque data (on electricity demand), offer guarantees on the performance of the proposed methodology.</summary></entry><entry><title type="html">On Flexible Inverse Probability of Treatment and Intensity Weighting: Informative Censoring, Variable Inclusion, and Weight Trimming</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/OnFlexibleInverseProbabilityofTreatmentandIntensityWeightingInformativeCensoringVariableInclusionandWeightTrimming.html" rel="alternate" type="text/html" title="On Flexible Inverse Probability of Treatment and Intensity Weighting: Informative Censoring, Variable Inclusion, and Weight Trimming" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/OnFlexibleInverseProbabilityofTreatmentandIntensityWeightingInformativeCensoringVariableInclusionandWeightTrimming</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/OnFlexibleInverseProbabilityofTreatmentandIntensityWeightingInformativeCensoringVariableInclusionandWeightTrimming.html">&lt;p&gt;Many observational studies feature irregular longitudinal data, where the observation times are not common across individuals in the study. Further, the observation times may be related to the longitudinal outcome. In this setting, failing to account for the informative observation process may result in biased causal estimates. This can be coupled with other sources of bias, including non-randomized treatment assignments and informative censoring. This paper provides an overview of a flexible weighting method used to adjust for informative observation processes and non-randomized treatment assignments. We investigate the sensitivity of the flexible weighting method to violations of the noninformative censoring assumption, examine variable selection for the observation process weighting model, known as inverse intensity weighting, and look at the impacts of weight trimming for the flexible weighting model. We show that the flexible weighting method is sensitive to violations of the noninformative censoring assumption and show that a previously proposed extension fails under such violations. We also show that variables confounding the observation and outcome processes should always be included in the observation intensity model. Finally, we show that weight trimming should be applied in the flexible weighting model when the treatment assignment process is highly informative and driving the extreme weights. We conclude with an application of the methodology to a real data set to examine the impacts of household water sources on malaria diagnoses.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15740&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Grace Tompkins, Joel A Dubin, Michael Wallace</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Many observational studies feature irregular longitudinal data, where the observation times are not common across individuals in the study. Further, the observation times may be related to the longitudinal outcome. In this setting, failing to account for the informative observation process may result in biased causal estimates. This can be coupled with other sources of bias, including non-randomized treatment assignments and informative censoring. This paper provides an overview of a flexible weighting method used to adjust for informative observation processes and non-randomized treatment assignments. We investigate the sensitivity of the flexible weighting method to violations of the noninformative censoring assumption, examine variable selection for the observation process weighting model, known as inverse intensity weighting, and look at the impacts of weight trimming for the flexible weighting model. We show that the flexible weighting method is sensitive to violations of the noninformative censoring assumption and show that a previously proposed extension fails under such violations. We also show that variables confounding the observation and outcome processes should always be included in the observation intensity model. Finally, we show that weight trimming should be applied in the flexible weighting model when the treatment assignment process is highly informative and driving the extreme weights. We conclude with an application of the methodology to a real data set to examine the impacts of household water sources on malaria diagnoses.</summary></entry><entry><title type="html">Online Changepoint Detection via Dynamic Mode Decomposition</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/OnlineChangepointDetectionviaDynamicModeDecomposition.html" rel="alternate" type="text/html" title="Online Changepoint Detection via Dynamic Mode Decomposition" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/OnlineChangepointDetectionviaDynamicModeDecomposition</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/OnlineChangepointDetectionviaDynamicModeDecomposition.html">&lt;p&gt;Detecting changes in data streams is a vital task in many applications. There is increasing interest in changepoint detection in the online setting, to enable real-time monitoring and support prompt responses and informed decision-making. Many approaches assume stationary sequences before encountering an abrupt change in the mean or variance. Notably less attention has focused on the challenging case where the monitored sequences exhibit trend, periodicity and seasonality. Dynamic mode decomposition is a data-driven dimensionality reduction technique that extracts the essential components of a dynamical system. We propose a changepoint detection method that leverages this technique to sequentially model the dynamics of a moving window of data and produce a low-rank reconstruction. A change is identified when there is a significant difference between this reconstruction and the observed data, and we provide theoretical justification for this approach. Extensive simulations demonstrate that our approach has superior detection performance compared to other methods for detecting small changes in mean, variance, periodicity, and second-order structure, among others, in data that exhibits seasonality. Results on real-world datasets also show excellent performance compared to contemporary approaches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15576&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Victor K. Khamesi, Niall M. Adams, Dean A. Bodenham, Edward A. K. Cohen</name></author><category term="stat.ME" /><summary type="html">Detecting changes in data streams is a vital task in many applications. There is increasing interest in changepoint detection in the online setting, to enable real-time monitoring and support prompt responses and informed decision-making. Many approaches assume stationary sequences before encountering an abrupt change in the mean or variance. Notably less attention has focused on the challenging case where the monitored sequences exhibit trend, periodicity and seasonality. Dynamic mode decomposition is a data-driven dimensionality reduction technique that extracts the essential components of a dynamical system. We propose a changepoint detection method that leverages this technique to sequentially model the dynamics of a moving window of data and produce a low-rank reconstruction. A change is identified when there is a significant difference between this reconstruction and the observed data, and we provide theoretical justification for this approach. Extensive simulations demonstrate that our approach has superior detection performance compared to other methods for detecting small changes in mean, variance, periodicity, and second-order structure, among others, in data that exhibits seasonality. Results on real-world datasets also show excellent performance compared to contemporary approaches.</summary></entry><entry><title type="html">On the Computational Complexity of Private High-dimensional Model Selection</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/OntheComputationalComplexityofPrivateHighdimensionalModelSelection.html" rel="alternate" type="text/html" title="On the Computational Complexity of Private High-dimensional Model Selection" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/OntheComputationalComplexityofPrivateHighdimensionalModelSelection</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/OntheComputationalComplexityofPrivateHighdimensionalModelSelection.html">&lt;p&gt;We consider the problem of model selection in a high-dimensional sparse linear regression model under privacy constraints. We propose a differentially private best subset selection method with strong utility properties by adopting the well-known exponential mechanism for selecting the best model. We propose an efficient Metropolis-Hastings algorithm and establish that it enjoys polynomial mixing time to its stationary distribution. Furthermore, we also establish approximate differential privacy for the estimates of the mixed Metropolis-Hastings chain. Finally, we perform some illustrative experiments that show the strong utility of our algorithm.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.07852&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Saptarshi Roy, Zehua Wang, Ambuj Tewari</name></author><category term="stat.ML," /><category term="stat.CO," /><category term="stat.ME" /><summary type="html">We consider the problem of model selection in a high-dimensional sparse linear regression model under privacy constraints. We propose a differentially private best subset selection method with strong utility properties by adopting the well-known exponential mechanism for selecting the best model. We propose an efficient Metropolis-Hastings algorithm and establish that it enjoys polynomial mixing time to its stationary distribution. Furthermore, we also establish approximate differential privacy for the estimates of the mixed Metropolis-Hastings chain. Finally, we perform some illustrative experiments that show the strong utility of our algorithm.</summary></entry><entry><title type="html">On the existence of powerful p-values and e-values for composite hypotheses</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Ontheexistenceofpowerfulpvaluesandevaluesforcompositehypotheses.html" rel="alternate" type="text/html" title="On the existence of powerful p-values and e-values for composite hypotheses" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Ontheexistenceofpowerfulpvaluesandevaluesforcompositehypotheses</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Ontheexistenceofpowerfulpvaluesandevaluesforcompositehypotheses.html">&lt;p&gt;Given a composite null $ \mathcal P$ and composite alternative $ \mathcal Q$, when and how can we construct a p-value whose distribution is exactly uniform under the null, and stochastically smaller than uniform under the alternative? Similarly, when and how can we construct an e-value whose expectation exactly equals one under the null, but its expected logarithm under the alternative is positive? We answer these basic questions, and other related ones, when $ \mathcal P$ and $ \mathcal Q$ are convex polytopes (in the space of probability measures). We prove that such constructions are possible if and only if $ \mathcal Q$ does not intersect the span of $ \mathcal P$. If the p-value is allowed to be stochastically larger than uniform under $P\in \mathcal P$, and the e-value can have expectation at most one under $P\in \mathcal P$, then it is achievable whenever $ \mathcal P$ and $ \mathcal Q$ are disjoint. More generally, even when $ \mathcal P$ and $ \mathcal Q$ are not polytopes, we characterize the existence of a bounded nontrivial e-variable whose expectation exactly equals one under any $P \in \mathcal P$. The proofs utilize recently developed techniques in simultaneous optimal transport. A key role is played by coarsening the filtration: sometimes, no such p-value or e-value exists in the richest data filtration, but it does exist in some reduced filtration, and our work provides the first general characterization of this phenomenon. We also provide an iterative construction that explicitly constructs such processes, and under certain conditions it finds the one that grows fastest under a specific alternative $Q$. We discuss implications for the construction of composite nonnegative (super)martingales, and end with some conjectures and open problems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.16539&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Zhenyuan Zhang, Aaditya Ramdas, Ruodu Wang</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Given a composite null $ \mathcal P$ and composite alternative $ \mathcal Q$, when and how can we construct a p-value whose distribution is exactly uniform under the null, and stochastically smaller than uniform under the alternative? Similarly, when and how can we construct an e-value whose expectation exactly equals one under the null, but its expected logarithm under the alternative is positive? We answer these basic questions, and other related ones, when $ \mathcal P$ and $ \mathcal Q$ are convex polytopes (in the space of probability measures). We prove that such constructions are possible if and only if $ \mathcal Q$ does not intersect the span of $ \mathcal P$. If the p-value is allowed to be stochastically larger than uniform under $P\in \mathcal P$, and the e-value can have expectation at most one under $P\in \mathcal P$, then it is achievable whenever $ \mathcal P$ and $ \mathcal Q$ are disjoint. More generally, even when $ \mathcal P$ and $ \mathcal Q$ are not polytopes, we characterize the existence of a bounded nontrivial e-variable whose expectation exactly equals one under any $P \in \mathcal P$. The proofs utilize recently developed techniques in simultaneous optimal transport. A key role is played by coarsening the filtration: sometimes, no such p-value or e-value exists in the richest data filtration, but it does exist in some reduced filtration, and our work provides the first general characterization of this phenomenon. We also provide an iterative construction that explicitly constructs such processes, and under certain conditions it finds the one that grows fastest under a specific alternative $Q$. We discuss implications for the construction of composite nonnegative (super)martingales, and end with some conjectures and open problems.</summary></entry><entry><title type="html">Parallel Approximations for High-Dimensional Multivariate Normal Probability Computation in Confidence Region Detection Applications</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ParallelApproximationsforHighDimensionalMultivariateNormalProbabilityComputationinConfidenceRegionDetectionApplications.html" rel="alternate" type="text/html" title="Parallel Approximations for High-Dimensional Multivariate Normal Probability Computation in Confidence Region Detection Applications" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ParallelApproximationsforHighDimensionalMultivariateNormalProbabilityComputationinConfidenceRegionDetectionApplications</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ParallelApproximationsforHighDimensionalMultivariateNormalProbabilityComputationinConfidenceRegionDetectionApplications.html">&lt;p&gt;Addressing the statistical challenge of computing the multivariate normal (MVN) probability in high dimensions holds significant potential for enhancing various applications. One common way to compute high-dimensional MVN probabilities is the Separation-of-Variables (SOV) algorithm. This algorithm is known for its high computational complexity of O(n^3) and space complexity of O(n^2), mainly due to a Cholesky factorization operation for an n X n covariance matrix, where $n$ represents the dimensionality of the MVN problem. This work proposes a high-performance computing framework that allows scaling the SOV algorithm and, subsequently, the confidence region detection algorithm. The framework leverages parallel linear algebra algorithms with a task-based programming model to achieve performance scalability in computing process probabilities, especially on large-scale systems. In addition, we enhance our implementation by incorporating Tile Low-Rank (TLR) approximation techniques to reduce algorithmic complexity without compromising the necessary accuracy. To evaluate the performance and accuracy of our framework, we conduct assessments using simulated data and a wind speed dataset. Our proposed implementation effectively handles high-dimensional multivariate normal (MVN) probability computations on shared and distributed-memory systems using finite precision arithmetics and TLR approximation computation. Performance results show a significant speedup of up to 20X in solving the MVN problem using TLR approximation compared to the reference dense solution without sacrificing the application’s accuracy. The qualitative results on synthetic and real datasets demonstrate how we maintain high accuracy in detecting confidence regions even when relying on TLR approximation to perform the underlying linear algebra operations.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14892&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Xiran Zhang, Sameh Abdulah, Jian Cao, Hatem Ltaief, Ying Sun, Marc G. Genton, David E. Keyes</name></author><category term="stat.CO" /><summary type="html">Addressing the statistical challenge of computing the multivariate normal (MVN) probability in high dimensions holds significant potential for enhancing various applications. One common way to compute high-dimensional MVN probabilities is the Separation-of-Variables (SOV) algorithm. This algorithm is known for its high computational complexity of O(n^3) and space complexity of O(n^2), mainly due to a Cholesky factorization operation for an n X n covariance matrix, where $n$ represents the dimensionality of the MVN problem. This work proposes a high-performance computing framework that allows scaling the SOV algorithm and, subsequently, the confidence region detection algorithm. The framework leverages parallel linear algebra algorithms with a task-based programming model to achieve performance scalability in computing process probabilities, especially on large-scale systems. In addition, we enhance our implementation by incorporating Tile Low-Rank (TLR) approximation techniques to reduce algorithmic complexity without compromising the necessary accuracy. To evaluate the performance and accuracy of our framework, we conduct assessments using simulated data and a wind speed dataset. Our proposed implementation effectively handles high-dimensional multivariate normal (MVN) probability computations on shared and distributed-memory systems using finite precision arithmetics and TLR approximation computation. Performance results show a significant speedup of up to 20X in solving the MVN problem using TLR approximation compared to the reference dense solution without sacrificing the application’s accuracy. The qualitative results on synthetic and real datasets demonstrate how we maintain high accuracy in detecting confidence regions even when relying on TLR approximation to perform the underlying linear algebra operations.</summary></entry><entry><title type="html">Post-selection inference for quantifying uncertainty in changes in variance</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Postselectioninferenceforquantifyinguncertaintyinchangesinvariance.html" rel="alternate" type="text/html" title="Post-selection inference for quantifying uncertainty in changes in variance" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Postselectioninferenceforquantifyinguncertaintyinchangesinvariance</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Postselectioninferenceforquantifyinguncertaintyinchangesinvariance.html">&lt;p&gt;Quantifying uncertainty in detected changepoints is an important problem. However it is challenging as the naive approach would use the data twice, first to detect the changes, and then to test them. This will bias the test, and can lead to anti-conservative p-values. One approach to avoid this is to use ideas from post-selection inference, which conditions on the information in the data used to choose which changes to test. As a result this produces valid p-values; that is, p-values that have a uniform distribution if there is no change. Currently such methods have been developed for detecting changes in mean only. This paper presents two approaches for constructing post-selection p-values for detecting changes in variance. These vary depending on the method use to detect the changes, but are general in terms of being applicable for a range of change-detection methods and a range of hypotheses that we may wish to test.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15670&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Rachel Carrington, Paul Fearnhead</name></author><category term="stat.ME" /><summary type="html">Quantifying uncertainty in detected changepoints is an important problem. However it is challenging as the naive approach would use the data twice, first to detect the changes, and then to test them. This will bias the test, and can lead to anti-conservative p-values. One approach to avoid this is to use ideas from post-selection inference, which conditions on the information in the data used to choose which changes to test. As a result this produces valid p-values; that is, p-values that have a uniform distribution if there is no change. Currently such methods have been developed for detecting changes in mean only. This paper presents two approaches for constructing post-selection p-values for detecting changes in variance. These vary depending on the method use to detect the changes, but are general in terms of being applicable for a range of change-detection methods and a range of hypotheses that we may wish to test.</summary></entry><entry><title type="html">Predicting Future Change-points in Time Series</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/PredictingFutureChangepointsinTimeSeries.html" rel="alternate" type="text/html" title="Predicting Future Change-points in Time Series" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/PredictingFutureChangepointsinTimeSeries</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/PredictingFutureChangepointsinTimeSeries.html">&lt;p&gt;Change-point detection and estimation procedures have been widely developed in the literature. However, commonly used approaches in change-point analysis have mainly been focusing on detecting change-points within an entire time series (off-line methods), or quickest detection of change-points in sequentially observed data (on-line methods). Both classes of methods are concerned with change-points that have already occurred. The arguably more important question of when future change-points may occur, remains largely unexplored. In this paper, we develop a novel statistical model that describes the mechanism of change-point occurrence. Specifically, the model assumes a latent process in the form of a random walk driven by non-negative innovations, and an observed process which behaves differently when the latent process belongs to different regimes. By construction, an occurrence of a change-point is equivalent to hitting a regime threshold by the latent process. Therefore, by predicting when the latent process will hit the next regime threshold, future change-points can be forecasted. The probabilistic properties of the model such as stationarity and ergodicity are established. A composite likelihood-based approach is developed for parameter estimation and model selection. Moreover, we construct the predictor and prediction interval for future change points based on the estimated model.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.09485&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Chak Fung Choi, Chunxue Li, Chun Yip Yau, Zifeng Zhao</name></author><category term="stat.ME" /><summary type="html">Change-point detection and estimation procedures have been widely developed in the literature. However, commonly used approaches in change-point analysis have mainly been focusing on detecting change-points within an entire time series (off-line methods), or quickest detection of change-points in sequentially observed data (on-line methods). Both classes of methods are concerned with change-points that have already occurred. The arguably more important question of when future change-points may occur, remains largely unexplored. In this paper, we develop a novel statistical model that describes the mechanism of change-point occurrence. Specifically, the model assumes a latent process in the form of a random walk driven by non-negative innovations, and an observed process which behaves differently when the latent process belongs to different regimes. By construction, an occurrence of a change-point is equivalent to hitting a regime threshold by the latent process. Therefore, by predicting when the latent process will hit the next regime threshold, future change-points can be forecasted. The probabilistic properties of the model such as stationarity and ergodicity are established. A composite likelihood-based approach is developed for parameter estimation and model selection. Moreover, we construct the predictor and prediction interval for future change points based on the estimated model.</summary></entry><entry><title type="html">Prediction De-Correlated Inference: A safe approach for post-prediction inference</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/PredictionDeCorrelatedInferenceAsafeapproachforpostpredictioninference.html" rel="alternate" type="text/html" title="Prediction De-Correlated Inference: A safe approach for post-prediction inference" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/PredictionDeCorrelatedInferenceAsafeapproachforpostpredictioninference</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/PredictionDeCorrelatedInferenceAsafeapproachforpostpredictioninference.html">&lt;p&gt;In modern data analysis, it is common to use machine learning methods to predict outcomes on unlabeled datasets and then use these pseudo-outcomes in subsequent statistical inference. Inference in this setting is often called post-prediction inference. We propose a novel assumption-lean framework for statistical inference under post-prediction setting, called Prediction De-Correlated Inference (PDC). Our approach is safe, in the sense that PDC can automatically adapt to any black-box machine-learning model and consistently outperform the supervised counterparts. The PDC framework also offers easy extensibility for accommodating multiple predictive models. Both numerical results and real-world data analysis demonstrate the superiority of PDC over the state-of-the-art methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.06478&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Feng Gan, Wanfeng Liang, Changliang Zou</name></author><category term="stat.ME," /><category term="stat.ML" /><summary type="html">In modern data analysis, it is common to use machine learning methods to predict outcomes on unlabeled datasets and then use these pseudo-outcomes in subsequent statistical inference. Inference in this setting is often called post-prediction inference. We propose a novel assumption-lean framework for statistical inference under post-prediction setting, called Prediction De-Correlated Inference (PDC). Our approach is safe, in the sense that PDC can automatically adapt to any black-box machine-learning model and consistently outperform the supervised counterparts. The PDC framework also offers easy extensibility for accommodating multiple predictive models. Both numerical results and real-world data analysis demonstrate the superiority of PDC over the state-of-the-art methods.</summary></entry><entry><title type="html">Predictive Uncertainty Quantification with Missing Covariates</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/PredictiveUncertaintyQuantificationwithMissingCovariates.html" rel="alternate" type="text/html" title="Predictive Uncertainty Quantification with Missing Covariates" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/PredictiveUncertaintyQuantificationwithMissingCovariates</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/PredictiveUncertaintyQuantificationwithMissingCovariates.html">&lt;p&gt;Predictive uncertainty quantification is crucial in decision-making problems. We investigate how to adequately quantify predictive uncertainty with missing covariates. A bottleneck is that missing values induce heteroskedasticity on the response’s predictive distribution given the observed covariates. Thus, we focus on building predictive sets for the response that are valid conditionally to the missing values pattern. We show that this goal is impossible to achieve informatively in a distribution-free fashion, and we propose useful restrictions on the distribution class. Motivated by these hardness results, we characterize how missing values and predictive uncertainty intertwine. Particularly, we rigorously formalize the idea that the more missing values, the higher the predictive uncertainty. Then, we introduce a generalized framework, coined CP-MDA-Nested&lt;em&gt;, outputting predictive sets in both regression and classification. Under independence between the missing value pattern and both the features and the response (an assumption justified by our hardness results), these predictive sets are valid conditionally to any pattern of missing values. Moreover, it provides great flexibility in the trade-off between statistical variability and efficiency. Finally, we experimentally assess the performances of CP-MDA-Nested&lt;/em&gt; beyond its scope of theoretical validity, demonstrating promising outcomes in more challenging configurations than independence.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15641&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Margaux Zaffran, Julie Josse, Yaniv Romano, Aymeric Dieuleveut</name></author><category term="stat.ME" /><summary type="html">Predictive uncertainty quantification is crucial in decision-making problems. We investigate how to adequately quantify predictive uncertainty with missing covariates. A bottleneck is that missing values induce heteroskedasticity on the response’s predictive distribution given the observed covariates. Thus, we focus on building predictive sets for the response that are valid conditionally to the missing values pattern. We show that this goal is impossible to achieve informatively in a distribution-free fashion, and we propose useful restrictions on the distribution class. Motivated by these hardness results, we characterize how missing values and predictive uncertainty intertwine. Particularly, we rigorously formalize the idea that the more missing values, the higher the predictive uncertainty. Then, we introduce a generalized framework, coined CP-MDA-Nested, outputting predictive sets in both regression and classification. Under independence between the missing value pattern and both the features and the response (an assumption justified by our hardness results), these predictive sets are valid conditionally to any pattern of missing values. Moreover, it provides great flexibility in the trade-off between statistical variability and efficiency. Finally, we experimentally assess the performances of CP-MDA-Nested beyond its scope of theoretical validity, demonstrating promising outcomes in more challenging configurations than independence.</summary></entry><entry><title type="html">Preferential Latent Space Models for Networks with Textual Edges</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/PreferentialLatentSpaceModelsforNetworkswithTextualEdges.html" rel="alternate" type="text/html" title="Preferential Latent Space Models for Networks with Textual Edges" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/PreferentialLatentSpaceModelsforNetworkswithTextualEdges</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/PreferentialLatentSpaceModelsforNetworkswithTextualEdges.html">&lt;p&gt;Many real-world networks contain rich textual information in the edges, such as email networks where an edge between two nodes is an email exchange. Other examples include co-author networks and social media networks. The useful textual information carried in the edges is often discarded in most network analyses, resulting in an incomplete view of the relationships between nodes. In this work, we propose to represent the text document between each pair of nodes as a vector counting the appearances of keywords extracted from the corpus, and introduce a new and flexible preferential latent space network model that can offer direct insights on how contents of the textual exchanges modulate the relationships between nodes. We establish identifiability conditions for the proposed model and tackle model estimation with a computationally efficient projected gradient descent algorithm. We further derive the non-asymptotic error bound of the estimator from each step of the algorithm. The efficacy of our proposed method is demonstrated through simulations and an analysis of the Enron email network.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15038&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Maoyu Zhang, Biao Cai, Dong Li, Xiaoyue Niu, Jingfei Zhang</name></author><category term="stat.ME" /><summary type="html">Many real-world networks contain rich textual information in the edges, such as email networks where an edge between two nodes is an email exchange. Other examples include co-author networks and social media networks. The useful textual information carried in the edges is often discarded in most network analyses, resulting in an incomplete view of the relationships between nodes. In this work, we propose to represent the text document between each pair of nodes as a vector counting the appearances of keywords extracted from the corpus, and introduce a new and flexible preferential latent space network model that can offer direct insights on how contents of the textual exchanges modulate the relationships between nodes. We establish identifiability conditions for the proposed model and tackle model estimation with a computationally efficient projected gradient descent algorithm. We further derive the non-asymptotic error bound of the estimator from each step of the algorithm. The efficacy of our proposed method is demonstrated through simulations and an analysis of the Enron email network.</summary></entry><entry><title type="html">Propagating moments in probabilistic graphical models with polynomial regression forms for decision support systems</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Propagatingmomentsinprobabilisticgraphicalmodelswithpolynomialregressionformsfordecisionsupportsystems.html" rel="alternate" type="text/html" title="Propagating moments in probabilistic graphical models with polynomial regression forms for decision support systems" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Propagatingmomentsinprobabilisticgraphicalmodelswithpolynomialregressionformsfordecisionsupportsystems</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Propagatingmomentsinprobabilisticgraphicalmodelswithpolynomialregressionformsfordecisionsupportsystems.html">&lt;p&gt;Probabilistic graphical models are widely used to model complex systems under uncertainty. Traditionally, Gaussian directed graphical models are applied for analysis of large networks with continuous variables as they can provide conditional and marginal distributions in closed form simplifying the inferential task. The Gaussianity and linearity assumptions are often adequate, yet can lead to poor performance when dealing with some practical applications. In this paper, we model each variable in graph G as a polynomial regression of its parents to capture complex relationships between individual variables and with a utility function of polynomial form. We develop a message-passing algorithm to propagate information throughout the network solely using moments which enables the expected utility scores to be calculated exactly. Our propagation method scales up well and enables to perform inference in terms of a finite number of expectations. We illustrate how the proposed methodology works with examples and in an application to decision problems in energy planning and for real-time clinical decision support.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.03643&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Victoria Volodina, Nikki Sonenberg, Peter Challenor, Jim Q. Smith</name></author><category term="stat.ME," /><category term="stat.AP" /><summary type="html">Probabilistic graphical models are widely used to model complex systems under uncertainty. Traditionally, Gaussian directed graphical models are applied for analysis of large networks with continuous variables as they can provide conditional and marginal distributions in closed form simplifying the inferential task. The Gaussianity and linearity assumptions are often adequate, yet can lead to poor performance when dealing with some practical applications. In this paper, we model each variable in graph G as a polynomial regression of its parents to capture complex relationships between individual variables and with a utility function of polynomial form. We develop a message-passing algorithm to propagate information throughout the network solely using moments which enables the expected utility scores to be calculated exactly. Our propagation method scales up well and enables to perform inference in terms of a finite number of expectations. We illustrate how the proposed methodology works with examples and in an application to decision problems in energy planning and for real-time clinical decision support.</summary></entry><entry><title type="html">Randomness of Shapes and Statistical Inference on Shapes via the Smooth Euler Characteristic Transform</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/RandomnessofShapesandStatisticalInferenceonShapesviatheSmoothEulerCharacteristicTransform.html" rel="alternate" type="text/html" title="Randomness of Shapes and Statistical Inference on Shapes via the Smooth Euler Characteristic Transform" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/RandomnessofShapesandStatisticalInferenceonShapesviatheSmoothEulerCharacteristicTransform</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/RandomnessofShapesandStatisticalInferenceonShapesviatheSmoothEulerCharacteristicTransform.html">&lt;p&gt;In this article, we establish the mathematical foundations for modeling the randomness of shapes and conducting statistical inference on shapes using the smooth Euler characteristic transform. Based on these foundations, we propose two chi-squared statistic-based algorithms for testing hypotheses on random shapes. Simulation studies are presented to validate our mathematical derivations and to compare our algorithms with state-of-the-art methods to demonstrate the utility of our proposed framework. As real applications, we analyze a data set of mandibular molars from four genera of primates and show that our algorithms have the power to detect significant shape differences that recapitulate known morphological variation across suborders. Altogether, our discussions bridge the following fields: algebraic and computational topology, probability theory and stochastic processes, Sobolev spaces and functional analysis, analysis of variance for functional data, and geometric morphometrics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2204.12699&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Kun Meng, Jinyu Wang, Lorin Crawford, Ani Eloyan</name></author><category term="stat.ME" /><summary type="html">In this article, we establish the mathematical foundations for modeling the randomness of shapes and conducting statistical inference on shapes using the smooth Euler characteristic transform. Based on these foundations, we propose two chi-squared statistic-based algorithms for testing hypotheses on random shapes. Simulation studies are presented to validate our mathematical derivations and to compare our algorithms with state-of-the-art methods to demonstrate the utility of our proposed framework. As real applications, we analyze a data set of mandibular molars from four genera of primates and show that our algorithms have the power to detect significant shape differences that recapitulate known morphological variation across suborders. Altogether, our discussions bridge the following fields: algebraic and computational topology, probability theory and stochastic processes, Sobolev spaces and functional analysis, analysis of variance for functional data, and geometric morphometrics.</summary></entry><entry><title type="html">Regularized Halfspace Depth for Functional Data</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/RegularizedHalfspaceDepthforFunctionalData.html" rel="alternate" type="text/html" title="Regularized Halfspace Depth for Functional Data" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/RegularizedHalfspaceDepthforFunctionalData</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/RegularizedHalfspaceDepthforFunctionalData.html">&lt;p&gt;Data depth is a powerful nonparametric tool originally proposed to rank multivariate data from center outward. In this context, one of the most archetypical depth notions is Tukey’s halfspace depth. In the last few decades notions of depth have also been proposed for functional data. However, Tukey’s depth cannot be extended to handle functional data because of its degeneracy. Here, we propose a new halfspace depth for functional data which avoids degeneracy by regularization. The halfspace projection directions are constrained to have a small reproducing kernel Hilbert space norm. Desirable theoretical properties of the proposed depth, such as isometry invariance, maximality at center, monotonicity relative to a deepest point, upper semi-continuity, and consistency are established. Moreover, the regularized halfspace depth can rank functional data with varying emphasis in shape or magnitude, depending on the regularization. A new outlier detection approach is also proposed, which is capable of detecting both shape and magnitude outliers. It is applicable to trajectories in $L^2$, a very general space of functions that include non-smooth trajectories. Based on extensive numerical studies, our methods are shown to perform well in terms of detecting outliers of different types. Three real data examples showcase the proposed depth notion.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.07034&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hyemin Yeon, Xiongtao Dai, Sara Lopez-Pintado</name></author><category term="stat.ME," /><category term="stat.TH" /><summary type="html">Data depth is a powerful nonparametric tool originally proposed to rank multivariate data from center outward. In this context, one of the most archetypical depth notions is Tukey’s halfspace depth. In the last few decades notions of depth have also been proposed for functional data. However, Tukey’s depth cannot be extended to handle functional data because of its degeneracy. Here, we propose a new halfspace depth for functional data which avoids degeneracy by regularization. The halfspace projection directions are constrained to have a small reproducing kernel Hilbert space norm. Desirable theoretical properties of the proposed depth, such as isometry invariance, maximality at center, monotonicity relative to a deepest point, upper semi-continuity, and consistency are established. Moreover, the regularized halfspace depth can rank functional data with varying emphasis in shape or magnitude, depending on the regularization. A new outlier detection approach is also proposed, which is capable of detecting both shape and magnitude outliers. It is applicable to trajectories in $L^2$, a very general space of functions that include non-smooth trajectories. Based on extensive numerical studies, our methods are shown to perform well in terms of detecting outliers of different types. Three real data examples showcase the proposed depth notion.</summary></entry><entry><title type="html">Seismic fragility curves fitting revisited: ordinal regression models and their generalization</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Seismicfragilitycurvesfittingrevisitedordinalregressionmodelsandtheirgeneralization.html" rel="alternate" type="text/html" title="Seismic fragility curves fitting revisited: ordinal regression models and their generalization" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Seismicfragilitycurvesfittingrevisitedordinalregressionmodelsandtheirgeneralization</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/Seismicfragilitycurvesfittingrevisitedordinalregressionmodelsandtheirgeneralization.html">&lt;p&gt;This research conducts a thorough reevaluation of seismic fragility curves by utilizing ordinal regression models, moving away from the commonly used log-normal distribution function known for its simplicity. It explores the nuanced differences and interrelations among various ordinal regression approaches, including Cumulative, Sequential, and Adjacent Category models, alongside their enhanced versions that incorporate category-specific effects and variance heterogeneity. The study applies these methodologies to empirical bridge damage data from the 2008 Wenchuan earthquake, using both frequentist and Bayesian inference methods, and conducts model diagnostics using surrogate residuals. The analysis covers eleven models, from basic to those with heteroscedastic extensions and category-specific effects. Through rigorous leave-one-out cross-validation, the Sequential model with category-specific effects emerges as the most effective. The findings underscore a notable divergence in damage probability predictions between this model and conventional Cumulative probit models, advocating for a substantial transition towards more adaptable fragility curve modeling techniques that enhance the precision of seismic risk assessments. In conclusion, this research not only readdresses the challenge of fitting seismic fragility curves but also advances methodological standards and expands the scope of seismic fragility analysis. It advocates for ongoing innovation and critical reevaluation of conventional methods to advance the predictive accuracy and applicability of seismic fragility models within the performance-based earthquake engineering domain.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15513&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Libo Chen</name></author><category term="stat.AP" /><summary type="html">This research conducts a thorough reevaluation of seismic fragility curves by utilizing ordinal regression models, moving away from the commonly used log-normal distribution function known for its simplicity. It explores the nuanced differences and interrelations among various ordinal regression approaches, including Cumulative, Sequential, and Adjacent Category models, alongside their enhanced versions that incorporate category-specific effects and variance heterogeneity. The study applies these methodologies to empirical bridge damage data from the 2008 Wenchuan earthquake, using both frequentist and Bayesian inference methods, and conducts model diagnostics using surrogate residuals. The analysis covers eleven models, from basic to those with heteroscedastic extensions and category-specific effects. Through rigorous leave-one-out cross-validation, the Sequential model with category-specific effects emerges as the most effective. The findings underscore a notable divergence in damage probability predictions between this model and conventional Cumulative probit models, advocating for a substantial transition towards more adaptable fragility curve modeling techniques that enhance the precision of seismic risk assessments. In conclusion, this research not only readdresses the challenge of fitting seismic fragility curves but also advances methodological standards and expands the scope of seismic fragility analysis. It advocates for ongoing innovation and critical reevaluation of conventional methods to advance the predictive accuracy and applicability of seismic fragility models within the performance-based earthquake engineering domain.</summary></entry><entry><title type="html">Semi-Supervised Learning guided by the Generalized Bayes Rule under Soft Revision</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/SemiSupervisedLearningguidedbytheGeneralizedBayesRuleunderSoftRevision.html" rel="alternate" type="text/html" title="Semi-Supervised Learning guided by the Generalized Bayes Rule under Soft Revision" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/SemiSupervisedLearningguidedbytheGeneralizedBayesRuleunderSoftRevision</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/SemiSupervisedLearningguidedbytheGeneralizedBayesRuleunderSoftRevision.html">&lt;p&gt;We provide a theoretical and computational investigation of the Gamma-Maximin method with soft revision, which was recently proposed as a robust criterion for pseudo-label selection (PLS) in semi-supervised learning. Opposed to traditional methods for PLS we use credal sets of priors (“generalized Bayes”) to represent the epistemic modeling uncertainty. These latter are then updated by the Gamma-Maximin method with soft revision. We eventually select pseudo-labeled data that are most likely in light of the least favorable distribution from the so updated credal set. We formalize the task of finding optimal pseudo-labeled data w.r.t. the Gamma-Maximin method with soft revision as an optimization problem. A concrete implementation for the class of logistic models then allows us to compare the predictive power of the method with competing approaches. It is observed that the Gamma-Maximin method with soft revision can achieve very promising results, especially when the proportion of labeled data is low.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15294&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Stefan Dietrich, Julian Rodemann, Christoph Jansen</name></author><category term="stat.ML," /><category term="stat.ME," /><category term="stat.TH" /><summary type="html">We provide a theoretical and computational investigation of the Gamma-Maximin method with soft revision, which was recently proposed as a robust criterion for pseudo-label selection (PLS) in semi-supervised learning. Opposed to traditional methods for PLS we use credal sets of priors (“generalized Bayes”) to represent the epistemic modeling uncertainty. These latter are then updated by the Gamma-Maximin method with soft revision. We eventually select pseudo-labeled data that are most likely in light of the least favorable distribution from the so updated credal set. We formalize the task of finding optimal pseudo-labeled data w.r.t. the Gamma-Maximin method with soft revision as an optimization problem. A concrete implementation for the class of logistic models then allows us to compare the predictive power of the method with competing approaches. It is observed that the Gamma-Maximin method with soft revision can achieve very promising results, especially when the proportion of labeled data is low.</summary></entry><entry><title type="html">Shrinkage for Extreme Partial Least-Squares</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ShrinkageforExtremePartialLeastSquares.html" rel="alternate" type="text/html" title="Shrinkage for Extreme Partial Least-Squares" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ShrinkageforExtremePartialLeastSquares</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/ShrinkageforExtremePartialLeastSquares.html">&lt;p&gt;This work focuses on dimension-reduction techniques for modelling conditional extreme values. Specifically, we investigate the idea that extreme values of a response variable can be explained by nonlinear functions derived from linear projections of an input random vector. In this context, the estimation of projection directions is examined, as approached by the Extreme Partial Least Squares (EPLS) method–an adaptation of the original Partial Least Squares (PLS) method tailored to the extreme-value framework. Further, a novel interpretation of EPLS directions as maximum likelihood estimators is introduced, utilizing the von Mises-Fisher distribution applied to hyperballs. The dimension reduction process is enhanced through the Bayesian paradigm, enabling the incorporation of prior information into the projection direction estimation. The maximum a posteriori estimator is derived in two specific cases, elucidating it as a regularization or shrinkage of the EPLS estimator. We also establish its asymptotic behavior as the sample size approaches infinity. A simulation data study is conducted in order to assess the practical utility of our proposed method. This clearly demonstrates its effectiveness even in moderate data problems within high-dimensional settings. Furthermore, we provide an illustrative example of the method’s applicability using French farm income data, highlighting its efficacy in real-world scenarios.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2403.09503&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Julyan Arbel, Stéphane Girard, Hadrien Lorenzo</name></author><category term="stat.ME," /><category term="stat.CO," /><category term="stat.TH" /><summary type="html">This work focuses on dimension-reduction techniques for modelling conditional extreme values. Specifically, we investigate the idea that extreme values of a response variable can be explained by nonlinear functions derived from linear projections of an input random vector. In this context, the estimation of projection directions is examined, as approached by the Extreme Partial Least Squares (EPLS) method–an adaptation of the original Partial Least Squares (PLS) method tailored to the extreme-value framework. Further, a novel interpretation of EPLS directions as maximum likelihood estimators is introduced, utilizing the von Mises-Fisher distribution applied to hyperballs. The dimension reduction process is enhanced through the Bayesian paradigm, enabling the incorporation of prior information into the projection direction estimation. The maximum a posteriori estimator is derived in two specific cases, elucidating it as a regularization or shrinkage of the EPLS estimator. We also establish its asymptotic behavior as the sample size approaches infinity. A simulation data study is conducted in order to assess the practical utility of our proposed method. This clearly demonstrates its effectiveness even in moderate data problems within high-dimensional settings. Furthermore, we provide an illustrative example of the method’s applicability using French farm income data, highlighting its efficacy in real-world scenarios.</summary></entry><entry><title type="html">Social and Economic Impact Analysis of Solar Mini-Grids in Rural Africa: A Cohort Study from Kenya and Nigeria</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/SocialandEconomicImpactAnalysisofSolarMiniGridsinRuralAfricaACohortStudyfromKenyaandNigeria.html" rel="alternate" type="text/html" title="Social and Economic Impact Analysis of Solar Mini-Grids in Rural Africa: A Cohort Study from Kenya and Nigeria" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/SocialandEconomicImpactAnalysisofSolarMiniGridsinRuralAfricaACohortStudyfromKenyaandNigeria</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/SocialandEconomicImpactAnalysisofSolarMiniGridsinRuralAfricaACohortStudyfromKenyaandNigeria.html">&lt;p&gt;This study presents the first comprehensive analysis of the social and economic effects of solar mini-grids in rural African settings, specifically in Kenya and Nigeria. A group of 2,658 household heads and business owners connected to mini-grids over the last five years were interviewed both before and one year after their connection. These interviews focused on changes in gender equality, productivity, health, safety, and economic activity. The results show notable improvements in all areas. Economic activities and productivity increased significantly among the connected households and businesses. The median income of rural Kenyan community members quadrupled. Gender equality also improved, with women gaining more opportunities in decision making and business. Health and safety enhancements were linked to reduced use of hazardous energy sources like kerosene lamps. The introduction of solar mini-grids not only transformed the energy landscape but also led to broad socioeconomic benefits in these rural areas. The research highlights the substantial impact of decentralized renewable energy on the social and economic development of rural African communities. Its findings are crucial for policymakers, development agencies, and stakeholders focused on promoting sustainable energy and development in Africa.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.02445&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Amy Town Carabajal, Akoua Orsot, Marie Pelagie Elimbi Moudio, Tracy Haggai, Chioma Joy Okonkwo, George Truett Jarrard III, Nicholas Stearns Selby</name></author><category term="stat.AP" /><summary type="html">This study presents the first comprehensive analysis of the social and economic effects of solar mini-grids in rural African settings, specifically in Kenya and Nigeria. A group of 2,658 household heads and business owners connected to mini-grids over the last five years were interviewed both before and one year after their connection. These interviews focused on changes in gender equality, productivity, health, safety, and economic activity. The results show notable improvements in all areas. Economic activities and productivity increased significantly among the connected households and businesses. The median income of rural Kenyan community members quadrupled. Gender equality also improved, with women gaining more opportunities in decision making and business. Health and safety enhancements were linked to reduced use of hazardous energy sources like kerosene lamps. The introduction of solar mini-grids not only transformed the energy landscape but also led to broad socioeconomic benefits in these rural areas. The research highlights the substantial impact of decentralized renewable energy on the social and economic development of rural African communities. Its findings are crucial for policymakers, development agencies, and stakeholders focused on promoting sustainable energy and development in Africa.</summary></entry><entry><title type="html">Strong screening rules for group-based SLOPE models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/StrongscreeningrulesforgroupbasedSLOPEmodels.html" rel="alternate" type="text/html" title="Strong screening rules for group-based SLOPE models" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/StrongscreeningrulesforgroupbasedSLOPEmodels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/StrongscreeningrulesforgroupbasedSLOPEmodels.html">&lt;p&gt;Tuning the regularization parameter in penalized regression models is an expensive task, requiring multiple models to be fit along a path of parameters. Strong screening rules drastically reduce computational costs by lowering the dimensionality of the input prior to fitting. We develop strong screening rules for group-based Sorted L-One Penalized Estimation (SLOPE) models: Group SLOPE and Sparse-group SLOPE. The developed rules are applicable for the wider family of group-based OWL models, including OSCAR. Our experiments on both synthetic and real data show that the screening rules significantly accelerate the fitting process. The screening rules make it accessible for group SLOPE and sparse-group SLOPE to be applied to high-dimensional datasets, particularly those encountered in genetics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15357&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Fabio Feser, Marina Evangelou</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">Tuning the regularization parameter in penalized regression models is an expensive task, requiring multiple models to be fit along a path of parameters. Strong screening rules drastically reduce computational costs by lowering the dimensionality of the input prior to fitting. We develop strong screening rules for group-based Sorted L-One Penalized Estimation (SLOPE) models: Group SLOPE and Sparse-group SLOPE. The developed rules are applicable for the wider family of group-based OWL models, including OSCAR. Our experiments on both synthetic and real data show that the screening rules significantly accelerate the fitting process. The screening rules make it accessible for group SLOPE and sparse-group SLOPE to be applied to high-dimensional datasets, particularly those encountered in genetics.</summary></entry><entry><title type="html">Study on spike-and-wave detection in epileptic signals using t-location-scale distribution and the K-nearest neighbors classifier</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/StudyonspikeandwavedetectioninepilepticsignalsusingtlocationscaledistributionandtheKnearestneighborsclassifier.html" rel="alternate" type="text/html" title="Study on spike-and-wave detection in epileptic signals using t-location-scale distribution and the K-nearest neighbors classifier" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/StudyonspikeandwavedetectioninepilepticsignalsusingtlocationscaledistributionandtheKnearestneighborsclassifier</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/StudyonspikeandwavedetectioninepilepticsignalsusingtlocationscaledistributionandtheKnearestneighborsclassifier.html">&lt;p&gt;Pattern classification in electroencephalography (EEG) signals is an important problem in biomedical engineering since it enables the detection of brain activity, particularly the early detection of epileptic seizures. In this paper, we propose a k-nearest neighbors classification for epileptic EEG signals based on a t-location-scale statistical representation to detect spike-and-waves. The proposed approach is demonstrated on a real dataset containing both spike-and-wave events and normal brain function signals, where our performance is evaluated in terms of classification accuracy, sensitivity, and specificity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.14896&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Antonio Quintero-Rincón, Jorge Prendes, Valeria Muro, Carlos D&apos;Giano</name></author><category term="stat.AP," /><category term="stat.CO," /><category term="stat.ML" /><summary type="html">Pattern classification in electroencephalography (EEG) signals is an important problem in biomedical engineering since it enables the detection of brain activity, particularly the early detection of epileptic seizures. In this paper, we propose a k-nearest neighbors classification for epileptic EEG signals based on a t-location-scale statistical representation to detect spike-and-waves. The proposed approach is demonstrated on a real dataset containing both spike-and-wave events and normal brain function signals, where our performance is evaluated in terms of classification accuracy, sensitivity, and specificity.</summary></entry><entry><title type="html">The Chained Difference-in-Differences</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/TheChainedDifferenceinDifferences.html" rel="alternate" type="text/html" title="The Chained Difference-in-Differences" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/TheChainedDifferenceinDifferences</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/TheChainedDifferenceinDifferences.html">&lt;p&gt;This paper studies the identification, estimation, and inference of long-term (binary) treatment effect parameters when balanced panel data is not available, or consists of only a subset of the available data. We develop a new estimator: the chained difference-in-differences, which leverages the overlapping structure of many unbalanced panel data sets. This approach consists in aggregating a collection of short-term treatment effects estimated on multiple incomplete panels. Our estimator accommodates (1) multiple time periods, (2) variation in treatment timing, (3) treatment effect heterogeneity, (4) general missing data patterns, and (5) sample selection on observables. We establish the asymptotic properties of the proposed estimator and discuss identification and efficiency gains in comparison to existing methods. Finally, we illustrate its relevance through (i) numerical simulations, and (ii) an application about the effects of an innovation policy in France.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.01085&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Christophe Bellégo, David Benatia, Vincent Dortet-Bernardet</name></author><category term="stat.AP" /><summary type="html">This paper studies the identification, estimation, and inference of long-term (binary) treatment effect parameters when balanced panel data is not available, or consists of only a subset of the available data. We develop a new estimator: the chained difference-in-differences, which leverages the overlapping structure of many unbalanced panel data sets. This approach consists in aggregating a collection of short-term treatment effects estimated on multiple incomplete panels. Our estimator accommodates (1) multiple time periods, (2) variation in treatment timing, (3) treatment effect heterogeneity, (4) general missing data patterns, and (5) sample selection on observables. We establish the asymptotic properties of the proposed estimator and discuss identification and efficiency gains in comparison to existing methods. Finally, we illustrate its relevance through (i) numerical simulations, and (ii) an application about the effects of an innovation policy in France.</summary></entry><entry><title type="html">Transfer Learning for Spatial Autoregressive Models</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/TransferLearningforSpatialAutoregressiveModels.html" rel="alternate" type="text/html" title="Transfer Learning for Spatial Autoregressive Models" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/TransferLearningforSpatialAutoregressiveModels</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/TransferLearningforSpatialAutoregressiveModels.html">&lt;p&gt;The spatial autoregressive (SAR) model has been widely applied in various empirical economic studies to characterize the spatial dependence among subjects. However, the precision of estimating the SAR model diminishes when the sample size of the target data is limited. In this paper, we propose a new transfer learning framework for the SAR model to borrow the information from similar source data to improve both estimation and prediction. When the informative source data sets are known, we introduce a two-stage algorithm, including a transferring stage and a debiasing stage, to estimate the unknown parameters and also establish the theoretical convergence rates for the resulting estimators. If we do not know which sources to transfer, a transferable source detection algorithm is proposed to detect informative sources data based on spatial residual bootstrap to retain the necessary spatial dependence. Its detection consistency is also derived. Simulation studies demonstrate that using informative source data, our transfer learning algorithm significantly enhances the performance of the classical two-stage least squares estimator. In the empirical application, we apply our method to the election prediction in swing states in the 2020 U.S. presidential election, utilizing polling data from the 2016 U.S. presidential election along with other demographic and geographical data. The empirical results show that our method outperforms traditional estimation methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15600&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Hao Zeng, Wei Zhong, Xingbai Xu</name></author><category term="stat.ML," /><category term="stat.ME" /><summary type="html">The spatial autoregressive (SAR) model has been widely applied in various empirical economic studies to characterize the spatial dependence among subjects. However, the precision of estimating the SAR model diminishes when the sample size of the target data is limited. In this paper, we propose a new transfer learning framework for the SAR model to borrow the information from similar source data to improve both estimation and prediction. When the informative source data sets are known, we introduce a two-stage algorithm, including a transferring stage and a debiasing stage, to estimate the unknown parameters and also establish the theoretical convergence rates for the resulting estimators. If we do not know which sources to transfer, a transferable source detection algorithm is proposed to detect informative sources data based on spatial residual bootstrap to retain the necessary spatial dependence. Its detection consistency is also derived. Simulation studies demonstrate that using informative source data, our transfer learning algorithm significantly enhances the performance of the classical two-stage least squares estimator. In the empirical application, we apply our method to the election prediction in swing states in the 2020 U.S. presidential election, utilizing polling data from the 2016 U.S. presidential election along with other demographic and geographical data. The empirical results show that our method outperforms traditional estimation methods.</summary></entry><entry><title type="html">realSEUDO for real-time calcium imaging analysis</title><link href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/realSEUDOforrealtimecalciumimaginganalysis.html" rel="alternate" type="text/html" title="realSEUDO for real-time calcium imaging analysis" /><published>2024-05-27T00:00:00+00:00</published><updated>2024-05-27T00:00:00+00:00</updated><id>https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/realSEUDOforrealtimecalciumimaginganalysis</id><content type="html" xml:base="https://emanuelealiverti.github.io/arxiv_rss/2024/05/27/realSEUDOforrealtimecalciumimaginganalysis.html">&lt;p&gt;Closed-loop neuroscience experimentation, where recorded neural activity is used to modify the experiment on-the-fly, is critical for deducing causal connections and optimizing experimental time. A critical step in creating a closed-loop experiment is real-time inference of neural activity from streaming recordings. One challenging modality for real-time processing is multi-photon calcium imaging (CI). CI enables the recording of activity in large populations of neurons however, often requires batch processing of the video data to extract single-neuron activity from the fluorescence videos. We use the recently proposed robust time-trace estimator-Sparse Emulation of Unused Dictionary Objects (SEUDO) algorithm-as a basis for a new on-line processing algorithm that simultaneously identifies neurons in the fluorescence video and infers their time traces in a way that is robust to as-yet unidentified neurons. To achieve real-time SEUDO (realSEUDO), we optimize the core estimator via both algorithmic improvements and an fast C-based implementation, and create a new cell finding loop to enable realSEUDO to also identify new cells. We demonstrate comparable performance to offline algorithms (e.g., CNMF), and improved performance over the current on-line approach (OnACID) at speeds of 120 Hz on average.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2405.15701&quot;&gt;Read more&lt;/a&gt;&lt;/p&gt;</content><author><name>Iuliia Dmitrieva, Sergey Babkin, Adam S. Charles</name></author><category term="stat.CO" /><summary type="html">Closed-loop neuroscience experimentation, where recorded neural activity is used to modify the experiment on-the-fly, is critical for deducing causal connections and optimizing experimental time. A critical step in creating a closed-loop experiment is real-time inference of neural activity from streaming recordings. One challenging modality for real-time processing is multi-photon calcium imaging (CI). CI enables the recording of activity in large populations of neurons however, often requires batch processing of the video data to extract single-neuron activity from the fluorescence videos. We use the recently proposed robust time-trace estimator-Sparse Emulation of Unused Dictionary Objects (SEUDO) algorithm-as a basis for a new on-line processing algorithm that simultaneously identifies neurons in the fluorescence video and infers their time traces in a way that is robust to as-yet unidentified neurons. To achieve real-time SEUDO (realSEUDO), we optimize the core estimator via both algorithmic improvements and an fast C-based implementation, and create a new cell finding loop to enable realSEUDO to also identify new cells. We demonstrate comparable performance to offline algorithms (e.g., CNMF), and improved performance over the current on-line approach (OnACID) at speeds of 120 Hz on average.</summary></entry></feed>