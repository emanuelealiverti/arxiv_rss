<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Quantifying Uncertainty in Classification Performance: ROC Confidence Bands Using Conformal Prediction</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Quantifying Uncertainty in Classification Performance: ROC Confidence Bands Using Conformal Prediction" />
<meta name="author" content="Zheshi Zheng, Bo Yang, Peter Song" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="To evaluate a classification algorithm, it is common practice to plot the ROC curve using test data. However, the inherent randomness in the test data can undermine our confidence in the conclusions drawn from the ROC curve, necessitating uncertainty quantification. In this article, we propose an algorithm to construct confidence bands for the ROC curve, quantifying the uncertainty of classification on the test data in terms of sensitivity and specificity. The algorithm is based on a procedure called conformal prediction, which constructs individualized confidence intervals for the test set and the confidence bands for the ROC curve can be obtained by combining the individualized intervals together. Furthermore, we address both scenarios where the test data are either iid or non-iid relative to the observed data set and propose distinct algorithms for each case with valid coverage probability. The proposed method is validated through both theoretical results and numerical experiments." />
<meta property="og:description" content="To evaluate a classification algorithm, it is common practice to plot the ROC curve using test data. However, the inherent randomness in the test data can undermine our confidence in the conclusions drawn from the ROC curve, necessitating uncertainty quantification. In this article, we propose an algorithm to construct confidence bands for the ROC curve, quantifying the uncertainty of classification on the test data in terms of sensitivity and specificity. The algorithm is based on a procedure called conformal prediction, which constructs individualized confidence intervals for the test set and the confidence bands for the ROC curve can be obtained by combining the individualized intervals together. Furthermore, we address both scenarios where the test data are either iid or non-iid relative to the observed data set and propose distinct algorithms for each case with valid coverage probability. The proposed method is validated through both theoretical results and numerical experiments." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/QuantifyingUncertaintyinClassificationPerformanceROCConfidenceBandsUsingConformalPrediction.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/QuantifyingUncertaintyinClassificationPerformanceROCConfidenceBandsUsingConformalPrediction.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-05-22T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Quantifying Uncertainty in Classification Performance: ROC Confidence Bands Using Conformal Prediction" />
<script type="application/ld+json">
{"description":"To evaluate a classification algorithm, it is common practice to plot the ROC curve using test data. However, the inherent randomness in the test data can undermine our confidence in the conclusions drawn from the ROC curve, necessitating uncertainty quantification. In this article, we propose an algorithm to construct confidence bands for the ROC curve, quantifying the uncertainty of classification on the test data in terms of sensitivity and specificity. The algorithm is based on a procedure called conformal prediction, which constructs individualized confidence intervals for the test set and the confidence bands for the ROC curve can be obtained by combining the individualized intervals together. Furthermore, we address both scenarios where the test data are either iid or non-iid relative to the observed data set and propose distinct algorithms for each case with valid coverage probability. The proposed method is validated through both theoretical results and numerical experiments.","author":{"@type":"Person","name":"Zheshi Zheng, Bo Yang, Peter Song"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/QuantifyingUncertaintyinClassificationPerformanceROCConfidenceBandsUsingConformalPrediction.html"},"dateModified":"2024-05-22T00:00:00+00:00","url":"https://emanuelealiverti.github.io/arxiv_rss/2024/05/22/QuantifyingUncertaintyinClassificationPerformanceROCConfidenceBandsUsingConformalPrediction.html","headline":"Quantifying Uncertainty in Classification Performance: ROC Confidence Bands Using Conformal Prediction","@type":"BlogPosting","datePublished":"2024-05-22T00:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-05-22 00:00:00 +0000">05-22</time>
  </p>
  
  <h1>Quantifying Uncertainty in Classification Performance: ROC Confidence Bands Using Conformal Prediction</h1>
  <br>Zheshi Zheng, Bo Yang, Peter Song</h3>
  <br> [stat.ME]

  <p>To evaluate a classification algorithm, it is common practice to plot the ROC curve using test data. However, the inherent randomness in the test data can undermine our confidence in the conclusions drawn from the ROC curve, necessitating uncertainty quantification. In this article, we propose an algorithm to construct confidence bands for the ROC curve, quantifying the uncertainty of classification on the test data in terms of sensitivity and specificity. The algorithm is based on a procedure called conformal prediction, which constructs individualized confidence intervals for the test set and the confidence bands for the ROC curve can be obtained by combining the individualized intervals together. Furthermore, we address both scenarios where the test data are either iid or non-iid relative to the observed data set and propose distinct algorithms for each case with valid coverage probability. The proposed method is validated through both theoretical results and numerical experiments.</p>

<p><a href="https://arxiv.org/abs/2405.12953">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>