<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Optimal Bias-Correction and Valid Inference in High-Dimensional Ridge Regression: A Closed-Form Solution</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Optimal Bias-Correction and Valid Inference in High-Dimensional Ridge Regression: A Closed-Form Solution" />
<meta name="author" content="Zhaoxing Gao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Ridge regression is an indispensable tool in big data econometrics but suffers from bias issues affecting both statistical efficiency and scalability. We introduce an iterative strategy to correct the bias effectively when the dimension $p$ is less than the sample size $n$. For $p&gt;n$, our method optimally reduces the bias to a level unachievable through linear transformations of the response. We employ a Ridge-Screening (RS) method to handle the remaining bias when $p&gt;n$, creating a reduced model suitable for bias-correction. Under certain conditions, the selected model nests the true one, making RS a novel variable selection approach. We establish the asymptotic properties and valid inferences of our de-biased ridge estimators for both $p&lt; n$ and $p&gt;n$, where $p$ and $n$ may grow towards infinity, along with the number of iterations. Our method is validated using simulated and real-world data examples, providing a closed-form solution to bias challenges in ridge regression inferences." />
<meta property="og:description" content="Ridge regression is an indispensable tool in big data econometrics but suffers from bias issues affecting both statistical efficiency and scalability. We introduce an iterative strategy to correct the bias effectively when the dimension $p$ is less than the sample size $n$. For $p&gt;n$, our method optimally reduces the bias to a level unachievable through linear transformations of the response. We employ a Ridge-Screening (RS) method to handle the remaining bias when $p&gt;n$, creating a reduced model suitable for bias-correction. Under certain conditions, the selected model nests the true one, making RS a novel variable selection approach. We establish the asymptotic properties and valid inferences of our de-biased ridge estimators for both $p&lt; n$ and $p&gt;n$, where $p$ and $n$ may grow towards infinity, along with the number of iterations. Our method is validated using simulated and real-world data examples, providing a closed-form solution to bias challenges in ridge regression inferences." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OptimalBiasCorrectionandValidInferenceinHighDimensionalRidgeRegressionAClosedFormSolution.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OptimalBiasCorrectionandValidInferenceinHighDimensionalRidgeRegressionAClosedFormSolution.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-05-02T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Optimal Bias-Correction and Valid Inference in High-Dimensional Ridge Regression: A Closed-Form Solution" />
<script type="application/ld+json">
{"description":"Ridge regression is an indispensable tool in big data econometrics but suffers from bias issues affecting both statistical efficiency and scalability. We introduce an iterative strategy to correct the bias effectively when the dimension $p$ is less than the sample size $n$. For $p&gt;n$, our method optimally reduces the bias to a level unachievable through linear transformations of the response. We employ a Ridge-Screening (RS) method to handle the remaining bias when $p&gt;n$, creating a reduced model suitable for bias-correction. Under certain conditions, the selected model nests the true one, making RS a novel variable selection approach. We establish the asymptotic properties and valid inferences of our de-biased ridge estimators for both $p&lt; n$ and $p&gt;n$, where $p$ and $n$ may grow towards infinity, along with the number of iterations. Our method is validated using simulated and real-world data examples, providing a closed-form solution to bias challenges in ridge regression inferences.","headline":"Optimal Bias-Correction and Valid Inference in High-Dimensional Ridge Regression: A Closed-Form Solution","dateModified":"2024-05-02T00:00:00+00:00","datePublished":"2024-05-02T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OptimalBiasCorrectionandValidInferenceinHighDimensionalRidgeRegressionAClosedFormSolution.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/05/02/OptimalBiasCorrectionandValidInferenceinHighDimensionalRidgeRegressionAClosedFormSolution.html","author":{"@type":"Person","name":"Zhaoxing Gao"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-05-02 00:00:00 +0000">05-02</time>
  </p>
  
  <h1>Optimal Bias-Correction and Valid Inference in High-Dimensional Ridge Regression: A Closed-Form Solution</h1>
  <br>Zhaoxing Gao</h3>
  <br> [stat.ME,stat.ML]

  <p>Ridge regression is an indispensable tool in big data econometrics but suffers from bias issues affecting both statistical efficiency and scalability. We introduce an iterative strategy to correct the bias effectively when the dimension $p$ is less than the sample size $n$. For $p&gt;n$, our method optimally reduces the bias to a level unachievable through linear transformations of the response. We employ a Ridge-Screening (RS) method to handle the remaining bias when $p&gt;n$, creating a reduced model suitable for bias-correction. Under certain conditions, the selected model nests the true one, making RS a novel variable selection approach. We establish the asymptotic properties and valid inferences of our de-biased ridge estimators for both $p&lt; n$ and $p&gt;n$, where $p$ and $n$ may grow towards infinity, along with the number of iterations. Our method is validated using simulated and real-world data examples, providing a closed-form solution to bias challenges in ridge regression inferences.</p>

<p><a href="https://arxiv.org/abs/2405.00424">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>