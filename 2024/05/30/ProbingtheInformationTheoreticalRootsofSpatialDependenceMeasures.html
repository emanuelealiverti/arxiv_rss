<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Probing the Information Theoretical Roots of Spatial Dependence Measures</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Probing the Information Theoretical Roots of Spatial Dependence Measures" />
<meta name="author" content="Zhangyu Wang, Krzysztof Janowicz, Gengchen Mai, Ivan Majic" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Intuitively, there is a relation between measures of spatial dependence and information theoretical measures of entropy. For instance, we can provide an intuition of why spatial data is special by stating that, on average, spatial data samples contain less than expected information. Similarly, spatial data, e.g., remotely sensed imagery, that is easy to compress is also likely to show significant spatial autocorrelation. Formulating our (highly specific) core concepts of spatial information theory in the widely used language of information theory opens new perspectives on their differences and similarities and also fosters cross-disciplinary collaboration, e.g., with the broader AI/ML communities. Interestingly, however, this intuitive relation is challenging to formalize and generalize, leading prior work to rely mostly on experimental results, e.g., for describing landscape patterns. In this work, we will explore the information theoretical roots of spatial autocorrelation, more specifically Moran’s I, through the lens of self-information (also known as surprisal) and provide both formal proofs and experiments." />
<meta property="og:description" content="Intuitively, there is a relation between measures of spatial dependence and information theoretical measures of entropy. For instance, we can provide an intuition of why spatial data is special by stating that, on average, spatial data samples contain less than expected information. Similarly, spatial data, e.g., remotely sensed imagery, that is easy to compress is also likely to show significant spatial autocorrelation. Formulating our (highly specific) core concepts of spatial information theory in the widely used language of information theory opens new perspectives on their differences and similarities and also fosters cross-disciplinary collaboration, e.g., with the broader AI/ML communities. Interestingly, however, this intuitive relation is challenging to formalize and generalize, leading prior work to rely mostly on experimental results, e.g., for describing landscape patterns. In this work, we will explore the information theoretical roots of spatial autocorrelation, more specifically Moran’s I, through the lens of self-information (also known as surprisal) and provide both formal proofs and experiments." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ProbingtheInformationTheoreticalRootsofSpatialDependenceMeasures.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ProbingtheInformationTheoreticalRootsofSpatialDependenceMeasures.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-05-30T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Probing the Information Theoretical Roots of Spatial Dependence Measures" />
<script type="application/ld+json">
{"description":"Intuitively, there is a relation between measures of spatial dependence and information theoretical measures of entropy. For instance, we can provide an intuition of why spatial data is special by stating that, on average, spatial data samples contain less than expected information. Similarly, spatial data, e.g., remotely sensed imagery, that is easy to compress is also likely to show significant spatial autocorrelation. Formulating our (highly specific) core concepts of spatial information theory in the widely used language of information theory opens new perspectives on their differences and similarities and also fosters cross-disciplinary collaboration, e.g., with the broader AI/ML communities. Interestingly, however, this intuitive relation is challenging to formalize and generalize, leading prior work to rely mostly on experimental results, e.g., for describing landscape patterns. In this work, we will explore the information theoretical roots of spatial autocorrelation, more specifically Moran’s I, through the lens of self-information (also known as surprisal) and provide both formal proofs and experiments.","author":{"@type":"Person","name":"Zhangyu Wang, Krzysztof Janowicz, Gengchen Mai, Ivan Majic"},"datePublished":"2024-05-30T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ProbingtheInformationTheoreticalRootsofSpatialDependenceMeasures.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/05/30/ProbingtheInformationTheoreticalRootsofSpatialDependenceMeasures.html","headline":"Probing the Information Theoretical Roots of Spatial Dependence Measures","@type":"BlogPosting","dateModified":"2024-05-30T00:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-05-30 00:00:00 +0000">05-30</time>
  </p>
  
  <h1>Probing the Information Theoretical Roots of Spatial Dependence Measures</h1>
  <br>Zhangyu Wang, Krzysztof Janowicz, Gengchen Mai, Ivan Majic</h3>
  <br> [stat.ME]

  <p>Intuitively, there is a relation between measures of spatial dependence and information theoretical measures of entropy. For instance, we can provide an intuition of why spatial data is special by stating that, on average, spatial data samples contain less than expected information. Similarly, spatial data, e.g., remotely sensed imagery, that is easy to compress is also likely to show significant spatial autocorrelation. Formulating our (highly specific) core concepts of spatial information theory in the widely used language of information theory opens new perspectives on their differences and similarities and also fosters cross-disciplinary collaboration, e.g., with the broader AI/ML communities. Interestingly, however, this intuitive relation is challenging to formalize and generalize, leading prior work to rely mostly on experimental results, e.g., for describing landscape patterns. In this work, we will explore the information theoretical roots of spatial autocorrelation, more specifically Moran’s I, through the lens of self-information (also known as surprisal) and provide both formal proofs and experiments.</p>

<p><a href="https://arxiv.org/abs/2405.18459">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>