<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Testing for no effect in regression problems: a permutation approach</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Testing for no effect in regression problems: a permutation approach" />
<meta name="author" content="Micha{\l} Ciszewski, Jakob Söhl, Ton Leenen, Bart van Trigt, Geurt Jongbloed" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Often the question arises whether $Y$ can be predicted based on $X$ using a certain model. Especially for highly flexible models such as neural networks one may ask whether a seemingly good prediction is actually better than fitting pure noise or whether it has to be attributed to the flexibility of the model. This paper proposes a rigorous permutation test to assess whether the prediction is better than the prediction of pure noise. The test avoids any sample splitting and is based instead on generating new pairings of $(X_i,Y_j)$. It introduces a new formulation of the null hypothesis and rigorous justification for the test, which distinguishes it from previous literature. The theoretical findings are applied both to simulated data and to sensor data of tennis serves in an experimental context. The simulation study underscores how the available information affects the test. It shows that the less informative the predictors, the lower the probability of rejecting the null hypothesis of fitting pure noise and emphasizes that detecting weaker dependence between variables requires a sufficient sample size." />
<meta property="og:description" content="Often the question arises whether $Y$ can be predicted based on $X$ using a certain model. Especially for highly flexible models such as neural networks one may ask whether a seemingly good prediction is actually better than fitting pure noise or whether it has to be attributed to the flexibility of the model. This paper proposes a rigorous permutation test to assess whether the prediction is better than the prediction of pure noise. The test avoids any sample splitting and is based instead on generating new pairings of $(X_i,Y_j)$. It introduces a new formulation of the null hypothesis and rigorous justification for the test, which distinguishes it from previous literature. The theoretical findings are applied both to simulated data and to sensor data of tennis serves in an experimental context. The simulation study underscores how the available information affects the test. It shows that the less informative the predictors, the lower the probability of rejecting the null hypothesis of fitting pure noise and emphasizes that detecting weaker dependence between variables requires a sufficient sample size." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Testingfornoeffectinregressionproblemsapermutationapproach.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Testingfornoeffectinregressionproblemsapermutationapproach.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-04-30T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Testing for no effect in regression problems: a permutation approach" />
<script type="application/ld+json">
{"description":"Often the question arises whether $Y$ can be predicted based on $X$ using a certain model. Especially for highly flexible models such as neural networks one may ask whether a seemingly good prediction is actually better than fitting pure noise or whether it has to be attributed to the flexibility of the model. This paper proposes a rigorous permutation test to assess whether the prediction is better than the prediction of pure noise. The test avoids any sample splitting and is based instead on generating new pairings of $(X_i,Y_j)$. It introduces a new formulation of the null hypothesis and rigorous justification for the test, which distinguishes it from previous literature. The theoretical findings are applied both to simulated data and to sensor data of tennis serves in an experimental context. The simulation study underscores how the available information affects the test. It shows that the less informative the predictors, the lower the probability of rejecting the null hypothesis of fitting pure noise and emphasizes that detecting weaker dependence between variables requires a sufficient sample size.","headline":"Testing for no effect in regression problems: a permutation approach","dateModified":"2024-04-30T00:00:00+00:00","datePublished":"2024-04-30T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Testingfornoeffectinregressionproblemsapermutationapproach.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/04/30/Testingfornoeffectinregressionproblemsapermutationapproach.html","author":{"@type":"Person","name":"Micha{\\l} Ciszewski, Jakob Söhl, Ton Leenen, Bart van Trigt, Geurt Jongbloed"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-04-30 00:00:00 +0000">04-30</time>
  </p>
  
  <h1>Testing for no effect in regression problems: a permutation approach</h1>
  <br>Micha{\l} Ciszewski, Jakob Söhl, Ton Leenen, Bart van Trigt, Geurt Jongbloed</h3>
  <br> [stat.ME,stat.TH]

  <p>Often the question arises whether $Y$ can be predicted based on $X$ using a certain model. Especially for highly flexible models such as neural networks one may ask whether a seemingly good prediction is actually better than fitting pure noise or whether it has to be attributed to the flexibility of the model. This paper proposes a rigorous permutation test to assess whether the prediction is better than the prediction of pure noise. The test avoids any sample splitting and is based instead on generating new pairings of $(X_i,Y_j)$. It introduces a new formulation of the null hypothesis and rigorous justification for the test, which distinguishes it from previous literature. The theoretical findings are applied both to simulated data and to sensor data of tennis serves in an experimental context. The simulation study underscores how the available information affects the test. It shows that the less informative the predictors, the lower the probability of rejecting the null hypothesis of fitting pure noise and emphasizes that detecting weaker dependence between variables requires a sufficient sample size.</p>

<p><a href="https://arxiv.org/abs/2305.02685">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>