<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Shrinkage MMSE estimators of covariances beyond the zero-mean and stationary variance assumptions</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Shrinkage MMSE estimators of covariances beyond the zero-mean and stationary variance assumptions" />
<meta name="author" content="Olivier Flasseur, Eric Thiébaut, Loïc Denis, Maud Langlois" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We tackle covariance estimation in low-sample scenarios, employing a structured covariance matrix with shrinkage methods. These involve convexly combining a low-bias/high-variance empirical estimate with a biased regularization estimator, striking a bias-variance trade-off. Literature provides optimal settings of the regularization amount through risk minimization between the true covariance and its shrunk counterpart. Such estimators were derived for zero-mean statistics with i.i.d. diagonal regularization matrices accounting for the average sample variance solely. We extend these results to regularization matrices accounting for the sample variances both for centered and non-centered samples. In the latter case, the empirical estimate of the true mean is incorporated into our shrinkage estimators. Introducing confidence weights into the statistics also enhance estimator robustness against outliers. We compare our estimators to other shrinkage methods both on numerical simulations and on real data to solve a detection problem in astronomy." />
<meta property="og:description" content="We tackle covariance estimation in low-sample scenarios, employing a structured covariance matrix with shrinkage methods. These involve convexly combining a low-bias/high-variance empirical estimate with a biased regularization estimator, striking a bias-variance trade-off. Literature provides optimal settings of the regularization amount through risk minimization between the true covariance and its shrunk counterpart. Such estimators were derived for zero-mean statistics with i.i.d. diagonal regularization matrices accounting for the average sample variance solely. We extend these results to regularization matrices accounting for the sample variances both for centered and non-centered samples. In the latter case, the empirical estimate of the true mean is incorporated into our shrinkage estimators. Introducing confidence weights into the statistics also enhance estimator robustness against outliers. We compare our estimators to other shrinkage methods both on numerical simulations and on real data to solve a detection problem in astronomy." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ShrinkageMMSEestimatorsofcovariancesbeyondthezeromeanandstationaryvarianceassumptions.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ShrinkageMMSEestimatorsofcovariancesbeyondthezeromeanandstationaryvarianceassumptions.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-28T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Shrinkage MMSE estimators of covariances beyond the zero-mean and stationary variance assumptions" />
<script type="application/ld+json">
{"description":"We tackle covariance estimation in low-sample scenarios, employing a structured covariance matrix with shrinkage methods. These involve convexly combining a low-bias/high-variance empirical estimate with a biased regularization estimator, striking a bias-variance trade-off. Literature provides optimal settings of the regularization amount through risk minimization between the true covariance and its shrunk counterpart. Such estimators were derived for zero-mean statistics with i.i.d. diagonal regularization matrices accounting for the average sample variance solely. We extend these results to regularization matrices accounting for the sample variances both for centered and non-centered samples. In the latter case, the empirical estimate of the true mean is incorporated into our shrinkage estimators. Introducing confidence weights into the statistics also enhance estimator robustness against outliers. We compare our estimators to other shrinkage methods both on numerical simulations and on real data to solve a detection problem in astronomy.","author":{"@type":"Person","name":"Olivier Flasseur, Eric Thiébaut, Loïc Denis, Maud Langlois"},"headline":"Shrinkage MMSE estimators of covariances beyond the zero-mean and stationary variance assumptions","dateModified":"2024-06-28T00:00:00+00:00","datePublished":"2024-06-28T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ShrinkageMMSEestimatorsofcovariancesbeyondthezeromeanandstationaryvarianceassumptions.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/28/ShrinkageMMSEestimatorsofcovariancesbeyondthezeromeanandstationaryvarianceassumptions.html","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-06-28 00:00:00 +0000">06-28</time>
  </p>
  
  <h1>Shrinkage MMSE estimators of covariances beyond the zero-mean and stationary variance assumptions</h1>
  <br>Olivier Flasseur, Eric Thiébaut, Loïc Denis, Maud Langlois</h3>
  <br> [stat.ME]

  <p>We tackle covariance estimation in low-sample scenarios, employing a structured covariance matrix with shrinkage methods. These involve convexly combining a low-bias/high-variance empirical estimate with a biased regularization estimator, striking a bias-variance trade-off. Literature provides optimal settings of the regularization amount through risk minimization between the true covariance and its shrunk counterpart. Such estimators were derived for zero-mean statistics with i.i.d. diagonal regularization matrices accounting for the average sample variance solely. We extend these results to regularization matrices accounting for the sample variances both for centered and non-centered samples. In the latter case, the empirical estimate of the true mean is incorporated into our shrinkage estimators. Introducing confidence weights into the statistics also enhance estimator robustness against outliers. We compare our estimators to other shrinkage methods both on numerical simulations and on real data to solve a detection problem in astronomy.</p>

<p><a href="https://arxiv.org/abs/2403.07104">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>