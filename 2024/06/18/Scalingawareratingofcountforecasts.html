<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Scaling-aware rating of count forecasts</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Scaling-aware rating of count forecasts" />
<meta name="author" content="Malte C. Tichy, Illia Babounikau, Nikolas Wolke, Stefan Ulbrich, Michael Feindt" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Forecast quality should be assessed in the context of what is possible in theory and what is reasonable to expect in practice. Often, one can identify an approximate upper bound to a probabilistic forecast’s sharpness, which sets a lower, not necessarily achievable, limit to error metrics. In retail forecasting, a simple, but often unconquerable sharpness limit is given by the Poisson distribution. When evaluating forecasts using traditional metrics such as Mean Absolute Error, it is hard to judge whether a certain achieved value reflects unavoidable Poisson noise or truly indicates an overdispersed prediction model. Moreover, every evaluation metric suffers from precision scaling: Perhaps surprisingly, the metric’s value is mostly defined by the selling rate and by the resulting rate-dependent Poisson noise, and only secondarily by the forecast quality. For any metric, comparing two groups of forecasted products often yields “the slow movers are performing worse than the fast movers” or vice versa, the na&quot;ive scaling trap. To distill the intrinsic quality of a forecast, we stratify predictions into buckets of approximately equal predicted value and evaluate metrics separately per bucket. By comparing the achieved value per bucket to benchmarks, we obtain an intuitive visualization of forecast quality, which can be summarized into a single rating that makes forecast quality comparable among different products or even industries. The thereby developed scaling-aware forecast rating is applied to forecasting models used on the M5 competition dataset as well as to real-life forecasts provided by Blue Yonder’s Demand Edge for Retail solution for grocery products in Sainsbury’s supermarkets in the United Kingdom. The results permit a clear interpretation and high-level understanding of model quality by non-experts." />
<meta property="og:description" content="Forecast quality should be assessed in the context of what is possible in theory and what is reasonable to expect in practice. Often, one can identify an approximate upper bound to a probabilistic forecast’s sharpness, which sets a lower, not necessarily achievable, limit to error metrics. In retail forecasting, a simple, but often unconquerable sharpness limit is given by the Poisson distribution. When evaluating forecasts using traditional metrics such as Mean Absolute Error, it is hard to judge whether a certain achieved value reflects unavoidable Poisson noise or truly indicates an overdispersed prediction model. Moreover, every evaluation metric suffers from precision scaling: Perhaps surprisingly, the metric’s value is mostly defined by the selling rate and by the resulting rate-dependent Poisson noise, and only secondarily by the forecast quality. For any metric, comparing two groups of forecasted products often yields “the slow movers are performing worse than the fast movers” or vice versa, the na&quot;ive scaling trap. To distill the intrinsic quality of a forecast, we stratify predictions into buckets of approximately equal predicted value and evaluate metrics separately per bucket. By comparing the achieved value per bucket to benchmarks, we obtain an intuitive visualization of forecast quality, which can be summarized into a single rating that makes forecast quality comparable among different products or even industries. The thereby developed scaling-aware forecast rating is applied to forecasting models used on the M5 competition dataset as well as to real-life forecasts provided by Blue Yonder’s Demand Edge for Retail solution for grocery products in Sainsbury’s supermarkets in the United Kingdom. The results permit a clear interpretation and high-level understanding of model quality by non-experts." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Scalingawareratingofcountforecasts.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Scalingawareratingofcountforecasts.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-18T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Scaling-aware rating of count forecasts" />
<script type="application/ld+json">
{"description":"Forecast quality should be assessed in the context of what is possible in theory and what is reasonable to expect in practice. Often, one can identify an approximate upper bound to a probabilistic forecast’s sharpness, which sets a lower, not necessarily achievable, limit to error metrics. In retail forecasting, a simple, but often unconquerable sharpness limit is given by the Poisson distribution. When evaluating forecasts using traditional metrics such as Mean Absolute Error, it is hard to judge whether a certain achieved value reflects unavoidable Poisson noise or truly indicates an overdispersed prediction model. Moreover, every evaluation metric suffers from precision scaling: Perhaps surprisingly, the metric’s value is mostly defined by the selling rate and by the resulting rate-dependent Poisson noise, and only secondarily by the forecast quality. For any metric, comparing two groups of forecasted products often yields “the slow movers are performing worse than the fast movers” or vice versa, the na&quot;ive scaling trap. To distill the intrinsic quality of a forecast, we stratify predictions into buckets of approximately equal predicted value and evaluate metrics separately per bucket. By comparing the achieved value per bucket to benchmarks, we obtain an intuitive visualization of forecast quality, which can be summarized into a single rating that makes forecast quality comparable among different products or even industries. The thereby developed scaling-aware forecast rating is applied to forecasting models used on the M5 competition dataset as well as to real-life forecasts provided by Blue Yonder’s Demand Edge for Retail solution for grocery products in Sainsbury’s supermarkets in the United Kingdom. The results permit a clear interpretation and high-level understanding of model quality by non-experts.","datePublished":"2024-06-18T00:00:00+00:00","dateModified":"2024-06-18T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Scalingawareratingofcountforecasts.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/Scalingawareratingofcountforecasts.html","author":{"@type":"Person","name":"Malte C. Tichy, Illia Babounikau, Nikolas Wolke, Stefan Ulbrich, Michael Feindt"},"@type":"BlogPosting","headline":"Scaling-aware rating of count forecasts","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-06-18 00:00:00 +0000">06-18</time>
  </p>
  
  <h1>Scaling-aware rating of count forecasts</h1>
  <br>Malte C. Tichy, Illia Babounikau, Nikolas Wolke, Stefan Ulbrich, Michael Feindt</h3>
  <br> [stat.AP]

  <p>Forecast quality should be assessed in the context of what is possible in theory and what is reasonable to expect in practice. Often, one can identify an approximate upper bound to a probabilistic forecast’s sharpness, which sets a lower, not necessarily achievable, limit to error metrics. In retail forecasting, a simple, but often unconquerable sharpness limit is given by the Poisson distribution. When evaluating forecasts using traditional metrics such as Mean Absolute Error, it is hard to judge whether a certain achieved value reflects unavoidable Poisson noise or truly indicates an overdispersed prediction model. Moreover, every evaluation metric suffers from precision scaling: Perhaps surprisingly, the metric’s value is mostly defined by the selling rate and by the resulting rate-dependent Poisson noise, and only secondarily by the forecast quality. For any metric, comparing two groups of forecasted products often yields “the slow movers are performing worse than the fast movers” or vice versa, the na"ive scaling trap. To distill the intrinsic quality of a forecast, we stratify predictions into buckets of approximately equal predicted value and evaluate metrics separately per bucket. By comparing the achieved value per bucket to benchmarks, we obtain an intuitive visualization of forecast quality, which can be summarized into a single rating that makes forecast quality comparable among different products or even industries. The thereby developed scaling-aware forecast rating is applied to forecasting models used on the M5 competition dataset as well as to real-life forecasts provided by Blue Yonder’s Demand Edge for Retail solution for grocery products in Sainsbury’s supermarkets in the United Kingdom. The results permit a clear interpretation and high-level understanding of model quality by non-experts.</p>

<p><a href="https://arxiv.org/abs/2211.16313">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>