<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Bayesian Inference for Consistent Predictions in Overparameterized Nonlinear Regression</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Bayesian Inference for Consistent Predictions in Overparameterized Nonlinear Regression" />
<meta name="author" content="Tomoya Wakayama" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The remarkable generalization performance of large-scale models has been challenging the conventional wisdom of the statistical learning theory. Although recent theoretical studies have shed light on this behavior in linear models and nonlinear classifiers, a comprehensive understanding of overparameterization in nonlinear regression models is still lacking. This study explores the predictive properties of overparameterized nonlinear regression within the Bayesian framework, extending the methodology of the adaptive prior considering the intrinsic spectral structure of the data. Posterior contraction is established for generalized linear and single-neuron models with Lipschitz continuous activation functions, demonstrating the consistency in the predictions of the proposed approach. Moreover, the Bayesian framework enables uncertainty estimation of the predictions. The proposed method was validated via numerical simulations and a real data application, showing its ability to achieve accurate predictions and reliable uncertainty estimates. This work provides a theoretical understanding of the advantages of overparameterization and a principled Bayesian approach to large nonlinear models." />
<meta property="og:description" content="The remarkable generalization performance of large-scale models has been challenging the conventional wisdom of the statistical learning theory. Although recent theoretical studies have shed light on this behavior in linear models and nonlinear classifiers, a comprehensive understanding of overparameterization in nonlinear regression models is still lacking. This study explores the predictive properties of overparameterized nonlinear regression within the Bayesian framework, extending the methodology of the adaptive prior considering the intrinsic spectral structure of the data. Posterior contraction is established for generalized linear and single-neuron models with Lipschitz continuous activation functions, demonstrating the consistency in the predictions of the proposed approach. Moreover, the Bayesian framework enables uncertainty estimation of the predictions. The proposed method was validated via numerical simulations and a real data application, showing its ability to achieve accurate predictions and reliable uncertainty estimates. This work provides a theoretical understanding of the advantages of overparameterization and a principled Bayesian approach to large nonlinear models." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianInferenceforConsistentPredictionsinOverparameterizedNonlinearRegression.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianInferenceforConsistentPredictionsinOverparameterizedNonlinearRegression.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-18T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Bayesian Inference for Consistent Predictions in Overparameterized Nonlinear Regression" />
<script type="application/ld+json">
{"description":"The remarkable generalization performance of large-scale models has been challenging the conventional wisdom of the statistical learning theory. Although recent theoretical studies have shed light on this behavior in linear models and nonlinear classifiers, a comprehensive understanding of overparameterization in nonlinear regression models is still lacking. This study explores the predictive properties of overparameterized nonlinear regression within the Bayesian framework, extending the methodology of the adaptive prior considering the intrinsic spectral structure of the data. Posterior contraction is established for generalized linear and single-neuron models with Lipschitz continuous activation functions, demonstrating the consistency in the predictions of the proposed approach. Moreover, the Bayesian framework enables uncertainty estimation of the predictions. The proposed method was validated via numerical simulations and a real data application, showing its ability to achieve accurate predictions and reliable uncertainty estimates. This work provides a theoretical understanding of the advantages of overparameterization and a principled Bayesian approach to large nonlinear models.","datePublished":"2024-06-18T00:00:00+00:00","dateModified":"2024-06-18T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianInferenceforConsistentPredictionsinOverparameterizedNonlinearRegression.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/18/BayesianInferenceforConsistentPredictionsinOverparameterizedNonlinearRegression.html","author":{"@type":"Person","name":"Tomoya Wakayama"},"@type":"BlogPosting","headline":"Bayesian Inference for Consistent Predictions in Overparameterized Nonlinear Regression","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-06-18 00:00:00 +0000">06-18</time>
  </p>
  
  <h1>Bayesian Inference for Consistent Predictions in Overparameterized Nonlinear Regression</h1>
  <br>Tomoya Wakayama</h3>
  <br> [stat.ML,stat.ME]

  <p>The remarkable generalization performance of large-scale models has been challenging the conventional wisdom of the statistical learning theory. Although recent theoretical studies have shed light on this behavior in linear models and nonlinear classifiers, a comprehensive understanding of overparameterization in nonlinear regression models is still lacking. This study explores the predictive properties of overparameterized nonlinear regression within the Bayesian framework, extending the methodology of the adaptive prior considering the intrinsic spectral structure of the data. Posterior contraction is established for generalized linear and single-neuron models with Lipschitz continuous activation functions, demonstrating the consistency in the predictions of the proposed approach. Moreover, the Bayesian framework enables uncertainty estimation of the predictions. The proposed method was validated via numerical simulations and a real data application, showing its ability to achieve accurate predictions and reliable uncertainty estimates. This work provides a theoretical understanding of the advantages of overparameterization and a principled Bayesian approach to large nonlinear models.</p>

<p><a href="https://arxiv.org/abs/2404.04498">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>