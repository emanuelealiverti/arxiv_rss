<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Assessment of Case Influence in the Lasso with a Case-weight Adjusted Solution Path</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Assessment of Case Influence in the Lasso with a Case-weight Adjusted Solution Path" />
<meta name="author" content="Zhenbang Jiao, Yoonkyung Lee" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We study case influence in the Lasso regression using Cook’s distance which measures overall change in the fitted values when one observation is deleted. Unlike in ordinary least squares regression, the estimated coefficients in the Lasso do not have a closed form due to the nondifferentiability of the $\ell_1$ penalty, and neither does Cook’s distance. To find the case-deleted Lasso solution without refitting the model, we approach it from the full data solution by introducing a weight parameter ranging from 1 to 0 and generating a solution path indexed by this parameter. We show that the solution path is piecewise linear with respect to a simple function of the weight parameter under a fixed penalty. The resulting case influence is a function of the penalty and weight, and it becomes Cook’s distance when the weight is 0. As the penalty parameter changes, selected variables change, and the magnitude of Cook’s distance for the same data point may vary with the subset of variables selected. In addition, we introduce a case influence graph to visualize how the contribution of each data point changes with the penalty parameter. From the graph, we can identify influential points at different penalty levels and make modeling decisions accordingly. Moreover, we find that case influence graphs exhibit different patterns between underfitting and overfitting phases, which can provide additional information for model selection." />
<meta property="og:description" content="We study case influence in the Lasso regression using Cook’s distance which measures overall change in the fitted values when one observation is deleted. Unlike in ordinary least squares regression, the estimated coefficients in the Lasso do not have a closed form due to the nondifferentiability of the $\ell_1$ penalty, and neither does Cook’s distance. To find the case-deleted Lasso solution without refitting the model, we approach it from the full data solution by introducing a weight parameter ranging from 1 to 0 and generating a solution path indexed by this parameter. We show that the solution path is piecewise linear with respect to a simple function of the weight parameter under a fixed penalty. The resulting case influence is a function of the penalty and weight, and it becomes Cook’s distance when the weight is 0. As the penalty parameter changes, selected variables change, and the magnitude of Cook’s distance for the same data point may vary with the subset of variables selected. In addition, we introduce a case influence graph to visualize how the contribution of each data point changes with the penalty parameter. From the graph, we can identify influential points at different penalty levels and make modeling decisions accordingly. Moreover, we find that case influence graphs exhibit different patterns between underfitting and overfitting phases, which can provide additional information for model selection." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AssessmentofCaseInfluenceintheLassowithaCaseweightAdjustedSolutionPath.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AssessmentofCaseInfluenceintheLassowithaCaseweightAdjustedSolutionPath.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-04T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Assessment of Case Influence in the Lasso with a Case-weight Adjusted Solution Path" />
<script type="application/ld+json">
{"description":"We study case influence in the Lasso regression using Cook’s distance which measures overall change in the fitted values when one observation is deleted. Unlike in ordinary least squares regression, the estimated coefficients in the Lasso do not have a closed form due to the nondifferentiability of the $\\ell_1$ penalty, and neither does Cook’s distance. To find the case-deleted Lasso solution without refitting the model, we approach it from the full data solution by introducing a weight parameter ranging from 1 to 0 and generating a solution path indexed by this parameter. We show that the solution path is piecewise linear with respect to a simple function of the weight parameter under a fixed penalty. The resulting case influence is a function of the penalty and weight, and it becomes Cook’s distance when the weight is 0. As the penalty parameter changes, selected variables change, and the magnitude of Cook’s distance for the same data point may vary with the subset of variables selected. In addition, we introduce a case influence graph to visualize how the contribution of each data point changes with the penalty parameter. From the graph, we can identify influential points at different penalty levels and make modeling decisions accordingly. Moreover, we find that case influence graphs exhibit different patterns between underfitting and overfitting phases, which can provide additional information for model selection.","author":{"@type":"Person","name":"Zhenbang Jiao, Yoonkyung Lee"},"datePublished":"2024-06-04T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AssessmentofCaseInfluenceintheLassowithaCaseweightAdjustedSolutionPath.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/04/AssessmentofCaseInfluenceintheLassowithaCaseweightAdjustedSolutionPath.html","headline":"Assessment of Case Influence in the Lasso with a Case-weight Adjusted Solution Path","@type":"BlogPosting","dateModified":"2024-06-04T00:00:00+00:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-06-04 00:00:00 +0000">06-04</time>
  </p>
  
  <h1>Assessment of Case Influence in the Lasso with a Case-weight Adjusted Solution Path</h1>
  <br>Zhenbang Jiao, Yoonkyung Lee</h3>
  <br> [stat.ME]

  <p>We study case influence in the Lasso regression using Cook’s distance which measures overall change in the fitted values when one observation is deleted. Unlike in ordinary least squares regression, the estimated coefficients in the Lasso do not have a closed form due to the nondifferentiability of the $\ell_1$ penalty, and neither does Cook’s distance. To find the case-deleted Lasso solution without refitting the model, we approach it from the full data solution by introducing a weight parameter ranging from 1 to 0 and generating a solution path indexed by this parameter. We show that the solution path is piecewise linear with respect to a simple function of the weight parameter under a fixed penalty. The resulting case influence is a function of the penalty and weight, and it becomes Cook’s distance when the weight is 0. As the penalty parameter changes, selected variables change, and the magnitude of Cook’s distance for the same data point may vary with the subset of variables selected. In addition, we introduce a case influence graph to visualize how the contribution of each data point changes with the penalty parameter. From the graph, we can identify influential points at different penalty levels and make modeling decisions accordingly. Moreover, we find that case influence graphs exhibit different patterns between underfitting and overfitting phases, which can provide additional information for model selection.</p>

<p><a href="https://arxiv.org/abs/2406.00493">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>