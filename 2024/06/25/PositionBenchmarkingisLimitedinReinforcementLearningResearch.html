<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Position: Benchmarking is Limited in Reinforcement Learning Research</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Position: Benchmarking is Limited in Reinforcement Learning Research" />
<meta name="author" content="Scott M. Jordan, Adam White, Bruno Castro da Silva, Martha White, Philip S. Thomas" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Novel reinforcement learning algorithms, or improvements on existing ones, are commonly justified by evaluating their performance on benchmark environments and are compared to an ever-changing set of standard algorithms. However, despite numerous calls for improvements, experimental practices continue to produce misleading or unsupported claims. One reason for the ongoing substandard practices is that conducting rigorous benchmarking experiments requires substantial computational time. This work investigates the sources of increased computation costs in rigorous experiment designs. We show that conducting rigorous performance benchmarks will likely have computational costs that are often prohibitive. As a result, we argue for using an additional experimentation paradigm to overcome the limitations of benchmarking." />
<meta property="og:description" content="Novel reinforcement learning algorithms, or improvements on existing ones, are commonly justified by evaluating their performance on benchmark environments and are compared to an ever-changing set of standard algorithms. However, despite numerous calls for improvements, experimental practices continue to produce misleading or unsupported claims. One reason for the ongoing substandard practices is that conducting rigorous benchmarking experiments requires substantial computational time. This work investigates the sources of increased computation costs in rigorous experiment designs. We show that conducting rigorous performance benchmarks will likely have computational costs that are often prohibitive. As a result, we argue for using an additional experimentation paradigm to overcome the limitations of benchmarking." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/PositionBenchmarkingisLimitedinReinforcementLearningResearch.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/PositionBenchmarkingisLimitedinReinforcementLearningResearch.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-25T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Position: Benchmarking is Limited in Reinforcement Learning Research" />
<script type="application/ld+json">
{"description":"Novel reinforcement learning algorithms, or improvements on existing ones, are commonly justified by evaluating their performance on benchmark environments and are compared to an ever-changing set of standard algorithms. However, despite numerous calls for improvements, experimental practices continue to produce misleading or unsupported claims. One reason for the ongoing substandard practices is that conducting rigorous benchmarking experiments requires substantial computational time. This work investigates the sources of increased computation costs in rigorous experiment designs. We show that conducting rigorous performance benchmarks will likely have computational costs that are often prohibitive. As a result, we argue for using an additional experimentation paradigm to overcome the limitations of benchmarking.","headline":"Position: Benchmarking is Limited in Reinforcement Learning Research","dateModified":"2024-06-25T00:00:00+00:00","datePublished":"2024-06-25T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/PositionBenchmarkingisLimitedinReinforcementLearningResearch.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/25/PositionBenchmarkingisLimitedinReinforcementLearningResearch.html","author":{"@type":"Person","name":"Scott M. Jordan, Adam White, Bruno Castro da Silva, Martha White, Philip S. Thomas"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-06-25 00:00:00 +0000">06-25</time>
  </p>
  
  <h1>Position: Benchmarking is Limited in Reinforcement Learning Research</h1>
  <br>Scott M. Jordan, Adam White, Bruno Castro da Silva, Martha White, Philip S. Thomas</h3>
  <br> [stat.ME]

  <p>Novel reinforcement learning algorithms, or improvements on existing ones, are commonly justified by evaluating their performance on benchmark environments and are compared to an ever-changing set of standard algorithms. However, despite numerous calls for improvements, experimental practices continue to produce misleading or unsupported claims. One reason for the ongoing substandard practices is that conducting rigorous benchmarking experiments requires substantial computational time. This work investigates the sources of increased computation costs in rigorous experiment designs. We show that conducting rigorous performance benchmarks will likely have computational costs that are often prohibitive. As a result, we argue for using an additional experimentation paradigm to overcome the limitations of benchmarking.</p>

<p><a href="https://arxiv.org/abs/2406.16241">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>