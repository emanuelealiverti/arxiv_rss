<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Testing the Fairness-Improvability of Algorithms</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Testing the Fairness-Improvability of Algorithms" />
<meta name="author" content="Eric Auerbach, Annie Liang, Kyohei Okumura, Max Tabord-Meehan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Many organizations use algorithms that have a disparate impact, i.e., the benefits or harms of the algorithm fall disproportionately on certain social groups. Addressing an algorithm’s disparate impact can be challenging, especially because it is often unclear whether reducing this impact is possible without sacrificing other important objectives of the organization, such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy-maker can formally substantiate, or refute, this necessity defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can be applied under any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and illustrate its practical application by evaluating a healthcare algorithm originally considered by Obermeyer et al 2019. In this application, we reject the null hypothesis that it is not possible to reduce the algorithm’s disparate impact without compromising on the accuracy of its predictions." />
<meta property="og:description" content="Many organizations use algorithms that have a disparate impact, i.e., the benefits or harms of the algorithm fall disproportionately on certain social groups. Addressing an algorithm’s disparate impact can be challenging, especially because it is often unclear whether reducing this impact is possible without sacrificing other important objectives of the organization, such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy-maker can formally substantiate, or refute, this necessity defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can be applied under any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and illustrate its practical application by evaluating a healthcare algorithm originally considered by Obermeyer et al 2019. In this application, we reject the null hypothesis that it is not possible to reduce the algorithm’s disparate impact without compromising on the accuracy of its predictions." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/TestingtheFairnessImprovabilityofAlgorithms.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/TestingtheFairnessImprovabilityofAlgorithms.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-21T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Testing the Fairness-Improvability of Algorithms" />
<script type="application/ld+json">
{"description":"Many organizations use algorithms that have a disparate impact, i.e., the benefits or harms of the algorithm fall disproportionately on certain social groups. Addressing an algorithm’s disparate impact can be challenging, especially because it is often unclear whether reducing this impact is possible without sacrificing other important objectives of the organization, such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy-maker can formally substantiate, or refute, this necessity defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can be applied under any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and illustrate its practical application by evaluating a healthcare algorithm originally considered by Obermeyer et al 2019. In this application, we reject the null hypothesis that it is not possible to reduce the algorithm’s disparate impact without compromising on the accuracy of its predictions.","datePublished":"2024-06-21T00:00:00+00:00","dateModified":"2024-06-21T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/TestingtheFairnessImprovabilityofAlgorithms.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/21/TestingtheFairnessImprovabilityofAlgorithms.html","author":{"@type":"Person","name":"Eric Auerbach, Annie Liang, Kyohei Okumura, Max Tabord-Meehan"},"@type":"BlogPosting","headline":"Testing the Fairness-Improvability of Algorithms","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-06-21 00:00:00 +0000">06-21</time>
  </p>
  
  <h1>Testing the Fairness-Improvability of Algorithms</h1>
  <br>Eric Auerbach, Annie Liang, Kyohei Okumura, Max Tabord-Meehan</h3>
  <br> [stat.AP]

  <p>Many organizations use algorithms that have a disparate impact, i.e., the benefits or harms of the algorithm fall disproportionately on certain social groups. Addressing an algorithm’s disparate impact can be challenging, especially because it is often unclear whether reducing this impact is possible without sacrificing other important objectives of the organization, such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy-maker can formally substantiate, or refute, this necessity defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can be applied under any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and illustrate its practical application by evaluating a healthcare algorithm originally considered by Obermeyer et al 2019. In this application, we reject the null hypothesis that it is not possible to reduce the algorithm’s disparate impact without compromising on the accuracy of its predictions.</p>

<p><a href="https://arxiv.org/abs/2405.04816">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>