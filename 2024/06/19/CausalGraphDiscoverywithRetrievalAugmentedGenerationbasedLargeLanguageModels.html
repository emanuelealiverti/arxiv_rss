<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <title>Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models</title><!-- Begin Jekyll SEO tag v2.7.1 -->
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models" />
<meta name="author" content="Yuzhe Zhang, Yipeng Zhang, Yidong Gan, Lina Yao, Chen Wang" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Causal graph recovery is traditionally done using statistical estimation-based methods or based on individual’s knowledge about variables of interests. They often suffer from data collection biases and limitations of individuals’ knowledge. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that leverages LLMs to deduce causal relationships in general causal graph recovery tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs extracted from scientific publication database as well as experiment data about factors of interest to achieve this goal. Our method gives a prompting strategy to extract associational relationships among those factors and a mechanism to perform causality verification for these associations. Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets. More importantly, as causality among some factors may change as new research results emerge, our method show sensitivity to new evidence in the literature and can provide useful information for updating causal graphs accordingly." />
<meta property="og:description" content="Causal graph recovery is traditionally done using statistical estimation-based methods or based on individual’s knowledge about variables of interests. They often suffer from data collection biases and limitations of individuals’ knowledge. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that leverages LLMs to deduce causal relationships in general causal graph recovery tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs extracted from scientific publication database as well as experiment data about factors of interest to achieve this goal. Our method gives a prompting strategy to extract associational relationships among those factors and a mechanism to perform causality verification for these associations. Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets. More importantly, as causality among some factors may change as new research results emerge, our method show sensitivity to new evidence in the literature and can provide useful information for updating causal graphs accordingly." />
<link rel="canonical" href="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/CausalGraphDiscoverywithRetrievalAugmentedGenerationbasedLargeLanguageModels.html" />
<meta property="og:url" content="https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/CausalGraphDiscoverywithRetrievalAugmentedGenerationbasedLargeLanguageModels.html" />
<meta property="og:site_name" content="Stat Arxiv of Today" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-19T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models" />
<script type="application/ld+json">
{"description":"Causal graph recovery is traditionally done using statistical estimation-based methods or based on individual’s knowledge about variables of interests. They often suffer from data collection biases and limitations of individuals’ knowledge. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that leverages LLMs to deduce causal relationships in general causal graph recovery tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs extracted from scientific publication database as well as experiment data about factors of interest to achieve this goal. Our method gives a prompting strategy to extract associational relationships among those factors and a mechanism to perform causality verification for these associations. Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets. More importantly, as causality among some factors may change as new research results emerge, our method show sensitivity to new evidence in the literature and can provide useful information for updating causal graphs accordingly.","datePublished":"2024-06-19T00:00:00+00:00","dateModified":"2024-06-19T00:00:00+00:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/CausalGraphDiscoverywithRetrievalAugmentedGenerationbasedLargeLanguageModels.html"},"url":"https://emanuelealiverti.github.io/arxiv_rss/2024/06/19/CausalGraphDiscoverywithRetrievalAugmentedGenerationbasedLargeLanguageModels.html","author":{"@type":"Person","name":"Yuzhe Zhang, Yipeng Zhang, Yidong Gan, Lina Yao, Chen Wang"},"@type":"BlogPosting","headline":"Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link type="application/atom+xml" rel="alternate" href="https://emanuelealiverti.github.io/arxiv_rss/feed.xml" title="Stat Arxiv of Today" /><link rel="shortcut icon" type="image/x-icon" href="" />
  <link rel="stylesheet" href="/arxiv_rss/assets/css/main.css" />


<script>
MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  },
  svg: {
    fontCache: 'global'
  }
};
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


</head>
<body a="light">
    <main class="page-content" aria-label="Content">
      <div class="w">
        <a href="/arxiv_rss/">..</a><article>
  <p class="post-meta">
    <time datetime="2024-06-19 00:00:00 +0000">06-19</time>
  </p>
  
  <h1>Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models</h1>
  <br>Yuzhe Zhang, Yipeng Zhang, Yidong Gan, Lina Yao, Chen Wang</h3>
  <br> [stat.ME]

  <p>Causal graph recovery is traditionally done using statistical estimation-based methods or based on individual’s knowledge about variables of interests. They often suffer from data collection biases and limitations of individuals’ knowledge. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that leverages LLMs to deduce causal relationships in general causal graph recovery tasks. This method leverages knowledge compressed in LLMs and knowledge LLMs extracted from scientific publication database as well as experiment data about factors of interest to achieve this goal. Our method gives a prompting strategy to extract associational relationships among those factors and a mechanism to perform causality verification for these associations. Comparing to other LLM-based methods that directly instruct LLMs to do the highly complex causal reasoning, our method shows clear advantage on causal graph quality on benchmark datasets. More importantly, as causality among some factors may change as new research results emerge, our method show sensitivity to new evidence in the literature and can provide useful information for updating causal graphs accordingly.</p>

<p><a href="https://arxiv.org/abs/2402.15301">Read more</a></p>

</article>

      </div>
    </main>
  </body>
</html>